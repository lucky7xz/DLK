{"textgrid.poem.55224": {"metadata": {"author": {"name": "Goethe, Johann Wolfgang", "birth": "N.A.", "death": "N.A."}, "title": "Sprichw\u00f6rtlich", "genre": "verse", "period": "N.A.", "pub_year": 1813, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn ich den Scherz will ernsthaft nehmen,", "tokens": ["Wenn", "ich", "den", "Scherz", "will", "ernst\u00b7haft", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VMFIN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So soll mich niemand drum besch\u00e4men;", "tokens": ["So", "soll", "mich", "nie\u00b7mand", "drum", "be\u00b7sch\u00e4\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PIS", "PAV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und wenn ich den Ernst will scherzhaft treiben,", "tokens": ["Und", "wenn", "ich", "den", "Ernst", "will", "scherz\u00b7haft", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ART", "NN", "VMFIN", "ADJD", "VVINF", "$,"], "meter": "----+-+-+-", "measure": "unknown.measure.tri"}, "line.4": {"text": "So werd ich immer derselbe bleiben.", "tokens": ["So", "werd", "ich", "im\u00b7mer", "der\u00b7sel\u00b7be", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PDAT", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Die Lust zu reden kommt zu rechter Stunde,", "tokens": ["Die", "Lust", "zu", "re\u00b7den", "kommt", "zu", "rech\u00b7ter", "Stun\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und wahrhaft flie\u00dft das Wort aus Herz und Munde.", "tokens": ["Und", "wahr\u00b7haft", "flie\u00dft", "das", "Wort", "aus", "Herz", "und", "Mun\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Ich sah mich um an vielen Orten", "tokens": ["Ich", "sah", "mich", "um", "an", "vie\u00b7len", "Or\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nach lustigen, gescheiten Worten;", "tokens": ["Nach", "lus\u00b7ti\u00b7gen", ",", "ge\u00b7schei\u00b7ten", "Wor\u00b7ten", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "An b\u00f6sen Tagen mu\u00dft ich mich freuen,", "tokens": ["An", "b\u00f6\u00b7sen", "Ta\u00b7gen", "mu\u00dft", "ich", "mich", "freu\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VMFIN", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da\u00df diese die besten Worte verleihen.", "tokens": ["Da\u00df", "die\u00b7se", "die", "bes\u00b7ten", "Wor\u00b7te", "ver\u00b7lei\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Im neuen Jahre Gl\u00fcck und Heil;", "tokens": ["Im", "neu\u00b7en", "Jah\u00b7re", "Gl\u00fcck", "und", "Heil", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auf Weh und Wunden gute Salbe!", "tokens": ["Auf", "Weh", "und", "Wun\u00b7den", "gu\u00b7te", "Sal\u00b7be", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Auf groben Klotz ein grober Keil!", "tokens": ["Auf", "gro\u00b7ben", "Klotz", "ein", "gro\u00b7ber", "Keil", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auf ", "tokens": ["Auf"], "token_info": ["word"], "pos": ["APPR"], "meter": "+", "measure": "single.up"}}, "stanza.5": {"line.1": {"text": "Willst lustig leben,", "tokens": ["Willst", "lus\u00b7tig", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "ADJD", "VVINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Geh mit zwei S\u00e4cken,", "tokens": ["Geh", "mit", "zwei", "S\u00e4\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "CARD", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Einen zum Geben,", "tokens": ["Ei\u00b7nen", "zum", "Ge\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "APPRART", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Einen, um einzustecken.", "tokens": ["Ei\u00b7nen", ",", "um", "ein\u00b7zu\u00b7ste\u00b7cken", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ART", "$,", "KOUI", "VVIZU", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.5": {"text": "Da gleichst du Prinzen,", "tokens": ["Da", "gleichst", "du", "Prin\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Pl\u00fcnderst und begl\u00fcckst Provinzen.", "tokens": ["Pl\u00fcn\u00b7derst", "und", "be\u00b7gl\u00fcckst", "Pro\u00b7vin\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "VVFIN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Was in der Zeiten Bildersaal", "tokens": ["Was", "in", "der", "Zei\u00b7ten", "Bil\u00b7der\u00b7saal"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Jemals ist trefflich gewesen,", "tokens": ["Je\u00b7mals", "ist", "treff\u00b7lich", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJD", "VAPP", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Das wird immer einer einmal", "tokens": ["Das", "wird", "im\u00b7mer", "ei\u00b7ner", "ein\u00b7mal"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "ART", "ADV"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Wieder auffrischen und lesen.", "tokens": ["Wie\u00b7der", "auf\u00b7fri\u00b7schen", "und", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "KON", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.7": {"line.1": {"text": "Nicht jeder wandelt nur gemeine Stege:", "tokens": ["Nicht", "je\u00b7der", "wan\u00b7delt", "nur", "ge\u00b7mei\u00b7ne", "Ste\u00b7ge", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIS", "VVFIN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Du siehst, die Spinnen bauen luft'ge Wege.", "tokens": ["Du", "siehst", ",", "die", "Spin\u00b7nen", "bau\u00b7en", "luft'\u00b7ge", "We\u00b7ge", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "ART", "NN", "VVINF", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Ein Kranz ist gar viel leichter binden,", "tokens": ["Ein", "Kranz", "ist", "gar", "viel", "leich\u00b7ter", "bin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als ihm ein w\u00fcrdig Haupt zu finden.", "tokens": ["Als", "ihm", "ein", "w\u00fcr\u00b7dig", "Haupt", "zu", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJD", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wie die Pflanzen zu wachsen belieben,", "tokens": ["Wie", "die", "Pflan\u00b7zen", "zu", "wach\u00b7sen", "be\u00b7lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PTKZU", "VVINF", "VVINF", "$,"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Darin wird jeder G\u00e4rtner sich \u00fcben;", "tokens": ["Da\u00b7rin", "wird", "je\u00b7der", "G\u00e4rt\u00b7ner", "sich", "\u00fc\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PIAT", "NN", "PRF", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Wo aber des Menschen Wachstum ruht,", "tokens": ["Wo", "a\u00b7ber", "des", "Men\u00b7schen", "Wachs\u00b7tum", "ruht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Dazu jeder selbst das Beste tut.", "tokens": ["Da\u00b7zu", "je\u00b7der", "selbst", "das", "Bes\u00b7te", "tut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.10": {"line.1": {"text": "Willst du dir aber das Beste tun,", "tokens": ["Willst", "du", "dir", "a\u00b7ber", "das", "Bes\u00b7te", "tun", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "So bleib nicht auf dir selber ruhn,", "tokens": ["So", "bleib", "nicht", "auf", "dir", "sel\u00b7ber", "ruhn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKNEG", "APPR", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sondern folg eines Meisters Sinn;", "tokens": ["Son\u00b7dern", "folg", "ei\u00b7nes", "Meis\u00b7ters", "Sinn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ART", "NN", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Mit ihm zu irren ist dir Gewinn.", "tokens": ["Mit", "ihm", "zu", "ir\u00b7ren", "ist", "dir", "Ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PTKZU", "VVINF", "VAFIN", "PPER", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.11": {"line.1": {"text": "Benutze redlich deine Zeit!", "tokens": ["Be\u00b7nut\u00b7ze", "red\u00b7lich", "dei\u00b7ne", "Zeit", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Willst was begreifen, such's nicht weit.", "tokens": ["Willst", "was", "be\u00b7grei\u00b7fen", ",", "such's", "nicht", "weit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "VVINF", "$,", "NE", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Zwischen heut und morgen", "tokens": ["Zwi\u00b7schen", "heut", "und", "mor\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "KON", "ADV"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Liegt eine lange Frist;", "tokens": ["Liegt", "ei\u00b7ne", "lan\u00b7ge", "Frist", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Lerne schnell besorgen,", "tokens": ["Ler\u00b7ne", "schnell", "be\u00b7sor\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Da du noch munter bist.", "tokens": ["Da", "du", "noch", "mun\u00b7ter", "bist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Die Dinte macht uns wohl gelehrt,", "tokens": ["Die", "Din\u00b7te", "macht", "uns", "wohl", "ge\u00b7lehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch \u00e4rgert sie, wo sie nicht hingeh\u00f6rt.", "tokens": ["Doch", "\u00e4r\u00b7gert", "sie", ",", "wo", "sie", "nicht", "hin\u00b7ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "PWAV", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Geschrieben Wort ist Perlen gleich;", "tokens": ["Ge\u00b7schrie\u00b7ben", "Wort", "ist", "Per\u00b7len", "gleich", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Dintenklecks ein b\u00f6ser Streich.", "tokens": ["Ein", "Din\u00b7ten\u00b7klecks", "ein", "b\u00f6\u00b7ser", "Streich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Wenn man f\u00fcrs K\u00fcnftige was erbaut,", "tokens": ["Wenn", "man", "f\u00fcrs", "K\u00fcnf\u00b7ti\u00b7ge", "was", "er\u00b7baut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "NN", "PWS", "VVPP", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Schief wird's von vielen angeschaut.", "tokens": ["Schief", "wird's", "von", "vie\u00b7len", "an\u00b7ge\u00b7schaut", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "APPR", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Tust du was f\u00fcr den Augenblick,", "tokens": ["Tust", "du", "was", "f\u00fcr", "den", "Au\u00b7gen\u00b7blick", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vor allem opfre du dem Gl\u00fcck.", "tokens": ["Vor", "al\u00b7lem", "opf\u00b7re", "du", "dem", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Mit einem Herren steht es gut,", "tokens": ["Mit", "ei\u00b7nem", "Her\u00b7ren", "steht", "es", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der, was er befohlen, selber tut.", "tokens": ["Der", ",", "was", "er", "be\u00b7foh\u00b7len", ",", "sel\u00b7ber", "tut", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "$,", "PWS", "PPER", "VVPP", "$,", "ADV", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.16": {"line.1": {"text": "Tu nur das Rechte in deinen Sachen;", "tokens": ["Tu", "nur", "das", "Rech\u00b7te", "in", "dei\u00b7nen", "Sa\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das andre wird sich von selber machen.", "tokens": ["Das", "and\u00b7re", "wird", "sich", "von", "sel\u00b7ber", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VAFIN", "PRF", "APPR", "ADV", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.17": {"line.1": {"text": "Wenn jemand sich wohl im Kleinen deucht,", "tokens": ["Wenn", "je\u00b7mand", "sich", "wohl", "im", "Klei\u00b7nen", "deucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PRF", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "So denke: der hat ein Gro\u00dfes erreicht.", "tokens": ["So", "den\u00b7ke", ":", "der", "hat", "ein", "Gro\u00b7\u00dfes", "er\u00b7reicht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$.", "ART", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-++-+--+", "measure": "iambic.penta.chol"}}, "stanza.18": {"line.1": {"text": "Glaube nur, du hast viel getan,", "tokens": ["Glau\u00b7be", "nur", ",", "du", "hast", "viel", "ge\u00b7tan", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "PPER", "VAFIN", "ADV", "VVPP", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wenn dir Geduld gew\u00f6hnest an.", "tokens": ["Wenn", "dir", "Ge\u00b7duld", "ge\u00b7w\u00f6h\u00b7nest", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Wer sich nicht nach der Decke streckt,", "tokens": ["Wer", "sich", "nicht", "nach", "der", "De\u00b7cke", "streckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "PTKNEG", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dem bleiben die F\u00fc\u00dfe unbedeckt.", "tokens": ["Dem", "blei\u00b7ben", "die", "F\u00fc\u00b7\u00dfe", "un\u00b7be\u00b7deckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.20": {"line.1": {"text": "Der Vogel ist froh in der Luft gem\u00fctet,", "tokens": ["Der", "Vo\u00b7gel", "ist", "froh", "in", "der", "Luft", "ge\u00b7m\u00fc\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "ADJD", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Wenn es da unten im Neste br\u00fctet.", "tokens": ["Wenn", "es", "da", "un\u00b7ten", "im", "Nes\u00b7te", "br\u00fc\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.21": {"line.1": {"text": "Wenn ein kluger Mann der Frau befiehlt,", "tokens": ["Wenn", "ein", "klu\u00b7ger", "Mann", "der", "Frau", "be\u00b7fiehlt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Dann sei es um ein Gro\u00dfes gespielt;", "tokens": ["Dann", "sei", "es", "um", "ein", "Gro\u00b7\u00dfes", "ge\u00b7spielt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Will die Frau dem Mann befehlen,", "tokens": ["Will", "die", "Frau", "dem", "Mann", "be\u00b7feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "So mu\u00df sie das Gro\u00dfe im Kleinen w\u00e4hlen.", "tokens": ["So", "mu\u00df", "sie", "das", "Gro\u00b7\u00dfe", "im", "Klei\u00b7nen", "w\u00e4h\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ART", "ADJA", "APPRART", "NN", "VVINF", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.22": {"line.1": {"text": "Welche Frau hat einen guten Mann,", "tokens": ["Wel\u00b7che", "Frau", "hat", "ei\u00b7nen", "gu\u00b7ten", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der sieht man's am Gesicht wohl an.", "tokens": ["Der", "sieht", "man's", "am", "Ge\u00b7sicht", "wohl", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "APPRART", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Eine Frau macht oft ein b\u00f6s Gesicht;", "tokens": ["Ei\u00b7ne", "Frau", "macht", "oft", "ein", "b\u00f6s", "Ge\u00b7sicht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der gute Mann verdient's wohl nicht.", "tokens": ["Der", "gu\u00b7te", "Mann", "ver\u00b7dient's", "wohl", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADV", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Ein braver Mann! ich kenn ihn ganz genau:", "tokens": ["Ein", "bra\u00b7ver", "Mann", "!", "ich", "kenn", "ihn", "ganz", "ge\u00b7nau", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Erst pr\u00fcgelt er, dann k\u00e4mmt er seine Frau.", "tokens": ["Erst", "pr\u00fc\u00b7gelt", "er", ",", "dann", "k\u00e4mmt", "er", "sei\u00b7ne", "Frau", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "ADV", "VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.25": {"line.1": {"text": "Ein sch\u00f6nes Ja, ein sch\u00f6nes Nein,", "tokens": ["Ein", "sch\u00f6\u00b7nes", "Ja", ",", "ein", "sch\u00f6\u00b7nes", "Nein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "PTKANT", "$,", "ART", "ADJA", "PTKANT", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur geschwind! soll mir willkommen sein.", "tokens": ["Nur", "ge\u00b7schwind", "!", "soll", "mir", "will\u00b7kom\u00b7men", "sein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "VMFIN", "PPER", "ADJD", "VAINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.26": {"line.1": {"text": "Januar, Februar, M\u00e4rz,", "tokens": ["Ja\u00b7nu\u00b7ar", ",", "Feb\u00b7ru\u00b7ar", ",", "M\u00e4rz", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Du bist mein liebes Herz.", "tokens": ["Du", "bist", "mein", "lie\u00b7bes", "Herz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Mai, Juni, Juli, August,", "tokens": ["Mai", ",", "Ju\u00b7ni", ",", "Ju\u00b7li", ",", "Au\u00b7gust", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "++-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Mir ist nichts mehr bewu\u00dft.", "tokens": ["Mir", "ist", "nichts", "mehr", "be\u00b7wu\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.27": {"line.1": {"text": "Neumond und gek\u00fc\u00dfter Mund", "tokens": ["Neu\u00b7mond", "und", "ge\u00b7k\u00fc\u00df\u00b7ter", "Mund"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "ADJA", "NN"], "meter": "++--+-+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Sind gleich wieder hell und frisch und gesund.", "tokens": ["Sind", "gleich", "wie\u00b7der", "hell", "und", "frisch", "und", "ge\u00b7sund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "KON", "ADJD", "KON", "ADJD", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.28": {"line.1": {"text": "Mir g\u00e4b es keine gr\u00f6\u00dfre Pein,", "tokens": ["Mir", "g\u00e4b", "es", "kei\u00b7ne", "gr\u00f6\u00df\u00b7re", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "W\u00e4r ich im Paradies allein.", "tokens": ["W\u00e4r", "ich", "im", "Pa\u00b7ra\u00b7dies", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPRART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Es lie\u00dfe sich alles trefflich schlichten,", "tokens": ["Es", "lie\u00b7\u00dfe", "sich", "al\u00b7les", "treff\u00b7lich", "schlich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PIS", "ADJD", "VVFIN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "K\u00f6nnte man die Sachen zweimal verrichten.", "tokens": ["K\u00f6nn\u00b7te", "man", "die", "Sa\u00b7chen", "zwei\u00b7mal", "ver\u00b7rich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ART", "NN", "ADV", "VVINF", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.30": {"line.1": {"text": "Nur heute, heute nur la\u00df dich nicht fangen,", "tokens": ["Nur", "heu\u00b7te", ",", "heu\u00b7te", "nur", "la\u00df", "dich", "nicht", "fan\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "ADV", "ADV", "VVIMP", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "So bist du hundertmal entgangen.", "tokens": ["So", "bist", "du", "hun\u00b7dert\u00b7mal", "ent\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Geht's in der Welt dir endlich schlecht,", "tokens": ["Geht's", "in", "der", "Welt", "dir", "end\u00b7lich", "schlecht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Tu, was du willst, nur habe nicht recht.", "tokens": ["Tu", ",", "was", "du", "willst", ",", "nur", "ha\u00b7be", "nicht", "recht", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PWS", "PPER", "VMFIN", "$,", "ADV", "VAFIN", "PTKNEG", "ADJD", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}}, "stanza.32": {"line.1": {"text": "Z\u00fccht'ge den Hund, den Wolf magst du peitschen;", "tokens": ["Z\u00fccht'\u00b7ge", "den", "Hund", ",", "den", "Wolf", "magst", "du", "peit\u00b7schen", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,", "ART", "NE", "VMFIN", "PPER", "VVINF", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Graue Haare sollst du nicht reizen.", "tokens": ["Grau\u00b7e", "Haa\u00b7re", "sollst", "du", "nicht", "rei\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.33": {"line.1": {"text": "Am Flusse kannst du stemmen und h\u00e4keln;", "tokens": ["Am", "Flus\u00b7se", "kannst", "du", "stem\u00b7men", "und", "h\u00e4\u00b7keln", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VMFIN", "PPER", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00dcberschwemmung l\u00e4\u00dft sich nicht m\u00e4keln.", "tokens": ["\u00dc\u00b7bersc\u00b7hwem\u00b7mung", "l\u00e4\u00dft", "sich", "nicht", "m\u00e4\u00b7keln", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "PTKNEG", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.34": {"line.1": {"text": "Tausend Fliegen hatt ich am Abend erschlagen,", "tokens": ["Tau\u00b7send", "Flie\u00b7gen", "hatt", "ich", "am", "A\u00b7bend", "er\u00b7schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PPER", "APPRART", "NN", "VVPP", "$,"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Doch werkte mich ", "tokens": ["Doch", "werk\u00b7te", "mich"], "token_info": ["word", "word", "word"], "pos": ["KON", "VVFIN", "PPER"], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.35": {"line.1": {"text": "Und w\u00e4rst du auch zum fernsten Ort,", "tokens": ["Und", "w\u00e4rst", "du", "auch", "zum", "ferns\u00b7ten", "Ort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zur kleinsten H\u00fctte durchgedrungen,", "tokens": ["Zur", "kleins\u00b7ten", "H\u00fct\u00b7te", "durch\u00b7ge\u00b7drun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was hilft es dir, du findest dort", "tokens": ["Was", "hilft", "es", "dir", ",", "du", "fin\u00b7dest", "dort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER", "PPER", "$,", "PPER", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Tabak und b\u00f6se Zungen.", "tokens": ["Ta\u00b7bak", "und", "b\u00f6\u00b7se", "Zun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.36": {"line.1": {"text": "W\u00fc\u00dfte nicht, was sie Bessers erfinden k\u00f6nnten,", "tokens": ["W\u00fc\u00df\u00b7te", "nicht", ",", "was", "sie", "Bes\u00b7sers", "er\u00b7fin\u00b7den", "k\u00f6nn\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "PRELS", "PPER", "NN", "VVINF", "VMFIN", "$,"], "meter": "+-+--+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Als wenn die Lichter ohne Putzen brennten.", "tokens": ["Als", "wenn", "die", "Lich\u00b7ter", "oh\u00b7ne", "Put\u00b7zen", "brenn\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.37": {"line.1": {"text": "Lief' das Brot, wie die Hasen laufen,", "tokens": ["Lie\u00b7f'", "das", "Brot", ",", "wie", "die", "Ha\u00b7sen", "lau\u00b7fen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,", "PWAV", "ART", "NN", "VVINF", "$,"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "Es kostete viel Schwei\u00df, es zu kaufen.", "tokens": ["Es", "kos\u00b7te\u00b7te", "viel", "Schwei\u00df", ",", "es", "zu", "kau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+---+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.38": {"line.1": {"text": "Will Vogelfang dir nicht geraten,", "tokens": ["Will", "Vo\u00b7gel\u00b7fang", "dir", "nicht", "ge\u00b7ra\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "NN", "PPER", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So magst du deinen Schuhu braten.", "tokens": ["So", "magst", "du", "dei\u00b7nen", "Schu\u00b7hu", "bra\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Das w\u00e4r dir ein sch\u00f6nes Gartengel\u00e4nde,", "tokens": ["Das", "w\u00e4r", "dir", "ein", "sch\u00f6\u00b7nes", "Gar\u00b7ten\u00b7ge\u00b7l\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wo man den Weinstock mit W\u00fcrsten b\u00e4nde.", "tokens": ["Wo", "man", "den", "Wein\u00b7stock", "mit", "W\u00fcrs\u00b7ten", "b\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.40": {"line.1": {"text": "Du mu\u00dft dich niemals mit Schwur vermessen:", "tokens": ["Du", "mu\u00dft", "dich", "nie\u00b7mals", "mit", "Schwur", "ver\u00b7mes\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Von dieser Speise will ich nicht essen.", "tokens": ["Von", "die\u00b7ser", "Spei\u00b7se", "will", "ich", "nicht", "es\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}}, "stanza.41": {"line.1": {"text": "Wer aber recht bequem ist und faul,", "tokens": ["Wer", "a\u00b7ber", "recht", "be\u00b7quem", "ist", "und", "faul", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADV", "ADJD", "VAFIN", "KON", "ADJD", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Fl\u00f6g dem eine gebratne Taube ins Maul,", "tokens": ["Fl\u00f6g", "dem", "ei\u00b7ne", "ge\u00b7brat\u00b7ne", "Tau\u00b7be", "ins", "Maul", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "+-+--+-+--+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Er w\u00fcrde h\u00f6chlich sich's verbitten,", "tokens": ["Er", "w\u00fcr\u00b7de", "h\u00f6ch\u00b7lich", "sich's", "ver\u00b7bit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "W\u00e4r sie nicht auch geschickt zerschnitten.", "tokens": ["W\u00e4r", "sie", "nicht", "auch", "ge\u00b7schickt", "zer\u00b7schnit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADV", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Freigebig ist der mit seinen Schritten,", "tokens": ["Frei\u00b7ge\u00b7big", "ist", "der", "mit", "sei\u00b7nen", "Schrit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Der kommt, von der Katze Speck zu erbitten.", "tokens": ["Der", "kommt", ",", "von", "der", "Kat\u00b7ze", "Speck", "zu", "er\u00b7bit\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "APPR", "ART", "NN", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.43": {"line.1": {"text": "Hast deine Kastanien zu lange gebraten;", "tokens": ["Hast", "dei\u00b7ne", "Kas\u00b7ta\u00b7ni\u00b7en", "zu", "lan\u00b7ge", "ge\u00b7bra\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "APPR", "ADV", "VVPP", "$."], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Sie sind dir alle zu Kohlen geraten.", "tokens": ["Sie", "sind", "dir", "al\u00b7le", "zu", "Koh\u00b7len", "ge\u00b7ra\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PIS", "APPR", "NN", "VVPP", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.44": {"line.1": {"text": "Das sind mir allzu b\u00f6se Bissen,", "tokens": ["Das", "sind", "mir", "all\u00b7zu", "b\u00f6\u00b7se", "Bis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "PTKA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "An denen die G\u00e4ste erw\u00fcrgen m\u00fcssen.", "tokens": ["An", "de\u00b7nen", "die", "G\u00e4s\u00b7te", "er\u00b7w\u00fcr\u00b7gen", "m\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ART", "NN", "VVINF", "VMINF", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.45": {"line.1": {"text": "Das ist eine von den gro\u00dfen Taten,", "tokens": ["Das", "ist", "ei\u00b7ne", "von", "den", "gro\u00b7\u00dfen", "Ta\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Sich in seinem eignen Fett zu braten.", "tokens": ["Sich", "in", "sei\u00b7nem", "eig\u00b7nen", "Fett", "zu", "bra\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "PPOSAT", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.46": {"line.1": {"text": "Gesotten oder gebraten!", "tokens": ["Ge\u00b7sot\u00b7ten", "o\u00b7der", "ge\u00b7bra\u00b7ten", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Er ist ans Feuer geraten.", "tokens": ["Er", "ist", "ans", "Feu\u00b7er", "ge\u00b7ra\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.47": {"line.1": {"text": "Gebraten oder gesotten!", "tokens": ["Ge\u00b7bra\u00b7ten", "o\u00b7der", "ge\u00b7sot\u00b7ten", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ihr sollt nicht meiner spotten.", "tokens": ["Ihr", "sollt", "nicht", "mei\u00b7ner", "spot\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "PPOSAT", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was ihr euch heute getr\u00f6stet,", "tokens": ["Was", "ihr", "euch", "heu\u00b7te", "ge\u00b7tr\u00f6s\u00b7tet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ihr seid doch morgen ger\u00f6stet.", "tokens": ["Ihr", "seid", "doch", "mor\u00b7gen", "ge\u00b7r\u00f6s\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.48": {"line.1": {"text": "Wer Ohren hat, soll h\u00f6ren;", "tokens": ["Wer", "Oh\u00b7ren", "hat", ",", "soll", "h\u00f6\u00b7ren", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Wer Geld hat, soll's verzehren.", "tokens": ["Wer", "Geld", "hat", ",", "soll's", "ver\u00b7zeh\u00b7ren", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "$,", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.49": {"line.1": {"text": "Der Mutter schenk ich,", "tokens": ["Der", "Mut\u00b7ter", "schenk", "ich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Die Tochter denk ich.", "tokens": ["Die", "Toch\u00b7ter", "denk", "ich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.50": {"line.1": {"text": "Kleid' eine S\u00e4ule,", "tokens": ["Kleid'", "ei\u00b7ne", "S\u00e4u\u00b7le", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Sie sieht wie eine Fr\u00e4ule.", "tokens": ["Sie", "sieht", "wie", "ei\u00b7ne", "Fr\u00e4u\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.51": {"line.1": {"text": "Schlaf ich, so schlaf ich mir bequem.", "tokens": ["Schlaf", "ich", ",", "so", "schlaf", "ich", "mir", "be\u00b7quem", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "$,", "ADV", "VVFIN", "PPER", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Arbeit ich, ja, ich wei\u00df nicht wem.", "tokens": ["Ar\u00b7beit", "ich", ",", "ja", ",", "ich", "wei\u00df", "nicht", "wem", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "$,", "PTKANT", "$,", "PPER", "VVFIN", "PTKNEG", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.52": {"line.1": {"text": "Ganz und gar", "tokens": ["Ganz", "und", "gar"], "token_info": ["word", "word", "word"], "pos": ["ADV", "KON", "ADV"], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Bin ich ein armer Wicht.", "tokens": ["Bin", "ich", "ein", "ar\u00b7mer", "Wicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Meine Tr\u00e4ume sind nicht wahr,", "tokens": ["Mei\u00b7ne", "Tr\u00e4u\u00b7me", "sind", "nicht", "wahr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Und meine Gedanken geraten nicht.", "tokens": ["Und", "mei\u00b7ne", "Ge\u00b7dan\u00b7ken", "ge\u00b7ra\u00b7ten", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "PTKNEG", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.53": {"line.1": {"text": "Mit meinem Willen mag's geschehn! \u2013", "tokens": ["Mit", "mei\u00b7nem", "Wil\u00b7len", "mag's", "ge\u00b7schehn", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NE", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Tr\u00e4ne wird mir in dem Auge stehn.", "tokens": ["Die", "Tr\u00e4\u00b7ne", "wird", "mir", "in", "dem", "Au\u00b7ge", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.54": {"line.1": {"text": "Wohl ungl\u00fcckselig ist der Mann,", "tokens": ["Wohl", "un\u00b7gl\u00fcck\u00b7se\u00b7lig", "ist", "der", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der unterl\u00e4\u00dft das, was er kann,", "tokens": ["Der", "un\u00b7ter\u00b7l\u00e4\u00dft", "das", ",", "was", "er", "kann", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PDS", "$,", "PWS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und unterf\u00e4ngt sich, was er nicht versteht;", "tokens": ["Und", "un\u00b7ter\u00b7f\u00e4ngt", "sich", ",", "was", "er", "nicht", "ver\u00b7steht", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "$,", "PWS", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Kein Wunder, da\u00df er zugrunde geht.", "tokens": ["Kein", "Wun\u00b7der", ",", "da\u00df", "er", "zu\u00b7grun\u00b7de", "geht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "KOUS", "PPER", "ADJA", "VVFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.55": {"line.1": {"text": "Du tr\u00e4gst sehr leicht, wenn du nichts hast;", "tokens": ["Du", "tr\u00e4gst", "sehr", "leicht", ",", "wenn", "du", "nichts", "hast", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "$,", "KOUS", "PPER", "PIS", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Aber Reichtum ist eine leichtere Last.", "tokens": ["A\u00b7ber", "Reich\u00b7tum", "ist", "ei\u00b7ne", "leich\u00b7te\u00b7re", "Last", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-++-+--+", "measure": "iambic.hexa.chol"}}, "stanza.56": {"line.1": {"text": "Alles in der Welt l\u00e4\u00dft sich ertragen,", "tokens": ["Al\u00b7les", "in", "der", "Welt", "l\u00e4\u00dft", "sich", "er\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "ART", "NN", "VVFIN", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Nur nicht eine Reihe von sch\u00f6nen Tagen.", "tokens": ["Nur", "nicht", "ei\u00b7ne", "Rei\u00b7he", "von", "sch\u00f6\u00b7nen", "Ta\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.57": {"line.1": {"text": "Was r\u00e4ucherst du nun deinem Toten?", "tokens": ["Was", "r\u00e4u\u00b7cherst", "du", "nun", "dei\u00b7nem", "To\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "H\u00e4ttst du's ihm so im Leben geboten!", "tokens": ["H\u00e4ttst", "du's", "ihm", "so", "im", "Le\u00b7ben", "ge\u00b7bo\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "ADV", "APPRART", "NN", "VVPP", "$."], "meter": "-----+--+-", "measure": "iambic.di.relaxed"}}, "stanza.58": {"line.1": {"text": "Ja! wer eure Verehrung nicht kennte:", "tokens": ["Ja", "!", "wer", "eu\u00b7re", "Ver\u00b7eh\u00b7rung", "nicht", "kenn\u00b7te", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$.", "PWS", "PPOSAT", "NN", "PTKNEG", "VVFIN", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Euch, nicht ihm baut ihr Monumente.", "tokens": ["Euch", ",", "nicht", "ihm", "baut", "ihr", "Mo\u00b7nu\u00b7men\u00b7te", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PTKNEG", "PPER", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}}, "stanza.59": {"line.1": {"text": "Willst du dich deines Wertes freuen,", "tokens": ["Willst", "du", "dich", "dei\u00b7nes", "Wer\u00b7tes", "freu\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PRF", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So mu\u00dft der Welt du Wert verleihen.", "tokens": ["So", "mu\u00dft", "der", "Welt", "du", "Wert", "ver\u00b7lei\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "NN", "NE", "NE", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "Will einer in die W\u00fcste pred'gen,", "tokens": ["Will", "ei\u00b7ner", "in", "die", "W\u00fcs\u00b7te", "pre\u00b7d'\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der mag sich von sich selbst erled'gen;", "tokens": ["Der", "mag", "sich", "von", "sich", "selbst", "er\u00b7le\u00b7d'\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PRF", "APPR", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Spricht aber einer zu seinen Br\u00fcdern,", "tokens": ["Spricht", "a\u00b7ber", "ei\u00b7ner", "zu", "sei\u00b7nen", "Br\u00fc\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PIS", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Werden sie's oft schlecht erwidern.", "tokens": ["Wer\u00b7den", "sie's", "oft", "schlecht", "er\u00b7wi\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.61": {"line.1": {"text": "La\u00df Neid und Mi\u00dfgunst sich verzehren,", "tokens": ["La\u00df", "Neid", "und", "Mi\u00df\u00b7gunst", "sich", "ver\u00b7zeh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "KON", "NN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Gute werden sie nicht wehren.", "tokens": ["Das", "Gu\u00b7te", "wer\u00b7den", "sie", "nicht", "weh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Denn, Gott sei Dank! es ist ein alter Brauch:", "tokens": ["Denn", ",", "Gott", "sei", "Dank", "!", "es", "ist", "ein", "al\u00b7ter", "Brauch", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "NN", "VAFIN", "NN", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "So weit die Sonne scheint, so weit erw\u00e4rmt sie auch.", "tokens": ["So", "weit", "die", "Son\u00b7ne", "scheint", ",", "so", "weit", "er\u00b7w\u00e4rmt", "sie", "auch", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "VVFIN", "$,", "ADV", "ADJD", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.62": {"line.1": {"text": "Das Interim", "tokens": ["Das", "In\u00b7te\u00b7rim"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Hat den Schalk hinter ihm.", "tokens": ["Hat", "den", "Schalk", "hin\u00b7ter", "ihm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPR", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wieviel Sch\u00e4lke mu\u00df es geben,", "tokens": ["Wie\u00b7viel", "Sch\u00e4l\u00b7ke", "mu\u00df", "es", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Da wir alle ad interim leben.", "tokens": ["Da", "wir", "al\u00b7le", "ad", "in\u00b7te\u00b7rim", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "FM", "FM", "VVINF", "$."], "meter": "--+-++-++-", "measure": "anapaest.init"}}, "stanza.63": {"line.1": {"text": "Was fragst du viel: Wo will's hinaus,", "tokens": ["Was", "fragst", "du", "viel", ":", "Wo", "will's", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "$.", "PWAV", "VMFIN", "APZR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wo oder wie kann's enden?", "tokens": ["Wo", "o\u00b7der", "wie", "kann's", "en\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "KON", "PWAV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich d\u00e4chte, Freund, du bliebst zu Haus", "tokens": ["Ich", "d\u00e4ch\u00b7te", ",", "Freund", ",", "du", "bliebst", "zu", "Haus"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "NN", "$,", "PPER", "VVFIN", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und spr\u00e4chst mit deinen W\u00e4nden.", "tokens": ["Und", "spr\u00e4chst", "mit", "dei\u00b7nen", "W\u00e4n\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.64": {"line.1": {"text": "Viele K\u00f6che versalzen den Brei;", "tokens": ["Vie\u00b7le", "K\u00f6\u00b7che", "ver\u00b7sal\u00b7zen", "den", "Brei", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ART", "NN", "$."], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Bewahr uns Gott vor vielen Dienern!", "tokens": ["Be\u00b7wahr", "uns", "Gott", "vor", "vie\u00b7len", "Die\u00b7nern", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wir aber sind, gesteht es frei,", "tokens": ["Wir", "a\u00b7ber", "sind", ",", "ge\u00b7steht", "es", "frei", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "$,", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Lazarett von Medizinern.", "tokens": ["Ein", "La\u00b7za\u00b7rett", "von", "Me\u00b7di\u00b7zi\u00b7nern", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.65": {"line.1": {"text": "Ihr meint, ich h\u00e4tt mich gewaltig betrogen;", "tokens": ["Ihr", "meint", ",", "ich", "h\u00e4tt", "mich", "ge\u00b7wal\u00b7tig", "be\u00b7tro\u00b7gen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hab's aber nicht aus den Fingern gesogen.", "tokens": ["Hab's", "a\u00b7ber", "nicht", "aus", "den", "Fin\u00b7gern", "ge\u00b7so\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "PTKNEG", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.66": {"line.1": {"text": "Noch spukt der Babylon'sche Turm,", "tokens": ["Noch", "spukt", "der", "Ba\u00b7by\u00b7lon'\u00b7sche", "Turm", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie sind nicht zu vereinen!", "tokens": ["Sie", "sind", "nicht", "zu", "ver\u00b7ei\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ein jeder Mann hat seinen Wurm,", "tokens": ["Ein", "je\u00b7der", "Mann", "hat", "sei\u00b7nen", "Wurm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kopernikus den seinen.", "tokens": ["Ko\u00b7per\u00b7ni\u00b7kus", "den", "sei\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ART", "PPOSAT", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.67": {"line.1": {"text": "Denn bei den alten, lieben Toten", "tokens": ["Denn", "bei", "den", "al\u00b7ten", ",", "lie\u00b7ben", "To\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Braucht man Erkl\u00e4rung, will man Noten.", "tokens": ["Braucht", "man", "Er\u00b7kl\u00e4\u00b7rung", ",", "will", "man", "No\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "NN", "$,", "VMFIN", "PIS", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Neuen glaubt man blank zu verstehn;", "tokens": ["Die", "Neu\u00b7en", "glaubt", "man", "blank", "zu", "ver\u00b7stehn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Doch ohne Dolmetsch wird's auch nicht gehn.", "tokens": ["Doch", "oh\u00b7ne", "Dol\u00b7metsch", "wird's", "auch", "nicht", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VAFIN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.68": {"line.1": {"text": "Sie sagen: Das mutet mich nicht an!", "tokens": ["Sie", "sa\u00b7gen", ":", "Das", "mu\u00b7tet", "mich", "nicht", "an", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$.", "PDS", "VVFIN", "PPER", "PTKNEG", "PTKVZ", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und meinen, sie h\u00e4tten's abgetan.", "tokens": ["Und", "mei\u00b7nen", ",", "sie", "h\u00e4t\u00b7ten's", "ab\u00b7ge\u00b7tan", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.69": {"line.1": {"text": "In meinem Revier", "tokens": ["In", "mei\u00b7nem", "Re\u00b7vier"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN"], "meter": "-+---", "measure": "dactylic.init"}, "line.2": {"text": "Sind Gelehrte gewesen,", "tokens": ["Sind", "Ge\u00b7lehr\u00b7te", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "VAPP", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Au\u00dfer ihrem eignen Brevier", "tokens": ["Au\u00b7\u00dfer", "ih\u00b7rem", "eig\u00b7nen", "Bre\u00b7vier"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Konnten sie keines lesen.", "tokens": ["Konn\u00b7ten", "sie", "kei\u00b7nes", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PIS", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.70": {"line.1": {"text": "Viel Rettungsmittel bietest du! was hei\u00dft's?", "tokens": ["Viel", "Ret\u00b7tungs\u00b7mit\u00b7tel", "bie\u00b7test", "du", "!", "was", "hei\u00dft's", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "$.", "PWS", "NE", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die beste Rettung: Gegenwart des Geists!", "tokens": ["Die", "bes\u00b7te", "Ret\u00b7tung", ":", "Ge\u00b7gen\u00b7wart", "des", "Geists", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$.", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.71": {"line.1": {"text": "La\u00df nur die Sorge sein,", "tokens": ["La\u00df", "nur", "die", "Sor\u00b7ge", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "ART", "NN", "VAINF", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Das gibt sich alles schon;", "tokens": ["Das", "gibt", "sich", "al\u00b7les", "schon", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "PIS", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und f\u00e4llt der Himmel ein,", "tokens": ["Und", "f\u00e4llt", "der", "Him\u00b7mel", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Kommt doch eine Lerche davon.", "tokens": ["Kommt", "doch", "ei\u00b7ne", "Ler\u00b7che", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "--+-+--+", "measure": "iambic.tri.chol"}}, "stanza.72": {"line.1": {"text": "Dann ist einer durchaus verarmt,", "tokens": ["Dann", "ist", "ei\u00b7ner", "durc\u00b7haus", "ver\u00b7armt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "ADV", "VVPP", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Wenn die Scham den Schaden umarmt.", "tokens": ["Wenn", "die", "Scham", "den", "Scha\u00b7den", "um\u00b7armt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.73": {"line.1": {"text": "Du treibst mir's gar zu toll.", "tokens": ["Du", "treibst", "mir's", "gar", "zu", "toll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "ADV", "PTKA", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Ich f\u00fcrcht, es breche!", "tokens": ["Ich", "f\u00fcrcht", ",", "es", "bre\u00b7che", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Nicht jeden Wochenschlu\u00df", "tokens": ["Nicht", "je\u00b7den", "Wo\u00b7chen\u00b7schlu\u00df"], "token_info": ["word", "word", "word"], "pos": ["PTKNEG", "PIAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Macht Gott die Zeche.", "tokens": ["Macht", "Gott", "die", "Ze\u00b7che", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.74": {"line.1": {"text": "Du bist sehr eilig, meiner Treu!", "tokens": ["Du", "bist", "sehr", "ei\u00b7lig", ",", "mei\u00b7ner", "Treu", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Du suchst die T\u00fcr und l\u00e4ufst vorbei.", "tokens": ["Du", "suchst", "die", "T\u00fcr", "und", "l\u00e4ufst", "vor\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "KON", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.75": {"line.1": {"text": "Sie glauben, miteinander zu streiten,", "tokens": ["Sie", "glau\u00b7ben", ",", "mi\u00b7tein\u00b7an\u00b7der", "zu", "strei\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und f\u00fchlen das Unrecht von beiden Seiten.", "tokens": ["Und", "f\u00fch\u00b7len", "das", "Un\u00b7recht", "von", "bei\u00b7den", "Sei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.76": {"line.1": {"text": "Haben's gekauft, es freut sie ba\u00df;", "tokens": ["Ha\u00b7ben's", "ge\u00b7kauft", ",", "es", "freut", "sie", "ba\u00df", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "$,", "PPER", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Eh man's denkt, so betr\u00fcbt sie das.", "tokens": ["Eh", "man's", "denkt", ",", "so", "be\u00b7tr\u00fcbt", "sie", "das", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$,", "ADV", "VVFIN", "PPER", "PDS", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.77": {"line.1": {"text": "Willst du nichts Unn\u00fctzes kaufen,", "tokens": ["Willst", "du", "nichts", "Un\u00b7n\u00fct\u00b7zes", "kau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PIS", "ADJA", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Mu\u00dft du nicht auf den Jahrmarkt laufen.", "tokens": ["Mu\u00dft", "du", "nicht", "auf", "den", "Jahr\u00b7markt", "lau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PTKNEG", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.78": {"line.1": {"text": "Langeweile ist ein b\u00f6ses Kraut,", "tokens": ["Lan\u00b7ge\u00b7wei\u00b7le", "ist", "ein", "b\u00f6\u00b7ses", "Kraut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Aber auch eine W\u00fcrze, die viel verdaut.", "tokens": ["A\u00b7ber", "auch", "ei\u00b7ne", "W\u00fcr\u00b7ze", ",", "die", "viel", "ver\u00b7daut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "$,", "PRELS", "ADV", "VVPP", "$."], "meter": "+--+-+--+-+", "measure": "iambic.penta.invert"}}, "stanza.79": {"line.1": {"text": "Wird uns eine rechte Qual zuteil,", "tokens": ["Wird", "uns", "ei\u00b7ne", "rech\u00b7te", "Qual", "zu\u00b7teil", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "ADJA", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Dann w\u00fcnschen wir uns Langeweil.", "tokens": ["Dann", "w\u00fcn\u00b7schen", "wir", "uns", "Lan\u00b7ge\u00b7weil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.80": {"line.1": {"text": "Da\u00df sie die Kinder erziehen k\u00f6nnten,", "tokens": ["Da\u00df", "sie", "die", "Kin\u00b7der", "er\u00b7zie\u00b7hen", "k\u00f6nn\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "M\u00fc\u00dften die M\u00fctter sein wie Enten:", "tokens": ["M\u00fc\u00df\u00b7ten", "die", "M\u00fct\u00b7ter", "sein", "wie", "En\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "VAINF", "KOKOM", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Sie schw\u00e4mmen mit ihrer Brut in Ruh;", "tokens": ["Sie", "schw\u00e4m\u00b7men", "mit", "ih\u00b7rer", "Brut", "in", "Ruh", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "APPR", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da geh\u00f6rt aber freilich Wasser dazu.", "tokens": ["Da", "ge\u00b7h\u00f6rt", "a\u00b7ber", "frei\u00b7lich", "Was\u00b7ser", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "NN", "PTKVZ", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}}, "stanza.81": {"line.1": {"text": "Das junge Volk, es bildet sich ein,", "tokens": ["Das", "jun\u00b7ge", "Volk", ",", "es", "bil\u00b7det", "sich", "ein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PPER", "VVFIN", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sein Tauftag sollte der Sch\u00f6pfungstag sein.", "tokens": ["Sein", "Tauf\u00b7tag", "soll\u00b7te", "der", "Sch\u00f6p\u00b7fungs\u00b7tag", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "ART", "NN", "VAINF", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "M\u00f6chten sie doch zugleich bedenken,", "tokens": ["M\u00f6ch\u00b7ten", "sie", "doch", "zu\u00b7gleich", "be\u00b7den\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Was wir ihnen als Eingebinde schenken.", "tokens": ["Was", "wir", "ih\u00b7nen", "als", "Ein\u00b7ge\u00b7bin\u00b7de", "schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PPER", "KOUS", "NN", "VVINF", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.82": {"line.1": {"text": "\u00bbnein! heut ist mir das Gl\u00fcck erbost!\u00ab", "tokens": ["\u00bb", "nein", "!", "heut", "ist", "mir", "das", "Gl\u00fcck", "er\u00b7bost", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PTKANT", "$.", "ADV", "VAFIN", "PPER", "ART", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Du, sattle gut und reite getrost!", "tokens": ["Du", ",", "satt\u00b7le", "gut", "und", "rei\u00b7te", "ge\u00b7trost", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "VVFIN", "ADJD", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.83": {"line.1": {"text": "\u00dcber ein Ding wird viel geplaudert,", "tokens": ["\u00dc\u00b7ber", "ein", "Ding", "wird", "viel", "ge\u00b7plau\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "ADV", "VVPP", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Viel beraten und lange gezaudert,", "tokens": ["Viel", "be\u00b7ra\u00b7ten", "und", "lan\u00b7ge", "ge\u00b7zau\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "KON", "ADV", "VVPP", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Und endlich gibt ein b\u00f6ses Mu\u00df", "tokens": ["Und", "end\u00b7lich", "gibt", "ein", "b\u00f6\u00b7ses", "Mu\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Sache widrig den Beschlu\u00df.", "tokens": ["Der", "Sa\u00b7che", "wid\u00b7rig", "den", "Be\u00b7schlu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.84": {"line.1": {"text": "Eine Bresche ist jeder Tag,", "tokens": ["Ei\u00b7ne", "Bre\u00b7sche", "ist", "je\u00b7der", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIAT", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Die viele Menschen erst\u00fcrmen.", "tokens": ["Die", "vie\u00b7le", "Men\u00b7schen", "er\u00b7st\u00fcr\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wer auch in die L\u00fccke fallen mag,", "tokens": ["Wer", "auch", "in", "die", "L\u00fc\u00b7cke", "fal\u00b7len", "mag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "APPR", "ART", "NN", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Die Toten sich niemals t\u00fcrmen.", "tokens": ["Die", "To\u00b7ten", "sich", "nie\u00b7mals", "t\u00fcr\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PRF", "ADV", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.85": {"line.1": {"text": "Wenn einer schiffet und reiset,", "tokens": ["Wenn", "ei\u00b7ner", "schif\u00b7fet", "und", "rei\u00b7set", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Sammelt er nach und nach immer ein,", "tokens": ["Sam\u00b7melt", "er", "nach", "und", "nach", "im\u00b7mer", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "KON", "APPR", "ADV", "PTKVZ", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Was sich am Leben mit mancher Pein", "tokens": ["Was", "sich", "am", "Le\u00b7ben", "mit", "man\u00b7cher", "Pein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PRF", "APPRART", "NN", "APPR", "PIAT", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wieder aussch\u00e4let und weiset.", "tokens": ["Wie\u00b7der", "aus\u00b7sch\u00e4\u00b7let", "und", "wei\u00b7set", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "KON", "VVFIN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.86": {"line.1": {"text": "Der Mensch erf\u00e4hrt, er sei auch, wer er mag,", "tokens": ["Der", "Mensch", "er\u00b7f\u00e4hrt", ",", "er", "sei", "auch", ",", "wer", "er", "mag", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "PPER", "VAFIN", "ADV", "$,", "PWS", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ein letztes Gl\u00fcck und einen letzten Tag.", "tokens": ["Ein", "letz\u00b7tes", "Gl\u00fcck", "und", "ei\u00b7nen", "letz\u00b7ten", "Tag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.87": {"line.1": {"text": "Das Gl\u00fcck deiner Tage", "tokens": ["Das", "Gl\u00fcck", "dei\u00b7ner", "Ta\u00b7ge"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "PPOSAT", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "W\u00e4ge nicht mit der Goldwaage.", "tokens": ["W\u00e4\u00b7ge", "nicht", "mit", "der", "Gold\u00b7waa\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "APPR", "ART", "NN", "$."], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.3": {"text": "Wirst du die Kr\u00e4merwaage nehmen,", "tokens": ["Wirst", "du", "die", "Kr\u00e4\u00b7mer\u00b7waa\u00b7ge", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So wirst du dich sch\u00e4men und dich bequemen.", "tokens": ["So", "wirst", "du", "dich", "sch\u00e4\u00b7men", "und", "dich", "be\u00b7que\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PRF", "VVINF", "KON", "PPER", "ADJA", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.88": {"line.1": {"text": "Hast du einmal das Rechte getan", "tokens": ["Hast", "du", "ein\u00b7mal", "das", "Rech\u00b7te", "ge\u00b7tan"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "ART", "NN", "VVPP"], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Und sieht ein Feind nur Scheeles daran,", "tokens": ["Und", "sieht", "ein", "Feind", "nur", "Schee\u00b7les", "da\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "NN", "PAV", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "So wird er gelegentlich, sp\u00e4t oder fr\u00fch,", "tokens": ["So", "wird", "er", "ge\u00b7le\u00b7gent\u00b7lich", ",", "sp\u00e4t", "o\u00b7der", "fr\u00fch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$,", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Dasselbe tun, er wei\u00df nicht wie.", "tokens": ["Das\u00b7sel\u00b7be", "tun", ",", "er", "wei\u00df", "nicht", "wie", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVINF", "$,", "PPER", "VVFIN", "PTKNEG", "PWAV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.89": {"line.1": {"text": "Willst du das Gute tun, mein Sohn,", "tokens": ["Willst", "du", "das", "Gu\u00b7te", "tun", ",", "mein", "Sohn", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "VVINF", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So lebe nur lange, da gibt sich's schon;", "tokens": ["So", "le\u00b7be", "nur", "lan\u00b7ge", ",", "da", "gibt", "sich's", "schon", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "$,", "ADV", "VVFIN", "PIS", "ADV", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Solltest du aber zu fr\u00fch ersterben,", "tokens": ["Soll\u00b7test", "du", "a\u00b7ber", "zu", "fr\u00fch", "ers\u00b7ter\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "PTKA", "ADJD", "VVPP", "$,"], "meter": "-+-+---+--", "measure": "unknown.measure.tri"}, "line.4": {"text": "Wirst du von K\u00fcnftigen Dank erwerben.", "tokens": ["Wirst", "du", "von", "K\u00fcnf\u00b7ti\u00b7gen", "Dank", "er\u00b7wer\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "APPR", "VVINF", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.90": {"line.1": {"text": "Was gibt uns wohl den sch\u00f6nsten Frieden,", "tokens": ["Was", "gibt", "uns", "wohl", "den", "sch\u00f6ns\u00b7ten", "Frie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als frei am eignen Gl\u00fcck zu schmieden.", "tokens": ["Als", "frei", "am", "eig\u00b7nen", "Gl\u00fcck", "zu", "schmie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "APPRART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.91": {"line.1": {"text": "La\u00dft mir die jungen Leute nur,", "tokens": ["La\u00dft", "mir", "die", "jun\u00b7gen", "Leu\u00b7te", "nur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "ADJA", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ergetzt euch an ihren Gaben!", "tokens": ["Und", "er\u00b7getzt", "euch", "an", "ih\u00b7ren", "Ga\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Es will doch Gro\u00dfmama Natur", "tokens": ["Es", "will", "doch", "Gro\u00df\u00b7ma\u00b7ma", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Manchmal einen n\u00e4rrischen Einfall haben.", "tokens": ["Manch\u00b7mal", "ei\u00b7nen", "n\u00e4r\u00b7ri\u00b7schen", "Ein\u00b7fall", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.92": {"line.1": {"text": "Ungebildet waren wir unangenehm;", "tokens": ["Un\u00b7ge\u00b7bil\u00b7det", "wa\u00b7ren", "wir", "un\u00b7an\u00b7ge\u00b7nehm", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Jetzt sind uns die Neuen sehr unbequem.", "tokens": ["Jetzt", "sind", "uns", "die", "Neu\u00b7en", "sehr", "un\u00b7be\u00b7quem", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.93": {"line.1": {"text": "Wo Anma\u00dfung mir wohlgef\u00e4llt?", "tokens": ["Wo", "An\u00b7ma\u00b7\u00dfung", "mir", "wohl\u00b7ge\u00b7f\u00e4llt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An Kindern: denen geh\u00f6rt die Welt.", "tokens": ["An", "Kin\u00b7dern", ":", "de\u00b7nen", "ge\u00b7h\u00f6rt", "die", "Welt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$.", "PDS", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.94": {"line.1": {"text": "Ihr z\u00e4hlt mich immer unter die Frohen,", "tokens": ["Ihr", "z\u00e4hlt", "mich", "im\u00b7mer", "un\u00b7ter", "die", "Fro\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Erst lebt ich roh, jetzt unter den Rohen.", "tokens": ["Erst", "lebt", "ich", "roh", ",", "jetzt", "un\u00b7ter", "den", "Ro\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "$,", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Den Fehler, den man selbst ge\u00fcbt,", "tokens": ["Den", "Feh\u00b7ler", ",", "den", "man", "selbst", "ge\u00b7\u00fcbt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PIS", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Man auch wohl an dem andern liebt.", "tokens": ["Man", "auch", "wohl", "an", "dem", "an\u00b7dern", "liebt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "ADV", "APPR", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.95": {"line.1": {"text": "Willst du mit mir hausen,", "tokens": ["Willst", "du", "mit", "mir", "hau\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "PPER", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "So la\u00df die Bestie drau\u00dfen.", "tokens": ["So", "la\u00df", "die", "Be\u00b7stie", "drau\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.96": {"line.1": {"text": "Wollen die Menschen Bestien sein,", "tokens": ["Wol\u00b7len", "die", "Men\u00b7schen", "Be\u00b7sti\u00b7en", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VVPP", "VAINF", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "So bringt nur Tiere zur Stube herein,", "tokens": ["So", "bringt", "nur", "Tie\u00b7re", "zur", "Stu\u00b7be", "her\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "NN", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das Widerw\u00e4rtige wird sich mindern.", "tokens": ["Das", "Wi\u00b7der\u00b7w\u00e4r\u00b7ti\u00b7ge", "wird", "sich", "min\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PRF", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wir sind eben alle von Adams Kindern.", "tokens": ["Wir", "sind", "e\u00b7ben", "al\u00b7le", "von", "A\u00b7dams", "Kin\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "APPR", "NE", "NN", "$."], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.97": {"line.1": {"text": "Mit Narren leben wird dir gar nicht schwer,", "tokens": ["Mit", "Nar\u00b7ren", "le\u00b7ben", "wird", "dir", "gar", "nicht", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Erhalte nur ein Tollhaus um dich her.", "tokens": ["Er\u00b7hal\u00b7te", "nur", "ein", "Toll\u00b7haus", "um", "dich", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.98": {"line.1": {"text": "Sag mir, was ein Hypochondrist", "tokens": ["Sag", "mir", ",", "was", "ein", "Hy\u00b7po\u00b7chond\u00b7rist"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "PPER", "$,", "PRELS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr ein wunderlicher Kunstfreund ist.", "tokens": ["F\u00fcr", "ein", "wun\u00b7der\u00b7li\u00b7cher", "Kunst\u00b7freund", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "In Bildergalerien geht er spazieren", "tokens": ["In", "Bil\u00b7der\u00b7ga\u00b7le\u00b7ri\u00b7en", "geht", "er", "spa\u00b7zie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPER", "VVINF"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Vor lauter Gem\u00e4lden, die ihn vexieren.", "tokens": ["Vor", "lau\u00b7ter", "Ge\u00b7m\u00e4l\u00b7den", ",", "die", "ihn", "ve\u00b7xie\u00b7ren", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,", "PRELS", "PPER", "VVINF", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.99": {"line.1": {"text": "Der Hypochonder ist bald kuriert,", "tokens": ["Der", "Hy\u00b7po\u00b7chon\u00b7der", "ist", "bald", "ku\u00b7riert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn euch das Leben recht kujoniert.", "tokens": ["Wenn", "euch", "das", "Le\u00b7ben", "recht", "ku\u00b7jo\u00b7niert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.100": {"line.1": {"text": "Du sollst mit dem Tode zufrieden sein,", "tokens": ["Du", "sollst", "mit", "dem", "To\u00b7de", "zu\u00b7frie\u00b7den", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "ART", "NN", "ADJD", "VAINF", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Warum machst du dir das Leben zur Pein?", "tokens": ["Wa\u00b7rum", "machst", "du", "dir", "das", "Le\u00b7ben", "zur", "Pein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "PPER", "ART", "NN", "APPRART", "NN", "$."], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.101": {"line.1": {"text": "Kein tolleres Versehn kann sein,", "tokens": ["Kein", "tol\u00b7le\u00b7res", "Ver\u00b7sehn", "kann", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VMFIN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gibst einem ein Fest und l\u00e4dst ihn nicht ein.", "tokens": ["Gibst", "ei\u00b7nem", "ein", "Fest", "und", "l\u00e4dst", "ihn", "nicht", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ART", "NN", "KON", "VVFIN", "PPER", "PTKNEG", "PTKVZ", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.102": {"line.1": {"text": "Da siehst du nun, wie's einem geht,", "tokens": ["Da", "siehst", "du", "nun", ",", "wie's", "ei\u00b7nem", "geht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "KOUS", "ART", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil sich der Beste von selbst versteht.", "tokens": ["Weil", "sich", "der", "Bes\u00b7te", "von", "selbst", "ver\u00b7steht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "NN", "APPR", "ADV", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.103": {"line.1": {"text": "Wenn ein Edler gegen dich fehlt,", "tokens": ["Wenn", "ein", "Ed\u00b7ler", "ge\u00b7gen", "dich", "fehlt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "PPER", "VVFIN", "$,"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "So tu, als h\u00e4ttest du's nicht gez\u00e4hlt:", "tokens": ["So", "tu", ",", "als", "h\u00e4t\u00b7test", "du's", "nicht", "ge\u00b7z\u00e4hlt", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "$,", "KOKOM", "VAFIN", "PIS", "PTKNEG", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Er wird es in sein Schuldbuch schreiben", "tokens": ["Er", "wird", "es", "in", "sein", "Schuld\u00b7buch", "schrei\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und dir nicht lange im Debet bleiben.", "tokens": ["Und", "dir", "nicht", "lan\u00b7ge", "im", "De\u00b7bet", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "PTKNEG", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.104": {"line.1": {"text": "Suche nicht vergebne Heilung!", "tokens": ["Su\u00b7che", "nicht", "ver\u00b7geb\u00b7ne", "Hei\u00b7lung", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unsrer Krankheit schwer Geheimnis", "tokens": ["Uns\u00b7rer", "Krank\u00b7heit", "schwer", "Ge\u00b7heim\u00b7nis"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADJD", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Schwankt zwischen \u00dcbereilung", "tokens": ["Schwankt", "zwi\u00b7schen", "\u00dc\u00b7be\u00b7rei\u00b7lung"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und zwischen Vers\u00e4umnis.", "tokens": ["Und", "zwi\u00b7schen", "Ver\u00b7s\u00e4um\u00b7nis", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.105": {"line.1": {"text": "Ja, schelte nur und fluche fort,", "tokens": ["Ja", ",", "schel\u00b7te", "nur", "und", "flu\u00b7che", "fort", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "VVFIN", "ADV", "KON", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es wird sich Be\u00dfres nie ergeben.", "tokens": ["Es", "wird", "sich", "Be\u00df\u00b7res", "nie", "er\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PRF", "NE", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Denn Trost ist ein absurdes Wort:", "tokens": ["Denn", "Trost", "ist", "ein", "ab\u00b7sur\u00b7des", "Wort", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Wer nicht verzweiflen kann, der mu\u00df nicht leben.", "tokens": ["Wer", "nicht", "ver\u00b7zwei\u00b7flen", "kann", ",", "der", "mu\u00df", "nicht", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "VVINF", "VMFIN", "$,", "ART", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.106": {"line.1": {"text": "Ich soll nicht auf den Meister schw\u00f6ren", "tokens": ["Ich", "soll", "nicht", "auf", "den", "Meis\u00b7ter", "schw\u00f6\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PTKNEG", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und immerfort den Meister h\u00f6ren!", "tokens": ["Und", "im\u00b7mer\u00b7fort", "den", "Meis\u00b7ter", "h\u00f6\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nein, ich wei\u00df, er kann nicht l\u00fcgen,", "tokens": ["Nein", ",", "ich", "wei\u00df", ",", "er", "kann", "nicht", "l\u00fc\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "$,", "PPER", "VMFIN", "PTKNEG", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Will mich gern mit ihm betriegen.", "tokens": ["Will", "mich", "gern", "mit", "ihm", "be\u00b7trie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "APPR", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.107": {"line.1": {"text": "Mich freuen die vielen Guten und T\u00fccht'gen,", "tokens": ["Mich", "freu\u00b7en", "die", "vie\u00b7len", "Gu\u00b7ten", "und", "T\u00fccht'\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "PIAT", "NN", "KON", "NN", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Obgleich so viele dazwischenbelfen.", "tokens": ["Ob\u00b7gleich", "so", "vie\u00b7le", "da\u00b7zwi\u00b7schen\u00b7bel\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die Deutschen wissen zu bericht'gen,", "tokens": ["Die", "Deut\u00b7schen", "wis\u00b7sen", "zu", "be\u00b7richt'\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Aber sie verstehen nicht nachzuhelfen.", "tokens": ["A\u00b7ber", "sie", "ver\u00b7ste\u00b7hen", "nicht", "nach\u00b7zu\u00b7hel\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PTKNEG", "VVINF", "$."], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}}, "stanza.108": {"line.1": {"text": "\u00bbdu kommst nicht ins Ideenland!\u00ab", "tokens": ["\u00bb", "du", "kommst", "nicht", "ins", "I\u00b7deen\u00b7land", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "PTKNEG", "APPRART", "NN", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "So bin ich doch am Ufer bekannt.", "tokens": ["So", "bin", "ich", "doch", "am", "U\u00b7fer", "be\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Wer die Inseln nicht zu erobern glaubt,", "tokens": ["Wer", "die", "In\u00b7seln", "nicht", "zu", "er\u00b7o\u00b7bern", "glaubt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "PTKNEG", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Dem ist Ankerwerfen doch wohl erlaubt.", "tokens": ["Dem", "ist", "An\u00b7ker\u00b7wer\u00b7fen", "doch", "wohl", "er\u00b7laubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "ADV", "ADV", "VVPP", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.109": {"line.1": {"text": "Meine Dichterglut war sehr gering,", "tokens": ["Mei\u00b7ne", "Dich\u00b7ter\u00b7glut", "war", "sehr", "ge\u00b7ring", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Solang ich dem Guten entgegenging;", "tokens": ["So\u00b7lang", "ich", "dem", "Gu\u00b7ten", "ent\u00b7ge\u00b7gen\u00b7ging", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Dagegen brannte sie lichterloh,", "tokens": ["Da\u00b7ge\u00b7gen", "brann\u00b7te", "sie", "lich\u00b7ter\u00b7loh", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wenn ich vor drohendem \u00dcbel floh.", "tokens": ["Wenn", "ich", "vor", "dro\u00b7hen\u00b7dem", "\u00dc\u00b7bel", "floh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.110": {"line.1": {"text": "Zart Gedicht, wie Regenbogen,", "tokens": ["Zart", "Ge\u00b7dicht", ",", "wie", "Re\u00b7gen\u00b7bo\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "PWAV", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wird nur auf dunklen Grund gezogen;", "tokens": ["Wird", "nur", "auf", "dunk\u00b7len", "Grund", "ge\u00b7zo\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Darum behagt dem Dichtergenie", "tokens": ["Da\u00b7rum", "be\u00b7hagt", "dem", "Dich\u00b7ter\u00b7ge\u00b7nie"], "token_info": ["word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ART", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Das Element der Melancholie.", "tokens": ["Das", "E\u00b7le\u00b7ment", "der", "Me\u00b7lan\u00b7cho\u00b7lie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.111": {"line.1": {"text": "Kaum hatt ich mich in die Welt gespielt", "tokens": ["Kaum", "hatt", "ich", "mich", "in", "die", "Welt", "ge\u00b7spielt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PRF", "APPR", "ART", "NN", "VVPP"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und fing an aufzutauchen,", "tokens": ["Und", "fing", "an", "auf\u00b7zu\u00b7tau\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Als man mich schon so vornehm hielt,", "tokens": ["Als", "man", "mich", "schon", "so", "vor\u00b7nehm", "hielt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PRF", "ADV", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mich zu mi\u00dfbrauchen.", "tokens": ["Mich", "zu", "mi\u00df\u00b7brau\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.112": {"line.1": {"text": "Wer dem Publikum dient, ist ein armes Tier;", "tokens": ["Wer", "dem", "Pub\u00b7li\u00b7kum", "dient", ",", "ist", "ein", "ar\u00b7mes", "Tier", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "VVFIN", "$,", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": "Er qu\u00e4lt sich ab, niemand bedankt sich daf\u00fcr.", "tokens": ["Er", "qu\u00e4lt", "sich", "ab", ",", "nie\u00b7mand", "be\u00b7dankt", "sich", "da\u00b7f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PTKVZ", "$,", "PIS", "VVFIN", "PRF", "PAV", "$."], "meter": "-+-+---+--+", "measure": "iambic.tetra.chol"}}, "stanza.113": {"line.1": {"text": "Gleich zu sein unter Gleichen,", "tokens": ["Gleich", "zu", "sein", "un\u00b7ter", "Glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKZU", "VAINF", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Das l\u00e4\u00dft sich schwer erreichen:", "tokens": ["Das", "l\u00e4\u00dft", "sich", "schwer", "er\u00b7rei\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADJD", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.114": {"line.1": {"text": "Du m\u00fc\u00dftest ohne Verdrie\u00dfen", "tokens": ["Du", "m\u00fc\u00df\u00b7test", "oh\u00b7ne", "Ver\u00b7drie\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wie der Schlechteste zu sein dich entschlie\u00dfen.", "tokens": ["Wie", "der", "Schlech\u00b7tes\u00b7te", "zu", "sein", "dich", "ent\u00b7schlie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PTKZU", "VAINF", "PPER", "VVFIN", "$."], "meter": "--+---+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.115": {"line.1": {"text": "Man kann nicht immer zusammen stehn,", "tokens": ["Man", "kann", "nicht", "im\u00b7mer", "zu\u00b7sam\u00b7men", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PTKNEG", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Am wenigsten mit gro\u00dfen Haufen.", "tokens": ["Am", "we\u00b7nigs\u00b7ten", "mit", "gro\u00b7\u00dfen", "Hau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "PIS", "APPR", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Seine Freunde, die l\u00e4\u00dft man gehn,", "tokens": ["Sei\u00b7ne", "Freun\u00b7de", ",", "die", "l\u00e4\u00dft", "man", "gehn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VVFIN", "PIS", "VVINF", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Die Menge l\u00e4\u00dft man laufen.", "tokens": ["Die", "Men\u00b7ge", "l\u00e4\u00dft", "man", "lau\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.116": {"line.1": {"text": "Du magst an dir das Falsche n\u00e4hren,", "tokens": ["Du", "magst", "an", "dir", "das", "Fal\u00b7sche", "n\u00e4h\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Allein wir lassen uns nicht st\u00f6ren;", "tokens": ["Al\u00b7lein", "wir", "las\u00b7sen", "uns", "nicht", "st\u00f6\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Du kannst uns loben, kannst uns schelten,", "tokens": ["Du", "kannst", "uns", "lo\u00b7ben", ",", "kannst", "uns", "schel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVINF", "$,", "VMFIN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wir lassen es nicht f\u00fcr das Rechte gelten.", "tokens": ["Wir", "las\u00b7sen", "es", "nicht", "f\u00fcr", "das", "Rech\u00b7te", "gel\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.117": {"line.1": {"text": "Man soll sich nicht mit Sp\u00f6ttern befassen;", "tokens": ["Man", "soll", "sich", "nicht", "mit", "Sp\u00f6t\u00b7tern", "be\u00b7fas\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PRF", "PTKNEG", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wer will sich f\u00fcr 'nen Narren halten lassen!", "tokens": ["Wer", "will", "sich", "f\u00fcr", "'nen", "Nar\u00b7ren", "hal\u00b7ten", "las\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PRF", "APPR", "ADJA", "NN", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Dar\u00fcber mu\u00df man sich aber zerrei\u00dfen,", "tokens": ["Da\u00b7r\u00fc\u00b7ber", "mu\u00df", "man", "sich", "a\u00b7ber", "zer\u00b7rei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PIS", "PRF", "ADV", "VVINF", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da\u00df man Narren nicht darf Narren hei\u00dfen.", "tokens": ["Da\u00df", "man", "Nar\u00b7ren", "nicht", "darf", "Nar\u00b7ren", "hei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "NN", "PTKNEG", "VMFIN", "NN", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.118": {"line.1": {"text": "Christkindlein tr\u00e4gt die S\u00fcnden der Welt,", "tokens": ["Christ\u00b7kin\u00b7dlein", "tr\u00e4gt", "die", "S\u00fcn\u00b7den", "der", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ART", "NN", "ART", "NN", "$,"], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sankt Christoph das Kind \u00fcber Wasser h\u00e4lt,", "tokens": ["Sankt", "Chris\u00b7toph", "das", "Kind", "\u00fc\u00b7ber", "Was\u00b7ser", "h\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Sie haben es beid' uns angetan,", "tokens": ["Sie", "ha\u00b7ben", "es", "beid'", "uns", "an\u00b7ge\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVFIN", "PPER", "VVPP", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Es geht mit uns von vornen an.", "tokens": ["Es", "geht", "mit", "uns", "von", "vor\u00b7nen", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "APPR", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.119": {"line.1": {"text": "Efeu und ein z\u00e4rtlich Gem\u00fct", "tokens": ["E\u00b7feu", "und", "ein", "z\u00e4rt\u00b7lich", "Ge\u00b7m\u00fct"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "KON", "ART", "ADJD", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Heftet sich an und gr\u00fcnt und bl\u00fcht.", "tokens": ["Hef\u00b7tet", "sich", "an", "und", "gr\u00fcnt", "und", "bl\u00fcht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PTKVZ", "KON", "VVFIN", "KON", "VVFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Kann es weder Stamm noch Mauer finden,", "tokens": ["Kann", "es", "we\u00b7der", "Stamm", "noch", "Mau\u00b7er", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "KON", "NN", "ADV", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Es mu\u00df verdorren, es mu\u00df verschwinden.", "tokens": ["Es", "mu\u00df", "ver\u00b7dor\u00b7ren", ",", "es", "mu\u00df", "ver\u00b7schwin\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "$,", "PPER", "VMFIN", "VVINF", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.120": {"line.1": {"text": "Zierlich Denken und s\u00fc\u00df Erinnern", "tokens": ["Zier\u00b7lich", "Den\u00b7ken", "und", "s\u00fc\u00df", "E\u00b7rin\u00b7nern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "NN", "KON", "ADJD", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Ist das Leben im tiefsten Innern.", "tokens": ["Ist", "das", "Le\u00b7ben", "im", "tiefs\u00b7ten", "In\u00b7nern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.121": {"line.1": {"text": "Ich tr\u00e4umt und liebte sonnenklar;", "tokens": ["Ich", "tr\u00e4umt", "und", "lieb\u00b7te", "son\u00b7nen\u00b7klar", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df ich lebte, ward ich gewahr.", "tokens": ["Da\u00df", "ich", "leb\u00b7te", ",", "ward", "ich", "ge\u00b7wahr", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$,", "VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.122": {"line.1": {"text": "Wer recht will tun, immer und mit Lust,", "tokens": ["Wer", "recht", "will", "tun", ",", "im\u00b7mer", "und", "mit", "Lust", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VMFIN", "VVINF", "$,", "ADV", "KON", "APPR", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der hege wahre Lieb in Sinn und Brust.", "tokens": ["Der", "he\u00b7ge", "wah\u00b7re", "Lieb", "in", "Sinn", "und", "Brust", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.123": {"line.1": {"text": "\u00bbwann magst du dich am liebsten b\u00fccken?\u00ab", "tokens": ["\u00bb", "wann", "magst", "du", "dich", "am", "liebs\u00b7ten", "b\u00fc\u00b7cken", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VMFIN", "PPER", "PRF", "PTKA", "ADJD", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dem Liebchen Fr\u00fchlingsblume zu pfl\u00fccken.", "tokens": ["Dem", "Lieb\u00b7chen", "Fr\u00fch\u00b7lings\u00b7blu\u00b7me", "zu", "pfl\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.124": {"line.1": {"text": "Doch das ist gar kein gro\u00df Verdienst,", "tokens": ["Doch", "das", "ist", "gar", "kein", "gro\u00df", "Ver\u00b7dienst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ADV", "PIAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Liebe bleibt der h\u00f6chste Gewinst.", "tokens": ["Denn", "Lie\u00b7be", "bleibt", "der", "h\u00f6chs\u00b7te", "Ge\u00b7winst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.125": {"line.1": {"text": "Die Zeit, sie m\u00e4ht so Rosen als Dornen,", "tokens": ["Die", "Zeit", ",", "sie", "m\u00e4ht", "so", "Ro\u00b7sen", "als", "Dor\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VVFIN", "ADV", "NN", "KOUS", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Aber das treibt immer wieder von vornen.", "tokens": ["A\u00b7ber", "das", "treibt", "im\u00b7mer", "wie\u00b7der", "von", "vor\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "ADV", "ADV", "APPR", "VVINF", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}}, "stanza.126": {"line.1": {"text": "Genie\u00dfe, was der Schmerz dir hinterlie\u00df!", "tokens": ["Ge\u00b7nie\u00b7\u00dfe", ",", "was", "der", "Schmerz", "dir", "hin\u00b7ter\u00b7lie\u00df", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "ART", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ist Not vor\u00fcber, sind die N\u00f6te s\u00fc\u00df.", "tokens": ["Ist", "Not", "vor\u00b7\u00fc\u00b7ber", ",", "sind", "die", "N\u00f6\u00b7te", "s\u00fc\u00df", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "PTKVZ", "$,", "VAFIN", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.127": {"line.1": {"text": "Gl\u00fcckselig ist, wer Liebe rein genie\u00dft,", "tokens": ["Gl\u00fcck\u00b7se\u00b7lig", "ist", ",", "wer", "Lie\u00b7be", "rein", "ge\u00b7nie\u00dft", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "$,", "PWS", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Weil doch zuletzt das Grab so Lieb als Ha\u00df verschlie\u00dft.", "tokens": ["Weil", "doch", "zu\u00b7letzt", "das", "Grab", "so", "Lieb", "als", "Ha\u00df", "ver\u00b7schlie\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "ART", "NN", "ADV", "NN", "KOUS", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.128": {"line.1": {"text": "Viele Lieb hab ich erlebet,", "tokens": ["Vie\u00b7le", "Lieb", "hab", "ich", "er\u00b7le\u00b7bet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wenn ich liebelos gestrebet;", "tokens": ["Wenn", "ich", "lie\u00b7be\u00b7los", "ge\u00b7stre\u00b7bet", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Und Verdrie\u00dfliches erworben,", "tokens": ["Und", "Ver\u00b7drie\u00df\u00b7li\u00b7ches", "er\u00b7wor\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenn ich fast f\u00fcr Lieb gestorben.", "tokens": ["Wenn", "ich", "fast", "f\u00fcr", "Lieb", "ge\u00b7stor\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "So du es zusammengezogen,", "tokens": ["So", "du", "es", "zu\u00b7sam\u00b7men\u00b7ge\u00b7zo\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "PPER", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "Bleibet Saldo dir gewogen.", "tokens": ["Blei\u00b7bet", "Sal\u00b7do", "dir", "ge\u00b7wo\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.129": {"line.1": {"text": "Tut dir jemand was zulieb,", "tokens": ["Tut", "dir", "je\u00b7mand", "was", "zu\u00b7lieb", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PIS", "PWS", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Nur geschwinde, gib nur, gib.", "tokens": ["Nur", "ge\u00b7schwin\u00b7de", ",", "gib", "nur", ",", "gib", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "ADJA", "$,", "VVIMP", "ADV", "$,", "VVIMP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenige getrost erwarten", "tokens": ["We\u00b7ni\u00b7ge", "ge\u00b7trost", "er\u00b7war\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "NN", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Dankesblume aus stillem Garten.", "tokens": ["Dan\u00b7kes\u00b7blu\u00b7me", "aus", "stil\u00b7lem", "Gar\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.130": {"line.1": {"text": "Doppelt gibt, wer gleich gibt,", "tokens": ["Dop\u00b7pelt", "gibt", ",", "wer", "gleich", "gibt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "PWS", "ADV", "VVFIN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Hundertfach, der gleich gibt,", "tokens": ["Hun\u00b7dert\u00b7fach", ",", "der", "gleich", "gibt", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "ADV", "VVFIN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Was man w\u00fcnscht und liebt.", "tokens": ["Was", "man", "w\u00fcnscht", "und", "liebt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "KON", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.131": {"line.1": {"text": "\u00bbwarum zauderst du so mit deinen Schritten?\u00ab", "tokens": ["\u00bb", "wa\u00b7rum", "zau\u00b7derst", "du", "so", "mit", "dei\u00b7nen", "Schrit\u00b7ten", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "$.", "$("], "meter": "---+-+-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Nur ungern mag ich ruhn,", "tokens": ["Nur", "un\u00b7gern", "mag", "ich", "ruhn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Will ich aber was Gutes tun,", "tokens": ["Will", "ich", "a\u00b7ber", "was", "Gu\u00b7tes", "tun", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "PWS", "NN", "VVINF", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Mu\u00df ich erst um Erlaubnis bitten.", "tokens": ["Mu\u00df", "ich", "erst", "um", "Er\u00b7laub\u00b7nis", "bit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.132": {"line.1": {"text": "Was willst du lange vigilieren,", "tokens": ["Was", "willst", "du", "lan\u00b7ge", "vi\u00b7gi\u00b7lie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dich mit der Welt herumvexieren?", "tokens": ["Dich", "mit", "der", "Welt", "her\u00b7um\u00b7ve\u00b7xie\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nur Heiterkeit und grader Sinn", "tokens": ["Nur", "Hei\u00b7ter\u00b7keit", "und", "gra\u00b7der", "Sinn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verschafft dir endlichen Gewinn.", "tokens": ["Ver\u00b7schafft", "dir", "end\u00b7li\u00b7chen", "Ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.133": {"line.1": {"text": "Wem wohl das Gl\u00fcck die sch\u00f6nste Palme beut?", "tokens": ["Wem", "wohl", "das", "Gl\u00fcck", "die", "sch\u00f6ns\u00b7te", "Pal\u00b7me", "beut", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wer freudig tut, sich des Getanen freut.", "tokens": ["Wer", "freu\u00b7dig", "tut", ",", "sich", "des", "Ge\u00b7ta\u00b7nen", "freut", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VVFIN", "$,", "PRF", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.134": {"line.1": {"text": "Gleich ist alles vers\u00f6hnt,", "tokens": ["Gleich", "ist", "al\u00b7les", "ver\u00b7s\u00f6hnt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "VVPP", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Wer redlich ficht, wird gekr\u00f6nt.", "tokens": ["Wer", "red\u00b7lich", "ficht", ",", "wird", "ge\u00b7kr\u00f6nt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "ADJD", "PTKVZ", "$,", "VAFIN", "VVPP", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.135": {"line.1": {"text": "Du wirkest nicht, alles bleibt so stumpf.", "tokens": ["Du", "wir\u00b7kest", "nicht", ",", "al\u00b7les", "bleibt", "so", "stumpf", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "$,", "PIS", "VVFIN", "ADV", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Sei guter Dinge!", "tokens": ["Sei", "gu\u00b7ter", "Din\u00b7ge", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Der Stein im Sumpf", "tokens": ["Der", "Stein", "im", "Sumpf"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Macht keine Ringe.", "tokens": ["Macht", "kei\u00b7ne", "Rin\u00b7ge", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PIAT", "NN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.136": {"line.1": {"text": "In des Weinstocks herrliche Gaben", "tokens": ["In", "des", "Wein\u00b7stocks", "herr\u00b7li\u00b7che", "Ga\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADJA", "NN"], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Gie\u00dft ihr mir schlechtes Gew\u00e4sser!", "tokens": ["Gie\u00dft", "ihr", "mir", "schlech\u00b7tes", "Ge\u00b7w\u00e4s\u00b7ser", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "ADJA", "NN", "$."], "meter": "++-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Ich soll immer unrecht haben", "tokens": ["Ich", "soll", "im\u00b7mer", "un\u00b7recht", "ha\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "NN", "VAFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und wei\u00df es besser.", "tokens": ["Und", "wei\u00df", "es", "bes\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.137": {"line.1": {"text": "Was ich mir gefallen lasse?", "tokens": ["Was", "ich", "mir", "ge\u00b7fal\u00b7len", "las\u00b7se", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PPER", "VVPP", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Zuschlagen mu\u00df die Masse,", "tokens": ["Zu\u00b7schla\u00b7gen", "mu\u00df", "die", "Mas\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Dann ist sie respektabel,", "tokens": ["Dann", "ist", "sie", "res\u00b7pek\u00b7ta\u00b7bel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Urteilen gelingt ihr miserabel.", "tokens": ["Ur\u00b7tei\u00b7len", "ge\u00b7lingt", "ihr", "mi\u00b7se\u00b7ra\u00b7bel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.138": {"line.1": {"text": "Es ist sehr schwer oft zu ergr\u00fcnden,", "tokens": ["Es", "ist", "sehr", "schwer", "oft", "zu", "er\u00b7gr\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Warum wir das angefangen;", "tokens": ["Wa\u00b7rum", "wir", "das", "an\u00b7ge\u00b7fan\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PDS", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wir m\u00fcssen oft Belohnung finden,", "tokens": ["Wir", "m\u00fcs\u00b7sen", "oft", "Be\u00b7loh\u00b7nung", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df es uns schlecht ergangen.", "tokens": ["Da\u00df", "es", "uns", "schlecht", "er\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.139": {"line.1": {"text": "Seh ich an andern gro\u00dfe Eigenschaften", "tokens": ["Seh", "ich", "an", "an\u00b7dern", "gro\u00b7\u00dfe", "Ei\u00b7gen\u00b7schaf\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und wollen die an mir auch haften,", "tokens": ["Und", "wol\u00b7len", "die", "an", "mir", "auch", "haf\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ART", "APPR", "PPER", "ADV", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "So werd ich sie in Liebe pflegen;", "tokens": ["So", "werd", "ich", "sie", "in", "Lie\u00b7be", "pfle\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Geht's nicht, so tu ich was anders dagegen.", "tokens": ["Geht's", "nicht", ",", "so", "tu", "ich", "was", "an\u00b7ders", "da\u00b7ge\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "$,", "ADV", "VVFIN", "PPER", "PIS", "ADV", "PAV", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}}, "stanza.140": {"line.1": {"text": "Ich, Egoist! \u2013 Wenn ich's nicht besser w\u00fc\u00dfte!", "tokens": ["Ich", ",", "E\u00b7goist", "!", "\u2013", "Wenn", "ich's", "nicht", "bes\u00b7ser", "w\u00fc\u00df\u00b7te", "!"], "token_info": ["word", "punct", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "NN", "$.", "$(", "KOUS", "PIS", "PTKNEG", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Der Neid, das ist der Egoiste;", "tokens": ["Der", "Neid", ",", "das", "ist", "der", "E\u00b7gois\u00b7te", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PDS", "VAFIN", "ART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und was ich auch f\u00fcr Wege geloffen,", "tokens": ["Und", "was", "ich", "auch", "f\u00fcr", "We\u00b7ge", "ge\u00b7lof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Auf 'm Neidpfad habt ihr mich nie betroffen.", "tokens": ["Auf", "'m", "Neid\u00b7pfad", "habt", "ihr", "mich", "nie", "be\u00b7trof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "PPER", "PRF", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.141": {"line.1": {"text": "Nicht \u00fcber Zeit- noch Landgenossen", "tokens": ["Nicht", "\u00fc\u00b7ber", "Zeit", "noch", "Land\u00b7ge\u00b7nos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PTKNEG", "APPR", "TRUNC", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00dft du dich beklagen;", "tokens": ["Mu\u00dft", "du", "dich", "be\u00b7kla\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PRF", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Nachbarn werden ganz andere Possen,", "tokens": ["Nach\u00b7barn", "wer\u00b7den", "ganz", "an\u00b7de\u00b7re", "Pos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "ADJA", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Und auch K\u00fcnftige, \u00fcber dich sagen.", "tokens": ["Und", "auch", "K\u00fcnf\u00b7ti\u00b7ge", ",", "\u00fc\u00b7ber", "dich", "sa\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "NN", "$,", "APPR", "PPER", "VVINF", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}}, "stanza.142": {"line.1": {"text": "Im Vaterlande", "tokens": ["Im", "Va\u00b7ter\u00b7lan\u00b7de"], "token_info": ["word", "word"], "pos": ["APPRART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Schreibe, was dir gef\u00e4llt:", "tokens": ["Schrei\u00b7be", ",", "was", "dir", "ge\u00b7f\u00e4llt", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PWS", "PPER", "VVPP", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "Da sind Liebesbande,", "tokens": ["Da", "sind", "Lie\u00b7bes\u00b7ban\u00b7de", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Da ist deine Welt.", "tokens": ["Da", "ist", "dei\u00b7ne", "Welt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "$."], "meter": "--+-+", "measure": "anapaest.init"}}, "stanza.143": {"line.1": {"text": "Drau\u00dfen zu wenig oder zu viel,", "tokens": ["Drau\u00b7\u00dfen", "zu", "we\u00b7nig", "o\u00b7der", "zu", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKA", "PIS", "KON", "APPR", "PIS", "$,"], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Zu Hause nur ist Ma\u00df und Ziel.", "tokens": ["Zu", "Hau\u00b7se", "nur", "ist", "Ma\u00df", "und", "Ziel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "VAFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.144": {"line.1": {"text": "Warum werden die Dichter beneidet?", "tokens": ["Wa\u00b7rum", "wer\u00b7den", "die", "Dich\u00b7ter", "be\u00b7nei\u00b7det", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Weil Unart sie zuweilen kleidet,", "tokens": ["Weil", "Un\u00b7art", "sie", "zu\u00b7wei\u00b7len", "klei\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und in der Welt ist's gro\u00dfe Pein,", "tokens": ["Und", "in", "der", "Welt", "ist's", "gro\u00b7\u00dfe", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df wir nicht d\u00fcrfen unartig sein.", "tokens": ["Da\u00df", "wir", "nicht", "d\u00fcr\u00b7fen", "un\u00b7ar\u00b7tig", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VMFIN", "ADJD", "VAINF", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.145": {"line.1": {"text": "So kommt denn auch das Dichtergenie", "tokens": ["So", "kommt", "denn", "auch", "das", "Dich\u00b7ter\u00b7ge\u00b7nie"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ART", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Durch die Welt und wei\u00df nicht wie.", "tokens": ["Durch", "die", "Welt", "und", "wei\u00df", "nicht", "wie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "VVFIN", "PTKNEG", "PWAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Guten Vorteil bringt ein heitrer Sinn;", "tokens": ["Gu\u00b7ten", "Vor\u00b7teil", "bringt", "ein", "hei\u00b7trer", "Sinn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Andern zerst\u00f6rt Verlust den Gewinn.", "tokens": ["An\u00b7dern", "zer\u00b7st\u00f6rt", "Ver\u00b7lust", "den", "Ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "VVFIN", "NN", "ART", "NN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}}, "stanza.146": {"line.1": {"text": "\u00bbimmer denk ich: mein Wunsch ist erreicht,", "tokens": ["\u00bb", "im\u00b7mer", "denk", "ich", ":", "mein", "Wunsch", "ist", "er\u00b7reicht", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "$.", "PPOSAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Und gleich geht's wieder anders her!\u00ab", "tokens": ["Und", "gleich", "geht's", "wie\u00b7der", "an\u00b7ders", "her", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADV", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zerst\u00fcckle das Leben, du machst dir's leicht;", "tokens": ["Zer\u00b7st\u00fcck\u00b7le", "das", "Le\u00b7ben", ",", "du", "machst", "dir's", "leicht", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "PPER", "VVFIN", "PIS", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Vereinige es, und du machst dir's schwer.", "tokens": ["Ver\u00b7ei\u00b7ni\u00b7ge", "es", ",", "und", "du", "machst", "dir's", "schwer", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KON", "PPER", "VVFIN", "PIS", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.147": {"line.1": {"text": "\u00bbbist du denn nicht auch zugrunde gerichtet?", "tokens": ["\u00bb", "bist", "du", "denn", "nicht", "auch", "zu\u00b7grun\u00b7de", "ge\u00b7rich\u00b7tet", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "PPER", "ADV", "PTKNEG", "ADV", "ADJA", "VVPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Von deinen Hoffnungen trifft nichts ein!\u00ab", "tokens": ["Von", "dei\u00b7nen", "Hoff\u00b7nun\u00b7gen", "trifft", "nichts", "ein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PIS", "PTKVZ", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die Hoffnung ist's, die sinnet und dichtet,", "tokens": ["Die", "Hoff\u00b7nung", "ist's", ",", "die", "sin\u00b7net", "und", "dich\u00b7tet", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$,", "PRELS", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und da kann ich noch immer lustig sein.", "tokens": ["Und", "da", "kann", "ich", "noch", "im\u00b7mer", "lus\u00b7tig", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PPER", "ADV", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.148": {"line.1": {"text": "Nicht alles ist an eins gebunden,", "tokens": ["Nicht", "al\u00b7les", "ist", "an", "eins", "ge\u00b7bun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIS", "VAFIN", "APPR", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Seid nur nicht mit euch selbst im Streit!", "tokens": ["Seid", "nur", "nicht", "mit", "euch", "selbst", "im", "Streit", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADV", "PTKNEG", "APPR", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.149": {"line.1": {"text": "Mit Liebe endigt man, was man erfunden;", "tokens": ["Mit", "Lie\u00b7be", "en\u00b7digt", "man", ",", "was", "man", "er\u00b7fun\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PIS", "$,", "PRELS", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Was man gelernt, mit Sicherheit.", "tokens": ["Was", "man", "ge\u00b7lernt", ",", "mit", "Si\u00b7cher\u00b7heit", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVPP", "$,", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.150": {"line.1": {"text": "Wer uns am strengsten kritisiert?", "tokens": ["Wer", "uns", "am", "strengs\u00b7ten", "kri\u00b7ti\u00b7siert", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPRART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Dilettant, der sich resigniert.", "tokens": ["Ein", "Di\u00b7let\u00b7tant", ",", "der", "sich", "re\u00b7sig\u00b7niert", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PRF", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.151": {"line.1": {"text": "Durch Vern\u00fcnfteln wird Poesie vertrieben,", "tokens": ["Durch", "Ver\u00b7n\u00fcnf\u00b7teln", "wird", "Poe\u00b7sie", "ver\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "NN", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Aber sie mag das Vern\u00fcnftige lieben.", "tokens": ["A\u00b7ber", "sie", "mag", "das", "Ver\u00b7n\u00fcnf\u00b7ti\u00b7ge", "lie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}}, "stanza.152": {"line.1": {"text": "\u00bbwo ist der Lehrer, dem man glaubt?\u00ab", "tokens": ["\u00bb", "wo", "ist", "der", "Leh\u00b7rer", ",", "dem", "man", "glaubt", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VAFIN", "ART", "NN", "$,", "PRELS", "PIS", "VVFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Tu, was dir dein kleines Gem\u00fct erlaubt.", "tokens": ["Tu", ",", "was", "dir", "dein", "klei\u00b7nes", "Ge\u00b7m\u00fct", "er\u00b7laubt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PWS", "PPER", "PPOSAT", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.153": {"line.1": {"text": "Glaubst dich zu kennen, wirst Gott nicht erkennen,", "tokens": ["Glaubst", "dich", "zu", "ken\u00b7nen", ",", "wirst", "Gott", "nicht", "er\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKZU", "VVINF", "$,", "VAFIN", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Auch wohl das Schlechte g\u00f6ttlich nennen.", "tokens": ["Auch", "wohl", "das", "Schlech\u00b7te", "g\u00f6tt\u00b7lich", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.154": {"line.1": {"text": "Wer Gott ahnet, ist hochzuhalten,", "tokens": ["Wer", "Gott", "ah\u00b7net", ",", "ist", "hoch\u00b7zu\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$,", "VAFIN", "ADJA", "$,"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Denn er wird nie im Schlechten walten.", "tokens": ["Denn", "er", "wird", "nie", "im", "Schlech\u00b7ten", "wal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.155": {"line.1": {"text": "Macht's einander nur nicht sauer,", "tokens": ["Macht's", "ein\u00b7an\u00b7der", "nur", "nicht", "sau\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Hier sind wir gleich, Baron und Bauer.", "tokens": ["Hier", "sind", "wir", "gleich", ",", "Ba\u00b7ron", "und", "Bau\u00b7er", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.156": {"line.1": {"text": "Warum uns Gott so wohl gef\u00e4llt?", "tokens": ["Wa\u00b7rum", "uns", "Gott", "so", "wohl", "ge\u00b7f\u00e4llt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "NN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil er sich uns nie in den Weg stellt.", "tokens": ["Weil", "er", "sich", "uns", "nie", "in", "den", "Weg", "stellt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "PPER", "ADV", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.157": {"line.1": {"text": "Wie wollten die Fischer sich n\u00e4hren und retten,", "tokens": ["Wie", "woll\u00b7ten", "die", "Fi\u00b7scher", "sich", "n\u00e4h\u00b7ren", "und", "ret\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "ART", "NN", "PRF", "VVINF", "KON", "VVINF", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Wenn die Fr\u00f6sche s\u00e4mtlich Z\u00e4hne h\u00e4tten?", "tokens": ["Wenn", "die", "Fr\u00f6\u00b7sche", "s\u00e4mt\u00b7lich", "Z\u00e4h\u00b7ne", "h\u00e4t\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADJD", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.158": {"line.1": {"text": "Wie Kirschen und Beeren behagen,", "tokens": ["Wie", "Kir\u00b7schen", "und", "Bee\u00b7ren", "be\u00b7ha\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Mu\u00dft du Kinder und Sperlinge fragen.", "tokens": ["Mu\u00dft", "du", "Kin\u00b7der", "und", "Sper\u00b7lin\u00b7ge", "fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "NN", "KON", "NN", "VVINF", "$."], "meter": "+-+-+++-+-", "measure": "unknown.measure.hexa"}}, "stanza.159": {"line.1": {"text": "\u00bbwarum hat dich das sch\u00f6ne Kind verlassen?\u00ab", "tokens": ["\u00bb", "wa\u00b7rum", "hat", "dich", "das", "sch\u00f6\u00b7ne", "Kind", "ver\u00b7las\u00b7sen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VAFIN", "PPER", "ART", "ADJA", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich kann sie darum doch nicht hassen:", "tokens": ["Ich", "kann", "sie", "da\u00b7rum", "doch", "nicht", "has\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PAV", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sie schien zu f\u00fcrchten und zu f\u00fchlen,", "tokens": ["Sie", "schien", "zu", "f\u00fcrch\u00b7ten", "und", "zu", "f\u00fch\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich werde das Pr\u00e4venire spielen.", "tokens": ["Ich", "wer\u00b7de", "das", "Pr\u00e4\u00b7ve\u00b7ni\u00b7re", "spie\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVINF", "$."], "meter": "-+---+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.160": {"line.1": {"text": "Glaube mir gar und ganz,", "tokens": ["Glau\u00b7be", "mir", "gar", "und", "ganz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "KON", "ADV", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.2": {"text": "M\u00e4dchen, la\u00df deine Bein' in Ruh,", "tokens": ["M\u00e4d\u00b7chen", ",", "la\u00df", "dei\u00b7ne", "Bein'", "in", "Ruh", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVIMP", "PPOSAT", "NN", "APPR", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Es geh\u00f6rt mehr zum Tanz", "tokens": ["Es", "ge\u00b7h\u00f6rt", "mehr", "zum", "Tanz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Als rote Schuh'.", "tokens": ["Als", "ro\u00b7te", "Schuh'", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.161": {"line.1": {"text": "Was ich nicht wei\u00df,", "tokens": ["Was", "ich", "nicht", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Macht mich nicht hei\u00df.", "tokens": ["Macht", "mich", "nicht", "hei\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "ADJD", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Und was ich wei\u00df,", "tokens": ["Und", "was", "ich", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Machte mich hei\u00df,", "tokens": ["Mach\u00b7te", "mich", "hei\u00df", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "Wenn ich nicht w\u00fc\u00dfte'", "tokens": ["Wenn", "ich", "nicht", "w\u00fc\u00df\u00b7te'"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PTKNEG", "VVFIN"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.6": {"text": "Wie's werden m\u00fc\u00dfte.", "tokens": ["Wie's", "wer\u00b7den", "m\u00fc\u00df\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VAINF", "VMFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.162": {"line.1": {"text": "Oft, wenn dir jeder Trost entflieht,", "tokens": ["Oft", ",", "wenn", "dir", "je\u00b7der", "Trost", "ent\u00b7flieht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PPER", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00dft du im stillen dich bequemen.", "tokens": ["Mu\u00dft", "du", "im", "stil\u00b7len", "dich", "be\u00b7que\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPRART", "VVFIN", "PPER", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nur dann, wenn dir Gewalt geschieht,", "tokens": ["Nur", "dann", ",", "wenn", "dir", "Ge\u00b7walt", "ge\u00b7schieht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "PPER", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird die Menge an dir Anteil nehmen;", "tokens": ["Wird", "die", "Men\u00b7ge", "an", "dir", "An\u00b7teil", "neh\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPR", "PPER", "NN", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.5": {"text": "Ums Unrecht, das dir widerf\u00e4hrt,", "tokens": ["Ums", "Un\u00b7recht", ",", "das", "dir", "wi\u00b7der\u00b7f\u00e4hrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kein Mensch den Blick zur Seite kehrt.", "tokens": ["Kein", "Mensch", "den", "Blick", "zur", "Sei\u00b7te", "kehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.163": {"line.1": {"text": "Was \u00e4rgerst du dich \u00fcber f\u00e4lschlich Erhobne!", "tokens": ["Was", "\u00e4r\u00b7gerst", "du", "dich", "\u00fc\u00b7ber", "f\u00e4lschlich", "Er\u00b7hob\u00b7ne", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PRF", "APPR", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wo g\u00e4b es denn nicht Eingeschobne?", "tokens": ["Wo", "g\u00e4b", "es", "denn", "nicht", "Ein\u00b7ge\u00b7schob\u00b7ne", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "PTKNEG", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.164": {"line.1": {"text": "Worauf alles ankommt? Das ist sehr simpel!", "tokens": ["Wo\u00b7rauf", "al\u00b7les", "an\u00b7kommt", "?", "Das", "ist", "sehr", "sim\u00b7pel", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "VVFIN", "$.", "PDS", "VAFIN", "ADV", "ADJD", "$."], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Vater, verf\u00fcge, eh's dein Gesind sp\u00fcrt!", "tokens": ["Va\u00b7ter", ",", "ver\u00b7f\u00fc\u00b7ge", ",", "eh's", "dein", "Ge\u00b7sind", "sp\u00fcrt", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "$,", "KOUS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.3": {"text": "Dahin oder dorthin flattert ein Wimpel,", "tokens": ["Da\u00b7hin", "o\u00b7der", "dor\u00b7thin", "flat\u00b7tert", "ein", "Wim\u00b7pel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "KON", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Steuermann wei\u00df, wohin euch der Wind f\u00fchrt.", "tokens": ["Steu\u00b7er\u00b7mann", "wei\u00df", ",", "wo\u00b7hin", "euch", "der", "Wind", "f\u00fchrt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "PWAV", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "+--+----+-", "measure": "dactylic.di.plus"}}, "stanza.165": {"line.1": {"text": "Eigenheiten, die werden schon haften;", "tokens": ["Ei\u00b7gen\u00b7hei\u00b7ten", ",", "die", "wer\u00b7den", "schon", "haf\u00b7ten", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "VAFIN", "ADV", "VVINF", "$."], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Kultiviere deine Eigenschaften.", "tokens": ["Kul\u00b7ti\u00b7vie\u00b7re", "dei\u00b7ne", "Ei\u00b7gen\u00b7schaf\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.166": {"line.1": {"text": "Viel Gewohnheiten darfst du haben,", "tokens": ["Viel", "Ge\u00b7wohn\u00b7hei\u00b7ten", "darfst", "du", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Aber keine Gewohnheit!", "tokens": ["A\u00b7ber", "kei\u00b7ne", "Ge\u00b7wohn\u00b7heit", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Dies Wort unter des Dichters Gaben", "tokens": ["Dies", "Wort", "un\u00b7ter", "des", "Dich\u00b7ters", "Ga\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "NN", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Halte nicht f\u00fcr Torheit.", "tokens": ["Hal\u00b7te", "nicht", "f\u00fcr", "Tor\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKNEG", "APPR", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.167": {"line.1": {"text": "Das Rechte, das ich viel getan,", "tokens": ["Das", "Rech\u00b7te", ",", "das", "ich", "viel", "ge\u00b7tan", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das ficht mich nun nicht weiter an,", "tokens": ["Das", "ficht", "mich", "nun", "nicht", "wei\u00b7ter", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PTKNEG", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Aber das Falsche, das mir entschl\u00fcpft,", "tokens": ["A\u00b7ber", "das", "Fal\u00b7sche", ",", "das", "mir", "ent\u00b7schl\u00fcpft", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Wie ein Gespenst mir vor Augen h\u00fcpft.", "tokens": ["Wie", "ein", "Ge\u00b7spenst", "mir", "vor", "Au\u00b7gen", "h\u00fcpft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PPER", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.168": {"line.1": {"text": "Gebt mir zu tun,", "tokens": ["Gebt", "mir", "zu", "tun", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Das sind reiche Gaben!", "tokens": ["Das", "sind", "rei\u00b7che", "Ga\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Das Herz kann nicht ruhn,", "tokens": ["Das", "Herz", "kann", "nicht", "ruhn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Will zu schaffen haben.", "tokens": ["Will", "zu", "schaf\u00b7fen", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PTKZU", "VVINF", "VAFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.169": {"line.1": {"text": "Ihrer viele wissen viel,", "tokens": ["Ih\u00b7rer", "vie\u00b7le", "wis\u00b7sen", "viel", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "PIS", "VVFIN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Von der Weisheit sind sie weit entfernt.", "tokens": ["Von", "der", "Weis\u00b7heit", "sind", "sie", "weit", "ent\u00b7fernt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Andre Leute sind euch ein Spiel;", "tokens": ["And\u00b7re", "Leu\u00b7te", "sind", "euch", "ein", "Spiel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Sich selbst hat niemand ausgelernt.", "tokens": ["Sich", "selbst", "hat", "nie\u00b7mand", "aus\u00b7ge\u00b7lernt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.170": {"line.1": {"text": "\u00bbman hat ein Schimpflied auf dich gemacht;", "tokens": ["\u00bb", "man", "hat", "ein", "Schimpf\u00b7lied", "auf", "dich", "ge\u00b7macht", ";"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "VAFIN", "ART", "NN", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Es hat's ein b\u00f6ser Feind erdacht.\u00ab", "tokens": ["Es", "hat's", "ein", "b\u00f6\u00b7ser", "Feind", "er\u00b7dacht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.171": {"line.1": {"text": "La\u00df sie's nur immer singen,", "tokens": ["La\u00df", "sie's", "nur", "im\u00b7mer", "sin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Denn es wird bald verklingen.", "tokens": ["Denn", "es", "wird", "bald", "ver\u00b7klin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADV", "VVINF", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.172": {"line.1": {"text": "Dauert nicht so lang in den Landen", "tokens": ["Dau\u00b7ert", "nicht", "so", "lang", "in", "den", "Lan\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "PTKNEG", "ADV", "ADJD", "APPR", "ART", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Als das: Christ ist erstanden.", "tokens": ["Als", "das", ":", "Christ", "ist", "er\u00b7stan\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "$.", "NN", "VAFIN", "VVPP", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}}, "stanza.173": {"line.1": {"text": "Das dauert schon achtzehnhundert Jahr'", "tokens": ["Das", "dau\u00b7ert", "schon", "acht\u00b7zehn\u00b7hun\u00b7dert", "Jahr'"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "CARD", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und ein paar dr\u00fcber, das ist wohl wahr!", "tokens": ["Und", "ein", "paar", "dr\u00fc\u00b7ber", ",", "das", "ist", "wohl", "wahr", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PIAT", "PAV", "$,", "PDS", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.174": {"line.1": {"text": "Wer ist denn der souver\u00e4ne Mann?", "tokens": ["Wer", "ist", "denn", "der", "sou\u00b7ve\u00b7r\u00e4\u00b7ne", "Mann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das ist bald gesagt:", "tokens": ["Das", "ist", "bald", "ge\u00b7sagt", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "VVPP", "$."], "meter": "----+", "measure": "unknown.measure.single"}, "line.3": {"text": "Der, den man nicht hindern kann,", "tokens": ["Der", ",", "den", "man", "nicht", "hin\u00b7dern", "kann", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PRELS", "PIS", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ob er nach Gutem oder B\u00f6sem jagt.", "tokens": ["Ob", "er", "nach", "Gu\u00b7tem", "o\u00b7der", "B\u00f6\u00b7sem", "jagt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "KON", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.175": {"line.1": {"text": "Entzwei' und gebiete! T\u00fcchtig Wort;", "tokens": ["Ent\u00b7zwei'", "und", "ge\u00b7bie\u00b7te", "!", "T\u00fcch\u00b7tig", "Wort", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "KON", "VVFIN", "$.", "ADJD", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Verein' und leite! Be\u00dfrer Hort.", "tokens": ["Ver\u00b7ein'", "und", "lei\u00b7te", "!", "Be\u00df\u00b7rer", "Hort", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVIMP", "KON", "VVFIN", "$.", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.176": {"line.1": {"text": "Magst du einmal mich hintergehen,", "tokens": ["Magst", "du", "ein\u00b7mal", "mich", "hin\u00b7ter\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Merk ich's, so la\u00df ich's wohl geschehen;", "tokens": ["Merk", "ich's", ",", "so", "la\u00df", "ich's", "wohl", "ge\u00b7sche\u00b7hen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PIS", "$,", "ADV", "VVFIN", "PIS", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Gestehst du mir's aber ins Gesicht,", "tokens": ["Ge\u00b7stehst", "du", "mir's", "a\u00b7ber", "ins", "Ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NE", "ADV", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "In meinem Leben verzeih ich's nicht.", "tokens": ["In", "mei\u00b7nem", "Le\u00b7ben", "ver\u00b7zeih", "ich's", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PIS", "PTKNEG", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.177": {"line.1": {"text": "Nicht gr\u00f6\u00dfern Vorteil w\u00fc\u00dft ich zu nennen,", "tokens": ["Nicht", "gr\u00f6\u00b7\u00dfern", "Vor\u00b7teil", "w\u00fc\u00dft", "ich", "zu", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJA", "NN", "VVFIN", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Als des Feindes Verdienst erkennen.", "tokens": ["Als", "des", "Fein\u00b7des", "Ver\u00b7dienst", "er\u00b7ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "NN", "VVINF", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.178": {"line.1": {"text": "\u00bbhat man das Gute dir erwidert?\u00ab", "tokens": ["\u00bb", "hat", "man", "das", "Gu\u00b7te", "dir", "er\u00b7wi\u00b7dert", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VAFIN", "PIS", "ART", "NN", "PPER", "VVPP", "$.", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Mein Pfeil flog ab, sehr sch\u00f6n befiedert,", "tokens": ["Mein", "Pfeil", "flog", "ab", ",", "sehr", "sch\u00f6n", "be\u00b7fie\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PTKVZ", "$,", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der ganze Himmel stand ihm offen,", "tokens": ["Der", "gan\u00b7ze", "Him\u00b7mel", "stand", "ihm", "of\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er hat wohl irgendwo getroffen.", "tokens": ["Er", "hat", "wohl", "ir\u00b7gend\u00b7wo", "ge\u00b7trof\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.179": {"line.1": {"text": "\u00bbwas schnitt dein Freund f\u00fcr ein Gesicht?\u00ab", "tokens": ["\u00bb", "was", "schnitt", "dein", "Freund", "f\u00fcr", "ein", "Ge\u00b7sicht", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPOSAT", "NN", "APPR", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Guter Geselle, das versteh ich nicht.", "tokens": ["Gu\u00b7ter", "Ge\u00b7sel\u00b7le", ",", "das", "ver\u00b7steh", "ich", "nicht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "PDS", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.3": {"text": "Ihm ist wohl sein S\u00fc\u00df Gesicht verleidet,", "tokens": ["Ihm", "ist", "wohl", "sein", "S\u00fc\u00df", "Ge\u00b7sicht", "ver\u00b7lei\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PPOSAT", "NN", "NN", "VVPP", "$,"], "meter": "----+-+-+-", "measure": "unknown.measure.tri"}, "line.4": {"text": "Da\u00df er heut saure Gesichter schneidet.", "tokens": ["Da\u00df", "er", "heut", "sau\u00b7re", "Ge\u00b7sich\u00b7ter", "schnei\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.180": {"line.1": {"text": "Ihr sucht die Menschen zu benennen", "tokens": ["Ihr", "sucht", "die", "Men\u00b7schen", "zu", "be\u00b7nen\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und glaubt am Namen sie zu kennen.", "tokens": ["Und", "glaubt", "am", "Na\u00b7men", "sie", "zu", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wer tiefer sieht, gesteht sich frei,", "tokens": ["Wer", "tie\u00b7fer", "sieht", ",", "ge\u00b7steht", "sich", "frei", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VVFIN", "$,", "VVFIN", "PRF", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es ist was Anonymes dabei.", "tokens": ["Es", "ist", "was", "An\u00b7o\u00b7ny\u00b7mes", "da\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "NN", "PAV", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.181": {"line.1": {"text": "\u00bbmancherlei hast du vers\u00e4umet:", "tokens": ["\u00bb", "man\u00b7cher\u00b7lei", "hast", "du", "ver\u00b7s\u00e4u\u00b7met", ":"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "VAFIN", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Statt zu handeln, hast getr\u00e4umet,", "tokens": ["Statt", "zu", "han\u00b7deln", ",", "hast", "ge\u00b7tr\u00e4u\u00b7met", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Statt zu danken, hast geschwiegen,", "tokens": ["Statt", "zu", "dan\u00b7ken", ",", "hast", "ge\u00b7schwie\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Solltest wandern, bliebest liegen.\u00ab", "tokens": ["Soll\u00b7test", "wan\u00b7dern", ",", "blie\u00b7best", "lie\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "$,", "VVFIN", "VVFIN", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.182": {"line.1": {"text": "Nein, ich habe nichts vers\u00e4umet!", "tokens": ["Nein", ",", "ich", "ha\u00b7be", "nichts", "ver\u00b7s\u00e4u\u00b7met", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VAFIN", "PIS", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wi\u00dft ihr denn, was ich getr\u00e4umet?", "tokens": ["Wi\u00dft", "ihr", "denn", ",", "was", "ich", "ge\u00b7tr\u00e4u\u00b7met", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "PWS", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nun will ich zum Danke fliegen,", "tokens": ["Nun", "will", "ich", "zum", "Dan\u00b7ke", "flie\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPRART", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Nur mein B\u00fcndel bleibe liegen.", "tokens": ["Nur", "mein", "B\u00fcn\u00b7del", "blei\u00b7be", "lie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVFIN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.183": {"line.1": {"text": "Heute geh ich. Komm ich wieder,", "tokens": ["Heu\u00b7te", "geh", "ich", ".", "Komm", "ich", "wie\u00b7der", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Singen wir ganz andre Lieder.", "tokens": ["Sin\u00b7gen", "wir", "ganz", "and\u00b7re", "Lie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo so viel sich hoffen l\u00e4\u00dft,", "tokens": ["Wo", "so", "viel", "sich", "hof\u00b7fen", "l\u00e4\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "PRF", "VVINF", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist der Abschied ja ein Fest.", "tokens": ["Ist", "der", "Ab\u00b7schied", "ja", "ein", "Fest", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.184": {"line.1": {"text": "Was soll ich viel lieben, was soll ich viel hassen;", "tokens": ["Was", "soll", "ich", "viel", "lie\u00b7ben", ",", "was", "soll", "ich", "viel", "has\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "ADV", "VVINF", "$,", "PWS", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Man lebt nur vom Lebenlassen.", "tokens": ["Man", "lebt", "nur", "vom", "Le\u00b7ben\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.185": {"line.1": {"text": "Nichts leichter, als dem D\u00fcrftigen schmeicheln;", "tokens": ["Nichts", "leich\u00b7ter", ",", "als", "dem", "D\u00fcrf\u00b7ti\u00b7gen", "schmei\u00b7cheln", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "$,", "KOUS", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wer mag aber ohne Vorteil heucheln.", "tokens": ["Wer", "mag", "a\u00b7ber", "oh\u00b7ne", "Vor\u00b7teil", "heu\u00b7cheln", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.186": {"line.1": {"text": "\u00bbwie konnte der ", "tokens": ["\u00bb", "wie", "konn\u00b7te", "der"], "token_info": ["punct", "word", "word", "word"], "pos": ["$(", "PWAV", "VMFIN", "ART"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Er ist auf Fingerchen gegangen.", "tokens": ["Er", "ist", "auf", "Fin\u00b7ge\u00b7rchen", "ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.187": {"line.1": {"text": "Sprichwort bezeichnet Nationen;", "tokens": ["Sprich\u00b7wort", "be\u00b7zeich\u00b7net", "Na\u00b7ti\u00b7o\u00b7nen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mu\u00dft aber erst unter ihnen wohnen.", "tokens": ["Mu\u00dft", "a\u00b7ber", "erst", "un\u00b7ter", "ih\u00b7nen", "woh\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.188": {"line.1": {"text": "Erkenne dich! \u2013 Was soll das hei\u00dfen?", "tokens": ["Er\u00b7ken\u00b7ne", "dich", "!", "\u2013", "Was", "soll", "das", "hei\u00b7\u00dfen", "?"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$.", "$(", "PWS", "VMFIN", "PDS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es hei\u00dft: Sei nur! und sei auch nicht!", "tokens": ["Es", "hei\u00dft", ":", "Sei", "nur", "!", "und", "sei", "auch", "nicht", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "VAFIN", "ADV", "$.", "KON", "VAFIN", "ADV", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es ist eben ein Spruch der lieben Weisen,", "tokens": ["Es", "ist", "e\u00b7ben", "ein", "Spruch", "der", "lie\u00b7ben", "Wei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "ART", "ADJA", "NN", "$,"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Der sich in der K\u00fcrze widerspricht.", "tokens": ["Der", "sich", "in", "der", "K\u00fcr\u00b7ze", "wi\u00b7der\u00b7spricht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.189": {"line.1": {"text": "Erkenne dich! \u2013 Was hab ich da f\u00fcr Lohn?", "tokens": ["Er\u00b7ken\u00b7ne", "dich", "!", "\u2013", "Was", "hab", "ich", "da", "f\u00fcr", "Lohn", "?"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$.", "$(", "PWS", "VAFIN", "PPER", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Erkenn ich mich, so mu\u00df ich gleich davon.", "tokens": ["Er\u00b7kenn", "ich", "mich", ",", "so", "mu\u00df", "ich", "gleich", "da\u00b7von", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "$,", "ADV", "VMFIN", "PPER", "ADV", "PAV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.190": {"line.1": {"text": "Als wenn ich auf den Maskenball k\u00e4me", "tokens": ["Als", "wenn", "ich", "auf", "den", "Mas\u00b7ken\u00b7ball", "k\u00e4\u00b7me"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "KOUS", "PPER", "APPR", "ART", "NN", "VVFIN"], "meter": "-+-+-+-++-", "measure": "unknown.measure.penta"}, "line.2": {"text": "Und gleich die Larve vom Angesicht n\u00e4hme.", "tokens": ["Und", "gleich", "die", "Lar\u00b7ve", "vom", "An\u00b7ge\u00b7sicht", "n\u00e4h\u00b7me", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+--", "measure": "iambic.tetra.relaxed"}}, "stanza.191": {"line.1": {"text": "Andre zu kennen, das mu\u00dft du probieren,", "tokens": ["And\u00b7re", "zu", "ken\u00b7nen", ",", "das", "mu\u00dft", "du", "pro\u00b7bie\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PTKZU", "VVINF", "$,", "PDS", "VMFIN", "PPER", "VVINF", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Ihnen zu schmeicheln oder sie zu vexieren.", "tokens": ["Ih\u00b7nen", "zu", "schmei\u00b7cheln", "o\u00b7der", "sie", "zu", "ve\u00b7xie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "KON", "PPER", "PTKZU", "VVINF", "$."], "meter": "+--+-+--+-+-", "measure": "iambic.penta.invert"}}, "stanza.192": {"line.1": {"text": "\u00bbwarum magst du gewisse Schriften nicht lesen?\u00ab", "tokens": ["\u00bb", "wa\u00b7rum", "magst", "du", "ge\u00b7wis\u00b7se", "Schrif\u00b7ten", "nicht", "le\u00b7sen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VMFIN", "PPER", "ADJA", "NN", "PTKNEG", "VVINF", "$.", "$("], "meter": "--+--+-+--+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Das ist auch sonst meine Speise gewesen;", "tokens": ["Das", "ist", "auch", "sonst", "mei\u00b7ne", "Spei\u00b7se", "ge\u00b7we\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "PPOSAT", "NN", "VAPP", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Eilt aber die Raupe, sich einzuspinnen,", "tokens": ["Eilt", "a\u00b7ber", "die", "Rau\u00b7pe", ",", "sich", "ein\u00b7zu\u00b7spin\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "$,", "PRF", "VVINF", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Nicht kann sie mehr Bl\u00e4ttern Geschmack abgewinnen.", "tokens": ["Nicht", "kann", "sie", "mehr", "Bl\u00e4t\u00b7tern", "Ge\u00b7schmack", "ab\u00b7ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "PPER", "PIAT", "NN", "NN", "VVINF", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.193": {"line.1": {"text": "Was dem Enkel so wie dem Ahn frommt,", "tokens": ["Was", "dem", "En\u00b7kel", "so", "wie", "dem", "Ahn", "frommt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "ADV", "KOKOM", "ART", "NN", "VVFIN", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Dar\u00fcber hat man viel getr\u00e4umet;", "tokens": ["Da\u00b7r\u00fc\u00b7ber", "hat", "man", "viel", "ge\u00b7tr\u00e4u\u00b7met", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PIS", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Aber worauf eben alles ankommt,", "tokens": ["A\u00b7ber", "wo\u00b7rauf", "e\u00b7ben", "al\u00b7les", "an\u00b7kommt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ADV", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Das wird vom Lehrer gew\u00f6hnlich vers\u00e4umet.", "tokens": ["Das", "wird", "vom", "Leh\u00b7rer", "ge\u00b7w\u00f6hn\u00b7lich", "ver\u00b7s\u00e4u\u00b7met", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPRART", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.194": {"line.1": {"text": "Verweile nicht, und sei dir selbst ein Traum,", "tokens": ["Ver\u00b7wei\u00b7le", "nicht", ",", "und", "sei", "dir", "selbst", "ein", "Traum", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "$,", "KON", "VAFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und wie du reisest, danke jedem Raum,", "tokens": ["Und", "wie", "du", "rei\u00b7sest", ",", "dan\u00b7ke", "je\u00b7dem", "Raum", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "VVFIN", "$,", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Bequeme dich dem Hei\u00dfen wie dem Kalten;", "tokens": ["Be\u00b7que\u00b7me", "dich", "dem", "Hei\u00b7\u00dfen", "wie", "dem", "Kal\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Dir wird die Welt, du wirst ihr nie veralten.", "tokens": ["Dir", "wird", "die", "Welt", ",", "du", "wirst", "ihr", "nie", "ver\u00b7al\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "PPER", "VAFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.195": {"line.1": {"text": "Ohne Umschweife", "tokens": ["Oh\u00b7ne", "Um\u00b7schwei\u00b7fe"], "token_info": ["word", "word"], "pos": ["APPR", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Begreife,", "tokens": ["Be\u00b7grei\u00b7fe", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Was dich mit der Welt entzweit;", "tokens": ["Was", "dich", "mit", "der", "Welt", "ent\u00b7zweit", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Nicht will sie Gem\u00fct, will H\u00f6flichkeit.", "tokens": ["Nicht", "will", "sie", "Ge\u00b7m\u00fct", ",", "will", "H\u00f6f\u00b7lich\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "PPER", "NN", "$,", "VMFIN", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.196": {"line.1": {"text": "Gem\u00fct mu\u00df verschleifen,", "tokens": ["Ge\u00b7m\u00fct", "mu\u00df", "ver\u00b7schlei\u00b7fen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "H\u00f6flichkeit l\u00e4\u00dft sich mit H\u00e4nden greifen.", "tokens": ["H\u00f6f\u00b7lich\u00b7keit", "l\u00e4\u00dft", "sich", "mit", "H\u00e4n\u00b7den", "grei\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PRF", "APPR", "NN", "VVINF", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.197": {"line.1": {"text": "Was eben wahr ist allerorten,", "tokens": ["Was", "e\u00b7ben", "wahr", "ist", "al\u00b7le\u00b7ror\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADJD", "VAFIN", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das sag ich mit ungescheuten Worten.", "tokens": ["Das", "sag", "ich", "mit", "un\u00b7ge\u00b7scheu\u00b7ten", "Wor\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.198": {"line.1": {"text": "Nichts taugt Ungeduld,", "tokens": ["Nichts", "taugt", "Un\u00b7ge\u00b7duld", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Noch weniger Reue;", "tokens": ["Noch", "we\u00b7ni\u00b7ger", "Reu\u00b7e", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Jene vermehrt die Schuld,", "tokens": ["Je\u00b7ne", "ver\u00b7mehrt", "die", "Schuld", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.4": {"text": "Diese schafft neue.", "tokens": ["Die\u00b7se", "schafft", "neu\u00b7e", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADJA", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.199": {"line.1": {"text": "Da\u00df von diesem wilden Sehnen,", "tokens": ["Da\u00df", "von", "die\u00b7sem", "wil\u00b7den", "Seh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Dieser reichen Saat von Tr\u00e4nen", "tokens": ["Die\u00b7ser", "rei\u00b7chen", "Saat", "von", "Tr\u00e4\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDAT", "ADJA", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "G\u00f6tterlust zu hoffen sei,", "tokens": ["G\u00f6t\u00b7ter\u00b7lust", "zu", "hof\u00b7fen", "sei", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Mache deine Seele frei!", "tokens": ["Ma\u00b7che", "dei\u00b7ne", "See\u00b7le", "frei", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.200": {"line.1": {"text": "Der entschlie\u00dft sich doch gleich,", "tokens": ["Der", "ent\u00b7schlie\u00dft", "sich", "doch", "gleich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADV", "ADV", "$,"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Den hei\u00df ich brav und k\u00fchn!", "tokens": ["Den", "hei\u00df", "ich", "brav", "und", "k\u00fchn", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "PPER", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Er springt in den Teich,", "tokens": ["Er", "springt", "in", "den", "Teich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Dem Regen zu entfliehn.", "tokens": ["Dem", "Re\u00b7gen", "zu", "ent\u00b7fliehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.201": {"line.1": {"text": "Da\u00df Gl\u00fcck ihm g\u00fcnstig sei,", "tokens": ["Da\u00df", "Gl\u00fcck", "ihm", "g\u00fcns\u00b7tig", "sei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Was hilft's dem St\u00f6ffel?", "tokens": ["Was", "hilft's", "dem", "St\u00f6f\u00b7fel", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Denn regnet's Brei,", "tokens": ["Denn", "reg\u00b7net's", "Brei", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Fehlt ihm der L\u00f6ffel.", "tokens": ["Fehlt", "ihm", "der", "L\u00f6f\u00b7fel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.202": {"line.1": {"text": "Dichter gleichen B\u00e4ren,", "tokens": ["Dich\u00b7ter", "glei\u00b7chen", "B\u00e4\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Die immer an eignen Pfoten zehren.", "tokens": ["Die", "im\u00b7mer", "an", "eig\u00b7nen", "Pfo\u00b7ten", "zeh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.203": {"line.1": {"text": "Die Welt ist nicht aus Brei und Mus geschaffen,", "tokens": ["Die", "Welt", "ist", "nicht", "aus", "Brei", "und", "Mus", "ge\u00b7schaf\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKNEG", "APPR", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Deswegen haltet euch nicht wie Schlaraffen;", "tokens": ["Des\u00b7we\u00b7gen", "hal\u00b7tet", "euch", "nicht", "wie", "Schla\u00b7raf\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PTKNEG", "KOKOM", "NN", "$."], "meter": "-+-+--+-++-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Harte Bissen gibt es zu kauen:", "tokens": ["Har\u00b7te", "Bis\u00b7sen", "gibt", "es", "zu", "kau\u00b7en", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "PPER", "PTKZU", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Wir m\u00fcssen erw\u00fcrgen oder sie verdauen.", "tokens": ["Wir", "m\u00fcs\u00b7sen", "er\u00b7w\u00fcr\u00b7gen", "o\u00b7der", "sie", "ver\u00b7dau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "KON", "PPER", "VVINF", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.204": {"line.1": {"text": "Ein kluges Volk wohnt nah dabei,", "tokens": ["Ein", "klu\u00b7ges", "Volk", "wohnt", "nah", "da\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADJD", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das immerfort sein Bestes wollte;", "tokens": ["Das", "im\u00b7mer\u00b7fort", "sein", "Bes\u00b7tes", "woll\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "PPOSAT", "NN", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Es gab dem niedrigen Kirchturm Brei,", "tokens": ["Es", "gab", "dem", "nied\u00b7ri\u00b7gen", "Kirch\u00b7turm", "Brei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Damit er gr\u00f6\u00dfer werden sollte.", "tokens": ["Da\u00b7mit", "er", "gr\u00f6\u00b7\u00dfer", "wer\u00b7den", "soll\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.205": {"line.1": {"text": "Sechsundzwanzig Groschen gilt mein Taler!", "tokens": ["Sech\u00b7sund\u00b7zwan\u00b7zig", "Gro\u00b7schen", "gilt", "mein", "Ta\u00b7ler", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Was hei\u00dft ihr mich denn einen Prahler?", "tokens": ["Was", "hei\u00dft", "ihr", "mich", "denn", "ei\u00b7nen", "Prah\u00b7ler", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PRF", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Habt ihr doch andre nicht gescholten,", "tokens": ["Habt", "ihr", "doch", "and\u00b7re", "nicht", "ge\u00b7schol\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PIS", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Deren Groschen einen Taler gegolten.", "tokens": ["De\u00b7ren", "Gro\u00b7schen", "ei\u00b7nen", "Ta\u00b7ler", "ge\u00b7gol\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.206": {"line.1": {"text": "Niedertr\u00e4chtigers wird nichts gereicht,", "tokens": ["Nie\u00b7der\u00b7tr\u00e4ch\u00b7ti\u00b7gers", "wird", "nichts", "ge\u00b7reicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Als wenn der Tag den Tag erzeugt.", "tokens": ["Als", "wenn", "der", "Tag", "den", "Tag", "er\u00b7zeugt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.207": {"line.1": {"text": "Was hat dir das arme Glas getan?", "tokens": ["Was", "hat", "dir", "das", "ar\u00b7me", "Glas", "ge\u00b7tan", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "----+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Sieh deinen Spiegel nicht so h\u00e4\u00dflich an.", "tokens": ["Sieh", "dei\u00b7nen", "Spie\u00b7gel", "nicht", "so", "h\u00e4\u00df\u00b7lich", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "NN", "PTKNEG", "ADV", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.208": {"line.1": {"text": "Liebesb\u00fccher und Jahrgedichte", "tokens": ["Lie\u00b7bes\u00b7b\u00fc\u00b7cher", "und", "Jahr\u00b7ge\u00b7dich\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Machen bleich und hager;", "tokens": ["Ma\u00b7chen", "bleich", "und", "ha\u00b7ger", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "KON", "ADJD", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Fr\u00f6sche plagten, sagt die Geschichte,", "tokens": ["Fr\u00f6\u00b7sche", "plag\u00b7ten", ",", "sagt", "die", "Ge\u00b7schich\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Pharaonem auf seinem Lager.", "tokens": ["Pha\u00b7rao\u00b7nem", "auf", "sei\u00b7nem", "La\u00b7ger", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.209": {"line.1": {"text": "So schlie\u00dfen wir, da\u00df in die L\u00e4ng", "tokens": ["So", "schlie\u00b7\u00dfen", "wir", ",", "da\u00df", "in", "die", "L\u00e4ng"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Euch nicht die Ohren gellen,", "tokens": ["Euch", "nicht", "die", "Oh\u00b7ren", "gel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKNEG", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Vernunft ist hoch, Verstand ist streng,", "tokens": ["Ver\u00b7nunft", "ist", "hoch", ",", "Ver\u00b7stand", "ist", "streng", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "$,", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wir rasseln drein mit Schellen.", "tokens": ["Wir", "ras\u00b7seln", "drein", "mit", "Schel\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.210": {"line.1": {"text": "Diese Worte sind nicht alle in Sachsen", "tokens": ["Die\u00b7se", "Wor\u00b7te", "sind", "nicht", "al\u00b7le", "in", "Sach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDAT", "NN", "VAFIN", "PTKNEG", "PIS", "APPR", "NE"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Noch auf meinem eignen Mist gewachsen,", "tokens": ["Noch", "auf", "mei\u00b7nem", "eig\u00b7nen", "Mist", "ge\u00b7wach\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Doch was f\u00fcr Samen die Fremde bringt,", "tokens": ["Doch", "was", "f\u00fcr", "Sa\u00b7men", "die", "Frem\u00b7de", "bringt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPR", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Erzog ich im Lande gut ged\u00fcngt.", "tokens": ["Er\u00b7zog", "ich", "im", "Lan\u00b7de", "gut", "ge\u00b7d\u00fcngt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "ADJD", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.211": {"line.1": {"text": "Und selbst den Leuten du bon ton", "tokens": ["Und", "selbst", "den", "Leu\u00b7ten", "du", "bon", "ton"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "PPER", "ADV", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ist dieses B\u00fcchlein lustig erschienen:", "tokens": ["Ist", "die\u00b7ses", "B\u00fcch\u00b7lein", "lus\u00b7tig", "er\u00b7schie\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Es ist kein Globe de compression,", "tokens": ["Es", "ist", "kein", "Glo\u00b7be", "de", "com\u00b7pres\u00b7si\u00b7on", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "NE", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sind lauter Flatterminen.", "tokens": ["Sind", "lau\u00b7ter", "Flat\u00b7ter\u00b7mi\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}