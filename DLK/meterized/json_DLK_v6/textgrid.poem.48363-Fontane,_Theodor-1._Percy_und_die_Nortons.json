{"textgrid.poem.48363": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "1. Percy und die Nortons", "genre": "verse", "period": "N.A.", "pub_year": 1851, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Graf Percy ging in den Garten sein,", "tokens": ["Graf", "Per\u00b7cy", "ging", "in", "den", "Gar\u00b7ten", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "APPR", "ART", "NN", "VAINF", "$,"], "meter": "-+-++-+-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Sein junges Gemahl geleitet ihn,", "tokens": ["Sein", "jun\u00b7ges", "Ge\u00b7mahl", "ge\u00b7lei\u00b7tet", "ihn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVPP", "PPER", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Er spricht: \u00bbMir singt ein Vogel ins Ohr,", "tokens": ["Er", "spricht", ":", "\u00bb", "Mir", "singt", "ein", "Vo\u00b7gel", "ins", "Ohr", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VVFIN", "ART", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Du mu\u00dft fechten, Percy, oder fliehn.\u00ab", "tokens": ["Du", "mu\u00dft", "fech\u00b7ten", ",", "Per\u00b7cy", ",", "o\u00b7der", "fliehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "$,", "NE", "$,", "KON", "VVINF", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.2": {"line.1": {"text": "Lady Percy spricht: \u00bbVerh\u00fcte das Gott!", "tokens": ["La\u00b7dy", "Per\u00b7cy", "spricht", ":", "\u00bb", "Ver\u00b7h\u00fc\u00b7te", "das", "Gott", "!"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "$.", "$(", "NN", "ART", "NN", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "O sei nicht so stolz, o sei nicht so scheu:", "tokens": ["O", "sei", "nicht", "so", "stolz", ",", "o", "sei", "nicht", "so", "scheu", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PTKNEG", "ADV", "ADJD", "$,", "FM", "VAFIN", "PTKNEG", "ADV", "ADJD", "$."], "meter": "+---+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Nach London geh, an der K\u00f6nigin Hof,", "tokens": ["Nach", "Lon\u00b7don", "geh", ",", "an", "der", "K\u00f6\u00b7ni\u00b7gin", "Hof", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$,", "APPR", "ART", "NN", "NE", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und beug' ihr dein Knie und leist' ihr die Treu.\u00ab", "tokens": ["Und", "beug'", "ihr", "dein", "Knie", "und", "leist'", "ihr", "die", "Treu", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "PPOSAT", "NN", "KON", "VVFIN", "PPER", "ART", "NN", "$.", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "\u00bbzu sp\u00e4t, zu sp\u00e4t, liebe Lady mein,", "tokens": ["\u00bb", "zu", "sp\u00e4t", ",", "zu", "sp\u00e4t", ",", "lie\u00b7be", "La\u00b7dy", "mein", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PTKA", "ADJD", "$,", "PTKA", "ADJD", "$,", "VVFIN", "NE", "PPOSAT", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Es ist nicht mehr, wie sonst es war,", "tokens": ["Es", "ist", "nicht", "mehr", ",", "wie", "sonst", "es", "war", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADV", "$,", "PWAV", "ADV", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Meine Feinde gelten bei Hofe jetzt,", "tokens": ["Mei\u00b7ne", "Fein\u00b7de", "gel\u00b7ten", "bei", "Ho\u00b7fe", "jetzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "NN", "ADV", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ich kann nicht gehn, mir droht Gefahr.\u00ab", "tokens": ["Ich", "kann", "nicht", "gehn", ",", "mir", "droht", "Ge\u00b7fahr", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "PPER", "VVFIN", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "\u00bbund doch, und doch \u2013 sonst reut es dich noch!", "tokens": ["\u00bb", "und", "doch", ",", "und", "doch", "\u2013", "sonst", "reut", "es", "dich", "noch", "!"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADV", "$,", "KON", "ADV", "$(", "ADV", "VVFIN", "PPER", "PRF", "ADV", "$."], "meter": "-+-+-+-++", "measure": "unknown.measure.penta"}, "line.2": {"text": "Leg ab deine Scheu, leg ab deinen Trutz,", "tokens": ["Leg", "ab", "dei\u00b7ne", "Scheu", ",", "leg", "ab", "dei\u00b7nen", "Trutz", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "$,", "NE", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Nimm all deine besten Mannen mit,", "tokens": ["Nimm", "all", "dei\u00b7ne", "bes\u00b7ten", "Man\u00b7nen", "mit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PIAT", "PPOSAT", "ADJA", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "So hast du Schirm, und so hast du Schutz.\u00ab", "tokens": ["So", "hast", "du", "Schirm", ",", "und", "so", "hast", "du", "Schutz", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "$,", "KON", "ADV", "VAFIN", "PPER", "NN", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "\u00bbzu sp\u00e4t, zu sp\u00e4t, liebe Lady mein,", "tokens": ["\u00bb", "zu", "sp\u00e4t", ",", "zu", "sp\u00e4t", ",", "lie\u00b7be", "La\u00b7dy", "mein", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PTKA", "ADJD", "$,", "PTKA", "ADJD", "$,", "VVFIN", "NE", "PPOSAT", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Der Hof ist klug, ist fein-verstrickt,", "tokens": ["Der", "Hof", "ist", "klug", ",", "ist", "fein\u00b7ver\u00b7strickt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und wenn ich morgen zu Hofe ging',", "tokens": ["Und", "wenn", "ich", "mor\u00b7gen", "zu", "Ho\u00b7fe", "ging'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "So h\u00e4tt' ich dich heute zuletzt erblickt.\u00ab", "tokens": ["So", "h\u00e4tt'", "ich", "dich", "heu\u00b7te", "zu\u00b7letzt", "er\u00b7blickt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PRF", "ADV", "ADV", "VVPP", "$.", "$("], "meter": "-+-++--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "\u00bbund doch, und doch \u2013 sonst reut es dich noch!", "tokens": ["\u00bb", "und", "doch", ",", "und", "doch", "\u2013", "sonst", "reut", "es", "dich", "noch", "!"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADV", "$,", "KON", "ADV", "$(", "ADV", "VVFIN", "PPER", "PRF", "ADV", "$."], "meter": "-+-+-+-++", "measure": "unknown.measure.penta"}, "line.2": {"text": "La\u00df satteln! ich will ja mit dir gehn", "tokens": ["La\u00df", "sat\u00b7teln", "!", "ich", "will", "ja", "mit", "dir", "gehn"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVIMP", "VVINF", "$.", "PPER", "VMFIN", "ADV", "APPR", "PPER", "VVINF"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und will bei Hofe, so Tag wie Nacht,", "tokens": ["Und", "will", "bei", "Ho\u00b7fe", ",", "so", "Tag", "wie", "Nacht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "NN", "$,", "ADV", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Meinem lieben Herrn zur Seite stehn.\u00ab", "tokens": ["Mei\u00b7nem", "lie\u00b7ben", "Herrn", "zur", "Sei\u00b7te", "stehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "APPRART", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "\u00bbhalt ein, halt ein, liebe Lady mein,", "tokens": ["\u00bb", "halt", "ein", ",", "halt", "ein", ",", "lie\u00b7be", "La\u00b7dy", "mein", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PTKVZ", "$,", "VVFIN", "PTKVZ", "$,", "VVFIN", "NE", "PPOSAT", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Es ist zu sp\u00e4t, ich bin nicht blind,", "tokens": ["Es", "ist", "zu", "sp\u00e4t", ",", "ich", "bin", "nicht", "blind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKA", "ADJD", "$,", "PPER", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Vogel hat Recht, und mein Herz hat Recht,", "tokens": ["Der", "Vo\u00b7gel", "hat", "Recht", ",", "und", "mein", "Herz", "hat", "Recht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "NN", "$,", "KON", "PPOSAT", "NN", "VAFIN", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Und fechten mu\u00df ich f\u00fcr Weib und Kind \u2013 \u2013", "tokens": ["Und", "fech\u00b7ten", "mu\u00df", "ich", "f\u00fcr", "Weib", "und", "Kind", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "APPR", "NN", "KON", "NN", "$(", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Tritt her, tritt her, mein Knappe jung,", "tokens": ["Tritt", "her", ",", "tritt", "her", ",", "mein", "Knap\u00b7pe", "jung", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "VVFIN", "PTKVZ", "$,", "PPOSAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und schaue mich an und horche wohl auf,", "tokens": ["Und", "schau\u00b7e", "mich", "an", "und", "hor\u00b7che", "wohl", "auf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKVZ", "KON", "VVFIN", "ADV", "PTKVZ", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Zu Richard Norton mu\u00df dieser Brief,", "tokens": ["Zu", "Ric\u00b7hard", "Nor\u00b7ton", "mu\u00df", "die\u00b7ser", "Brief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "VMFIN", "PDAT", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Noch eh' vor\u00fcber des Tages Lauf.", "tokens": ["Noch", "eh'", "vor\u00b7\u00fc\u00b7ber", "des", "Ta\u00b7ges", "Lauf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Empfiehl mich dem Squire und sag' ihm das Wort:", "tokens": ["Emp\u00b7fiehl", "mich", "dem", "Squi\u00b7re", "und", "sag'", "ihm", "das", "Wort", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PRF", "ART", "NN", "KON", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Die Stunde sei da, und wir seien bereit,", "tokens": ["Die", "Stun\u00b7de", "sei", "da", ",", "und", "wir", "sei\u00b7en", "be\u00b7reit", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "$,", "KON", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Und wenn er noch Richard Norton w\u00e4r',", "tokens": ["Und", "wenn", "er", "noch", "Ric\u00b7hard", "Nor\u00b7ton", "w\u00e4r'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "NE", "NE", "VAFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "So m\u00fcss' er kommen zu dieser Zeit.\u00ab", "tokens": ["So", "m\u00fcss'", "er", "kom\u00b7men", "zu", "die\u00b7ser", "Zeit", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VMFIN", "PPER", "VVFIN", "APPR", "PDAT", "NN", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.10": {"line.1": {"text": "Der Percy sprach's, der Knappe brach auf,", "tokens": ["Der", "Per\u00b7cy", "sprach's", ",", "der", "Knap\u00b7pe", "brach", "auf", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "$,", "ART", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Eine Weile er ging, eine Weile er lief,", "tokens": ["Ei\u00b7ne", "Wei\u00b7le", "er", "ging", ",", "ei\u00b7ne", "Wei\u00b7le", "er", "lief", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "VVFIN", "$,", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Und eh' die Sonne hernieder war,", "tokens": ["Und", "eh'", "die", "Son\u00b7ne", "her\u00b7nie\u00b7der", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "PTKVZ", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da hatte der Squire des Grafen Brief.", "tokens": ["Da", "hat\u00b7te", "der", "Squi\u00b7re", "des", "Gra\u00b7fen", "Brief", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ART", "NN", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.11": {"line.1": {"text": "Er las voll Ernst, er las zweimal,", "tokens": ["Er", "las", "voll", "Ernst", ",", "er", "las", "zwei\u00b7mal", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "NN", "$,", "PPER", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Seine S\u00f6hne sahen ihn fragend an,", "tokens": ["Sei\u00b7ne", "S\u00f6h\u00b7ne", "sa\u00b7hen", "ihn", "fra\u00b7gend", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "ADJD", "PTKVZ", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Und als er las zum dritten Mal,", "tokens": ["Und", "als", "er", "las", "zum", "drit\u00b7ten", "Mal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Eine Tr\u00e4n' ihm \u00fcber das Antlitz rann.", "tokens": ["Ei\u00b7ne", "Tr\u00e4n'", "ihm", "\u00fc\u00b7ber", "das", "Ant\u00b7litz", "rann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.12": {"line.1": {"text": "\u00bbsag' an, sag' an, Christopher, mein Sohn,", "tokens": ["\u00bb", "sag'", "an", ",", "sag'", "an", ",", "Chris\u00b7to\u00b7pher", ",", "mein", "Sohn", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["$(", "VVIMP", "PTKVZ", "$,", "VVFIN", "PTKVZ", "$,", "NE", "$,", "PPOSAT", "NN", "$,"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Dein junges Herz hat braven Mut,", "tokens": ["Dein", "jun\u00b7ges", "Herz", "hat", "bra\u00b7ven", "Mut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Graf Percy ziehet in b\u00f6sen Streit,", "tokens": ["Graf", "Per\u00b7cy", "zie\u00b7het", "in", "b\u00f6\u00b7sen", "Streit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Was sollen wir tun, welch Rat ist gut?\u00ab", "tokens": ["Was", "sol\u00b7len", "wir", "tun", ",", "welch", "Rat", "ist", "gut", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "VVINF", "$,", "PWAT", "NN", "VAFIN", "ADJD", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.13": {"line.1": {"text": "\u00bbund soll ich raten, so rat' ich frei:", "tokens": ["\u00bb", "und", "soll", "ich", "ra\u00b7ten", ",", "so", "rat'", "ich", "frei", ":"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "VMFIN", "PPER", "VVFIN", "$,", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Graf Percy ist ein edler Lord,", "tokens": ["Graf", "Per\u00b7cy", "ist", "ein", "ed\u00b7ler", "Lord", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und was es immer uns bringen mag,", "tokens": ["Und", "was", "es", "im\u00b7mer", "uns", "brin\u00b7gen", "mag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wir m\u00fcssen ihm halten unser Wort.\u00ab", "tokens": ["Wir", "m\u00fcs\u00b7sen", "ihm", "hal\u00b7ten", "un\u00b7ser", "Wort", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "PPOSAT", "NN", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "\u00bbhab' Dank, hab' Dank, Christopher, mein Sohn.", "tokens": ["\u00bb", "hab'", "Dank", ",", "hab'", "Dank", ",", "Chris\u00b7to\u00b7pher", ",", "mein", "Sohn", "."], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["$(", "VAFIN", "NN", "$,", "VAFIN", "NN", "$,", "NE", "$,", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Dein Rat ist gut, Gott schenk' ihm Gedeihn,", "tokens": ["Dein", "Rat", "ist", "gut", ",", "Gott", "schenk'", "ihm", "Ge\u00b7deihn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$,", "NN", "VVFIN", "PPER", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und kommen wir mit dem Leben davon,", "tokens": ["Und", "kom\u00b7men", "wir", "mit", "dem", "Le\u00b7ben", "da\u00b7von", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "PAV", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "So soll dir's nicht vergessen sein.", "tokens": ["So", "soll", "dir's", "nicht", "ver\u00b7ges\u00b7sen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "PTKNEG", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Was aber sprecht ihr, ihr andern acht?", "tokens": ["Was", "a\u00b7ber", "sprecht", "ihr", ",", "ihr", "an\u00b7dern", "acht", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVFIN", "PPER", "$,", "PPOSAT", "ADJA", "CARD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sagt ja, sagt nein, ich la\u00df es geschehn.\u00ab", "tokens": ["Sagt", "ja", ",", "sagt", "nein", ",", "ich", "la\u00df", "es", "ge\u00b7schehn", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "ADV", "$,", "VVFIN", "PTKANT", "$,", "PPER", "VVFIN", "PPER", "VVPP", "$.", "$("], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Da sprachen sieben: \u00bbWie's kommen mag,", "tokens": ["Da", "spra\u00b7chen", "sie\u00b7ben", ":", "\u00bb", "Wie's", "kom\u00b7men", "mag", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "VVINF", "$.", "$(", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wir wollen zu unserm Vater stehn.\u00ab", "tokens": ["Wir", "wol\u00b7len", "zu", "un\u00b7serm", "Va\u00b7ter", "stehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.16": {"line.1": {"text": "\u00bbhabt Dank, habt Dank, meine Kinder brav,", "tokens": ["\u00bb", "habt", "Dank", ",", "habt", "Dank", ",", "mei\u00b7ne", "Kin\u00b7der", "brav", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "NN", "$,", "VAFIN", "NN", "$,", "PPOSAT", "NN", "ADJD", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Unser s\u00e4chsisch Blut, ihr haltet es rein,", "tokens": ["Un\u00b7ser", "s\u00e4ch\u00b7sisch", "Blut", ",", "ihr", "hal\u00b7tet", "es", "rein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJD", "NN", "$,", "PPER", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Und ob ich leben, ob sterben mag,", "tokens": ["Und", "ob", "ich", "le\u00b7ben", ",", "ob", "ster\u00b7ben", "mag", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVINF", "$,", "KOUS", "VVINF", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Eures Vaters Segen soll mit euch sein.", "tokens": ["Eu\u00b7res", "Va\u00b7ters", "Se\u00b7gen", "soll", "mit", "euch", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "VMFIN", "APPR", "PPER", "VAINF", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.17": {"line.1": {"text": "Doch was sagst du, Franz Norton, mein Sohn,", "tokens": ["Doch", "was", "sagst", "du", ",", "Franz", "Nor\u00b7ton", ",", "mein", "Sohn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "$,", "NE", "NE", "$,", "PPOSAT", "NN", "$,"], "meter": "-----+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Mein \u00c4ltester du und mein Erbe dazu!", "tokens": ["Mein", "\u00c4l\u00b7tes\u00b7ter", "du", "und", "mein", "Er\u00b7be", "da\u00b7zu", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPER", "KON", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Ich seh' was br\u00fcten in deiner Brust;", "tokens": ["Ich", "seh'", "was", "br\u00fc\u00b7ten", "in", "dei\u00b7ner", "Brust", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Deine Br\u00fcder sprachen, so sprich auch du.\u00ab", "tokens": ["Dei\u00b7ne", "Br\u00fc\u00b7der", "spra\u00b7chen", ",", "so", "sprich", "auch", "du", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$,", "ADV", "ADV", "ADV", "PPER", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.18": {"line.1": {"text": "\u00bbund soll ich sprechen, lieb Vater mein:", "tokens": ["\u00bb", "und", "soll", "ich", "spre\u00b7chen", ",", "lieb", "Va\u00b7ter", "mein", ":"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "KON", "VMFIN", "PPER", "VVINF", "$,", "ADJD", "NN", "PPOSAT", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Dein Bart ist grau, dein Haupt ist wei\u00df;", "tokens": ["Dein", "Bart", "ist", "grau", ",", "dein", "Haupt", "ist", "wei\u00df", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$,", "PPOSAT", "NN", "VAFIN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Setz' nicht an faulen, schimpflichen Kampf", "tokens": ["Setz'", "nicht", "an", "fau\u00b7len", ",", "schimpf\u00b7li\u00b7chen", "Kampf"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PTKNEG", "APPR", "VVINF", "$,", "ADJA", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Deiner siebzig Jahre ehrlichen Preis.\u00ab", "tokens": ["Dei\u00b7ner", "sieb\u00b7zig", "Jah\u00b7re", "ehr\u00b7li\u00b7chen", "Preis", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "CARD", "NN", "ADJA", "NN", "$.", "$("], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.19": {"line.1": {"text": "\u00bbhalt ein, Franz Norton! der Schimpf ist dein!", "tokens": ["\u00bb", "halt", "ein", ",", "Franz", "Nor\u00b7ton", "!", "der", "Schimpf", "ist", "dein", "!"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PTKVZ", "$,", "NE", "NE", "$.", "ART", "NN", "VAFIN", "PPOSAT", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Mein Sohn, mein Sohn, wer hat dich bet\u00f6rt?", "tokens": ["Mein", "Sohn", ",", "mein", "Sohn", ",", "wer", "hat", "dich", "be\u00b7t\u00f6rt", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PWS", "VAFIN", "PPER", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Als Kind auf deines Vaters Knien,", "tokens": ["Als", "Kind", "auf", "dei\u00b7nes", "Va\u00b7ters", "Kni\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da hab' ich dich andre Sprache gelehrt.\u00ab \u2013", "tokens": ["Da", "hab'", "ich", "dich", "and\u00b7re", "Spra\u00b7che", "ge\u00b7lehrt", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PRF", "ADJA", "NN", "VVPP", "$.", "$(", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.20": {"line.1": {"text": "Der Alte rief's. \u2013 Vor Tagesschein", "tokens": ["Der", "Al\u00b7te", "rie\u00b7f'", "s.", "\u2013", "Vor", "Ta\u00b7ges\u00b7schein"], "token_info": ["word", "word", "word", "abbreviation", "punct", "word", "word"], "pos": ["ART", "NN", "VVFIN", "NE", "$(", "APPR", "NN"], "meter": "-+-+-++-+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Brachen sie auf mit Mann und Ro\u00df,", "tokens": ["Bra\u00b7chen", "sie", "auf", "mit", "Mann", "und", "Ro\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "APPR", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Und ehe die Sonne in Mittag stand,", "tokens": ["Und", "e\u00b7he", "die", "Son\u00b7ne", "in", "Mit\u00b7tag", "stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Hielten sie schon vor des Percy Schlo\u00df.", "tokens": ["Hiel\u00b7ten", "sie", "schon", "vor", "des", "Per\u00b7cy", "Schlo\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "APPR", "ART", "NE", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.21": {"line.1": {"text": "Bald auch die Nevils kamen heran,", "tokens": ["Bald", "auch", "die", "Ne\u00b7vils", "ka\u00b7men", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Die stolzen Grafen von Westmorland,", "tokens": ["Die", "stol\u00b7zen", "Gra\u00b7fen", "von", "West\u00b7mor\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NE", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und \u2013 eh' die Sonne zu R\u00fcste ging,", "tokens": ["Und", "\u2013", "eh'", "die", "Son\u00b7ne", "zu", "R\u00fcs\u00b7te", "ging", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "KOUS", "ART", "NN", "APPR", "NE", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Sie dreizehntausend beisammen fand.", "tokens": ["Sie", "drei\u00b7zehn\u00b7tau\u00b7send", "bei\u00b7sam\u00b7men", "fand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVINF", "VVFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.22": {"line.1": {"text": "Das Nevil-Banner, zum ersten dann", "tokens": ["Das", "Ne\u00b7vil\u00b7Ban\u00b7ner", ",", "zum", "ers\u00b7ten", "dann"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "APPRART", "ADJA", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Im Morgenwinde ward es entrollt;", "tokens": ["Im", "Mor\u00b7gen\u00b7win\u00b7de", "ward", "es", "ent\u00b7rollt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "VVFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Sein Zeichen war ein silberner Stier,", "tokens": ["Sein", "Zei\u00b7chen", "war", "ein", "sil\u00b7ber\u00b7ner", "Stier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Der trug eine blinkende Kette von Gold.", "tokens": ["Der", "trug", "ei\u00b7ne", "blin\u00b7ken\u00b7de", "Ket\u00b7te", "von", "Gold", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.23": {"line.1": {"text": "Die Percys lie\u00dfen zum zweiten dann", "tokens": ["Die", "Per\u00b7cys", "lie\u00b7\u00dfen", "zum", "zwei\u00b7ten", "dann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NE", "VVFIN", "APPRART", "ADJA", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ihren schimmernden Halbmond flattern und wehn;", "tokens": ["Ih\u00b7ren", "schim\u00b7mern\u00b7den", "Halb\u00b7mond", "flat\u00b7tern", "und", "wehn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVINF", "KON", "VVINF", "$."], "meter": "+-+--+-+--+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Die Nortons aber f\u00fchrten ein Kreuz,", "tokens": ["Die", "Nor\u00b7tons", "a\u00b7ber", "f\u00fchr\u00b7ten", "ein", "Kreuz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Dran waren die Wunden des Heilands zu sehn.", "tokens": ["Dran", "wa\u00b7ren", "die", "Wun\u00b7den", "des", "Hei\u00b7lands", "zu", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.24": {"line.1": {"text": "Sie zogen ins Feld, und sie jagten wie Spreu", "tokens": ["Sie", "zo\u00b7gen", "ins", "Feld", ",", "und", "sie", "jag\u00b7ten", "wie", "Spreu"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "$,", "KON", "PPER", "VVFIN", "KOKOM", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.2": {"text": "Der K\u00f6nigin Volk \u00fcbers Clifford-Moor;", "tokens": ["Der", "K\u00f6\u00b7ni\u00b7gin", "Volk", "\u00fc\u00b7bers", "Clif\u00b7ford\u00b7Moor", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Siebenhundert retteten sich aufs Schlo\u00df \u2013", "tokens": ["Sie\u00b7ben\u00b7hun\u00b7dert", "ret\u00b7te\u00b7ten", "sich", "aufs", "Schlo\u00df", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PRF", "APPRART", "NN", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Bald aber lagen die Grafen davor.", "tokens": ["Bald", "a\u00b7ber", "la\u00b7gen", "die", "Gra\u00b7fen", "da\u00b7vor", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ART", "NN", "PAV", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.25": {"line.1": {"text": "Sie griffen an am kommenden Tag,", "tokens": ["Sie", "grif\u00b7fen", "an", "am", "kom\u00b7men\u00b7den", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und am dritten Tage da gl\u00fcckte der Sturm:", "tokens": ["Und", "am", "drit\u00b7ten", "Ta\u00b7ge", "da", "gl\u00fcck\u00b7te", "der", "Sturm", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "NN", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die Percys nahmen den Felsenwall,", "tokens": ["Die", "Per\u00b7cys", "nah\u00b7men", "den", "Fel\u00b7sen\u00b7wall", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die Nortons nahmen den Backsteinturm.", "tokens": ["Die", "Nor\u00b7tons", "nah\u00b7men", "den", "Back\u00b7stein\u00b7turm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.26": {"line.1": {"text": "Ihre Banner wehten von Schlo\u00df zu Schlo\u00df,", "tokens": ["Ih\u00b7re", "Ban\u00b7ner", "weh\u00b7ten", "von", "Schlo\u00df", "zu", "Schlo\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "NN", "APPR", "NN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Bleicher Schrecken lief gen London hin,", "tokens": ["Blei\u00b7cher", "Schre\u00b7cken", "lief", "gen", "Lon\u00b7don", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "APPR", "NE", "PTKVZ", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da aber ward der Schrecken zu Wut", "tokens": ["Da", "a\u00b7ber", "ward", "der", "Schre\u00b7cken", "zu", "Wut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Im Herzen unsrer K\u00f6nigin.", "tokens": ["Im", "Her\u00b7zen", "uns\u00b7rer", "K\u00f6\u00b7ni\u00b7gin", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Sie rief: \u00bbWohlan denn, Blut um Blut!", "tokens": ["Sie", "rief", ":", "\u00bb", "Wo\u00b7hlan", "denn", ",", "Blut", "um", "Blut", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "ADV", "ADV", "$,", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie sollen ernten, was sie ges\u00e4t,", "tokens": ["Sie", "sol\u00b7len", "ern\u00b7ten", ",", "was", "sie", "ge\u00b7s\u00e4t", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und das Beil mag beugen ihren Kopf,", "tokens": ["Und", "das", "Beil", "mag", "beu\u00b7gen", "ih\u00b7ren", "Kopf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VMFIN", "VVINF", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Der so trotzig auf ihren H\u00e4lsen steht.\u00ab", "tokens": ["Der", "so", "trot\u00b7zig", "auf", "ih\u00b7ren", "H\u00e4l\u00b7sen", "steht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADV", "ADJD", "APPR", "PPOSAT", "NN", "VVFIN", "$.", "$("], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.28": {"line.1": {"text": "Sie musterte drei\u00dfigtausend Mann,", "tokens": ["Sie", "mus\u00b7ter\u00b7te", "drei\u00b7\u00dfig\u00b7tau\u00b7send", "Mann", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "CARD", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die f\u00fchrte der h\u00f6fische Warwick-Graf,", "tokens": ["Die", "f\u00fchr\u00b7te", "der", "h\u00f6\u00b7fi\u00b7sche", "Wa\u00b7rwick\u00b7Graf", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Und am elften Tag, am Humber-Strom,", "tokens": ["Und", "am", "elf\u00b7ten", "Tag", ",", "am", "Hum\u00b7ber\u00b7Strom", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "NN", "$,", "APPRART", "NN", "$,"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Da war es, wo er die Grafen traf.", "tokens": ["Da", "war", "es", ",", "wo", "er", "die", "Gra\u00b7fen", "traf", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$,", "PWAV", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.29": {"line.1": {"text": "Er rief hin\u00fcber, voll Spott und Hohn:", "tokens": ["Er", "rief", "hin\u00b7\u00fc\u00b7ber", ",", "voll", "Spott", "und", "Hohn", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "ADJD", "NN", "KON", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbnun Nevil-Stier, st\u00fcrm' an in Wut,", "tokens": ["\u00bb", "nun", "Ne\u00b7vil\u00b7S\u00b7tier", ",", "st\u00fcrm'", "an", "in", "Wut", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "NE", "$,", "VVFIN", "APPR", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nun Percy-Mond, geh' auf, geh' auf,", "tokens": ["Nun", "Per\u00b7cy\u00b7Mond", ",", "geh'", "auf", ",", "geh'", "auf", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "NE", "$,", "VVFIN", "PTKVZ", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nun Norton, sieh, was dein Heiland tut.\u00ab", "tokens": ["Nun", "Nor\u00b7ton", ",", "sieh", ",", "was", "dein", "Hei\u00b7land", "tut", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "NE", "$,", "VVFIN", "$,", "PRELS", "PPOSAT", "NN", "VVFIN", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.30": {"line.1": {"text": "Der Nevil-Stier und das Norton-Kreuz,", "tokens": ["Der", "Ne\u00b7vil\u00b7S\u00b7tier", "und", "das", "Nor\u00b7ton\u00b7Kreuz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wohl t\u00e4ten sie hoch in L\u00fcften wehn,", "tokens": ["Wohl", "t\u00e4\u00b7ten", "sie", "hoch", "in", "L\u00fcf\u00b7ten", "wehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "APPR", "NN", "VVINF", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der Percy-Mond, wohl ging er auf,", "tokens": ["Der", "Per\u00b7cy\u00b7Mond", ",", "wohl", "ging", "er", "auf", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Doch er ging nur auf, um unterzugehn.", "tokens": ["Doch", "er", "ging", "nur", "auf", ",", "um", "un\u00b7ter\u00b7zu\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "PTKVZ", "$,", "KOUI", "VVIZU", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.31": {"line.1": {"text": "Graf Percy floh gen Schottland hin,", "tokens": ["Graf", "Per\u00b7cy", "floh", "gen", "Schott\u00b7land", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "APPR", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Graf Nevil floh weit \u00fcber die See,", "tokens": ["Graf", "Ne\u00b7vil", "floh", "weit", "\u00fc\u00b7ber", "die", "See", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Die Nortons aber wollten nicht fliehn,", "tokens": ["Die", "Nor\u00b7tons", "a\u00b7ber", "woll\u00b7ten", "nicht", "fliehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Sprach jeder: \u00bbIch falle, wo ich steh'.\u00ab", "tokens": ["Sprach", "je\u00b7der", ":", "\u00bb", "Ich", "fal\u00b7le", ",", "wo", "ich", "steh'", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PIS", "$.", "$(", "PPER", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.32": {"line.1": {"text": "Sie fielen nicht, nicht Vater, nicht Sohn,", "tokens": ["Sie", "fie\u00b7len", "nicht", ",", "nicht", "Va\u00b7ter", ",", "nicht", "Sohn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PTKNEG", "NN", "$,", "PTKNEG", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und litten doch alle blutigen Tod;", "tokens": ["Und", "lit\u00b7ten", "doch", "al\u00b7le", "blu\u00b7ti\u00b7gen", "Tod", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Vergebens war seine Locke so wei\u00df,", "tokens": ["Ver\u00b7ge\u00b7bens", "war", "sei\u00b7ne", "Lo\u00b7cke", "so", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "ADV", "VVFIN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Vergebens war ihre Wange so rot.", "tokens": ["Ver\u00b7ge\u00b7bens", "war", "ih\u00b7re", "Wan\u00b7ge", "so", "rot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.33": {"line.1": {"text": "Sie fielen nicht auf ehrlichem Feld,", "tokens": ["Sie", "fie\u00b7len", "nicht", "auf", "ehr\u00b7li\u00b7chem", "Feld", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Sie fielen, wo der Drei-Baum stand;", "tokens": ["Sie", "fie\u00b7len", ",", "wo", "der", "Drei\u00b7Baum", "stand", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Der W\u00fcrger ging von T\u00fcr zu T\u00fcr,", "tokens": ["Der", "W\u00fcr\u00b7ger", "ging", "von", "T\u00fcr", "zu", "T\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und ein Schrei ging \u00fcber Northumberland.", "tokens": ["Und", "ein", "Schrei", "ging", "\u00fc\u00b7ber", "Nor\u00b7thum\u00b7ber\u00b7land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "APPR", "NE", "$."], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}}}}}