{"textgrid.poem.47909": {"metadata": {"author": {"name": "Klaj, Johann", "birth": "N.A.", "death": "N.A."}, "title": "1L: Di\u00df ist der Zeiten Lauf von Anbeginn der Zeiten/", "genre": "verse", "period": "N.A.", "pub_year": 1636, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Di\u00df ist der Zeiten Lauf von Anbeginn der Zeiten/", "tokens": ["Di\u00df", "ist", "der", "Zei\u00b7ten", "Lauf", "von", "An\u00b7be\u00b7ginn", "der", "Zei\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "NN", "APPR", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "es ist der Welt ihr Lauf/ es ist der Lauf der Welt/", "tokens": ["es", "ist", "der", "Welt", "ihr", "Lauf", "/", "es", "ist", "der", "Lauf", "der", "Welt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "PPOSAT", "NN", "$(", "PPER", "VAFIN", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "di\u00df f\u00e4llt/ und jenes steigt/ di\u00df steigt/ und jenes f\u00e4llt/", "tokens": ["di\u00df", "f\u00e4llt", "/", "und", "je\u00b7nes", "steigt", "/", "di\u00df", "steigt", "/", "und", "je\u00b7nes", "f\u00e4llt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$(", "KON", "PDS", "VVFIN", "$(", "PDS", "VVFIN", "$(", "KON", "PDS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "das Ziel/ das abgesteckt/ kan kein Ding \u00fcberschreiten.", "tokens": ["das", "Ziel", "/", "das", "ab\u00b7ge\u00b7steckt", "/", "kan", "kein", "Ding", "\u00fc\u00b7bersc\u00b7hrei\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PDS", "VVPP", "$(", "VMFIN", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Di\u00df zeigt das Riesenbild/ das jener Mann", "tokens": ["Di\u00df", "zeigt", "das", "Rie\u00b7sen\u00b7bild", "/", "das", "je\u00b7ner", "Mann"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ART", "NN", "$(", "ART", "PDAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "der Mann/ der halb ein Ochs/ der Ochs/ der halb ein Mann/", "tokens": ["der", "Mann", "/", "der", "halb", "ein", "Ochs", "/", "der", "Ochs", "/", "der", "halb", "ein", "Mann", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "ADJD", "ART", "NN", "$(", "ART", "NN", "$(", "ART", "ADJD", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "von dessen Fall di\u00df Rund ersch\u00fcttert \u00fcm und an/", "tokens": ["von", "des\u00b7sen", "Fall", "di\u00df", "Rund", "er\u00b7sch\u00fct\u00b7tert", "\u00fcm", "und", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELAT", "NN", "PDS", "ADJD", "VVFIN", "ADJD", "KON", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "di\u00df zeigte/ was geschicht/ was wird und ist geschehen.", "tokens": ["di\u00df", "zeig\u00b7te", "/", "was", "ge\u00b7schicht", "/", "was", "wird", "und", "ist", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$(", "PWS", "VVPP", "$(", "PWS", "VAFIN", "KON", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Der Perser hat das Haubt de\u00df Goldes abgesebelt/", "tokens": ["Der", "Per\u00b7ser", "hat", "das", "Haubt", "de\u00df", "Gol\u00b7des", "ab\u00b7ge\u00b7se\u00b7belt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "den Griechen ward das Hertz der Silber-Brust zu Theil/", "tokens": ["den", "Grie\u00b7chen", "ward", "das", "Hertz", "der", "Sil\u00b7ber\u00b7Brust", "zu", "Theil", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "ART", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "das Ertz-Ged\u00e4rme stund zu Rom am Marckte feil/", "tokens": ["das", "Ertz\u00b7Ge\u00b7d\u00e4r\u00b7me", "stund", "zu", "Rom", "am", "Marck\u00b7te", "feil", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "NE", "APPRART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "der Eisen-F\u00fcsse Thon sind Mengwerck und vernebelt.", "tokens": ["der", "Ei\u00b7sen\u00b7F\u00fcs\u00b7se", "Thon", "sind", "Meng\u00b7werck", "und", "ver\u00b7ne\u00b7belt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "NN", "KON", "VVFIN", "$."], "meter": "-+-+-+-++-+-+", "measure": "unknown.measure.septa"}}, "stanza.4": {"line.1": {"text": "Es ist wie Tennenspreu vom Sommerwind", "tokens": ["Es", "ist", "wie", "Ten\u00b7nen\u00b7spreu", "vom", "Som\u00b7mer\u00b7wind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "KOKOM", "NE", "APPRART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "die Teutschen fielen an/ die haben Rom", "tokens": ["die", "Teut\u00b7schen", "fie\u00b7len", "an", "/", "die", "ha\u00b7ben", "Rom"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$(", "PDS", "VAFIN", "NE"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "da\u00df die Besiegerin in der Besiegten ligt/", "tokens": ["da\u00df", "die", "Be\u00b7sie\u00b7ge\u00b7rin", "in", "der", "Be\u00b7sieg\u00b7ten", "ligt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "nichts ist vom R\u00f6mer mehr als nur der Name blieben.", "tokens": ["nichts", "ist", "vom", "R\u00f6\u00b7mer", "mehr", "als", "nur", "der", "Na\u00b7me", "blie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "APPRART", "NN", "PIAT", "KOKOM", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Da dann das Teutsche Volck sich hin und her gewendet/", "tokens": ["Da", "dann", "das", "Teut\u00b7sche", "Volck", "sich", "hin", "und", "her", "ge\u00b7wen\u00b7det", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "PRF", "PTKVZ", "KON", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "sich hin und her gesetzt/ besetzet St\u00e4dt und Land/", "tokens": ["sich", "hin", "und", "her", "ge\u00b7setzt", "/", "be\u00b7set\u00b7zet", "St\u00e4dt", "und", "Land", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PTKVZ", "KON", "ADV", "VVPP", "$(", "VVFIN", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Theils von sich selbst benamt/ Theils von dem Donausand/", "tokens": ["Theils", "von", "sich", "selbst", "be\u00b7namt", "/", "Theils", "von", "dem", "Do\u00b7nau\u00b7sand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PRF", "ADV", "VVPP", "$(", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Theils von dem reinen Rhein/ wo der sich Seewarts lendet.", "tokens": ["Theils", "von", "dem", "rei\u00b7nen", "Rhein", "/", "wo", "der", "sich", "See\u00b7warts", "len\u00b7det", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "NE", "$(", "PWAV", "ART", "PRF", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Ihr Dach und Fach war schlecht/ der Grund war eine Gabel/", "tokens": ["Ihr", "Dach", "und", "Fach", "war", "schlecht", "/", "der", "Grund", "war", "ei\u00b7ne", "Ga\u00b7bel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "NN", "VAFIN", "ADJD", "$(", "ART", "NN", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "es deckte Hirt und Herd ein Strohbeworfner Baum/", "tokens": ["es", "deck\u00b7te", "Hirt", "und", "Herd", "ein", "Stroh\u00b7be\u00b7worf\u00b7ner", "Baum", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "KON", "NN", "ART", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "so gro\u00df der Hausmann war/ so gro\u00df de\u00df Hauses Raum/", "tokens": ["so", "gro\u00df", "der", "Haus\u00b7mann", "war", "/", "so", "gro\u00df", "de\u00df", "Hau\u00b7ses", "Raum", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "VAFIN", "$(", "ADV", "ADJD", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "ein Baumkleid kleidet ihn/ bis an de\u00df Leibes Nabel.", "tokens": ["ein", "Baum\u00b7kleid", "klei\u00b7det", "ihn", "/", "bis", "an", "de\u00df", "Lei\u00b7bes", "Na\u00b7bel", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$(", "ADV", "APPR", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}}, "stanza.7": {"line.1": {"text": "Was Scham au\u00df Scham befahl/ hat Scham au\u00df Scham bedecket", "tokens": ["Was", "Scham", "au\u00df", "Scham", "be\u00b7fahl", "/", "hat", "Scham", "au\u00df", "Scham", "be\u00b7de\u00b7cket"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "NN", "APPR", "NN", "VVFIN", "$(", "VAFIN", "NN", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "mit ein\u1ebd Weidenbusch; Kein Mein- und Dein seyn", "tokens": ["mit", "ein\u1ebd", "Wei\u00b7den\u00b7busch", ";", "Kein", "Mein", "und", "Dein", "seyn"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$.", "PIAT", "TRUNC", "KON", "PPOSAT", "VAINF"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "hat einen Zaun gez\u00e4unt; Kein Grentzstein war gericht/", "tokens": ["hat", "ei\u00b7nen", "Zaun", "ge\u00b7z\u00e4unt", ";", "Kein", "Grentz\u00b7stein", "war", "ge\u00b7richt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "VVPP", "$.", "PIAT", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "ein jeder sich mit Recht in jedes Feld hinstrecket.", "tokens": ["ein", "je\u00b7der", "sich", "mit", "Recht", "in", "je\u00b7des", "Feld", "hins\u00b7tre\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "PRF", "APPR", "NN", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Wann Eicheln nicht genug de\u00df Magens Z\u00fcrnen stillten/", "tokens": ["Wann", "Ei\u00b7cheln", "nicht", "ge\u00b7nug", "de\u00df", "Ma\u00b7gens", "Z\u00fcr\u00b7nen", "still\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "PTKNEG", "ADV", "ART", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "dann kam ein Butterweck/ da fra\u00df ein jeder frey/", "tokens": ["dann", "kam", "ein", "But\u00b7ter\u00b7weck", "/", "da", "fra\u00df", "ein", "je\u00b7der", "frey", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$(", "ADV", "VVFIN", "ART", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "die Hand war L\u00f6ffelsstatt beym siedendheissen Brey/", "tokens": ["die", "Hand", "war", "L\u00f6f\u00b7felss\u00b7tatt", "beym", "sie\u00b7dend\u00b7heis\u00b7sen", "Brey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "NN", "APPRART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "die dann bi\u00df obenan den Krag- und Magen f\u00fcllten.", "tokens": ["die", "dann", "bi\u00df", "o\u00b7be\u00b7nan", "den", "Krag", "und", "Ma\u00b7gen", "f\u00fcll\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADV", "ART", "TRUNC", "KON", "NN", "VVFIN", "$."], "meter": "-++--+-+-+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.9": {"line.1": {"text": "Ein jeder war vergn\u00fcgt mit dem/ was ihm bescheret;", "tokens": ["Ein", "je\u00b7der", "war", "ver\u00b7gn\u00fcgt", "mit", "dem", "/", "was", "ihm", "be\u00b7sche\u00b7ret", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "VVPP", "APPR", "ART", "$(", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Im Fall ein Donnerkeil sein Eichenhaus ber\u00fchrt/", "tokens": ["Im", "Fall", "ein", "Don\u00b7ner\u00b7keil", "sein", "Ei\u00b7chen\u00b7haus", "be\u00b7r\u00fchrt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "PPOSAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "stund er frisch aufgericht/ kein Feig-kein Bleich-seyn sp\u00fcrt/", "tokens": ["stund", "er", "frisch", "auf\u00b7ge\u00b7richt", "/", "kein", "Feig\u00b7\u00b7kein", "Bleich\u00b7seyn", "sp\u00fcrt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "VVPP", "$(", "PIAT", "NN", "NN", "VVFIN", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.4": {"text": "weil er mit Wissen nie Gewissen je beschweret.", "tokens": ["weil", "er", "mit", "Wis\u00b7sen", "nie", "Ge\u00b7wis\u00b7sen", "je", "be\u00b7schwe\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "ADV", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Ein Mann ein Wort/ ein Wort ein Mann/ hat es geheissen/", "tokens": ["Ein", "Mann", "ein", "Wort", "/", "ein", "Wort", "ein", "Mann", "/", "hat", "es", "ge\u00b7heis\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$(", "ART", "NN", "ART", "NN", "$(", "VAFIN", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "so bald das Wort von Mund/ ward es zu hartem Stein/", "tokens": ["so", "bald", "das", "Wort", "von", "Mund", "/", "ward", "es", "zu", "har\u00b7tem", "Stein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "APPR", "NN", "$(", "VAFIN", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "das Wort/ de\u00df Mannes Wort Ja must und Amen seyn/", "tokens": ["das", "Wort", "/", "de\u00df", "Man\u00b7nes", "Wort", "Ja", "must", "und", "A\u00b7men", "seyn", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "NN", "NN", "NN", "VVFIN", "KON", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Versprechen ward auch Thun/ nicht/ wie jetzt/ W\u00f6rtergleissen.", "tokens": ["Ver\u00b7spre\u00b7chen", "ward", "auch", "Thun", "/", "nicht", "/", "wie", "jetzt", "/", "W\u00f6r\u00b7ter\u00b7gleis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "NN", "$(", "PTKNEG", "$(", "KOKOM", "ADV", "$(", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Ein Bissen Brod im Saltz begliederte die Glieder/", "tokens": ["Ein", "Bis\u00b7sen", "Brod", "im", "Saltz", "be\u00b7glie\u00b7der\u00b7te", "die", "Glie\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPRART", "NN", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "kein Anstrich hielte Strich vom K\u00fchmist au\u00dfgebrant/", "tokens": ["kein", "An\u00b7strich", "hiel\u00b7te", "Strich", "vom", "K\u00fch\u00b7mist", "au\u00df\u00b7ge\u00b7brant", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "NE", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "die Jungfern giengen au\u00df mit Buhlen \u00fcber Land/", "tokens": ["die", "Jung\u00b7fern", "gien\u00b7gen", "au\u00df", "mit", "Buh\u00b7len", "\u00fc\u00b7ber", "Land", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "APPR", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "sie giengen Jungfern weg und kamen Jungfern wieder.", "tokens": ["sie", "gien\u00b7gen", "Jung\u00b7fern", "weg", "und", "ka\u00b7men", "Jung\u00b7fern", "wie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "PTKVZ", "KON", "VVFIN", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Als Teutschland aber sich gantz \u00fcppig wolte kleiden/", "tokens": ["Als", "Teutschland", "a\u00b7ber", "sich", "gantz", "\u00fcp\u00b7pig", "wol\u00b7te", "klei\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ADV", "PRF", "ADV", "ADJD", "VMFIN", "VVINF", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "wards eine Helene/ die Troja \u00e4schert ein/", "tokens": ["wards", "ei\u00b7ne", "He\u00b7le\u00b7ne", "/", "die", "Tro\u00b7ja", "\u00e4sc\u00b7hert", "ein", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$(", "ART", "NE", "VVFIN", "ART", "$("], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "dann wer mit Fremden bult/ bult ihm selbst fremde Pein/", "tokens": ["dann", "wer", "mit", "Frem\u00b7den", "bult", "/", "bult", "ihm", "selbst", "frem\u00b7de", "Pein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "APPR", "NN", "VVFIN", "$(", "VVFIN", "PPER", "ADV", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "und mu\u00df mit rechtem Recht auch fremde Straffen leiden.", "tokens": ["und", "mu\u00df", "mit", "rech\u00b7tem", "Recht", "auch", "frem\u00b7de", "Straf\u00b7fen", "lei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "ADJA", "NN", "ADV", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Bis da\u00df es so weit kam/ da\u00df es auch muste decken", "tokens": ["Bis", "da\u00df", "es", "so", "weit", "kam", "/", "da\u00df", "es", "auch", "mus\u00b7te", "de\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "KOUS", "PPER", "ADV", "ADJD", "VVFIN", "$(", "KOUS", "PPER", "ADV", "VMFIN", "VVINF"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "ein Haderlumpenrock; Statt der Zitronen Safft/", "tokens": ["ein", "Ha\u00b7der\u00b7lum\u00b7pen\u00b7rock", ";", "Statt", "der", "Zit\u00b7ro\u00b7nen", "Safft", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "NN", "ART", "NN", "NN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "must ihr ein nasses Moo\u00df einfl\u00f6ssen Lebenskrafft;", "tokens": ["must", "ihr", "ein", "nas\u00b7ses", "Moo\u00df", "ein\u00b7fl\u00f6s\u00b7sen", "Le\u00b7bens\u00b7krafft", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "ADJA", "NN", "VVPP", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Es mu\u00df ihr au\u00df der Hand nicht Porzellanen schmecken.", "tokens": ["Es", "mu\u00df", "ihr", "au\u00df", "der", "Hand", "nicht", "Por\u00b7zel\u00b7la\u00b7nen", "schme\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "APPR", "ART", "NN", "PTKNEG", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Ihr eckelte vor ihr. Die runtzelschlaffen Wunden", "tokens": ["Ihr", "ec\u00b7kel\u00b7te", "vor", "ihr", ".", "Die", "runt\u00b7zel\u00b7schlaf\u00b7fen", "Wun\u00b7den"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "$.", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "verstellten ihre Haut/ die Z\u00e4hne stunden lo\u00df/", "tokens": ["ver\u00b7stell\u00b7ten", "ih\u00b7re", "Haut", "/", "die", "Z\u00e4h\u00b7ne", "stun\u00b7den", "lo\u00df", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "$(", "ART", "NN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "die Fenster brachen schon/ die Knochen hiengen blo\u00df/", "tokens": ["die", "Fens\u00b7ter", "bra\u00b7chen", "schon", "/", "die", "Kno\u00b7chen", "hien\u00b7gen", "blo\u00df", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "$(", "ART", "NN", "VVFIN", "ADV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "die Glieder waren schlaff/ das gantze Fleisch verschwunden.", "tokens": ["die", "Glie\u00b7der", "wa\u00b7ren", "schlaff", "/", "das", "gant\u00b7ze", "Fleisch", "ver\u00b7schwun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$(", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "In solcher Jammerangst schrieb es auff jhren Knien/", "tokens": ["In", "sol\u00b7cher", "Jam\u00b7me\u00b7rangst", "schrieb", "es", "auff", "jhren", "Kni\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "die Feder war ein Rohr/ die Rinde grob Papyr/", "tokens": ["die", "Fe\u00b7der", "war", "ein", "Rohr", "/", "die", "Rin\u00b7de", "grob", "Pa\u00b7pyr", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$(", "ART", "NN", "ADJD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "die Dint ein Erdenklo\u00df. Ach weh/ ach wehe mir/", "tokens": ["die", "Dint", "ein", "Er\u00b7den\u00b7klo\u00df", ".", "Ach", "weh", "/", "ach", "we\u00b7he", "mir", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$.", "NN", "PTKVZ", "$(", "XY", "VVFIN", "PPER", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "ich werde nimmer nicht als eine Rose bl\u00fcen!", "tokens": ["ich", "wer\u00b7de", "nim\u00b7mer", "nicht", "als", "ei\u00b7ne", "Ro\u00b7se", "bl\u00fcen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "KOUS", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Ach Teutschland nicht mehr Teutsch im Kinderauferziehen/", "tokens": ["Ach", "Teutschland", "nicht", "mehr", "Teutsch", "im", "Kin\u00b7der\u00b7au\u00b7fer\u00b7zie\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "NN", "PTKNEG", "PIAT", "NN", "APPRART", "NN", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "im Fall der Vatter spielt/ so wei\u00df das kleinste Kind/", "tokens": ["im", "Fall", "der", "Vat\u00b7ter", "spielt", "/", "so", "wei\u00df", "das", "kleins\u00b7te", "Kind", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "VVFIN", "$(", "ADV", "VVFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "obs Eicheln oder Gr\u00fcn/ Hertz oder Schellen sind/", "tokens": ["obs", "Ei\u00b7cheln", "o\u00b7der", "Gr\u00fcn", "/", "Hertz", "o\u00b7der", "Schel\u00b7len", "sind", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "$(", "NN", "KON", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "die Tochter auch/ die sich im Spinnen solte m\u00fchen/", "tokens": ["die", "Toch\u00b7ter", "auch", "/", "die", "sich", "im", "Spin\u00b7nen", "sol\u00b7te", "m\u00fc\u00b7hen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$(", "PRELS", "PRF", "APPRART", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.17": {"line.1": {"text": "die schmincket sich mit Schminck; Sie mu\u00df sich pr\u00e4chtig kleiden/", "tokens": ["die", "schmin\u00b7cket", "sich", "mit", "Schminck", ";", "Sie", "mu\u00df", "sich", "pr\u00e4ch\u00b7tig", "klei\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "VVFIN", "PRF", "APPR", "NN", "$.", "PPER", "VMFIN", "PRF", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "spricht gleich der Kasten nein. Der Spiegel sagets ihr/", "tokens": ["spricht", "gleich", "der", "Kas\u00b7ten", "nein", ".", "Der", "Spie\u00b7gel", "sa\u00b7gets", "ihr", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "PTKANT", "$.", "ART", "NN", "VVFIN", "PPER", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.3": {"text": "wie da\u00df das Muschelkorn sey jhres Halses Zier/", "tokens": ["wie", "da\u00df", "das", "Mu\u00b7schel\u00b7korn", "sey", "jhres", "Hal\u00b7ses", "Zier", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOKOM", "KOUS", "ART", "NN", "VAFIN", "PPOSAT", "NN", "NN", "$("], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "die Falcken-Augen gehn/ sich auf der Gasse weiden.", "tokens": ["die", "Fal\u00b7cken\u00b7Au\u00b7gen", "gehn", "/", "sich", "auf", "der", "Gas\u00b7se", "wei\u00b7den", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$(", "PRF", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Die Belgen balgten sich \u00fcm B\u00e4r- und Ochsenh\u00e4ute/", "tokens": ["Die", "Bel\u00b7gen", "balg\u00b7ten", "sich", "\u00fcm", "B\u00e4r", "und", "Och\u00b7sen\u00b7h\u00e4u\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPRART", "TRUNC", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "die Haut bedeckte Haut/ die H\u00f6rner ragten vor/", "tokens": ["die", "Haut", "be\u00b7deck\u00b7te", "Haut", "/", "die", "H\u00f6r\u00b7ner", "rag\u00b7ten", "vor", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$(", "ART", "NN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "es deckte lange", "tokens": ["es", "deck\u00b7te", "lan\u00b7ge"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV"], "meter": "-+-+-", "measure": "iambic.di"}, "line.4": {"text": "H\u00fclff Teut/ h\u00fclf grosser Teut/", "tokens": ["H\u00fclff", "Teut", "/", "h\u00fclf", "gros\u00b7ser", "Teut", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$(", "ADJD", "ADJA", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Im Fall das Wachs noch weich/ kan man mit ihm verfahren;", "tokens": ["Im", "Fall", "das", "Wachs", "noch", "weich", "/", "kan", "man", "mit", "ihm", "ver\u00b7fah\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "ADV", "ADJD", "$(", "VMFIN", "PIS", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Hart/ ist es hart wie Stein/ man dr\u00fcckt und nichts eindr\u00fcckt.", "tokens": ["Hart", "/", "ist", "es", "hart", "wie", "Stein", "/", "man", "dr\u00fcckt", "und", "nichts", "ein\u00b7dr\u00fcckt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "VAFIN", "PPER", "ADJD", "KOKOM", "NN", "$(", "PIS", "VVFIN", "KON", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was klein nicht wird gezogn/ das bleibet ungeschickt/", "tokens": ["Was", "klein", "nicht", "wird", "ge\u00b7zogn", "/", "das", "blei\u00b7bet", "un\u00b7ge\u00b7schickt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "PTKNEG", "VAFIN", "VVPP", "$(", "PDS", "VVFIN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "die Laster nemen zu und wachsen mit den Jahren.", "tokens": ["die", "Las\u00b7ter", "ne\u00b7men", "zu", "und", "wach\u00b7sen", "mit", "den", "Jah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "PTKZU", "KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.20": {"line.1": {"text": "Ach weh/ ach weh/ ich seh an den gestirnten H\u00f6hen", "tokens": ["Ach", "weh", "/", "ach", "weh", "/", "ich", "seh", "an", "den", "ge\u00b7stirn\u00b7ten", "H\u00f6\u00b7hen"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ITJ", "ADV", "$(", "XY", "PTKVZ", "$(", "PPER", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "das Sternliecht/", "tokens": ["das", "Stern\u00b7liecht", "/"], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "das von der Himmelwag l\u00e4ufft durch das J\u00e4gerh\u00fcfft/", "tokens": ["das", "von", "der", "Him\u00b7mel\u00b7wag", "l\u00e4ufft", "durch", "das", "J\u00e4\u00b7ger\u00b7h\u00fcfft", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "ART", "NN", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "und auf den Drachenschwantz mit seinem Schwantze trifft.", "tokens": ["und", "auf", "den", "Dra\u00b7chen\u00b7schwantz", "mit", "sei\u00b7nem", "Schwant\u00b7ze", "trifft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Was h\u00f6ret man doch nicht vor Wunder in den Tagen", "tokens": ["Was", "h\u00f6\u00b7ret", "man", "doch", "nicht", "vor", "Wun\u00b7der", "in", "den", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PIS", "ADV", "PTKNEG", "APPR", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "von Blutgef\u00e4rbter Flut in Wassergr\u00e4ben sagen/", "tokens": ["von", "Blut\u00b7ge\u00b7f\u00e4rb\u00b7ter", "Flut", "in", "Was\u00b7ser\u00b7gr\u00e4\u00b7ben", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "von Bircken in dem Wald/ es lermet in der Lufft", "tokens": ["von", "Bir\u00b7cken", "in", "dem", "Wald", "/", "es", "ler\u00b7met", "in", "der", "Lufft"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "APPR", "ART", "NN", "$(", "PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "geharnschte Reiterey; Es schwermet in der Tufft", "tokens": ["ge\u00b7harnschte", "Rei\u00b7te\u00b7rey", ";", "Es", "schwer\u00b7met", "in", "der", "Tufft"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "$.", "PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.5": {"text": "der Musqvetirer Blitz; Die V\u00f6lcker ziehn zusammen/", "tokens": ["der", "Mus\u00b7qve\u00b7ti\u00b7rer", "Blitz", ";", "Die", "V\u00f6l\u00b7cker", "ziehn", "zu\u00b7sam\u00b7men", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$("], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.6": {"text": "zur Feldschlacht angefrischt; die Wolcken speyen Flammen/", "tokens": ["zur", "Feld\u00b7schlacht", "an\u00b7ge\u00b7frischt", ";", "die", "Wol\u00b7cken", "spe\u00b7yen", "Flam\u00b7men", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$.", "ART", "NN", "VVFIN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "die R\u00fcstung r\u00fchret sich au\u00df heimlicher Gewalt/", "tokens": ["die", "R\u00fcs\u00b7tung", "r\u00fch\u00b7ret", "sich", "au\u00df", "heim\u00b7li\u00b7cher", "Ge\u00b7walt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "h\u00f6rt/ h\u00f6rt/ O b\u00f6se Post! was der Comete halt:", "tokens": ["h\u00f6rt", "/", "h\u00f6rt", "/", "O", "b\u00f6\u00b7se", "Post", "!", "was", "der", "Co\u00b7me\u00b7te", "halt", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "VVFIN", "$(", "NE", "ADJA", "NN", "$.", "PWS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Weh von Ost/ Sud/ Nord/ West! Weh/ weh von allen Wind\u1ebd!", "tokens": ["Weh", "von", "Ost", "/", "Sud", "/", "Nord", "/", "West", "!", "Weh", "/", "weh", "von", "al\u00b7len", "Win\u00b7d\u1ebd", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$.", "NN", "$(", "ADV", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-++-+-+-", "measure": "unknown.measure.septa"}, "line.10": {"text": "Weh/ weh/ weh/ weh/ weh/ weh den Himmelschreier S\u00fcnden/", "tokens": ["Weh", "/", "weh", "/", "weh", "/", "weh", "/", "weh", "/", "weh", "den", "Him\u00b7mel\u00b7schrei\u00b7er", "S\u00fcn\u00b7den", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PTKVZ", "$(", "PTKVZ", "$(", "PTKVZ", "$(", "PTKVZ", "$(", "ADV", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "die Teutschland hat ver\u00fcbt! Weh Tempel! Weh Altar!", "tokens": ["die", "Teutschland", "hat", "ver\u00b7\u00fcbt", "!", "Weh", "Tem\u00b7pel", "!", "Weh", "Al\u00b7tar", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$.", "NN", "NN", "$.", "NN", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "Weh Br\u00e4utigam/ weh Braut! Weh wei\u00dfbereiftes Haar!", "tokens": ["Weh", "Br\u00e4u\u00b7ti\u00b7gam", "/", "weh", "Braut", "!", "Weh", "wei\u00df\u00b7be\u00b7reif\u00b7tes", "Haar", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "NE", "$(", "ADV", "NN", "$.", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Weh noch milchweisses Kien!", "tokens": ["Weh", "noch", "milch\u00b7weis\u00b7ses", "Ki\u00b7en", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.14": {"text": "Ich Mutter sch\u00f6ner St\u00e4dte/", "tokens": ["Ich", "Mut\u00b7ter", "sch\u00f6\u00b7ner", "St\u00e4d\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "ADJA", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.15": {"text": "wo Gott sein Feuerherd/ sein Wohnhaus und Ger\u00e4te/", "tokens": ["wo", "Gott", "sein", "Feu\u00b7er\u00b7herd", "/", "sein", "Wohn\u00b7haus", "und", "Ge\u00b7r\u00e4\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "PPOSAT", "NN", "$(", "PPOSAT", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Ich Teutschland/ weiland ich/ nicht Weltbeherrscherin/", "tokens": ["Ich", "Teutschland", "/", "wei\u00b7land", "ich", "/", "nicht", "Welt\u00b7be\u00b7herr\u00b7sche\u00b7rin", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "NE", "$(", "VVFIN", "PPER", "$(", "PTKNEG", "NN", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.17": {"text": "Amazonin/ ach nicht/ nicht L\u00e4nderk\u00f6niginn", "tokens": ["A\u00b7ma\u00b7zo\u00b7nin", "/", "ach", "nicht", "/", "nicht", "L\u00e4n\u00b7der\u00b7k\u00f6\u00b7ni\u00b7ginn"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word"], "pos": ["NE", "$(", "XY", "PTKNEG", "$(", "PTKNEG", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Gott hat mir Vngel\u00fcck/ ach Vngel\u00fcck! bestellet/", "tokens": ["Gott", "hat", "mir", "Vn\u00b7ge\u00b7l\u00fcck", "/", "ach", "Vn\u00b7ge\u00b7l\u00fcck", "!", "be\u00b7stel\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "NN", "$(", "XY", "NN", "$.", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "da\u00df dem/ ders h\u00f6ren wird/ das Par der Ohren gellet/", "tokens": ["da\u00df", "dem", "/", "ders", "h\u00f6\u00b7ren", "wird", "/", "das", "Par", "der", "Oh\u00b7ren", "gel\u00b7let", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "$(", "ADV", "VVINF", "VAFIN", "$(", "ART", "NN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Blut/ Gut/ Mut gehen drauf. Wo vor raucht ein Altar/", "tokens": ["Blut", "/", "Gut", "/", "Mut", "ge\u00b7hen", "drauf", ".", "Wo", "vor", "raucht", "ein", "Al\u00b7tar", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ADJD", "$(", "NN", "VVFIN", "PTKVZ", "$.", "PWAV", "APPR", "VVFIN", "ART", "NN", "$("], "meter": "+-+--+--++-+", "measure": "trochaic.hexa.relaxed"}, "line.21": {"text": "da w\u00e4chst jetzt Dresp und Dorn. Der dicken B\u00e4ume Schar", "tokens": ["da", "w\u00e4chst", "jetzt", "Dresp", "und", "Dorn", ".", "Der", "di\u00b7cken", "B\u00e4u\u00b7me", "Schar"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "NE", "KON", "NN", "$.", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "beschattet Land und Sand. Mit vier/ f\u00fcnf/ sechs Buchstaben", "tokens": ["be\u00b7schat\u00b7tet", "Land", "und", "Sand", ".", "Mit", "vier", "/", "f\u00fcnf", "/", "sechs", "Buch\u00b7sta\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["VVFIN", "NN", "KON", "NN", "$.", "APPR", "CARD", "$(", "CARD", "$(", "CARD", "NN"], "meter": "-+-+-+-+--++-", "measure": "iambic.hexa.relaxed"}, "line.23": {"text": "hat mich Gott heimgesucht; die wilden Landsknecht haben", "tokens": ["hat", "mich", "Gott", "heim\u00b7ge\u00b7sucht", ";", "die", "wil\u00b7den", "Lands\u00b7knecht", "ha\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "NN", "VVFIN", "$.", "ART", "ADJA", "NN", "VAFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "He/ He/ He jubilirt. Ihr Reiter auch zu Hauff/", "tokens": ["He", "/", "He", "/", "He", "ju\u00b7bi\u00b7lirt", ".", "Ihr", "Rei\u00b7ter", "auch", "zu", "Hauff", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "VVPP", "$.", "PPOSAT", "NN", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "macht euch \u00fcm Teutschland her/ werfft Sch\u00fctt und Schantzen auf.", "tokens": ["macht", "euch", "\u00fcm", "Teutschland", "her", "/", "werfft", "Sch\u00fctt", "und", "Schant\u00b7zen", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "PTKVZ", "$(", "VVFIN", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.26": {"text": "Ich armes Teutschland h\u00e4ng die Harfen an die Weiden/", "tokens": ["Ich", "ar\u00b7mes", "Teutschland", "h\u00e4ng", "die", "Har\u00b7fen", "an", "die", "Wei\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.27": {"text": "ich wei\u00df kein einig Lied zu spielen in dem Leiden/", "tokens": ["ich", "wei\u00df", "kein", "ei\u00b7nig", "Lied", "zu", "spie\u00b7len", "in", "dem", "Lei\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJD", "NN", "PTKZU", "VVINF", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "wann mir mein edles Land nur zu Gedancken k\u00f6mmt/", "tokens": ["wann", "mir", "mein", "ed\u00b7les", "Land", "nur", "zu", "Ge\u00b7dan\u00b7cken", "k\u00f6mmt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPOSAT", "ADJA", "NN", "ADV", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "es mir au\u00df meinen Sinn die Hertzens-Freude nimmt.", "tokens": ["es", "mir", "au\u00df", "mei\u00b7nen", "Sinn", "die", "Hert\u00b7zens\u00b7Freu\u00b7de", "nimmt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "APPR", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Die Priesterschafft ist weg/ die B\u00fcrger au\u00dfgezogen/", "tokens": ["Die", "Pries\u00b7ter\u00b7schafft", "ist", "weg", "/", "die", "B\u00fcr\u00b7ger", "au\u00df\u00b7ge\u00b7zo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "$(", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "der Wald ist ohne Wild/ die V\u00f6gel weggeflogen/", "tokens": ["der", "Wald", "ist", "oh\u00b7ne", "Wild", "/", "die", "V\u00f6\u00b7gel", "weg\u00b7ge\u00b7flo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPR", "NE", "$(", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "mir ist/ als einer ist in erster Kindes-Noht:", "tokens": ["mir", "ist", "/", "als", "ei\u00b7ner", "ist", "in", "ers\u00b7ter", "Kin\u00b7des\u00b7Noht", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$(", "KOUS", "PIS", "VAFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Ach Angst/ die mehr als Angst! O Tod/ der mehr als Tod!", "tokens": ["Ach", "Angst", "/", "die", "mehr", "als", "Angst", "!", "O", "Tod", "/", "der", "mehr", "als", "Tod", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "NN", "$(", "ART", "PIAT", "KOKOM", "NN", "$.", "NE", "NN", "$(", "ART", "PIAT", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Nit Schau-Thal/ W\u00fcrge-Thal.", "tokens": ["Nit", "Schau\u00b7Thal", "/", "W\u00fcr\u00b7ge\u00b7Thal", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PTKNEG", "NN", "$(", "NE", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.35": {"text": "man h\u00f6rt kein Freudenlied/", "tokens": ["man", "h\u00f6rt", "kein", "Freu\u00b7den\u00b7lied", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.36": {"text": "es klipklapt klippeklapt kein M\u00fchlwerck nah und fern/", "tokens": ["es", "klipk\u00b7lapt", "klip\u00b7pe\u00b7klapt", "kein", "M\u00fchl\u00b7werck", "nah", "und", "fern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "VVFIN", "PIAT", "NN", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.37": {"text": "kein Weirauch rauchet mehr/ kein Liecht leucht in Latern.", "tokens": ["kein", "Wei\u00b7rauch", "rau\u00b7chet", "mehr", "/", "kein", "Liecht", "leucht", "in", "La\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "ADV", "$(", "PIAT", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.38": {"text": "Es ist das Schwert gefegt/ gefegt/ da\u00df es soll blincken/", "tokens": ["Es", "ist", "das", "Schwert", "ge\u00b7fegt", "/", "ge\u00b7fegt", "/", "da\u00df", "es", "soll", "blin\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVPP", "$(", "VVPP", "$(", "KOUS", "PPER", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "es blinckt das Schwert/ es flinckt zur Rechten und zur Lincken/", "tokens": ["es", "blinckt", "das", "Schwert", "/", "es", "flinckt", "zur", "Rech\u00b7ten", "und", "zur", "Lin\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$(", "PPER", "VVFIN", "APPRART", "NN", "KON", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "der F\u00fcrst ist L\u00f6wenart/ der Seher ist ein Schalck/", "tokens": ["der", "F\u00fcrst", "ist", "L\u00f6\u00b7wen\u00b7art", "/", "der", "Se\u00b7her", "ist", "ein", "Schalck", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "NN", "$(", "ART", "NN", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "er plappert Fabelwerck und t\u00fcncht mit losem Kalck.", "tokens": ["er", "plap\u00b7pert", "Fa\u00b7bel\u00b7werck", "und", "t\u00fcncht", "mit", "lo\u00b7sem", "Kalck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "KON", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Dr\u00fcm mu\u00df nun Gott der Herr sich Letztens mit mir letzen/", "tokens": ["Dr\u00fcm", "mu\u00df", "nun", "Gott", "der", "Herr", "sich", "Letz\u00b7tens", "mit", "mir", "let\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ADV", "NN", "ART", "NN", "PRF", "ADV", "APPR", "PPER", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "der Straffen Langsamseyn mit Sch\u00e4rferseyn", "tokens": ["der", "Straf\u00b7fen", "Lang\u00b7sam\u00b7seyn", "mit", "Sch\u00e4r\u00b7fer\u00b7seyn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Dann wo kein Warnen hilfft/ greifft er ergrimmet ein.", "tokens": ["Dann", "wo", "kein", "War\u00b7nen", "hilfft", "/", "greifft", "er", "er\u00b7grim\u00b7met", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWAV", "PIAT", "NN", "VVFIN", "$(", "VVFIN", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Wie wann ein Quaderst\u00fcck vom j\u00fcngstgebrochnem Stein", "tokens": ["Wie", "wann", "ein", "Qua\u00b7der\u00b7st\u00fcck", "vom", "j\u00fcngst\u00b7ge\u00b7broch\u00b7nem", "Stein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PWAV", "ART", "NN", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "sich schwerlich heben l\u00e4sst/ k\u00f6mmt es einst in das Lauffen/", "tokens": ["sich", "schwer\u00b7lich", "he\u00b7ben", "l\u00e4sst", "/", "k\u00f6mmt", "es", "einst", "in", "das", "Lauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "VVINF", "VVFIN", "$(", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "so schmeist es Stock und Stein/ und was es trifft/ in Hauffen:", "tokens": ["so", "schmeist", "es", "Stock", "und", "Stein", "/", "und", "was", "es", "trifft", "/", "in", "Hauf\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "KON", "NN", "$(", "KON", "PWS", "PPER", "VVFIN", "$(", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So grimmt de\u00df H\u00f6chsten Grimm. Es ist gantz mit mir au\u00df/", "tokens": ["So", "grimmt", "de\u00df", "H\u00f6chs\u00b7ten", "Grimm", ".", "Es", "ist", "gantz", "mit", "mir", "au\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NE", "$.", "PPER", "VAFIN", "ADV", "APPR", "PPER", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "es mu\u00df geschleiffet seyn der Alemannen", "tokens": ["es", "mu\u00df", "ge\u00b7schleif\u00b7fet", "seyn", "der", "A\u00b7le\u00b7man\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "VVPP", "VAINF", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Wie treu die Kluckhe\u00f1", "tokens": ["Wie", "treu", "die", "Kluck\u00b7he\u00f1"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ART", "NN"], "meter": "-+---", "measure": "dactylic.init"}, "line.10": {"text": "dem noch nicht pfl\u00fccken Volck geklocket und gelocket", "tokens": ["dem", "noch", "nicht", "pfl\u00fc\u00b7cken", "Volck", "ge\u00b7klo\u00b7cket", "und", "ge\u00b7lo\u00b7cket"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "PTKNEG", "VVINF", "NN", "VVPP", "KON", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "klock/ klock/ doch sonder Frucht. Ich bin zu Grund gericht/", "tokens": ["klock", "/", "klock", "/", "doch", "son\u00b7der", "Frucht", ".", "Ich", "bin", "zu", "Grund", "ge\u00b7richt", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "XY", "$(", "ADV", "ADJA", "NN", "$.", "PPER", "VAFIN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "kein Rahten r\u00e4htet mir/ kein Helfen hilfet nicht", "tokens": ["kein", "Rah\u00b7ten", "r\u00e4h\u00b7tet", "mir", "/", "kein", "Hel\u00b7fen", "hil\u00b7fet", "nicht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "$(", "PIAT", "NN", "VVFIN", "PTKNEG"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "als dieses blancke Schwert.", "tokens": ["als", "die\u00b7ses", "blan\u00b7cke", "Schwert", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.14": {"text": "mein h\u00f6rend Ohr wird taub/ der Augen Liecht gebricht/", "tokens": ["mein", "h\u00f6\u00b7rend", "Ohr", "wird", "taub", "/", "der", "Au\u00b7gen", "Liecht", "ge\u00b7bricht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJD", "NN", "VAFIN", "ADJD", "$(", "ART", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "die Zunge steht gehemmt/ der m\u00fcde Puls schl\u00e4gt nicht/", "tokens": ["die", "Zun\u00b7ge", "steht", "ge\u00b7hemmt", "/", "der", "m\u00fc\u00b7de", "Puls", "schl\u00e4gt", "nicht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "VVPP", "$(", "ART", "ADJA", "NN", "VVFIN", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "kein Landkind streicht mich an/ man l\u00e4sst mich Krafftlo\u00df ligen.", "tokens": ["kein", "Land\u00b7kind", "streicht", "mich", "an", "/", "man", "l\u00e4sst", "mich", "Kraff\u00b7tlo\u00df", "li\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPER", "PTKVZ", "$(", "PIS", "VVFIN", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Psy/ psy! Ich niese noch/ kein Landsmann/ helff Gott/ saget/", "tokens": ["Psy", "/", "psy", "!", "Ich", "nie\u00b7se", "noch", "/", "kein", "Lands\u00b7mann", "/", "helff", "Gott", "/", "sa\u00b7get", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$.", "PPER", "VVFIN", "ADV", "$(", "PIAT", "NN", "$(", "VVFIN", "NN", "$(", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "nun tausend gute Nacht/ Ich habe mich geletzt/", "tokens": ["nun", "tau\u00b7send", "gu\u00b7te", "Nacht", "/", "Ich", "ha\u00b7be", "mich", "ge\u00b7letzt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "ADJA", "NN", "$(", "PPER", "VAFIN", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "und keinen Menschen nicht zum Erben eingesetzt", "tokens": ["und", "kei\u00b7nen", "Men\u00b7schen", "nicht", "zum", "Er\u00b7ben", "ein\u00b7ge\u00b7setzt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "PTKNEG", "APPRART", "NN", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "weil mich kein Mensche nicht mit Klageweibern klaget.", "tokens": ["weil", "mich", "kein", "Men\u00b7sche", "nicht", "mit", "Kla\u00b7ge\u00b7wei\u00b7bern", "kla\u00b7get", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "NN", "PTKNEG", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Ich Teutschland bin ein As/ bin selbst mein Todengr\u00e4ber/", "tokens": ["Ich", "Teutschland", "bin", "ein", "As", "/", "bin", "selbst", "mein", "To\u00b7den\u00b7gr\u00e4\u00b7ber", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VAFIN", "ART", "NN", "$(", "VAFIN", "ADV", "PPOSAT", "NN", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.22": {"text": "schick mir mein Grabmahl zu/ z\u00fcnd meinen Holtzsto\u00df", "tokens": ["schick", "mir", "mein", "Grab\u00b7mahl", "zu", "/", "z\u00fcnd", "mei\u00b7nen", "Holtz\u00b7sto\u00df"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVIMP", "PPER", "PPOSAT", "NN", "PTKZU", "$(", "VVFIN", "PPOSAT", "NN"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.23": {"text": "So brante Troja vor/ sagt fr\u00f6lich jederman/", "tokens": ["So", "bran\u00b7te", "Tro\u00b7ja", "vor", "/", "sagt", "fr\u00f6\u00b7lich", "je\u00b7der\u00b7man", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "PTKVZ", "$(", "VVFIN", "ADJD", "PIS", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "ich brenn und werd ein Brand/ mein Land ein Leichenl\u00e4ger.", "tokens": ["ich", "brenn", "und", "werd", "ein", "Brand", "/", "mein", "Land", "ein", "Lei\u00b7chen\u00b7l\u00e4\u00b7ger", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VAFIN", "ART", "NN", "$(", "PPOSAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.23": {"line.1": {"text": "Teutschland/ Teutschland die betr\u00fcbte/ gantzzergliedert/ abgesehnt", "tokens": ["Teutschland", "/", "Teutschland", "die", "be\u00b7tr\u00fcb\u00b7te", "/", "gantz\u00b7zer\u00b7glie\u00b7dert", "/", "ab\u00b7ge\u00b7sehnt"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$(", "NE", "ART", "VVFIN", "$(", "VVPP", "$(", "VVPP"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "fast entmarcket alles Marcks/ Schlaffesvoll und au\u00dfgethrent/", "tokens": ["fast", "ent\u00b7mar\u00b7cket", "al\u00b7les", "Marcks", "/", "Schlaf\u00b7fes\u00b7voll", "und", "au\u00df\u00b7ge\u00b7thrent", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "$(", "ADJD", "KON", "VVPP", "$("], "meter": "+-+-+--+-+-+-+", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "hielt sich dort am Pegnitzsande", "tokens": ["hielt", "sich", "dort", "am", "Peg\u00b7nitz\u00b7san\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADV", "APPRART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "ihrer Heimat Mittellande.", "tokens": ["ih\u00b7rer", "Hei\u00b7mat", "Mit\u00b7tel\u00b7lan\u00b7de", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Vm zu sehen/\u00fcm zu h\u00f6rn/ ob kein Pflaster sey zu finden/", "tokens": ["Vm", "zu", "se\u00b7hen", "/", "\u00fcm", "zu", "h\u00f6rn", "/", "ob", "kein", "Pflas\u00b7ter", "sey", "zu", "fin\u00b7den", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKZU", "VVINF", "$(", "ADJD", "PTKZU", "VVINF", "$(", "KOUS", "PIAT", "NN", "VAFIN", "PTKZU", "VVINF", "$("], "meter": "+-+-+-+--+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.6": {"text": "ob dann keine Salbe nicht/ die Verwundte zu verbinden/", "tokens": ["ob", "dann", "kei\u00b7ne", "Sal\u00b7be", "nicht", "/", "die", "Ver\u00b7wund\u00b7te", "zu", "ver\u00b7bin\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "NN", "PTKNEG", "$(", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "+-+-+--+-+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.7": {"text": "strecket sich hin in die Matten/", "tokens": ["stre\u00b7cket", "sich", "hin", "in", "die", "Mat\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "schlummert unter Bl\u00e4tterschatten.", "tokens": ["schlum\u00b7mert", "un\u00b7ter", "Bl\u00e4t\u00b7ter\u00b7schat\u00b7ten", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.9": {"text": "Sie tr\u00e4umt in dem s\u00fcssen Schlaff\u1ebd/ ihr Heil sey nun nimmer fern/", "tokens": ["Sie", "tr\u00e4umt", "in", "dem", "s\u00fcs\u00b7sen", "Schlaff\u1ebd", "/", "ihr", "Heil", "sey", "nun", "nim\u00b7mer", "fern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$(", "PPOSAT", "NN", "VAFIN", "ADV", "ADV", "ADJD", "$("], "meter": "-+--+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "weil nun dreissig Jahr verwichen/ seid der Weck-u\u00f1 Schreckestern", "tokens": ["weil", "nun", "dreis\u00b7sig", "Jahr", "ver\u00b7wi\u00b7chen", "/", "seid", "der", "Weck\u00b7u\u00f1", "Schre\u00b7ckes\u00b7tern"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "CARD", "NN", "VVINF", "$(", "VAFIN", "ART", "NN", "NN"], "meter": "+-+-+-+-+-+-+--", "measure": "unknown.measure.septa"}, "line.11": {"text": "an der blauen Burg ward funden/", "tokens": ["an", "der", "blau\u00b7en", "Burg", "ward", "fun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VAFIN", "VVFIN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.12": {"text": "Besengleich vom Reis gebunden.", "tokens": ["Be\u00b7sen\u00b7gleich", "vom", "Reis", "ge\u00b7bun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPRART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.24": {"line.1": {"text": "Der/ die im Traume ligt/ k\u00f6mmt vor/ als k\u00e4m geflogen", "tokens": ["Der", "/", "die", "im", "Trau\u00b7me", "ligt", "/", "k\u00f6mmt", "vor", "/", "als", "k\u00e4m", "ge\u00b7flo\u00b7gen"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "$(", "ART", "APPRART", "NN", "VVFIN", "$(", "VVFIN", "PTKVZ", "$(", "KOUS", "ADJD", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "ein Adler/ der gekr\u00f6nt/ der br\u00e4cht vom Wolckenbogen", "tokens": ["ein", "Ad\u00b7ler", "/", "der", "ge\u00b7kr\u00f6nt", "/", "der", "br\u00e4cht", "vom", "Wol\u00b7cken\u00b7bo\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "ART", "VVPP", "$(", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "ihr einen Lorberkrantz/ mit welchem wird \u00fcmlaubt", "tokens": ["ihr", "ei\u00b7nen", "Lor\u00b7ber\u00b7krantz", "/", "mit", "wel\u00b7chem", "wird", "\u00fcm\u00b7laubt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ART", "NN", "$(", "APPR", "PWAT", "VAFIN", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "ihr gantz verworfnes Haar/ ihr gantz verwornes Haubt.", "tokens": ["ihr", "gantz", "ver\u00b7worf\u00b7nes", "Haar", "/", "ihr", "gantz", "ver\u00b7wor\u00b7nes", "Haubt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADJA", "NN", "$(", "PPER", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ein J\u00fcngling st\u00fcnd vor ihr/ der ihr zur Nasen streckte", "tokens": ["Ein", "J\u00fcng\u00b7ling", "st\u00fcnd", "vor", "ihr", "/", "der", "ihr", "zur", "Na\u00b7sen", "streck\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPR", "PPOSAT", "$(", "PRELS", "PPER", "APPRART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "den Liljenp\u00fcschel hin; die Hand ein L\u00f6we leckte", "tokens": ["den", "Lil\u00b7jen\u00b7p\u00fc\u00b7schel", "hin", ";", "die", "Hand", "ein", "L\u00f6\u00b7we", "leck\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PTKVZ", "$.", "ART", "NN", "ART", "NE", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "mit frommer Freundlichkeit. Ihr Haubt bek\u00f6mmet Krafft/", "tokens": ["mit", "from\u00b7mer", "Freund\u00b7lich\u00b7keit", ".", "Ihr", "Haubt", "be\u00b7k\u00f6m\u00b7met", "Krafft", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$.", "PPOSAT", "NN", "VVFIN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "es pochet st\u00e4rcker an der rote Lebenssafft.", "tokens": ["es", "po\u00b7chet", "st\u00e4r\u00b7cker", "an", "der", "ro\u00b7te", "Le\u00b7bens\u00b7safft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Sie/ sie die fromme Frau im Schlaffe heimlich lachte", "tokens": ["Sie", "/", "sie", "die", "from\u00b7me", "Frau", "im", "Schlaf\u00b7fe", "heim\u00b7lich", "lach\u00b7te"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "$(", "PPER", "ART", "ADJA", "NN", "APPRART", "NN", "ADJD", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "ob dem vers\u00fcsten Traum; inde\u00df sie selbst erwachte", "tokens": ["ob", "dem", "ver\u00b7s\u00fcs\u00b7ten", "Traum", ";", "in\u00b7de\u00df", "sie", "selbst", "er\u00b7wach\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN", "$.", "ADV", "PPER", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "halb froh und halb gesund. Dann Tr\u00e4ume tr\u00fcgen offt", "tokens": ["halb", "froh", "und", "halb", "ge\u00b7sund", ".", "Dann", "Tr\u00e4u\u00b7me", "tr\u00fc\u00b7gen", "offt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "ADJD", "KON", "ADJD", "VVPP", "$.", "ADV", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "und sagt der Au\u00dfgang au\u00df/ da\u00df/ was gehofft/ verhofft.", "tokens": ["und", "sagt", "der", "Au\u00df\u00b7gang", "au\u00df", "/", "da\u00df", "/", "was", "ge\u00b7hofft", "/", "ver\u00b7hofft", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKVZ", "$(", "KOUS", "$(", "PWS", "VVPP", "$(", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.25": {"line.1": {"text": "Es mu\u00df ein b\u00f6ses/ b\u00f6ses Leben", "tokens": ["Es", "mu\u00df", "ein", "b\u00f6\u00b7ses", "/", "b\u00f6\u00b7ses", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "$(", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "ein b\u00f6ses/ b\u00f6ses Ende geben.", "tokens": ["ein", "b\u00f6\u00b7ses", "/", "b\u00f6\u00b7ses", "En\u00b7de", "ge\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$(", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Man hat den Himmel aufgereitzet/", "tokens": ["Man", "hat", "den", "Him\u00b7mel", "auf\u00b7ge\u00b7reit\u00b7zet", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "gew\u00fcrgt/ geraubet und gegeitzet;", "tokens": ["ge\u00b7w\u00fcrgt", "/", "ge\u00b7rau\u00b7bet", "und", "ge\u00b7geit\u00b7zet", ";"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Au\u00df Menschen Gold und Geld geschmeltzet/", "tokens": ["Au\u00df", "Men\u00b7schen", "Gold", "und", "Geld", "ge\u00b7schmelt\u00b7zet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "in allen Lastern sich geweltzet/", "tokens": ["in", "al\u00b7len", "Las\u00b7tern", "sich", "ge\u00b7welt\u00b7zet", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "PRF", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "die gantze Nacht verbracht mit Sauffen/", "tokens": ["die", "gant\u00b7ze", "Nacht", "ver\u00b7bracht", "mit", "Sauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "APPR", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "au\u00df Brunst ins Hurenhaus gelauffen/", "tokens": ["au\u00df", "Brunst", "ins", "Hu\u00b7ren\u00b7haus", "ge\u00b7lauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "de\u00df Tags geschnarcht/ de\u00df Nachts gewachet/", "tokens": ["de\u00df", "Tags", "ge\u00b7schnarcht", "/", "de\u00df", "Nachts", "ge\u00b7wa\u00b7chet", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$(", "ART", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "au\u00df Nacht Tag/ Tag au\u00df Nacht gemachet/", "tokens": ["au\u00df", "Nacht", "Tag", "/", "Tag", "au\u00df", "Nacht", "ge\u00b7ma\u00b7chet", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "$(", "NN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "die reichen Wasser au\u00dfgeleeret/", "tokens": ["die", "rei\u00b7chen", "Was\u00b7ser", "au\u00df\u00b7ge\u00b7lee\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "die V\u00f6gel in der Lufft verzehret.", "tokens": ["die", "V\u00f6\u00b7gel", "in", "der", "Lufft", "ver\u00b7zeh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Doch muntert sie sich auf/ und geht nach dem Geth\u00f6ne/", "tokens": ["Doch", "mun\u00b7tert", "sie", "sich", "auf", "/", "und", "geht", "nach", "dem", "Ge\u00b7th\u00f6\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PRF", "APPR", "$(", "KON", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "durch Leitung ihres Ohrs/ da lachet sie Irene/", "tokens": ["durch", "Lei\u00b7tung", "ih\u00b7res", "Ohrs", "/", "da", "la\u00b7chet", "sie", "I\u00b7re\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PPOSAT", "NN", "$(", "ADV", "VVFIN", "PPER", "NE", "$("], "meter": "-+-+-+-+--+--", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "die Friedens-G\u00f6ttin/ an/ in ihrer Zier und Pracht/", "tokens": ["die", "Frie\u00b7dens\u00b7G\u00f6t\u00b7tin", "/", "an", "/", "in", "ih\u00b7rer", "Zier", "und", "Pracht", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PTKVZ", "$(", "APPR", "PPOSAT", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "hat Teutschland in dem Traum gelacht/ es noch mehr lacht", "tokens": ["hat", "Teutschland", "in", "dem", "Traum", "ge\u00b7lacht", "/", "es", "noch", "mehr", "lacht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "NE", "APPR", "ART", "NN", "VVPP", "$(", "PPER", "ADV", "ADV", "VVFIN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "und voller Freuden steht; Der Fried ist sch\u00f6n bekr\u00e4ntzet", "tokens": ["und", "vol\u00b7ler", "Freu\u00b7den", "steht", ";", "Der", "Fried", "ist", "sch\u00f6n", "be\u00b7kr\u00b7\u00e4nt\u00b7zet"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "VVFIN", "$.", "ART", "NN", "VAFIN", "ADJD", "VVFIN"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "mit Fr\u00fcchten \u00fcm und \u00fcm; in seiner Lincken gl\u00e4ntzet", "tokens": ["mit", "Fr\u00fcch\u00b7ten", "\u00fcm", "und", "\u00fcm", ";", "in", "sei\u00b7ner", "Lin\u00b7cken", "gl\u00e4nt\u00b7zet"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJD", "KON", "ADJD", "$.", "APPR", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "der gr\u00fcne Palmenzweig; es gr\u00fcnt die rechte Hand", "tokens": ["der", "gr\u00fc\u00b7ne", "Pal\u00b7men\u00b7zweig", ";", "es", "gr\u00fcnt", "die", "rech\u00b7te", "Hand"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$.", "PPER", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "von einem Lorbeerkrantz/ es spielet sein Gewand", "tokens": ["von", "ei\u00b7nem", "Lor\u00b7beer\u00b7krantz", "/", "es", "spie\u00b7let", "sein", "Ge\u00b7wand"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$(", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "im Friede mit dem Wind/ die viergetheilte Zeiten", "tokens": ["im", "Frie\u00b7de", "mit", "dem", "Wind", "/", "die", "vier\u00b7ge\u00b7theil\u00b7te", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "de\u00df Friedenreichen Jahrs die h\u00fcpfen auf den Seiten", "tokens": ["de\u00df", "Frie\u00b7den\u00b7rei\u00b7chen", "Jahrs", "die", "h\u00fcp\u00b7fen", "auf", "den", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "au\u00df Dienstpflichtschuldigkeit/ ein jede macht sich kraus", "tokens": ["au\u00df", "Dienst\u00b7pflicht\u00b7schul\u00b7dig\u00b7keit", "/", "ein", "je\u00b7de", "macht", "sich", "kraus"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$(", "ART", "PIAT", "VVFIN", "PRF", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "und streichet ihren Dienst mit Springereimen au\u00df.", "tokens": ["und", "strei\u00b7chet", "ih\u00b7ren", "Dienst", "mit", "Sprin\u00b7ge\u00b7rei\u00b7men", "au\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}