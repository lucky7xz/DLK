{"textgrid.poem.24302": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "Prolog zur Jobsiade", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie der Verfasser der Jobsiade", "tokens": ["Wie", "der", "Ver\u00b7fas\u00b7ser", "der", "Job\u00b7si\u00b7a\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "ART", "NN"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "Lebte und meinte, und was er tate,", "tokens": ["Leb\u00b7te", "und", "mein\u00b7te", ",", "und", "was", "er", "ta\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "VVFIN", "$,", "KON", "PWS", "PPER", "VVFIN", "$,"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Steht im Meyer und im Brockhaus", "tokens": ["Steht", "im", "Me\u00b7yer", "und", "im", "Brock\u00b7haus"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPRART", "NN", "KON", "APPRART", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Gr\u00fcndlich, ausf\u00fchrlich und durchaus.", "tokens": ["Gr\u00fcnd\u00b7lich", ",", "aus\u00b7f\u00fchr\u00b7lich", "und", "durc\u00b7haus", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADJD", "KON", "ADV", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.2": {"line.1": {"text": "Auch ist vollkommen klein schon gespalten,", "tokens": ["Auch", "ist", "voll\u00b7kom\u00b7men", "klein", "schon", "ge\u00b7spal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJD", "ADJD", "ADV", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Was man von seinem Humore mu\u00df halten,", "tokens": ["Was", "man", "von", "sei\u00b7nem", "Hu\u00b7mo\u00b7re", "mu\u00df", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "APPR", "PPOSAT", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und was von seiner Knittelei", "tokens": ["Und", "was", "von", "sei\u00b7ner", "Knit\u00b7te\u00b7lei"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Quoad \u00c4sthetik zu meinen sei.", "tokens": ["Quo\u00b7ad", "\u00c4s\u00b7the\u00b7tik", "zu", "mei\u00b7nen", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPR", "PPOSAT", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Dar\u00fcber legten so Bartels wie Meyer", "tokens": ["Da\u00b7r\u00fc\u00b7ber", "leg\u00b7ten", "so", "Bar\u00b7tels", "wie", "Me\u00b7yer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ADV", "NN", "KOKOM", "NE"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Entz\u00fcckend ovale kritische Eier,", "tokens": ["Ent\u00b7z\u00fc\u00b7ckend", "o\u00b7va\u00b7le", "kri\u00b7ti\u00b7sche", "Ei\u00b7er", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "ADJA", "NN", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und \u00fcbrigens liebt das Publikum", "tokens": ["Und", "\u00fcb\u00b7ri\u00b7gens", "liebt", "das", "Pub\u00b7li\u00b7kum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hundert Jahre und l\u00e4nger Hieronimum.", "tokens": ["Hun\u00b7dert", "Jah\u00b7re", "und", "l\u00e4n\u00b7ger", "Hie\u00b7ro\u00b7ni\u00b7mum", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "KON", "ADJD", "NE", "$."], "meter": "+-+--+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Bin also einigerma\u00dfen verlegen,", "tokens": ["Bin", "al\u00b7so", "ei\u00b7ni\u00b7ger\u00b7ma\u00b7\u00dfen", "ver\u00b7le\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Was ", "tokens": ["Was"], "token_info": ["word"], "pos": ["PWS"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Bei allem Dr\u00fccken und aller Qual", "tokens": ["Bei", "al\u00b7lem", "Dr\u00fc\u00b7cken", "und", "al\u00b7ler", "Qual"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIS", "NN", "KON", "PIAT", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wird schlie\u00dflich meins auch blo\u00df oval.", "tokens": ["Wird", "schlie\u00df\u00b7lich", "meins", "auch", "blo\u00df", "o\u00b7val", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVFIN", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Seis drum: ich legs. Gleich andern wackern", "tokens": ["Seis", "drum", ":", "ich", "legs", ".", "Gleich", "an\u00b7dern", "wa\u00b7ckern"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["FM.la", "FM.la", "$.", "PPER", "ADV", "$.", "ADV", "PIS", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kritischen Hennen kann ich auch gackern,", "tokens": ["Kri\u00b7ti\u00b7schen", "Hen\u00b7nen", "kann", "ich", "auch", "ga\u00b7ckern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Und, legt mein Ei man in Kortums Sol,", "tokens": ["Und", ",", "legt", "mein", "Ei", "man", "in", "Kor\u00b7tums", "Sol", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "VVFIN", "PPOSAT", "NN", "PIS", "APPR", "NN", "VMFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Findt mans auch schlie\u00dflich gesalzen wohl.", "tokens": ["Findt", "mans", "auch", "schlie\u00df\u00b7lich", "ge\u00b7sal\u00b7zen", "wohl", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "ADJD", "VVPP", "ADV", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.6": {"line.1": {"text": "Und so beginn ich denn unverweilen", "tokens": ["Und", "so", "be\u00b7ginn", "ich", "denn", "un\u00b7ver\u00b7wei\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "ADJA"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das Allerwichtigste mitzuteilen:", "tokens": ["Das", "Al\u00b7ler\u00b7wich\u00b7tigs\u00b7te", "mit\u00b7zu\u00b7tei\u00b7len", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Karl Arnold Kortum, Doktor der Medizin", "tokens": ["Karl", "Ar\u00b7nold", "Kor\u00b7tum", ",", "Dok\u00b7tor", "der", "Me\u00b7di\u00b7zin"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "NE", "NE", "$,", "NN", "ART", "NN"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "war ", "tokens": ["war"], "token_info": ["word"], "pos": ["VAFIN"], "meter": "-", "measure": "single.down"}}, "stanza.7": {"line.1": {"text": "Denn er hielte nicht wenig auf seinen Magen", "tokens": ["Denn", "er", "hiel\u00b7te", "nicht", "we\u00b7nig", "auf", "sei\u00b7nen", "Ma\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "PPOSAT", "NN"], "meter": "--+--+--+-+-", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Und meinte, das Hungert\u00fcchernagen", "tokens": ["Und", "mein\u00b7te", ",", "das", "Hun\u00b7ge\u00b7rt\u00fc\u00b7cher\u00b7na\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["KON", "VVFIN", "$,", "ART", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Sei weder gesund, noch angenehm;", "tokens": ["Sei", "we\u00b7der", "ge\u00b7sund", ",", "noch", "an\u00b7ge\u00b7nehm", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAIMP", "KON", "ADJD", "$,", "ADV", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Drum dichtete er blo\u00df au\u00dferdem.", "tokens": ["Drum", "dich\u00b7te\u00b7te", "er", "blo\u00df", "au\u00b7\u00dfer\u00b7dem", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "PAV", "$."], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}}, "stanza.8": {"line.1": {"text": "Und legte sich flei\u00dfig aufs Krankekurieren,", "tokens": ["Und", "leg\u00b7te", "sich", "flei\u00b7\u00dfig", "aufs", "Kran\u00b7ke\u00b7ku\u00b7rie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADJD", "APPRART", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Oper-, Purg-, Ordin- und Medizinieren,", "tokens": ["O\u00b7per", ",", "Pur\u00b7g", ",", "Or\u00b7din", "und", "Me\u00b7di\u00b7zi\u00b7nie\u00b7ren", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["TRUNC", "$,", "TRUNC", "$,", "TRUNC", "KON", "NN", "$,"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Weshalb ihm die ", "tokens": ["We\u00b7shalb", "ihm", "die"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "PPER", "ART"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Und nicht die Poeten ein Denkmal gesetzt.", "tokens": ["Und", "nicht", "die", "Po\u00b7et\u00b7en", "ein", "Denk\u00b7mal", "ge\u00b7setzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+---+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.9": {"line.1": {"text": "Doch ist es gewi\u00df, da\u00df von seinen Rezepten", "tokens": ["Doch", "ist", "es", "ge\u00b7wi\u00df", ",", "da\u00df", "von", "sei\u00b7nen", "Re\u00b7zep\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PPER", "ADV", "$,", "KOUS", "APPR", "PPOSAT", "NN"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Ihn keine, doch Verse viel \u00fcberlebten.", "tokens": ["Ihn", "kei\u00b7ne", ",", "doch", "Ver\u00b7se", "viel", "\u00fc\u00b7ber\u00b7leb\u00b7ten", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "$,", "KON", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Das Rezipe schuf den Bauch ihm breit,", "tokens": ["Das", "Re\u00b7zi\u00b7pe", "schuf", "den", "Bauch", "ihm", "breit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "PPER", "ADJD", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der Pegasus wiehert Unsterblichkeit.", "tokens": ["Der", "Pe\u00b7ga\u00b7sus", "wie\u00b7hert", "U\u00b7nsterb\u00b7lich\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "NN", "$."], "meter": "-+-+-+---+", "measure": "zehnsilber"}}, "stanza.10": {"line.1": {"text": "Sonst ist nicht viel von ihm zu berichten.", "tokens": ["Sonst", "ist", "nicht", "viel", "von", "ihm", "zu", "be\u00b7rich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "ADV", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "+---+-+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Was tat er denn Gro\u00dfes? Heilen und dichten.", "tokens": ["Was", "tat", "er", "denn", "Gro\u00b7\u00dfes", "?", "Hei\u00b7len", "und", "dich\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "NN", "$.", "NN", "KON", "ADJA", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Er war kein Heiliger und kein Held,", "tokens": ["Er", "war", "kein", "Hei\u00b7li\u00b7ger", "und", "kein", "Held", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hat nirgends nichts, krach, auf den Kopf gestellt.", "tokens": ["Hat", "nir\u00b7gends", "nichts", ",", "krach", ",", "auf", "den", "Kopf", "ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIS", "$,", "VVFIN", "$,", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Lebte blo\u00df so mit seinen Talenten,", "tokens": ["Leb\u00b7te", "blo\u00df", "so", "mit", "sei\u00b7nen", "Ta\u00b7len\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Medikamenten und Instrumenten", "tokens": ["Me\u00b7di\u00b7ka\u00b7men\u00b7ten", "und", "Inst\u00b7ru\u00b7men\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+-+---+-+-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Unscheinbar dahin zu Bochum der Stadt,", "tokens": ["Un\u00b7schein\u00b7bar", "da\u00b7hin", "zu", "Bo\u00b7ch\u00b7um", "der", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "PAV", "APPR", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Die jetzt mehr als damals Einwohner hat.", "tokens": ["Die", "jetzt", "mehr", "als", "da\u00b7mals", "Ein\u00b7woh\u00b7ner", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PIAT", "KOKOM", "ADV", "NN", "VAFIN", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Es gab da noch keine Metallgie\u00dfereien,", "tokens": ["Es", "gab", "da", "noch", "kei\u00b7ne", "Me\u00b7tall\u00b7gie\u00b7\u00dfe\u00b7rei\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PIAT", "NN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Doch h\u00f6rte man zahlreiche Vierf\u00fc\u00dfler schreien;", "tokens": ["Doch", "h\u00f6r\u00b7te", "man", "zahl\u00b7rei\u00b7che", "Vier\u00b7f\u00fc\u00df\u00b7ler", "schrei\u00b7en", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADJA", "NN", "VVFIN", "$."], "meter": "-+---+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "An Stelle der Gu\u00dfstahlindustrie", "tokens": ["An", "Stel\u00b7le", "der", "Gu\u00df\u00b7stah\u00b7lin\u00b7dust\u00b7rie"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "ART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Pr\u00e4dominierte das n\u00fctzliche Vieh.", "tokens": ["Pr\u00e4\u00b7do\u00b7mi\u00b7nier\u00b7te", "das", "n\u00fctz\u00b7li\u00b7che", "Vieh", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.13": {"line.1": {"text": "Das fand schon auf der Stra\u00dfe sein Futter,", "tokens": ["Das", "fand", "schon", "auf", "der", "Stra\u00b7\u00dfe", "sein", "Fut\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "APPR", "ART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Revanchierte sich flei\u00dfig mit Milch und Butter", "tokens": ["Re\u00b7van\u00b7chier\u00b7te", "sich", "flei\u00b7\u00dfig", "mit", "Milch", "und", "But\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "ADJD", "APPR", "NN", "KON", "NN"], "meter": "+-+--+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Und gab am Ende, wenns leider tot,", "tokens": ["Und", "gab", "am", "En\u00b7de", ",", "wenns", "lei\u00b7der", "tot", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "$,", "KOUS", "ADV", "ADJD", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Das Material zum Boeuf ", "tokens": ["Das", "Ma\u00b7te\u00b7ri\u00b7al", "zum", "Bo\u00b7euf"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.14": {"line.1": {"text": "Doch war man wenig auf Fleisch versessen,", "tokens": ["Doch", "war", "man", "we\u00b7nig", "auf", "Fleisch", "ver\u00b7ses\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIS", "ADV", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hat lieber Gem\u00fcse und Brot gegessen,", "tokens": ["Hat", "lie\u00b7ber", "Ge\u00b7m\u00fc\u00b7se", "und", "Brot", "ge\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Stand nur der Wippap in der N\u00e4h,", "tokens": ["Stand", "nur", "der", "Wip\u00b7pap", "in", "der", "N\u00e4h", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das ist der Kessel voll d\u00fcnnem Kaffee.", "tokens": ["Das", "ist", "der", "Kes\u00b7sel", "voll", "d\u00fcn\u00b7nem", "Kaf\u00b7fee", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.15": {"line.1": {"text": "Aus diesem Grunde (meint Doktor Kortum)", "tokens": ["Aus", "die\u00b7sem", "Grun\u00b7de", "(", "meint", "Dok\u00b7tor", "Kor\u00b7tum", ")"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "$(", "VVFIN", "NN", "NE", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ging selten gef\u00e4hrliche Kr\u00e4nke dort um.", "tokens": ["Ging", "sel\u00b7ten", "ge\u00b7f\u00e4hr\u00b7li\u00b7che", "Kr\u00e4n\u00b7ke", "dort", "um", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ADJA", "NN", "ADV", "PTKVZ", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Item: man war nicht \u00fcppig bei Kost,", "tokens": ["I\u00b7tem", ":", "man", "war", "nicht", "\u00fcp\u00b7pig", "bei", "Kost", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PIS", "VAFIN", "PTKNEG", "ADJD", "APPR", "NN", "$,"], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Auch hatte das St\u00e4dtchen keine fahrende Post.", "tokens": ["Auch", "hat\u00b7te", "das", "St\u00e4dt\u00b7chen", "kei\u00b7ne", "fah\u00b7ren\u00b7de", "Post", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PIAT", "ADJA", "NN", "$."], "meter": "-+--+-+-+--+", "measure": "iambic.penta.relaxed"}}, "stanza.16": {"line.1": {"text": "Auch das war dem Wohlsein kaum gegenteilig;", "tokens": ["Auch", "das", "war", "dem", "Wohl\u00b7sein", "kaum", "ge\u00b7gen\u00b7tei\u00b7lig", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VAFIN", "ART", "NN", "ADV", "ADJD", "$."], "meter": "--+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Man wird nerv\u00f6s, hat man es eilig.", "tokens": ["Man", "wird", "ner\u00b7v\u00f6s", ",", "hat", "man", "es", "ei\u00b7lig", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "NE", "$,", "VAFIN", "PIS", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Von Kortums Patienten klagte nie", "tokens": ["Von", "Kor\u00b7tums", "Pa\u00b7ti\u00b7en\u00b7ten", "klag\u00b7te", "nie"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ein einziger \u00fcber Neurasthenie.", "tokens": ["Ein", "ein\u00b7zi\u00b7ger", "\u00fc\u00b7ber", "Neu\u00b7ras\u00b7the\u00b7nie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "APPR", "NN", "$."], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.17": {"line.1": {"text": "Wie aber will bei solchen Umst\u00e4nden", "tokens": ["Wie", "a\u00b7ber", "will", "bei", "sol\u00b7chen", "Um\u00b7st\u00e4n\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VMFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ein geistreicher Arzt seine Zeit verwenden?", "tokens": ["Ein", "geist\u00b7rei\u00b7cher", "Arzt", "sei\u00b7ne", "Zeit", "ver\u00b7wen\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.3": {"text": "Entweder: er s\u00e4uft (und das ist gemein),", "tokens": ["Ent\u00b7we\u00b7der", ":", "er", "s\u00e4uft", "(", "und", "das", "ist", "ge\u00b7mein", ")", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "$.", "PPER", "VVFIN", "$(", "KON", "PDS", "VAFIN", "ADJD", "$(", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Oder: es fallen ihm Verse ein.", "tokens": ["O\u00b7der", ":", "es", "fal\u00b7len", "ihm", "Ver\u00b7se", "ein", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$.", "PPER", "VVFIN", "PPER", "NN", "PTKVZ", "$."], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}}, "stanza.18": {"line.1": {"text": "O Spiritus! Von allen Produkten", "tokens": ["O", "Spi\u00b7ri\u00b7tus", "!", "Von", "al\u00b7len", "Pro\u00b7duk\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NE", "NE", "$.", "APPR", "PIAT", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der G\u00e4rung von Lissabon bis Mukden", "tokens": ["Der", "G\u00e4\u00b7rung", "von", "Lis\u00b7sa\u00b7bon", "bis", "Muk\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NE", "APPR", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Bist du das st\u00e4rkste. Von Pol zu Pol", "tokens": ["Bist", "du", "das", "st\u00e4rks\u00b7te", ".", "Von", "Pol", "zu", "Pol"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PDS", "VVFIN", "$.", "APPR", "NN", "APPR", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "R\u00fchmen die V\u00f6lker den Alkohol.", "tokens": ["R\u00fch\u00b7men", "die", "V\u00f6l\u00b7ker", "den", "Al\u00b7ko\u00b7hol", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ART", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.19": {"line.1": {"text": "Es tranken die Griechen, R\u00f6mer, Hebr\u00e4er,", "tokens": ["Es", "tran\u00b7ken", "die", "Grie\u00b7chen", ",", "R\u00f6\u00b7mer", ",", "Heb\u00b7r\u00e4\u00b7er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "NE", "$,", "NN", "$,"], "meter": "-+--+-+-+--", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "(makkab\u00e4-, Mannich\u00e4-, Sadduz\u00e4er),", "tokens": ["(", "mak\u00b7ka\u00b7b\u00e4", ",", "Man\u00b7ni\u00b7ch\u00e4", ",", "Sad\u00b7du\u00b7z\u00e4\u00b7er", ")", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "TRUNC", "$,", "TRUNC", "$,", "NN", "$(", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Und auch beim Turmbau zu Babylon", "tokens": ["Und", "auch", "beim", "Turm\u00b7bau", "zu", "Ba\u00b7by\u00b7lon"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPRART", "NN", "APPR", "NE"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Tranken so Maurer wie Zimmerer schon.", "tokens": ["Tran\u00b7ken", "so", "Mau\u00b7rer", "wie", "Zim\u00b7me\u00b7rer", "schon", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "NN", "KOKOM", "NN", "ADV", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}}, "stanza.20": {"line.1": {"text": "Am Euphrat, am Tigris, am Ganges, am Nile,", "tokens": ["Am", "Eu\u00b7ph\u00b7rat", ",", "am", "Tig\u00b7ris", ",", "am", "Gan\u00b7ges", ",", "am", "Ni\u00b7le", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NE", "$,", "APPRART", "NE", "$,", "APPRART", "NN", "$,", "APPRART", "NN", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "In Mexiko, Mecklenburg, Lobenstein, Chile,", "tokens": ["In", "Me\u00b7xi\u00b7ko", ",", "Meck\u00b7len\u00b7burg", ",", "Lo\u00b7bens\u00b7tein", ",", "Chi\u00b7le", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NE", "$,", "NE", "$,", "NN", "$,", "NN", "$,"], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Auf dem Himalaya, der Jungfrau, selbst auf dem Popo-", "tokens": ["Auf", "dem", "Hi\u00b7ma\u00b7la\u00b7ya", ",", "der", "Jung\u00b7frau", ",", "selbst", "auf", "dem", "Po\u00b7po"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "ART", "NN", "$,", "ADV", "APPR", "ART", "TRUNC"], "meter": "+-+--+-+-+-+-+", "measure": "trochaic.septa.relaxed"}, "line.4": {"text": "Katepetl war immer des Trunkes man froh.", "tokens": ["Ka\u00b7te\u00b7petl", "war", "im\u00b7mer", "des", "Trun\u00b7kes", "man", "froh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "ART", "NN", "PIS", "ADJD", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.21": {"line.1": {"text": "Engl\u00e4nder und Russen, Sachs-, Preu\u00dfen, Franzosen,", "tokens": ["En\u00b7gl\u00e4n\u00b7der", "und", "Rus\u00b7sen", ",", "Sachs", ",", "Preu\u00b7\u00dfen", ",", "Fran\u00b7zo\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "KON", "NN", "$,", "TRUNC", "$,", "NE", "$,", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Worin sind sie einig? In Spirituosen;", "tokens": ["Wo\u00b7rin", "sind", "sie", "ei\u00b7nig", "?", "In", "Spi\u00b7ri\u00b7tu\u00b7o\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADJD", "$.", "APPR", "NN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Man bechert von Potschappel bis Paris", "tokens": ["Man", "be\u00b7chert", "von", "Pot\u00b7schap\u00b7pel", "bis", "Pa\u00b7ris"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR", "NE", "APPR", "NE"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Und denkt nicht an Rheuma und Nierengries.", "tokens": ["Und", "denkt", "nicht", "an", "Rheu\u00b7ma", "und", "Nie\u00b7ren\u00b7gries", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "APPR", "NE", "KON", "NE", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.22": {"line.1": {"text": "Doch Kortum war Arzt, und er wu\u00dfte: die Schn\u00e4pse", "tokens": ["Doch", "Kor\u00b7tum", "war", "Arzt", ",", "und", "er", "wu\u00df\u00b7te", ":", "die", "Schn\u00e4p\u00b7se"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "NE", "VAFIN", "NN", "$,", "KON", "PPER", "VVFIN", "$.", "ART", "NN"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Haben zur Folge verschiedene Koll\u00e4pse,", "tokens": ["Ha\u00b7ben", "zur", "Fol\u00b7ge", "ver\u00b7schie\u00b7de\u00b7ne", "Kol\u00b7l\u00e4p\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPRART", "NN", "ADJA", "NN", "$,"], "meter": "+--+--+-+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Und auch nach zu vielem Bier und Wein", "tokens": ["Und", "auch", "nach", "zu", "vie\u00b7lem", "Bier", "und", "Wein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "PTKA", "PIS", "NN", "KON", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Stellt sich allzugern allerhand Kr\u00e4nkliches ein.", "tokens": ["Stellt", "sich", "all\u00b7zu\u00b7gern", "al\u00b7ler\u00b7hand", "Kr\u00e4nk\u00b7li\u00b7ches", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "PIAT", "NN", "PTKVZ", "$."], "meter": "+-+--+-++--+", "measure": "trochaic.hexa.relaxed"}}, "stanza.23": {"line.1": {"text": "Viel ungef\u00e4hrlicher ist das Skandieren;", "tokens": ["Viel", "un\u00b7ge\u00b7f\u00e4hr\u00b7li\u00b7cher", "ist", "das", "Skan\u00b7die\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "ART", "NN", "$."], "meter": "-+-+--+-+--", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Man kann dabei h\u00f6chstens den Verstand verlieren,", "tokens": ["Man", "kann", "da\u00b7bei", "h\u00f6chs\u00b7tens", "den", "Ver\u00b7stand", "ver\u00b7lie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PAV", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und das auch nur dann, wenn man sowieso", "tokens": ["Und", "das", "auch", "nur", "dann", ",", "wenn", "man", "so\u00b7wi\u00b7e\u00b7so"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PDS", "ADV", "ADV", "ADV", "$,", "KOUS", "PIS", "ADV"], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Nicht ganz grundfest ist im Kapitolio.", "tokens": ["Nicht", "ganz", "grund\u00b7fest", "ist", "im", "Ka\u00b7pi\u00b7to\u00b7lio", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "ADJD", "VAFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.24": {"line.1": {"text": "Im ganzen ist der Verkehr mit den Musen", "tokens": ["Im", "gan\u00b7zen", "ist", "der", "Ver\u00b7kehr", "mit", "den", "Mu\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "VAFIN", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Vorzuziehen dem mit Spiritussen,", "tokens": ["Vor\u00b7zu\u00b7zie\u00b7hen", "dem", "mit", "Spi\u00b7ri\u00b7tus\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "APPR", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Wenn man, wie sichs am Rand versteht,", "tokens": ["Wenn", "man", ",", "wie", "sichs", "am", "Rand", "ver\u00b7steht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "$,", "PWAV", "PIS", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dabei nicht gleich bis zum Laster geht.", "tokens": ["Da\u00b7bei", "nicht", "gleich", "bis", "zum", "Las\u00b7ter", "geht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PTKNEG", "ADV", "APPR", "APPRART", "NN", "VVFIN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.25": {"line.1": {"text": "Das wu\u00dfte Karl Arnold. Er trieb es mit Ma\u00dfen", "tokens": ["Das", "wu\u00df\u00b7te", "Karl", "Ar\u00b7nold", ".", "Er", "trieb", "es", "mit", "Ma\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "NE", "NE", "$.", "PPER", "VVFIN", "PPER", "APPR", "NN"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Griff sie, wo sie weich sind, nahm sie aufs Knie,", "tokens": ["Griff", "sie", ",", "wo", "sie", "weich", "sind", ",", "nahm", "sie", "aufs", "Knie", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "$,", "PWAV", "PPER", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Aber D\u00e9bauchen beging er nie.", "tokens": ["A\u00b7ber", "D\u00e9\u00b7bau\u00b7chen", "be\u00b7ging", "er", "nie", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PPER", "ADV", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.26": {"line.1": {"text": "Doch eins, ja, das: Er hatte ne Neigung", "tokens": ["Doch", "eins", ",", "ja", ",", "das", ":", "Er", "hat\u00b7te", "ne", "Nei\u00b7gung"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PIS", "$,", "PTKANT", "$,", "PRELS", "$.", "PPER", "VAFIN", "ADJA", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Zu nicht immer ganz s\u00e4nftlicher Liebesbezeigung,", "tokens": ["Zu", "nicht", "im\u00b7mer", "ganz", "s\u00e4nft\u00b7li\u00b7cher", "Lie\u00b7bes\u00b7be\u00b7zei\u00b7gung", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PTKNEG", "ADV", "ADV", "ADJA", "NN", "$,"], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Zerkn\u00fcllte gerne R\u00f6ckchen sowohl wie Frisur, \u2013", "tokens": ["Zer\u00b7kn\u00fcll\u00b7te", "ger\u00b7ne", "R\u00f6ck\u00b7chen", "so\u00b7wohl", "wie", "Fri\u00b7sur", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "ADV", "NN", "KON", "PWAV", "NN", "$,", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Kurz, er machte den Musen handgreiflich die Cour.", "tokens": ["Kurz", ",", "er", "mach\u00b7te", "den", "Mu\u00b7sen", "hand\u00b7greif\u00b7lich", "die", "Cour", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PPER", "VVFIN", "ART", "NN", "ADJD", "ART", "NN", "$."], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.27": {"line.1": {"text": "Da rutschte manch Schleifchen, zerri\u00df manche Spitze,", "tokens": ["Da", "rutschte", "manch", "Schleif\u00b7chen", ",", "zer\u00b7ri\u00df", "man\u00b7che", "Spit\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "$,", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Man sah auch manchmal dabei in der Hitze", "tokens": ["Man", "sah", "auch", "manch\u00b7mal", "da\u00b7bei", "in", "der", "Hit\u00b7ze"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADV", "ADV", "PAV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ein nackigtes St\u00fcckchen an Stellen, wo", "tokens": ["Ein", "na\u00b7ckig\u00b7tes", "St\u00fcck\u00b7chen", "an", "Stel\u00b7len", ",", "wo"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "$,", "PWAV"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Es weder damals noch heut comme il faut.", "tokens": ["Es", "we\u00b7der", "da\u00b7mals", "noch", "heut", "com\u00b7me", "il", "faut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "KON", "ADV", "ADV", "ADV", "FM", "FM", "FM", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.28": {"line.1": {"text": "Doch das war am Ende nicht weiter bedenklich,", "tokens": ["Doch", "das", "war", "am", "En\u00b7de", "nicht", "wei\u00b7ter", "be\u00b7denk\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "APPRART", "NN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die Madames waren damals recht leicht einhenklich,", "tokens": ["Die", "Ma\u00b7da\u00b7mes", "wa\u00b7ren", "da\u00b7mals", "recht", "leicht", "ein\u00b7hen\u00b7klich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "ADJD", "ADJD", "$,"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.3": {"text": "Denn mit der Moral stands leider b\u00f6s.", "tokens": ["Denn", "mit", "der", "Mo\u00b7ral", "stands", "lei\u00b7der", "b\u00f6s", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "ADV", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Das achtzehnte Jahrhundert war amour\u00f6s.", "tokens": ["Das", "acht\u00b7zehn\u00b7te", "Jahr\u00b7hun\u00b7dert", "war", "a\u00b7mou\u00b7r\u00f6s", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "NE", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}}, "stanza.29": {"line.1": {"text": "Dagegen war eins nicht \u00e0 la mode", "tokens": ["Da\u00b7ge\u00b7gen", "war", "eins", "nicht", "\u00e0", "la", "mo\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PIS", "PTKNEG", "FM", "FM", "FM"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "In dieser mehr zierlichen Zeitperiode,", "tokens": ["In", "die\u00b7ser", "mehr", "zier\u00b7li\u00b7chen", "Zeit\u00b7pe\u00b7ri\u00b7o\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "PIAT", "ADJA", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Was Doktor Kortum gar sehr behagt:", "tokens": ["Was", "Dok\u00b7tor", "Kor\u00b7tum", "gar", "sehr", "be\u00b7hagt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "NE", "ADV", "ADV", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Er hat gerne alles graderaus gesagt.", "tokens": ["Er", "hat", "ger\u00b7ne", "al\u00b7les", "gra\u00b7de\u00b7raus", "ge\u00b7sagt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "ADV", "VVPP", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.30": {"line.1": {"text": "Er war nicht f\u00fcrs Wortepomadisieren,", "tokens": ["Er", "war", "nicht", "f\u00fcrs", "Wor\u00b7te\u00b7po\u00b7ma\u00b7di\u00b7sie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "APPRART", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Und, mochte Euterpchen sich noch so sehr zieren,", "tokens": ["Und", ",", "moch\u00b7te", "Eu\u00b7terpc\u00b7hen", "sich", "noch", "so", "sehr", "zie\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "VVFIN", "NE", "PRF", "ADV", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Er brachte ihr ungeniert Ausdr\u00fccke bei,", "tokens": ["Er", "brach\u00b7te", "ihr", "un\u00b7ge\u00b7niert", "Aus\u00b7dr\u00fc\u00b7cke", "bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "NN", "PTKVZ", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Als ob sie in Bochum geboren sei.", "tokens": ["Als", "ob", "sie", "in", "Bo\u00b7ch\u00b7um", "ge\u00b7bo\u00b7ren", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "APPR", "NN", "VVPP", "VAFIN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.31": {"line.1": {"text": "\u00bbkomm, Phyllis, zu wallen im Lustparadiese!\u00ab", "tokens": ["\u00bb", "komm", ",", "Phyl\u00b7lis", ",", "zu", "wal\u00b7len", "im", "Lust\u00b7pa\u00b7ra\u00b7die\u00b7se", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "$,", "NE", "$,", "PTKZU", "VVINF", "APPRART", "NN", "$.", "$("], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Unsinn: Komm Grete, geh mit auf die Wiese.", "tokens": ["Un\u00b7sinn", ":", "Komm", "Gre\u00b7te", ",", "geh", "mit", "auf", "die", "Wie\u00b7se", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "NE", "$,", "VVFIN", "APPR", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "\u00bbo siehe, wie Luna die Lilien k\u00fc\u00dft!\u00ab", "tokens": ["\u00bb", "o", "sie\u00b7he", ",", "wie", "Lu\u00b7na", "die", "Li\u00b7li\u00b7en", "k\u00fc\u00dft", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "FM", "FM", "$,", "PWAV", "NN", "ART", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Unsinn: Komm raus, weils Vollmond ist.", "tokens": ["Un\u00b7sinn", ":", "Komm", "raus", ",", "weils", "Voll\u00b7mond", "ist", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "VVFIN", "PTKVZ", "$,", "KOUS", "NN", "VAFIN", "$."], "meter": "+--+-++-", "measure": "iambic.tetra.invert"}}, "stanza.32": {"line.1": {"text": "Und, wie er kein Freund war vom Phrasenscherwenzeln,", "tokens": ["Und", ",", "wie", "er", "kein", "Freund", "war", "vom", "Phra\u00b7sen\u00b7scher\u00b7wen\u00b7zeln", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWAV", "PPER", "PIAT", "NN", "VAFIN", "APPRART", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "So liebte er wenig das zierliche T\u00e4nzeln", "tokens": ["So", "lieb\u00b7te", "er", "we\u00b7nig", "das", "zier\u00b7li\u00b7che", "T\u00e4n\u00b7zeln"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "In h\u00f6fischen Formen, galant und kokett;", "tokens": ["In", "h\u00f6\u00b7fi\u00b7schen", "For\u00b7men", ",", "ga\u00b7lant", "und", "ko\u00b7kett", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "ADJD", "KON", "ADJD", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Er war mehr f\u00fcrn Hopser, als f\u00fcrs Menuett.", "tokens": ["Er", "war", "mehr", "f\u00fcrn", "Hop\u00b7ser", ",", "als", "f\u00fcrs", "Me\u00b7nu\u00b7ett", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NE", "$,", "KOUS", "APPRART", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.33": {"line.1": {"text": "Nach Fl\u00f6te und Geige gef\u00e4llig zu schleifen", "tokens": ["Nach", "Fl\u00f6\u00b7te", "und", "Gei\u00b7ge", "ge\u00b7f\u00e4l\u00b7lig", "zu", "schlei\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN", "ADJD", "PTKZU", "VVINF"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "War nicht seine Sache, die Dudelsackpfeifen", "tokens": ["War", "nicht", "sei\u00b7ne", "Sa\u00b7che", ",", "die", "Du\u00b7del\u00b7sack\u00b7pfei\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "PTKNEG", "PPOSAT", "NN", "$,", "ART", "NN"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Gaben seinem Gestampfe den holprigen Takt,", "tokens": ["Ga\u00b7ben", "sei\u00b7nem", "Ge\u00b7stamp\u00b7fe", "den", "holp\u00b7ri\u00b7gen", "Takt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+--+--+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Wenn er Fr\u00e4ulein Euterpen hat h\u00fcftlings gepackt.", "tokens": ["Wenn", "er", "Fr\u00e4u\u00b7lein", "Eu\u00b7ter\u00b7pen", "hat", "h\u00fcft\u00b7lings", "ge\u00b7packt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "NE", "VAFIN", "ADV", "VVPP", "$."], "meter": "--+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.34": {"line.1": {"text": "Doch das sind schlie\u00dflich blo\u00df \u00c4u\u00dferlichkeiten.", "tokens": ["Doch", "das", "sind", "schlie\u00df\u00b7lich", "blo\u00df", "\u00c4u\u00b7\u00dfer\u00b7lich\u00b7kei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ADV", "ADV", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Den Pegasus kann man verschiedentlich reiten:", "tokens": ["Den", "Pe\u00b7ga\u00b7sus", "kann", "man", "ver\u00b7schie\u00b7dent\u00b7lich", "rei\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VMFIN", "PIS", "ADJD", "VVFIN", "$."], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Im spanischen Tritt und im Bauerngalopp,", "tokens": ["Im", "spa\u00b7ni\u00b7schen", "Tritt", "und", "im", "Bau\u00b7ern\u00b7ga\u00b7lopp", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "KON", "APPRART", "NN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Der Sitz macht den Reiter, nicht Trab oder Hopp.", "tokens": ["Der", "Sitz", "macht", "den", "Rei\u00b7ter", ",", "nicht", "Trab", "o\u00b7der", "Hopp", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,", "PTKNEG", "NN", "KON", "NE", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.35": {"line.1": {"text": "Und das wird man Kortumen nachsagen m\u00fcssen:", "tokens": ["Und", "das", "wird", "man", "Kor\u00b7tu\u00b7men", "nach\u00b7sa\u00b7gen", "m\u00fcs\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "PIS", "NN", "VVINF", "VMINF", "$."], "meter": "--+--+--+-+-", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Sein struppiger Gaul hat nicht ab ihn geschmissen,", "tokens": ["Sein", "strup\u00b7pi\u00b7ger", "Gaul", "hat", "nicht", "ab", "ihn", "ge\u00b7schmis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "PTKNEG", "PTKVZ", "PPER", "VVPP", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Wie sehr auch manchmal ausschlug das Beest,", "tokens": ["Wie", "sehr", "auch", "manch\u00b7mal", "aus\u00b7schlug", "das", "Beest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Doktor Kortum ist immer oben gewest.", "tokens": ["Dok\u00b7tor", "Kor\u00b7tum", "ist", "im\u00b7mer", "o\u00b7ben", "ge\u00b7west", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "+-+--+-+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.36": {"line.1": {"text": "Ihm machten Vergn\u00fcgen die wilden Kapriolen,", "tokens": ["Ihm", "mach\u00b7ten", "Ver\u00b7gn\u00fc\u00b7gen", "die", "wil\u00b7den", "Kap\u00b7ri\u00b7o\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Hat sich in Freiheit dressiert das Dichterfohlen", "tokens": ["Hat", "sich", "in", "Frei\u00b7heit", "dres\u00b7siert", "das", "Dich\u00b7ter\u00b7foh\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PRF", "APPR", "NN", "VVFIN", "ART", "NN"], "meter": "+--+--+-+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Just grade auf Quersprung und Zuckeltrott.", "tokens": ["Just", "gra\u00b7de", "auf", "Quer\u00b7sprung", "und", "Zu\u00b7ckel\u00b7trott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Gefolgt hats doch seinem H\u00fch und Hott.", "tokens": ["Ge\u00b7folgt", "hats", "doch", "sei\u00b7nem", "H\u00fch", "und", "Hott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "ADV", "PPOSAT", "NN", "KON", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.37": {"line.1": {"text": "Mir scheint, als ob man nicht ganz nach Verdienste", "tokens": ["Mir", "scheint", ",", "als", "ob", "man", "nicht", "ganz", "nach", "Ver\u00b7diens\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOKOM", "KOUS", "PIS", "PTKNEG", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sch\u00e4tzte des Jobsdoktors Reiterk\u00fcnste.", "tokens": ["Sch\u00e4tz\u00b7te", "des", "Jobs\u00b7dok\u00b7tors", "Rei\u00b7ter\u00b7k\u00fcns\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NE", "NE", "$."], "meter": "+--++-+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Sie sehen ja aus, als obs gar nichts w\u00e4r,", "tokens": ["Sie", "se\u00b7hen", "ja", "aus", ",", "als", "obs", "gar", "nichts", "w\u00e4r", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "$,", "KOKOM", "KOUS", "ADV", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und doch ist diese Art Zotteln schwer.", "tokens": ["Und", "doch", "ist", "die\u00b7se", "Art", "Zot\u00b7teln", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PDAT", "NN", "NN", "ADJD", "$."], "meter": "-+-+-++-+", "measure": "unknown.measure.penta"}}, "stanza.38": {"line.1": {"text": "Er glaubte Hans Sachsen nachzuspotten", "tokens": ["Er", "glaub\u00b7te", "Hans", "Sach\u00b7sen", "nach\u00b7zu\u00b7spot\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "NE", "VVFIN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Mit seinem stolpernden, holpernden Trotten.", "tokens": ["Mit", "sei\u00b7nem", "stol\u00b7pern\u00b7den", ",", "hol\u00b7pern\u00b7den", "Trot\u00b7ten", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Doch hat er dem nichts am Zeuge geflickt;", "tokens": ["Doch", "hat", "er", "dem", "nichts", "am", "Zeu\u00b7ge", "ge\u00b7flickt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ART", "PIS", "APPRART", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der Schuster ritt anders und sehr geschickt.", "tokens": ["Der", "Schus\u00b7ter", "ritt", "an\u00b7ders", "und", "sehr", "ge\u00b7schickt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "KON", "ADV", "VVPP", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.39": {"line.1": {"text": "Hansens Verse sind gar nicht komisch und holprig;", "tokens": ["Han\u00b7sens", "Ver\u00b7se", "sind", "gar", "nicht", "ko\u00b7misch", "und", "holp\u00b7rig", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VAFIN", "ADV", "PTKNEG", "ADJD", "KON", "ADJD", "$."], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Nur auf dem Papiere macht es sich stolprig;", "tokens": ["Nur", "auf", "dem", "Pa\u00b7pie\u00b7re", "macht", "es", "sich", "stolp\u00b7rig", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVFIN", "PPER", "PRF", "ADJD", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Gelesen sind sie schmiegsam wie Wachs.", "tokens": ["Ge\u00b7le\u00b7sen", "sind", "sie", "schmieg\u00b7sam", "wie", "Wachs", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PPER", "ADJD", "KOKOM", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Respekt vor dem Versemeister Hans Sachs!", "tokens": ["Res\u00b7pekt", "vor", "dem", "Ver\u00b7se\u00b7meis\u00b7ter", "Hans", "Sachs", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "NE", "NE", "$."], "meter": "+---+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.40": {"line.1": {"text": "Respekt aber auch vor dem Knitteler Kortum!", "tokens": ["Res\u00b7pekt", "a\u00b7ber", "auch", "vor", "dem", "Knit\u00b7te\u00b7ler", "Kor\u00b7tum", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "APPR", "ART", "NN", "NE", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Er f\u00fchrte herbei manchen Versabortum", "tokens": ["Er", "f\u00fchr\u00b7te", "her\u00b7bei", "man\u00b7chen", "Ver\u00b7sa\u00b7bor\u00b7tum"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und meinte, das w\u00e4re blo\u00df Travestie.", "tokens": ["Und", "mein\u00b7te", ",", "das", "w\u00e4\u00b7re", "blo\u00df", "Tra\u00b7ves\u00b7tie", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PDS", "VAFIN", "ADV", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "War aber mehr. War Harmonie", "tokens": ["War", "a\u00b7ber", "mehr", ".", "War", "Har\u00b7mo\u00b7nie"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "ADV", "ADV", "$.", "VAFIN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Im Disharmonischen, war ein Treffer", "tokens": ["Im", "Dis\u00b7har\u00b7mo\u00b7ni\u00b7schen", ",", "war", "ein", "Tref\u00b7fer"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "NN", "$,", "VAFIN", "ART", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Im komischen Stile. Kein Nach\u00e4ffer", "tokens": ["Im", "ko\u00b7mi\u00b7schen", "Sti\u00b7le", ".", "Kein", "Nach\u00b7\u00e4f\u00b7fer"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "$.", "PIAT", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "War unser Doktor, war originell.", "tokens": ["War", "un\u00b7ser", "Dok\u00b7tor", ",", "war", "o\u00b7rig\u00b7i\u00b7nell", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "$,", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Drum sa\u00df im Ohre sein Ton so schnell.", "tokens": ["Drum", "sa\u00df", "im", "Oh\u00b7re", "sein", "Ton", "so", "schnell", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "APPRART", "NN", "PPOSAT", "NN", "ADV", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.42": {"line.1": {"text": "Weil ihm nicht weniger als Hans Sachsen", "tokens": ["Weil", "ihm", "nicht", "we\u00b7ni\u00b7ger", "als", "Hans", "Sach\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PTKNEG", "PIS", "KOKOM", "NE", "NE"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Ein eigener Schnabel zum Singen war gewachsen,", "tokens": ["Ein", "ei\u00b7ge\u00b7ner", "Schna\u00b7bel", "zum", "Sin\u00b7gen", "war", "ge\u00b7wach\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Weil er nicht sang, wie jedermann sung:", "tokens": ["Weil", "er", "nicht", "sang", ",", "wie", "je\u00b7der\u00b7mann", "sung", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVFIN", "$,", "PWAV", "PIS", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das brachte sein Lied so rasch in Schwung.", "tokens": ["Das", "brach\u00b7te", "sein", "Lied", "so", "rasch", "in", "Schwung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPOSAT", "NN", "ADV", "ADJD", "APPR", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.43": {"line.1": {"text": "Er war, um zu reden mit Liliencronen,", "tokens": ["Er", "war", ",", "um", "zu", "re\u00b7den", "mit", "Li\u00b7lien\u00b7cro\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "KOUI", "PTKZU", "VVINF", "APPR", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Ein Neut\u00f6ner in den sterilen Zonen", "tokens": ["Ein", "Neu\u00b7t\u00f6\u00b7ner", "in", "den", "ste\u00b7ri\u00b7len", "Zo\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Des deutschen komischen Heldengedichts,", "tokens": ["Des", "deut\u00b7schen", "ko\u00b7mi\u00b7schen", "Hel\u00b7den\u00b7ge\u00b7dichts", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Schuf seine Manier sich aus dem Nichts.", "tokens": ["Schuf", "sei\u00b7ne", "Ma\u00b7nier", "sich", "aus", "dem", "Nichts", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "PRF", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.44": {"line.1": {"text": "Sonst w\u00e4re trotz all seinem Geist und Humore", "tokens": ["Sonst", "w\u00e4\u00b7re", "trotz", "all", "sei\u00b7nem", "Geist", "und", "Hu\u00b7mo\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "APPR", "PIAT", "PPOSAT", "NN", "KON", "NE"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Hieronimus Jobs nicht durch so viele Tore", "tokens": ["Hie\u00b7ro\u00b7ni\u00b7mus", "Jobs", "nicht", "durch", "so", "vie\u00b7le", "To\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "PTKNEG", "APPR", "ADV", "PIAT", "NN"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Jahrzehnt auf Jahrzehnt geschritten bis heut,", "tokens": ["Jahr\u00b7zehnt", "auf", "Jahr\u00b7zehnt", "ge\u00b7schrit\u00b7ten", "bis", "heut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVPP", "KON", "ADV", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da die \u00bbInsel\u00ab ihn nagelneu alt herbeut.", "tokens": ["Da", "die", "\u00bb", "In\u00b7sel", "\u00ab", "ihn", "na\u00b7gel\u00b7neu", "alt", "her\u00b7beut", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "$(", "NN", "$(", "PPER", "ADJD", "ADJD", "VVPP", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}}, "stanza.45": {"line.1": {"text": "Mehr w\u00fc\u00dfte ich eigentlich nicht zu sagen,", "tokens": ["Mehr", "w\u00fc\u00df\u00b7te", "ich", "ei\u00b7gent\u00b7lich", "nicht", "zu", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Denn reichlich unn\u00fctz scheint mir das Fragen,", "tokens": ["Denn", "reich\u00b7lich", "un\u00b7n\u00fctz", "scheint", "mir", "das", "Fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADJD", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ob niedrig, ob hoch die Gattung sei", "tokens": ["Ob", "nied\u00b7rig", ",", "ob", "hoch", "die", "Gat\u00b7tung", "sei"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADJD", "$,", "KOUS", "ADJD", "ART", "NN", "VAFIN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Dieser Art komischen Poesei.", "tokens": ["Die\u00b7ser", "Art", "ko\u00b7mi\u00b7schen", "Poe\u00b7sei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.46": {"line.1": {"text": "Ich behaupte getrost: der Jobs ist klassisch,", "tokens": ["Ich", "be\u00b7haup\u00b7te", "ge\u00b7trost", ":", "der", "Jobs", "ist", "klas\u00b7sisch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "VVPP", "$.", "ART", "NE", "VAFIN", "ADJD", "$,"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Sei er blo\u00df bochumsch oder parnassisch.", "tokens": ["Sei", "er", "blo\u00df", "bo\u00b7ch\u00b7um\u00b7sch", "o\u00b7der", "par\u00b7nas\u00b7sisch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "+-+-+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Was sich unmariniert so lange frisch erh\u00e4lt,", "tokens": ["Was", "sich", "un\u00b7ma\u00b7ri\u00b7niert", "so", "lan\u00b7ge", "frisch", "er\u00b7h\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "ADJD", "ADV", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Sei, ob es auch klein, neben Gro\u00dfes gestellt.", "tokens": ["Sei", ",", "ob", "es", "auch", "klein", ",", "ne\u00b7ben", "Gro\u00b7\u00dfes", "ge\u00b7stellt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "ADV", "ADJD", "$,", "APPR", "NN", "VVPP", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.47": {"line.1": {"text": "Doch eins noch, das: Es geht das Gerede,", "tokens": ["Doch", "eins", "noch", ",", "das", ":", "Es", "geht", "das", "Ge\u00b7re\u00b7de", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ADV", "$,", "PRELS", "$.", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die zwei Fortsetzungen, alle beede,", "tokens": ["Die", "zwei", "Fort\u00b7set\u00b7zun\u00b7gen", ",", "al\u00b7le", "bee\u00b7de", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "CARD", "NN", "$,", "PIAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Seien durchaus vom \u00dcberflu\u00df;", "tokens": ["Sei\u00b7en", "durc\u00b7haus", "vom", "\u00dc\u00b7berf\u00b7lu\u00df", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Tot h\u00e4tte solln bleiben Hieronimus.", "tokens": ["Tot", "h\u00e4t\u00b7te", "solln", "blei\u00b7ben", "Hie\u00b7ro\u00b7ni\u00b7mus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "VMFIN", "VVINF", "NE", "$."], "meter": "-+-++--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.48": {"line.1": {"text": "Dann h\u00e4tte das Kunstwerk seine Rundheit", "tokens": ["Dann", "h\u00e4t\u00b7te", "das", "Kunst\u00b7werk", "sei\u00b7ne", "Rund\u00b7heit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "PPOSAT", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Bewahrt und streng \u00e4sthetische Gesundheit,", "tokens": ["Be\u00b7wahrt", "und", "streng", "\u00e4s\u00b7the\u00b7ti\u00b7sche", "Ge\u00b7sund\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Indessen jetzt Teil zwei und drei", "tokens": ["In\u00b7des\u00b7sen", "jetzt", "Teil", "zwei", "und", "drei"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "NN", "CARD", "KON", "CARD"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Nichts weiter als sch\u00e4dliche Wucherung sei.", "tokens": ["Nichts", "wei\u00b7ter", "als", "sch\u00e4d\u00b7li\u00b7che", "Wu\u00b7che\u00b7rung", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADV", "KOKOM", "ADJA", "NN", "VAFIN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.49": {"line.1": {"text": "O Gott, ihr Herren vom kritischen Knaster,", "tokens": ["O", "Gott", ",", "ihr", "Her\u00b7ren", "vom", "kri\u00b7ti\u00b7schen", "Knas\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "PPOSAT", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "La\u00dft ihr nicht endlich mal ab von dem Laster,", "tokens": ["La\u00dft", "ihr", "nicht", "end\u00b7lich", "mal", "ab", "von", "dem", "Las\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PTKNEG", "ADV", "ADV", "PTKVZ", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Immer nur Warzen und Auswuchs zu sehn,", "tokens": ["Im\u00b7mer", "nur", "War\u00b7zen", "und", "Aus\u00b7wuchs", "zu", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "NN", "KON", "NN", "PTKZU", "VVINF", "$,"], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.4": {"text": "Wo die Triebe der Kraft etwas \u00fcppiger stehn?", "tokens": ["Wo", "die", "Trie\u00b7be", "der", "Kraft", "et\u00b7was", "\u00fcp\u00b7pi\u00b7ger", "stehn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ART", "NN", "ADV", "ADJD", "VVINF", "$."], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}}, "stanza.50": {"line.1": {"text": "Ich d\u00e4chte, wir haben uns nicht zu beklagen,", "tokens": ["Ich", "d\u00e4ch\u00b7te", ",", "wir", "ha\u00b7ben", "uns", "nicht", "zu", "be\u00b7kla\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "PPER", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Da\u00df Hieronimus auferstand aus dem Schragen,", "tokens": ["Da\u00df", "Hie\u00b7ro\u00b7ni\u00b7mus", "auf\u00b7er\u00b7stand", "aus", "dem", "Schra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+---+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Denn so der zweite wie dritte Teil", "tokens": ["Denn", "so", "der", "zwei\u00b7te", "wie", "drit\u00b7te", "Teil"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "ADJA", "KOKOM", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Bereiten uns gar keine Langeweil.", "tokens": ["Be\u00b7rei\u00b7ten", "uns", "gar", "kei\u00b7ne", "Lan\u00b7ge\u00b7weil", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.51": {"line.1": {"text": "Ich m\u00f6chte sie beide durchaus nicht missen", "tokens": ["Ich", "m\u00f6ch\u00b7te", "sie", "bei\u00b7de", "durc\u00b7haus", "nicht", "mis\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "PIS", "ADV", "PTKNEG", "VVINF"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Und bin sehr gl\u00fccklich, aus ihnen zu wissen,", "tokens": ["Und", "bin", "sehr", "gl\u00fcck\u00b7lich", ",", "aus", "ih\u00b7nen", "zu", "wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADJD", "$,", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da\u00df Doktor Kortum noch allerhand", "tokens": ["Da\u00df", "Dok\u00b7tor", "Kor\u00b7tum", "noch", "al\u00b7ler\u00b7hand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "NE", "ADV", "PIAT"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Au\u00dfer dem scharfen Schinden verstand.", "tokens": ["Au\u00b7\u00dfer", "dem", "schar\u00b7fen", "Schin\u00b7den", "ver\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}}, "stanza.52": {"line.1": {"text": "Zum Beispiel: Ein M\u00e4dchen zu malen wie Esther.", "tokens": ["Zum", "Bei\u00b7spiel", ":", "Ein", "M\u00e4d\u00b7chen", "zu", "ma\u00b7len", "wie", "E\u00b7sther", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$.", "ART", "NN", "PTKZU", "VVINF", "KOKOM", "NN", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Ich w\u00fcnschte, ich h\u00e4tte so eine Schwester.", "tokens": ["Ich", "w\u00fcnschte", ",", "ich", "h\u00e4t\u00b7te", "so", "ei\u00b7ne", "Schwes\u00b7ter", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Wobei ich gar nicht b\u00f6se w\u00e4r,", "tokens": ["Wo\u00b7bei", "ich", "gar", "nicht", "b\u00f6\u00b7se", "w\u00e4r", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PTKNEG", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "W\u00fcrde der Baron dann mein Schwageer.", "tokens": ["W\u00fcr\u00b7de", "der", "Ba\u00b7ron", "dann", "mein", "Schwa\u00b7geer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADV", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.53": {"line.1": {"text": "Auch mu\u00df ich gestehen: Wie der Nachtw\u00e4chter,", "tokens": ["Auch", "mu\u00df", "ich", "ge\u00b7ste\u00b7hen", ":", "Wie", "der", "Nacht\u00b7w\u00e4ch\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "VVPP", "$.", "PWAV", "ART", "NN", "$,"], "meter": "+-+-+-+-++-", "measure": "unknown.measure.hexa"}, "line.2": {"text": "Gef\u00e4llt mir der ", "tokens": ["Ge\u00b7f\u00e4llt", "mir", "der"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPER", "ART"], "meter": "-+--", "measure": "dactylic.init"}, "line.3": {"text": "Zug scheint mir auch das zu sein,", "tokens": ["Zug", "scheint", "mir", "auch", "das", "zu", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "PDS", "PTKZU", "VAINF", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Da\u00df er begraben sein will beim Amalein;", "tokens": ["Da\u00df", "er", "be\u00b7gra\u00b7ben", "sein", "will", "beim", "A\u00b7mal\u00b7ein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "VAINF", "VMFIN", "APPRART", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.54": {"line.1": {"text": "Obwohl die, wie wir es deutlich lesen,", "tokens": ["Ob\u00b7wohl", "die", ",", "wie", "wir", "es", "deut\u00b7lich", "le\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "$,", "PWAV", "PPER", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Durchaus kein Tugendausbund gewesen.", "tokens": ["Durc\u00b7haus", "kein", "Tu\u00b7gend\u00b7aus\u00b7bund", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VAPP", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Fehlte das, k\u00e4m mir des Doktors Humor", "tokens": ["Fehl\u00b7te", "das", ",", "k\u00e4m", "mir", "des", "Dok\u00b7tors", "Hu\u00b7mor"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PDS", "$,", "VVFIN", "PPER", "ART", "NN", "NN"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.4": {"text": "Betr\u00e4chtlich weniger s\u00fc\u00dfe vor.", "tokens": ["Be\u00b7tr\u00e4cht\u00b7lich", "we\u00b7ni\u00b7ger", "s\u00fc\u00b7\u00dfe", "vor", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PTKVZ", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.55": {"line.1": {"text": "Ja, ich bekenne: die Fortsetzungen", "tokens": ["Ja", ",", "ich", "be\u00b7ken\u00b7ne", ":", "die", "Fort\u00b7set\u00b7zun\u00b7gen"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "$.", "ART", "NN"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Haben mich immer erst ganz bezwungen,", "tokens": ["Ha\u00b7ben", "mich", "im\u00b7mer", "erst", "ganz", "be\u00b7zwun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Weil ich daraus mit Freuden erfuhr:", "tokens": ["Weil", "ich", "da\u00b7raus", "mit", "Freu\u00b7den", "er\u00b7fuhr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PAV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Karl Arnold hatte nicht Sch\u00e4rfe nur.", "tokens": ["Karl", "Ar\u00b7nold", "hat\u00b7te", "nicht", "Sch\u00e4r\u00b7fe", "nur", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VAFIN", "PTKNEG", "NN", "ADV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.56": {"line.1": {"text": "Er war ein Poet auch aus dem ", "tokens": ["Er", "war", "ein", "Po\u00b7et", "auch", "aus", "dem"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADV", "APPR", "ART"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Er konnte auch ohne H\u00f6llenstein scherzen.", "tokens": ["Er", "konn\u00b7te", "auch", "oh\u00b7ne", "H\u00f6l\u00b7len\u00b7stein", "scher\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+--+-+-+--", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der rasse Wein wird m\u00e4hlich mild,", "tokens": ["Der", "ras\u00b7se", "Wein", "wird", "m\u00e4h\u00b7lich", "mild", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der schroffe Schnitt wird zum runden Bild.", "tokens": ["Der", "schrof\u00b7fe", "Schnitt", "wird", "zum", "run\u00b7den", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.57": {"line.1": {"text": "Und keiner kann sagen: die Sache wird so\u00dfig,", "tokens": ["Und", "kei\u00b7ner", "kann", "sa\u00b7gen", ":", "die", "Sa\u00b7che", "wird", "so\u00b7\u00dfig", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VMFIN", "VVINF", "$.", "ART", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Der Witz wird schal, der Humor wird klosig.", "tokens": ["Der", "Witz", "wird", "schal", ",", "der", "Hu\u00b7mor", "wird", "klo\u00b7sig", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Es geht nur, wie es im Leben geht:", "tokens": ["Es", "geht", "nur", ",", "wie", "es", "im", "Le\u00b7ben", "geht", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PWAV", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Der Gang wird ruhig, beschaulich, st\u00e4t.", "tokens": ["Der", "Gang", "wird", "ru\u00b7hig", ",", "be\u00b7schau\u00b7lich", ",", "st\u00e4t", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "ADJD", "$,", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.58": {"line.1": {"text": "Was der Dichter mit splitternden Hieben begonnen,", "tokens": ["Was", "der", "Dich\u00b7ter", "mit", "split\u00b7tern\u00b7den", "Hie\u00b7ben", "be\u00b7gon\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "--+--+--+--+-", "measure": "anapaest.tetra.plus"}, "line.2": {"text": "Hat schlie\u00dflich das Ansehn von Schnitzwerk gewonnen;", "tokens": ["Hat", "schlie\u00df\u00b7lich", "das", "An\u00b7sehn", "von", "Schnitz\u00b7werk", "ge\u00b7won\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "APPR", "NN", "VVPP", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Die harte Kontur kriegt weicheren Schwung.", "tokens": ["Die", "har\u00b7te", "Kon\u00b7tur", "kriegt", "wei\u00b7che\u00b7ren", "Schwung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "ADJA", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}}}}