{"textgrid.poem.60719": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Der Leichtsinn trat an eine Schildkr\u00f6te heran:", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Leichtsinn trat an eine Schildkr\u00f6te heran:", "tokens": ["Der", "Leicht\u00b7sinn", "trat", "an", "ei\u00b7ne", "Schild\u00b7kr\u00f6\u00b7te", "he\u00b7ran", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+---+", "measure": "unknown.measure.penta"}, "line.2": {"text": "Des Uferloches m\u00fcd will sie die Welt besehen.", "tokens": ["Des", "U\u00b7fer\u00b7lo\u00b7ches", "m\u00fcd", "will", "sie", "die", "Welt", "be\u00b7se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Man schaut sich gern ein neues St\u00fcckchen Erde an,", "tokens": ["Man", "schaut", "sich", "gern", "ein", "neu\u00b7es", "St\u00fcck\u00b7chen", "Er\u00b7de", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "ADV", "ART", "ADJA", "NN", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Wer hinkt, pflegt gern aus dem verha\u00dften Haus zu gehen,", "tokens": ["Wer", "hinkt", ",", "pflegt", "gern", "aus", "dem", "ver\u00b7ha\u00df\u00b7ten", "Haus", "zu", "ge\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "VVFIN", "ADV", "APPR", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Sie, teilt zwei Enten mit, was sie ersann.", "tokens": ["Sie", ",", "teilt", "zwei", "En\u00b7ten", "mit", ",", "was", "sie", "er\u00b7sann", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "VVFIN", "CARD", "NN", "PTKVZ", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die stimmen bei und bieten sich ihr als Gespann", "tokens": ["Die", "stim\u00b7men", "bei", "und", "bie\u00b7ten", "sich", "ihr", "als", "Ge\u00b7spann"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "KON", "VVFIN", "PRF", "PPER", "KOUS", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "F\u00fcr eine Luftfahrt an bis nach Amerika.", "tokens": ["F\u00fcr", "ei\u00b7ne", "Luft\u00b7fahrt", "an", "bis", "nach", "A\u00b7me\u00b7ri\u00b7ka", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "KON", "APPR", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Sie sagten: \u00bbVieles siehst du da,", "tokens": ["Sie", "sag\u00b7ten", ":", "\u00bb", "Vie\u00b7les", "siehst", "du", "da", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PIS", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Gar manche Republik und manches K\u00f6nigreich,", "tokens": ["Gar", "man\u00b7che", "Re\u00b7pub\u00b7lik", "und", "man\u00b7ches", "K\u00f6\u00b7nig\u00b7reich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "V\u00f6lker und Sitten andrer Art als hier am Teich.", "tokens": ["V\u00f6l\u00b7ker", "und", "Sit\u00b7ten", "an\u00b7drer", "Art", "als", "hier", "am", "Teich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "ADJA", "NN", "KOKOM", "ADV", "APPRART", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.7": {"text": "Da lernt man! Auch Ulysses hat es so gemacht.\u00ab", "tokens": ["Da", "lernt", "man", "!", "Auch", "U\u00b7lys\u00b7ses", "hat", "es", "so", "ge\u00b7macht", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PIS", "$.", "ADV", "NE", "VAFIN", "PPER", "ADV", "VVPP", "$.", "$("], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.8": {"text": "Ulysses \u2013 welch verwegener Vergleich!", "tokens": ["U\u00b7lys\u00b7ses", "\u2013", "welch", "ver\u00b7we\u00b7ge\u00b7ner", "Ver\u00b7gleich", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$(", "PWAT", "ADJA", "NN", "$."], "meter": "+--+-+---+", "measure": "iambic.tetra.invert"}, "line.9": {"text": "Wer h\u00e4tte hier an den gedacht! \u2013", "tokens": ["Wer", "h\u00e4t\u00b7te", "hier", "an", "den", "ge\u00b7dacht", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VAFIN", "ADV", "APPR", "ART", "VVPP", "$.", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.10": {"text": "Bald war man einig, wie die Fahrt zu machen sei.", "tokens": ["Bald", "war", "man", "ei\u00b7nig", ",", "wie", "die", "Fahrt", "zu", "ma\u00b7chen", "sei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "ADJD", "$,", "PWAV", "ART", "NN", "PTKZU", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Im Vorbereiten waren sie nicht faul:", "tokens": ["Im", "Vor\u00b7be\u00b7rei\u00b7ten", "wa\u00b7ren", "sie", "nicht", "faul", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Die Enten brachten einen Stock herbei,", "tokens": ["Die", "En\u00b7ten", "brach\u00b7ten", "ei\u00b7nen", "Stock", "her\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Den schoben sie der Schildkr\u00f6t quer durchs Maul.", "tokens": ["Den", "scho\u00b7ben", "sie", "der", "Schild\u00b7kr\u00f6t", "quer", "durchs", "Maul", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "ART", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "\u00bbjetzt gilt es,\u00ab sagten sie, \u00bbrecht fest zu fassen,", "tokens": ["\u00bb", "jetzt", "gilt", "es", ",", "\u00ab", "sag\u00b7ten", "sie", ",", "\u00bb", "recht", "fest", "zu", "fas\u00b7sen", ","], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "ADV", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Und h\u00fcte dich, ihn loszulassen!\u00ab", "tokens": ["Und", "h\u00fc\u00b7te", "dich", ",", "ihn", "los\u00b7zu\u00b7las\u00b7sen", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "$,", "PPER", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Das Vogelpaar ergriff den Stock an beiden Enden", "tokens": ["Das", "Vo\u00b7gel\u00b7paar", "er\u00b7griff", "den", "Stock", "an", "bei\u00b7den", "En\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Und flog mit seiner B\u00fcrde auf.", "tokens": ["Und", "flog", "mit", "sei\u00b7ner", "B\u00fcr\u00b7de", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Da gab es \u00fcberall ein Augenwenden,", "tokens": ["Da", "gab", "es", "\u00fc\u00b7be\u00b7rall", "ein", "Au\u00b7gen\u00b7wen\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.19": {"text": "Verwundert sah man in die Luft hinauf.", "tokens": ["Ver\u00b7wun\u00b7dert", "sah", "man", "in", "die", "Luft", "hin\u00b7auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VVFIN", "PIS", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.20": {"text": "\u00bbkommt,\u00ab rief man, \u00bbkommt und seht! Die K\u00f6nigin", "tokens": ["\u00bb", "kommt", ",", "\u00ab", "rief", "man", ",", "\u00bb", "kommt", "und", "seht", "!", "Die", "K\u00f6\u00b7ni\u00b7gin"], "token_info": ["punct", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["$(", "VVFIN", "$,", "$(", "VVFIN", "PIS", "$,", "$(", "VVFIN", "KON", "VVFIN", "$.", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.21": {"text": "Der Schildkr\u00f6ten zieht durch die Wolken hin.\u00ab", "tokens": ["Der", "Schild\u00b7kr\u00f6\u00b7ten", "zieht", "durch", "die", "Wol\u00b7ken", "hin", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.22": {"text": "\u00bbja, in der Tat: die K\u00f6nigin,\u00ab", "tokens": ["\u00bb", "ja", ",", "in", "der", "Tat", ":", "die", "K\u00f6\u00b7ni\u00b7gin", ",", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "PTKANT", "$,", "APPR", "ART", "NN", "$.", "ART", "NN", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Erklang's von droben; \u00bbkeiner soll zu spotten wagen!\u00ab", "tokens": ["Er\u00b7klang's", "von", "dro\u00b7ben", ";", "\u00bb", "kei\u00b7ner", "soll", "zu", "spot\u00b7ten", "wa\u00b7gen", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "APPR", "ADV", "$.", "$(", "PIS", "VMFIN", "PTKZU", "VVINF", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Sie h\u00e4tte besser dran getan,", "tokens": ["Sie", "h\u00e4t\u00b7te", "bes\u00b7ser", "dran", "ge\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "PAV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Wort darob zu sagen", "tokens": ["Kein", "Wort", "da\u00b7rob", "zu", "sa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "PAV", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und schweigend fortzuziehn auf ihrer hohen Bahn.", "tokens": ["Und", "schwei\u00b7gend", "fort\u00b7zu\u00b7ziehn", "auf", "ih\u00b7rer", "ho\u00b7hen", "Bahn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Da\u00df sprechend sie das Maul ge\u00f6ffnet, mu\u00df sie b\u00fc\u00dfen:", "tokens": ["Da\u00df", "spre\u00b7chend", "sie", "das", "Maul", "ge\u00b7\u00f6ff\u00b7net", ",", "mu\u00df", "sie", "b\u00fc\u00b7\u00dfen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "PPER", "ART", "NN", "VVPP", "$,", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Sie sank vom Stecken, der ihr St\u00fctze bot,", "tokens": ["Sie", "sank", "vom", "Ste\u00b7cken", ",", "der", "ihr", "St\u00fct\u00b7ze", "bot", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "$,", "PRELS", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und lag zerschellt den Schauenden zu F\u00fc\u00dfen.", "tokens": ["Und", "lag", "zer\u00b7schellt", "den", "Schau\u00b7en\u00b7den", "zu", "F\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "VVFIN", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Schwatzhaftigkeit war schuld an ihrem Tod.", "tokens": ["Schwatz\u00b7haf\u00b7tig\u00b7keit", "war", "schuld", "an", "ih\u00b7rem", "Tod", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Neugier, Dummheit, Albernheit,", "tokens": ["Neu\u00b7gier", ",", "Dumm\u00b7heit", ",", "Al\u00b7bern\u00b7heit", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Prahlsucht und Geschw\u00e4tzigkeit", "tokens": ["Prahl\u00b7sucht", "und", "Ge\u00b7schw\u00e4t\u00b7zig\u00b7keit"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Sind einander eng verwandt,", "tokens": ["Sind", "ein\u00b7an\u00b7der", "eng", "ver\u00b7wandt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sind f\u00fcnf Finger einer Hand.", "tokens": ["Sind", "f\u00fcnf", "Fin\u00b7ger", "ei\u00b7ner", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "CARD", "NN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}