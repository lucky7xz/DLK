{"textgrid.poem.61713": {"metadata": {"author": {"name": "Geibel, Emanuel", "birth": "N.A.", "death": "N.A."}, "title": "Schulgeschichten", "genre": "verse", "period": "N.A.", "pub_year": 1833, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wer jemals, war es noch so kurz, auf schmaler Bank", "tokens": ["Wer", "je\u00b7mals", ",", "war", "es", "noch", "so", "kurz", ",", "auf", "schma\u00b7ler", "Bank"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "ADV", "$,", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$,", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Am schr\u00e4gen, vielzerschnittnen Tisch als Sch\u00fcler sa\u00df,", "tokens": ["Am", "schr\u00e4\u00b7gen", ",", "viel\u00b7zer\u00b7schnitt\u00b7nen", "Tisch", "als", "Sch\u00fc\u00b7ler", "sa\u00df", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$,", "ADJA", "NN", "KOUS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der kennt den Reiz von Schulgeschichten. La\u00dft mich denn", "tokens": ["Der", "kennt", "den", "Reiz", "von", "Schul\u00b7ge\u00b7schich\u00b7ten", ".", "La\u00dft", "mich", "denn"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPR", "NN", "$.", "VVIMP", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Derart ein paar berichten! Aber du vergib,", "tokens": ["Der\u00b7art", "ein", "paar", "be\u00b7rich\u00b7ten", "!", "A\u00b7ber", "du", "ver\u00b7gib", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "PIAT", "VVINF", "$.", "KON", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Mein w\u00fcrd'ger Rektor, wenn ich heute scherzend dein", "tokens": ["Mein", "w\u00fcrd'\u00b7ger", "Rek\u00b7tor", ",", "wenn", "ich", "heu\u00b7te", "scher\u00b7zend", "dein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "KOUS", "PPER", "ADV", "ADJD", "PPOSAT"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Im Lied gedenke, z\u00fcrne nicht dem \u00dcbermut;", "tokens": ["Im", "Lied", "ge\u00b7den\u00b7ke", ",", "z\u00fcr\u00b7ne", "nicht", "dem", "\u00dc\u00b7ber\u00b7mut", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "$,", "VVFIN", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Nein, wenn noch Schatten l\u00e4cheln k\u00f6nnen, l\u00e4chle mit!", "tokens": ["Nein", ",", "wenn", "noch", "Schat\u00b7ten", "l\u00e4\u00b7cheln", "k\u00f6n\u00b7nen", ",", "l\u00e4ch\u00b7le", "mit", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KOUS", "ADV", "NN", "VVFIN", "VMINF", "$,", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Noch seh' ich dich im langen Rock von braunem Fries,", "tokens": ["Noch", "seh'", "ich", "dich", "im", "lan\u00b7gen", "Rock", "von", "brau\u00b7nem", "Fries", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPRART", "ADJA", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Kniehoch gestiefelt, hager, auf dem Schulhof stehn,", "tokens": ["Knie\u00b7hoch", "ge\u00b7stie\u00b7felt", ",", "ha\u00b7ger", ",", "auf", "dem", "Schul\u00b7hof", "stehn", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "ADJD", "$,", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Die Uhr in H\u00e4nden, mit gestrengem Herrscherblick", "tokens": ["Die", "Uhr", "in", "H\u00e4n\u00b7den", ",", "mit", "ge\u00b7stren\u00b7gem", "Herr\u00b7scher\u00b7blick"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN", "$,", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Jedweden L\u00e4rm des allzu lauten Knabenschwarms,", "tokens": ["Jed\u00b7we\u00b7den", "L\u00e4rm", "des", "all\u00b7zu", "lau\u00b7ten", "Kna\u00b7ben\u00b7schwarms", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "PTKA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Jedweden Unfug d\u00e4mpfend, bis des Gl\u00f6ckleins Ton", "tokens": ["Jed\u00b7we\u00b7den", "Un\u00b7fug", "d\u00e4mp\u00b7fend", ",", "bis", "des", "Gl\u00f6c\u00b7kleins", "Ton"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVPP", "$,", "APPR", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Vom Pappelplatz uns wieder in die Klassen trieb.", "tokens": ["Vom", "Pap\u00b7pel\u00b7platz", "uns", "wie\u00b7der", "in", "die", "Klas\u00b7sen", "trieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPER", "ADV", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Dein ganzes Wesen \u2013 denn du nanntest nicht umsonst", "tokens": ["Dein", "gan\u00b7zes", "We\u00b7sen", "\u2013", "denn", "du", "nann\u00b7test", "nicht", "um\u00b7sonst"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$(", "KON", "PPER", "VVFIN", "PTKNEG", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Kant deinen Meister \u2013 trug des kategorischen", "tokens": ["Kant", "dei\u00b7nen", "Meis\u00b7ter", "\u2013", "trug", "des", "ka\u00b7te\u00b7go\u00b7ri\u00b7schen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "PPOSAT", "NN", "$(", "VVFIN", "ART", "ADJA"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Imperativus Stempel; jede Miene war", "tokens": ["Im\u00b7pe\u00b7ra\u00b7ti\u00b7vus", "Stem\u00b7pel", ";", "je\u00b7de", "Mie\u00b7ne", "war"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NE", "NE", "$.", "PIAT", "NN", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Und jedes Wort unweigerlicher Machtbefehl.", "tokens": ["Und", "je\u00b7des", "Wort", "un\u00b7wei\u00b7ger\u00b7li\u00b7cher", "Macht\u00b7be\u00b7fehl", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Doch wohnt' in harter Schale dir ein weich Gem\u00fct;", "tokens": ["Doch", "wohnt'", "in", "har\u00b7ter", "Scha\u00b7le", "dir", "ein", "weich", "Ge\u00b7m\u00fct", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN", "PPER", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Denn wohl erinnr' ich's, wie beim herben Leidbericht", "tokens": ["Denn", "wohl", "er\u00b7inn\u00b7r'", "ich's", ",", "wie", "beim", "her\u00b7ben", "Leid\u00b7be\u00b7richt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PIS", "$,", "PWAV", "APPRART", "ADJA", "NN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.20": {"text": "Vom fr\u00fchen Tode Konradins, von Magdeburgs", "tokens": ["Vom", "fr\u00fc\u00b7hen", "To\u00b7de", "Kon\u00b7ra\u00b7dins", ",", "von", "Mag\u00b7de\u00b7burgs"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "NE", "$,", "APPR", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Zerst\u00f6rung pl\u00f6tzlich schluchzend dir die Stimme brach,", "tokens": ["Zer\u00b7st\u00f6\u00b7rung", "pl\u00f6tz\u00b7lich", "schluch\u00b7zend", "dir", "die", "Stim\u00b7me", "brach", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ADJD", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Erstickt von Tr\u00e4nen menschlichwarmen Mitgef\u00fchls.", "tokens": ["Er\u00b7stickt", "von", "Tr\u00e4\u00b7nen", "menschlich\u00b7war\u00b7men", "Mit\u00b7ge\u00b7f\u00fchls", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.23": {"text": "So stehst du fest in meiner Seel', ein w\u00fcrdig Bild.", "tokens": ["So", "stehst", "du", "fest", "in", "mei\u00b7ner", "Seel'", ",", "ein", "w\u00fcr\u00b7dig", "Bild", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJD", "APPR", "PPOSAT", "NN", "$,", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Doch nun erz\u00e4hl' ich, was ich lachend miterlebt,", "tokens": ["Doch", "nun", "er\u00b7z\u00e4hl'", "ich", ",", "was", "ich", "la\u00b7chend", "mi\u00b7ter\u00b7lebt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "$,", "PWS", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Als du zerstreut einst, ohnedies ein wenig taub,", "tokens": ["Als", "du", "zer\u00b7streut", "einst", ",", "oh\u00b7ne\u00b7dies", "ein", "we\u00b7nig", "taub", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ADV", "$,", "KOUI", "ART", "PIS", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Geschichte wiederholte und, den Blick aufs Buch,", "tokens": ["Ge\u00b7schich\u00b7te", "wie\u00b7der\u00b7hol\u00b7te", "und", ",", "den", "Blick", "aufs", "Buch", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "KON", "$,", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Antwort von einem heischtest, der abwesend war.", "tokens": ["Ant\u00b7wort", "von", "ei\u00b7nem", "heischtest", ",", "der", "ab\u00b7we\u00b7send", "war", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "VVFIN", "$,", "PRELS", "ADJD", "VAFIN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "\u00bbwer schlug die Schlacht bei Bautzen, Meyer?\u00ab \u2013 \u00bbMeyer fehlt!\u00ab \u2013", "tokens": ["\u00bb", "wer", "schlug", "die", "Schlacht", "bei", "Baut\u00b7zen", ",", "Me\u00b7yer", "?", "\u00ab", "\u2013", "\u00bb", "Me\u00b7yer", "fehlt", "!", "\u00ab", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "punct", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "ART", "NN", "APPR", "NN", "$,", "NE", "$.", "$(", "$(", "$(", "NE", "VVFIN", "$.", "$(", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "\u00bb's ist falsch. Der N\u00e4chste!\u00ab \u2013 \u00bbMeyer fehlt!\u00ab \u2013 \u00bb's ist wieder falsch.", "tokens": ["\u00bb", "'s", "ist", "falsch", ".", "Der", "N\u00e4chs\u00b7te", "!", "\u00ab", "\u2013", "\u00bb", "Me\u00b7yer", "fehlt", "!", "\u00ab", "\u2013", "\u00bb", "'s", "ist", "wie\u00b7der", "falsch", "."], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ADJD", "$.", "ART", "NN", "$.", "$(", "$(", "$(", "NE", "VVFIN", "$.", "$(", "$(", "$(", "PPER", "VAFIN", "ADV", "ADJD", "$."], "meter": "+-+-+----+-+-+", "measure": "unknown.measure.hexa"}, "line.3": {"text": "Der N\u00e4chste!\u00ab \u2013 \u00bbMeyer ist nicht da!\u00ab \u2013 \u00bbDer Folgende!\u00ab \u2013", "tokens": ["Der", "N\u00e4chs\u00b7te", "!", "\u00ab", "\u2013", "\u00bb", "Me\u00b7yer", "ist", "nicht", "da", "!", "\u00ab", "\u2013", "\u00bb", "Der", "Fol\u00b7gen\u00b7de", "!", "\u00ab", "\u2013"], "token_info": ["word", "word", "punct", "punct", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "$.", "$(", "$(", "$(", "NE", "VAFIN", "PTKNEG", "ADV", "$.", "$(", "$(", "$(", "ART", "NN", "$.", "$(", "$("], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.4": {"text": "\u00bbder Alte scheint im Kopf verr\u00fcckt!\u00ab \u2013 \u00bbGanz recht, mein Sohn.", "tokens": ["\u00bb", "der", "Al\u00b7te", "scheint", "im", "Kopf", "ver\u00b7r\u00fcckt", "!", "\u00ab", "\u2013", "\u00bb", "Ganz", "recht", ",", "mein", "Sohn", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "APPRART", "NN", "VVPP", "$.", "$(", "$(", "$(", "ADV", "ADJD", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nur h\u00e4tt' es Meyer wissen m\u00fcssen so wie du.\u00ab \u2013", "tokens": ["Nur", "h\u00e4tt'", "es", "Me\u00b7yer", "wis\u00b7sen", "m\u00fcs\u00b7sen", "so", "wie", "du", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NE", "VVINF", "VMFIN", "ADV", "KOKOM", "PPER", "$.", "$(", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ein kaum verhaltnes Kichern folgte, doch du fuhrst,", "tokens": ["Ein", "kaum", "ver\u00b7halt\u00b7nes", "Ki\u00b7chern", "folg\u00b7te", ",", "doch", "du", "fuhrst", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "VVFIN", "$,", "KON", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Nichts ahnend, ruhig im Examinieren fort.", "tokens": ["Nichts", "ah\u00b7nend", ",", "ru\u00b7hig", "im", "E\u00b7xa\u00b7mi\u00b7nie\u00b7ren", "fort", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVPP", "$,", "ADJD", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ein andermal ergl\u00fchte freilich zorniger", "tokens": ["Ein", "an\u00b7der\u00b7mal", "er\u00b7gl\u00fch\u00b7te", "frei\u00b7lich", "zor\u00b7ni\u00b7ger"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "VVFIN", "ADV", "ADJA"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Stirne dir, und b\u00f6sen Sturm verhei\u00dfend klang", "tokens": ["Die", "Stir\u00b7ne", "dir", ",", "und", "b\u00f6\u00b7sen", "Sturm", "ver\u00b7hei\u00b7\u00dfend", "klang"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PPER", "$,", "KON", "ADJA", "NN", "VVPP", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Dein s\u00e4chsisch Deutsch ins Ohr mir, als du pl\u00f6tzlich mich", "tokens": ["Dein", "s\u00e4ch\u00b7sisch", "Deutsch", "ins", "Ohr", "mir", ",", "als", "du", "pl\u00f6tz\u00b7lich", "mich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJD", "NN", "APPRART", "NN", "PPER", "$,", "KOUS", "PPER", "ADJD", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Hinweg vom Nepos auf den Gang hinausberiefst.", "tokens": ["Hin\u00b7weg", "vom", "Ne\u00b7pos", "auf", "den", "Gang", "hin\u00b7aus\u00b7be\u00b7riefst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nicht eben herzhaft folgt' ich, war am Tag zuvor", "tokens": ["Nicht", "e\u00b7ben", "herz\u00b7haft", "folgt'", "ich", ",", "war", "am", "Tag", "zu\u00b7vor"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKNEG", "ADV", "ADJD", "VVFIN", "PPER", "$,", "VAFIN", "APPRART", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Doch auf dem Kirchhof von der Jugend Tertias", "tokens": ["Doch", "auf", "dem", "Kirch\u00b7hof", "von", "der", "Ju\u00b7gend", "Ter\u00b7ti\u00b7as"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "APPR", "ART", "NN", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ein blut'ger Hauptstreich wider die Verb\u00fcndeten", "tokens": ["Ein", "blut'\u00b7ger", "Haupt\u00b7streich", "wi\u00b7der", "die", "Ver\u00b7b\u00fcn\u00b7de\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Der Nachbarschulen nur zu siegreich ausgef\u00fchrt.", "tokens": ["Der", "Nach\u00b7bar\u00b7schu\u00b7len", "nur", "zu", "sieg\u00b7reich", "aus\u00b7ge\u00b7f\u00fchrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKA", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Denn mehr als einer war geschunden heimgekehrt,", "tokens": ["Denn", "mehr", "als", "ei\u00b7ner", "war", "ge\u00b7schun\u00b7den", "heim\u00b7ge\u00b7kehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KOUS", "PIS", "VAFIN", "VVPP", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Und nach den R\u00e4delsf\u00fchrern, deren \u00e4rgsten ich", "tokens": ["Und", "nach", "den", "R\u00e4\u00b7dels\u00b7f\u00fch\u00b7rern", ",", "de\u00b7ren", "\u00e4rgs\u00b7ten", "ich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "$,", "PDS", "VVFIN", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Mich selber wu\u00dfte, wurde nun im peinlichen", "tokens": ["Mich", "sel\u00b7ber", "wu\u00df\u00b7te", ",", "wur\u00b7de", "nun", "im", "pein\u00b7li\u00b7chen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "VVFIN", "$,", "VAFIN", "ADV", "APPRART", "ADJA"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Verh\u00f6r geforscht, als g\u00e4lt' es Catilinas Haupt.", "tokens": ["Ver\u00b7h\u00f6r", "ge\u00b7forscht", ",", "als", "g\u00e4lt'", "es", "Ca\u00b7ti\u00b7li\u00b7nas", "Haupt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "KOUS", "VVFIN", "PPER", "NE", "NN", "$."], "meter": "-+-+-+-+---+", "measure": "unknown.measure.penta"}, "line.13": {"text": "Bald war die Schuld ermittelt, und gelind genug", "tokens": ["Bald", "war", "die", "Schuld", "er\u00b7mit\u00b7telt", ",", "und", "ge\u00b7lind", "ge\u00b7nug"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVFIN", "$,", "KON", "ADJD", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Erging der Spruch auf Karzer. Doch nun sollt' ich noch", "tokens": ["Er\u00b7ging", "der", "Spruch", "auf", "Kar\u00b7zer", ".", "Doch", "nun", "sollt'", "ich", "noch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "APPR", "NN", "$.", "KON", "ADV", "VMFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Angeben, wer zugleich mit mir das Volk verf\u00fchrt,", "tokens": ["An\u00b7ge\u00b7ben", ",", "wer", "zu\u00b7gleich", "mit", "mir", "das", "Volk", "ver\u00b7f\u00fchrt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PWS", "ADV", "APPR", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.16": {"text": "Vor allem aber, ob ich mich der F\u00e4uste blo\u00df", "tokens": ["Vor", "al\u00b7lem", "a\u00b7ber", ",", "ob", "ich", "mich", "der", "F\u00e4us\u00b7te", "blo\u00df"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIS", "ADV", "$,", "KOUS", "PPER", "PRF", "ART", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Bedient im Treffen oder zur Bekr\u00e4ftigung", "tokens": ["Be\u00b7dient", "im", "Tref\u00b7fen", "o\u00b7der", "zur", "Be\u00b7kr\u00e4f\u00b7ti\u00b7gung"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "KON", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Der ungl\u00fcckseligen Pr\u00fcgel einen Stock gebraucht,", "tokens": ["Der", "un\u00b7gl\u00fcck\u00b7se\u00b7li\u00b7gen", "Pr\u00fc\u00b7gel", "ei\u00b7nen", "Stock", "ge\u00b7braucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.19": {"text": "Ein ", "tokens": ["Ein"], "token_info": ["word"], "pos": ["ART"], "meter": "+", "measure": "single.up"}, "line.20": {"text": "\u00bbich nicht,\u00ab versetzt' ich, \u00bbaber von den anderen", "tokens": ["\u00bb", "ich", "nicht", ",", "\u00ab", "ver\u00b7setzt'", "ich", ",", "\u00bb", "a\u00b7ber", "von", "den", "an\u00b7de\u00b7ren"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["$(", "PPER", "PTKNEG", "$,", "$(", "VVFIN", "PPER", "$,", "$(", "ADV", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Etwelche m\u00f6gen \u2013\u00ab", "tokens": ["Et\u00b7wel\u00b7che", "m\u00f6\u00b7gen", "\u2013", "\u00ab"], "token_info": ["word", "word", "punct", "punct"], "pos": ["PIS", "VMFIN", "$(", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.22": {"text": "\u00bbm\u00f6gen!!\u00ab fiel er heftig ein,", "tokens": ["\u00bb", "m\u00f6\u00b7gen", "!!", "\u00ab", "fiel", "er", "hef\u00b7tig", "ein", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "$.", "$(", "VVFIN", "PPER", "ADJD", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.23": {"text": "Gleich tief emp\u00f6rt als Rektor und Grammatikus,", "tokens": ["Gleich", "tief", "em\u00b7p\u00f6rt", "als", "Rek\u00b7tor", "und", "Gram\u00b7ma\u00b7ti\u00b7kus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJD", "KOKOM", "NN", "KON", "NN", "$,"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.24": {"text": "\u00bbfalsch angewandter Konjunktiv! Ein Faktum ist's!\u00ab", "tokens": ["\u00bb", "falsch", "an\u00b7ge\u00b7wand\u00b7ter", "Kon\u00b7junk\u00b7tiv", "!", "Ein", "Fak\u00b7tum", "ist's", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "ADJA", "NN", "$.", "ART", "NN", "VAFIN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Und eh' ich dessen mich versehen, hatt' er mir", "tokens": ["Und", "eh'", "ich", "des\u00b7sen", "mich", "ver\u00b7se\u00b7hen", ",", "hatt'", "er", "mir"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "PDS", "PRF", "VVINF", "$,", "VAFIN", "PPER", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Mit schlaffer Hand die Regel ins Gesicht gepr\u00e4gt,", "tokens": ["Mit", "schlaf\u00b7fer", "Hand", "die", "Re\u00b7gel", "ins", "Ge\u00b7sicht", "ge\u00b7pr\u00e4gt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ART", "NN", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Da\u00df mir der Backen stundenlang wie Feuer war.", "tokens": ["Da\u00df", "mir", "der", "Ba\u00b7cken", "stun\u00b7den\u00b7lang", "wie", "Feu\u00b7er", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADJD", "KOKOM", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Doch trug mir dieses Argument ", "tokens": ["Doch", "trug", "mir", "die\u00b7ses", "Ar\u00b7gu\u00b7ment"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.29": {"text": "Heilsame Fr\u00fcchte. Nimmer hab' ich mich seitdem", "tokens": ["Heil\u00b7sa\u00b7me", "Fr\u00fcch\u00b7te", ".", "Nim\u00b7mer", "hab'", "ich", "mich", "seit\u00b7dem"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "$.", "ADV", "VAFIN", "PPER", "PRF", "PAV"], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.30": {"text": "Des Konjunktivs beflissen, wo's ein Faktum galt;", "tokens": ["Des", "Kon\u00b7junk\u00b7tivs", "be\u00b7flis\u00b7sen", ",", "wo's", "ein", "Fak\u00b7tum", "galt", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,", "PWAV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Selbst nicht bei Hof. Und das war manchmal schwer genug.", "tokens": ["Selbst", "nicht", "bei", "Hof", ".", "Und", "das", "war", "manch\u00b7mal", "schwer", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "APPR", "NE", "$.", "KON", "PDS", "VAFIN", "ADV", "ADJD", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}