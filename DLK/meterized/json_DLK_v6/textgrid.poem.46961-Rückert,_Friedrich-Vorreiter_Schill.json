{"textgrid.poem.46961": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "Vorreiter Schill", "genre": "verse", "period": "N.A.", "pub_year": 1827, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Die ihr reitet im Mondenlicht,", "tokens": ["Die", "ihr", "rei\u00b7tet", "im", "Mon\u00b7den\u00b7licht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Verge\u00dft doch euren Vorreiter nicht.", "tokens": ["Ver\u00b7ge\u00dft", "doch", "eu\u00b7ren", "Vor\u00b7rei\u00b7ter", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPOSAT", "NN", "PTKNEG", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wo reitet ihr hin im Mondenlicht?", "tokens": ["Wo", "rei\u00b7tet", "ihr", "hin", "im", "Mon\u00b7den\u00b7licht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Kennt ihr eueren Vorreiter nicht?", "tokens": ["Kennt", "ihr", "eu\u00b7e\u00b7ren", "Vor\u00b7rei\u00b7ter", "nicht", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "PTKNEG", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.3": {"line.1": {"text": "Ich bin vor euch her geritten,", "tokens": ["Ich", "bin", "vor", "euch", "her", "ge\u00b7rit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ich hab' im stillen euch Bahn gemacht;", "tokens": ["Ich", "hab'", "im", "stil\u00b7len", "euch", "Bahn", "ge\u00b7macht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "VVFIN", "PPER", "NN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ich bin vor euch her geritten,", "tokens": ["Ich", "bin", "vor", "euch", "her", "ge\u00b7rit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Vier Jahre schon vor der L\u00fctzener Schlacht.", "tokens": ["Vier", "Jah\u00b7re", "schon", "vor", "der", "L\u00fct\u00b7ze\u00b7ner", "Schlacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.4": {"line.1": {"text": "Ich bin vor euch her geritten,", "tokens": ["Ich", "bin", "vor", "euch", "her", "ge\u00b7rit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und h\u00e4tten alle wie ich es gemacht,", "tokens": ["Und", "h\u00e4t\u00b7ten", "al\u00b7le", "wie", "ich", "es", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIS", "KOKOM", "PPER", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "So w\u00e4re die Freiheit erstritten,", "tokens": ["So", "w\u00e4\u00b7re", "die", "Frei\u00b7heit", "er\u00b7strit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und h\u00e4tte bedurft nicht der L\u00fctzener Schlacht.", "tokens": ["Und", "h\u00e4t\u00b7te", "be\u00b7durft", "nicht", "der", "L\u00fct\u00b7ze\u00b7ner", "Schlacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "VVFIN", "PTKNEG", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.5": {"line.1": {"text": "Ich bin vor euch her geritten,", "tokens": ["Ich", "bin", "vor", "euch", "her", "ge\u00b7rit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Mit kleinerem H\u00e4uflein als ihr noch seid,", "tokens": ["Mit", "klei\u00b7ne\u00b7rem", "H\u00e4uf\u00b7lein", "als", "ihr", "noch", "seid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KOUS", "PPER", "ADV", "VAFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Freihin durch Deutschlands Mitten,", "tokens": ["Frei\u00b7hin", "durch", "Deutschlands", "Mit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Es war gar nicht vor den Feinden mir leid.", "tokens": ["Es", "war", "gar", "nicht", "vor", "den", "Fein\u00b7den", "mir", "leid", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "APPR", "ART", "NN", "PPER", "ADJD", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "Ich bin hindurch geritten,", "tokens": ["Ich", "bin", "hin\u00b7durch", "ge\u00b7rit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PAV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Es hat mich gefangen kein Franzenheer,", "tokens": ["Es", "hat", "mich", "ge\u00b7fan\u00b7gen", "kein", "Fran\u00b7zen\u00b7heer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "PIAT", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Ich habe mich durchgestritten,", "tokens": ["Ich", "ha\u00b7be", "mich", "durch\u00b7ge\u00b7strit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und bin geritten bis an das Meer.", "tokens": ["Und", "bin", "ge\u00b7rit\u00b7ten", "bis", "an", "das", "Meer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "VVPP", "KON", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Ich habe mich durchgestritten,", "tokens": ["Ich", "ha\u00b7be", "mich", "durch\u00b7ge\u00b7strit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ich bin geritten bis nach Stralsund;", "tokens": ["Ich", "bin", "ge\u00b7rit\u00b7ten", "bis", "nach", "Stral\u00b7sund", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "KON", "APPR", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da wollt' ich hin\u00fcber zum Britten,", "tokens": ["Da", "wollt'", "ich", "hin\u00b7\u00fc\u00b7ber", "zum", "Brit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Da hat mich gebissen ein franzischer Hund.", "tokens": ["Da", "hat", "mich", "ge\u00b7bis\u00b7sen", "ein", "fran\u00b7zi\u00b7scher", "Hund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVPP", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.8": {"line.1": {"text": "Er hat mich in'n Schenkel gebissen,", "tokens": ["Er", "hat", "mich", "in'n", "Schen\u00b7kel", "ge\u00b7bis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPRART", "NN", "VVPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Da\u00df ich von meinem Schimmel fiel;", "tokens": ["Da\u00df", "ich", "von", "mei\u00b7nem", "Schim\u00b7mel", "fiel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er hat mir den Kopf abgerissen,", "tokens": ["Er", "hat", "mir", "den", "Kopf", "ab\u00b7ge\u00b7ris\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hat damit getrieben sein Spiel.", "tokens": ["Und", "hat", "da\u00b7mit", "ge\u00b7trie\u00b7ben", "sein", "Spiel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PAV", "VVPP", "VAINF", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.9": {"line.1": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Nehmt euch vor den franzischen Hunden in acht,", "tokens": ["Nehmt", "euch", "vor", "den", "fran\u00b7zi\u00b7schen", "Hun\u00b7den", "in", "acht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "ADJA", "NN", "APPR", "CARD", "$,"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Da\u00df sie's nicht euch machen, ihr J\u00e4ger,", "tokens": ["Da\u00df", "sie's", "nicht", "euch", "ma\u00b7chen", ",", "ihr", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "PPER", "VVINF", "$,", "PPOSAT", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Wie sie's eurem Vorreiter gemacht.", "tokens": ["Wie", "sie's", "eu\u00b7rem", "Vor\u00b7rei\u00b7ter", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-++--+", "measure": "iambic.penta.chol"}}, "stanza.10": {"line.1": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Die ihr reitet im Mondenlicht,", "tokens": ["Die", "ihr", "rei\u00b7tet", "im", "Mon\u00b7den\u00b7licht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Ihr schwarzen Gewandes Tr\u00e4ger,", "tokens": ["Ihr", "schwar\u00b7zen", "Ge\u00b7wan\u00b7des", "Tr\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ihr R\u00e4cher, verge\u00dft euern Vorreiter nicht.", "tokens": ["Ihr", "R\u00e4\u00b7cher", ",", "ver\u00b7ge\u00dft", "eu\u00b7ern", "Vor\u00b7rei\u00b7ter", "nicht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VVFIN", "PPOSAT", "NN", "PTKNEG", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.11": {"line.1": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wo reitet ihr hin im Mondenschein?", "tokens": ["Wo", "rei\u00b7tet", "ihr", "hin", "im", "Mon\u00b7den\u00b7schein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ich bin nur ein Geist, doch kein tr\u00e4ger,", "tokens": ["Ich", "bin", "nur", "ein", "Geist", ",", "doch", "kein", "tr\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "$,", "ADV", "PIAT", "ADJA", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Ich kann noch jetzt euer Vorreiter sein.", "tokens": ["Ich", "kann", "noch", "jetzt", "eu\u00b7er", "Vor\u00b7rei\u00b7ter", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "PPOSAT", "NN", "VAINF", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.12": {"line.1": {"text": "Ihr k\u00fchnen L\u00fctzowschen J\u00e4ger,", "tokens": ["Ihr", "k\u00fch\u00b7nen", "L\u00fct\u00b7zow\u00b7schen", "J\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "La\u00dft mich euern Vorreiter sein;", "tokens": ["La\u00dft", "mich", "eu\u00b7ern", "Vor\u00b7rei\u00b7ter", "sein", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPOSAT", "NN", "VAINF", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Ihr deutscher Rache Tr\u00e4ger,", "tokens": ["Ihr", "deut\u00b7scher", "Ra\u00b7che", "Tr\u00e4\u00b7ger", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Mir nach! Ich reit' euch voran zum Rhein.", "tokens": ["Mir", "nach", "!", "Ich", "reit'", "euch", "vo\u00b7ran", "zum", "Rhein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKVZ", "$.", "PPER", "VVFIN", "PPER", "ADV", "APPRART", "NE", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}}}}