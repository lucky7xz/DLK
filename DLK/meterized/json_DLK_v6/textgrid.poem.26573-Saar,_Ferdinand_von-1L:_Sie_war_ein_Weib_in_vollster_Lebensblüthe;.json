{"textgrid.poem.26573": {"metadata": {"author": {"name": "Saar, Ferdinand von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Sie war ein Weib in vollster Lebensbl\u00fcthe;", "genre": "verse", "period": "N.A.", "pub_year": 1869, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sie war ein Weib in vollster Lebensbl\u00fcthe;", "tokens": ["Sie", "war", "ein", "Weib", "in", "volls\u00b7ter", "Le\u00b7bens\u00b7bl\u00fct\u00b7he", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Nicht ohne Launen ganz und ohne Schw\u00e4chen \u2013", "tokens": ["Nicht", "oh\u00b7ne", "Lau\u00b7nen", "ganz", "und", "oh\u00b7ne", "Schw\u00e4\u00b7chen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "NN", "ADV", "KON", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Im Inn'ren aber war sie laut're G\u00fcte.", "tokens": ["Im", "Inn'\u00b7ren", "a\u00b7ber", "war", "sie", "laut'\u00b7re", "G\u00fc\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "VAFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Sie ha\u00dfte Redeprunk und Silbenstechen;", "tokens": ["Sie", "ha\u00df\u00b7te", "Re\u00b7de\u00b7prunk", "und", "Sil\u00b7ben\u00b7ste\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Stumm sa\u00df sie meist mit sinnenden Geberden,", "tokens": ["Stumm", "sa\u00df", "sie", "meist", "mit", "sin\u00b7nen\u00b7den", "Ge\u00b7ber\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Nur ihre dunklen Augen lie\u00df sie sprechen.", "tokens": ["Nur", "ih\u00b7re", "dunk\u00b7len", "Au\u00b7gen", "lie\u00df", "sie", "spre\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "VVFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Versagt geblieben war ihr nichts auf Erden \u2013", "tokens": ["Ver\u00b7sagt", "ge\u00b7blie\u00b7ben", "war", "ihr", "nichts", "auf", "Er\u00b7den", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "VVPP", "VAFIN", "PPER", "PIS", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Doch in Erf\u00fcllung konnte nie ermatten", "tokens": ["Doch", "in", "Er\u00b7f\u00fcl\u00b7lung", "konn\u00b7te", "nie", "er\u00b7mat\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NN", "VMFIN", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ihr Wunsch, zu lieben und geliebt zu werden.", "tokens": ["Ihr", "Wunsch", ",", "zu", "lie\u00b7ben", "und", "ge\u00b7liebt", "zu", "wer\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PTKZU", "VVINF", "KON", "VVPP", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Nicht zu den Klugen z\u00e4hlte sie und Satten;", "tokens": ["Nicht", "zu", "den", "Klu\u00b7gen", "z\u00e4hl\u00b7te", "sie", "und", "Sat\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "ART", "NN", "VVFIN", "PPER", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ich ahnte wohl \u2013 doch mocht' ich's nie erfahren,", "tokens": ["Ich", "ahn\u00b7te", "wohl", "\u2013", "doch", "mocht'", "ich's", "nie", "er\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$(", "ADV", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Warum sie treulos ward dem jungen Gatten.", "tokens": ["Wa\u00b7rum", "sie", "treu\u00b7los", "ward", "dem", "jun\u00b7gen", "Gat\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADJD", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Ich wei\u00df nur, da\u00df mit stillem Offenbaren", "tokens": ["Ich", "wei\u00df", "nur", ",", "da\u00df", "mit", "stil\u00b7lem", "Of\u00b7fen\u00b7ba\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "$,", "KOUS", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ihr Mund mich k\u00fc\u00dfte, wie im Widerstreiten \u2013", "tokens": ["Ihr", "Mund", "mich", "k\u00fc\u00df\u00b7te", ",", "wie", "im", "Wi\u00b7der\u00b7strei\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPER", "VVFIN", "$,", "PWAV", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und da\u00df wir Beide fortan gl\u00fccklich waren.", "tokens": ["Und", "da\u00df", "wir", "Bei\u00b7de", "for\u00b7tan", "gl\u00fcck\u00b7lich", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PIS", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "O gold'ne Zeit verschwiegner Seligkeiten,", "tokens": ["O", "gold'\u00b7ne", "Zeit", "ver\u00b7schwieg\u00b7ner", "Se\u00b7lig\u00b7kei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Die du so reich mir damals angebrochen \u2013", "tokens": ["Die", "du", "so", "reich", "mir", "da\u00b7mals", "an\u00b7ge\u00b7bro\u00b7chen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "ADJD", "PPER", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "La\u00df deinen Wiederschein mein Herz durchgleiten!", "tokens": ["La\u00df", "dei\u00b7nen", "Wie\u00b7der\u00b7schein", "mein", "Herz", "durch\u00b7glei\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPOSAT", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "O holde Tage, o vertr\u00e4umte Wochen", "tokens": ["O", "hol\u00b7de", "Ta\u00b7ge", ",", "o", "ver\u00b7tr\u00e4um\u00b7te", "Wo\u00b7chen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADJA", "NN", "$,", "FM", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Auf hohem Schlo\u00df, in freien Sommerfluren \u2013", "tokens": ["Auf", "ho\u00b7hem", "Schlo\u00df", ",", "in", "frei\u00b7en", "Som\u00b7mer\u00b7flu\u00b7ren", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Im Bann der Stadt auch und in ihren Jochen!", "tokens": ["Im", "Bann", "der", "Stadt", "auch", "und", "in", "ih\u00b7ren", "Jo\u00b7chen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "ADV", "KON", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Da schritten wir vereint auf Rosenspuren,", "tokens": ["Da", "schrit\u00b7ten", "wir", "ver\u00b7eint", "auf", "Ro\u00b7sen\u00b7spu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Da sa\u00dfen wir gen\u00fcber uns im Wagen,", "tokens": ["Da", "sa\u00b7\u00dfen", "wir", "ge\u00b7n\u00fc\u00b7ber", "uns", "im", "Wa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wenn wir zum Wald \u2013 wenn wir zur Oper fuhren!", "tokens": ["Wenn", "wir", "zum", "Wald", "\u2013", "wenn", "wir", "zur", "O\u00b7per", "fuh\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "$(", "KOUS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "O sel'ges Gl\u00fcck, den weichen Shawl zu tragen \u2013", "tokens": ["O", "sel'\u00b7ges", "Gl\u00fcck", ",", "den", "wei\u00b7chen", "Shawl", "zu", "tra\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dicht hinter sie in Logen mich zu schmiegen", "tokens": ["Dicht", "hin\u00b7ter", "sie", "in", "Lo\u00b7gen", "mich", "zu", "schmie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "APPR", "PPER", "APPR", "NN", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und einen Ku\u00df auf wei\u00dfe Schultern wagen.", "tokens": ["Und", "ei\u00b7nen", "Ku\u00df", "auf", "wei\u00b7\u00dfe", "Schul\u00b7tern", "wa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Und dann in hohen Freuden sich zu wiegen,", "tokens": ["Und", "dann", "in", "ho\u00b7hen", "Freu\u00b7den", "sich", "zu", "wie\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN", "PRF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wie sie nur bieten kann verbot'nes Minnen,", "tokens": ["Wie", "sie", "nur", "bie\u00b7ten", "kann", "ver\u00b7bot'\u00b7nes", "Min\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "VVINF", "VMFIN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Im Tanz vereint, den weiten Saal durchfliegen!", "tokens": ["Im", "Tanz", "ver\u00b7eint", ",", "den", "wei\u00b7ten", "Saal", "durch\u00b7flie\u00b7gen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$,", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Da lernten wir den Augenblick gewinnen", "tokens": ["Da", "lern\u00b7ten", "wir", "den", "Au\u00b7gen\u00b7blick", "ge\u00b7win\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und \u2013 konnt' uns doch der n\u00e4chste schon gef\u00e4hrden!", "tokens": ["Und", "\u2013", "konnt'", "uns", "doch", "der", "n\u00e4chs\u00b7te", "schon", "ge\u00b7f\u00e4hr\u00b7den", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "VMFIN", "PPER", "ADV", "ART", "ADJA", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Im Augenblick Unm\u00f6gliches ersinnen.", "tokens": ["Im", "Au\u00b7gen\u00b7blick", "Un\u00b7m\u00f6g\u00b7li\u00b7ches", "er\u00b7sin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "Vorbei! Vorbei! Ein Ende mu\u00dfte werden,", "tokens": ["Vor\u00b7bei", "!", "Vor\u00b7bei", "!", "Ein", "En\u00b7de", "mu\u00df\u00b7te", "wer\u00b7den", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "ADV", "$.", "ART", "NN", "VMFIN", "VAINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und bin ich auch nicht ungestraft geblieben \u2013", "tokens": ["Und", "bin", "ich", "auch", "nicht", "un\u00b7ge\u00b7straft", "ge\u00b7blie\u00b7ben", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Denn welche Schuld entr\u00e4nne hier auf Erden:", "tokens": ["Denn", "wel\u00b7che", "Schuld", "ent\u00b7r\u00e4n\u00b7ne", "hier", "auf", "Er\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAT", "NN", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.13": {"line.1": {"text": "So wu\u00dft' ich doch, was ", "tokens": ["So", "wu\u00dft'", "ich", "doch", ",", "was"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PWS"], "meter": "-+--+", "measure": "iambic.di.chol"}}}}}