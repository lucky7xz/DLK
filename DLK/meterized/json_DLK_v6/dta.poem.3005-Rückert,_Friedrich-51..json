{"dta.poem.3005": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "51.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1838", "urn": "urn:nbn:de:kobv:b4-200905195108", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Sag': Ich bin Ich! Und wie du sagest, f\u00fchl' es auch:", "tokens": ["Sag'", ":", "Ich", "bin", "Ich", "!", "Und", "wie", "du", "sa\u00b7gest", ",", "f\u00fchl'", "es", "auch", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "VAFIN", "PPER", "$.", "KON", "PWAV", "PPER", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In deinem kleinen Ich des gro\u00dfen Iches Hauch.", "tokens": ["In", "dei\u00b7nem", "klei\u00b7nen", "Ich", "des", "gro\u00b7\u00dfen", "I\u00b7ches", "Hauch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "PPER", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Sag': Ich bin Ich! und dich in den Gedanken senke:", "tokens": ["Sag'", ":", "Ich", "bin", "Ich", "!", "und", "dich", "in", "den", "Ge\u00b7dan\u00b7ken", "sen\u00b7ke", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "VAFIN", "PPER", "$.", "KON", "PRF", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Ich denke was ich bin, und bin das was ich denke.", "tokens": ["Ich", "den\u00b7ke", "was", "ich", "bin", ",", "und", "bin", "das", "was", "ich", "den\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PWS", "PPER", "VAFIN", "$,", "KON", "VAFIN", "PDS", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ich von mir selber kann nicht unterschieden seyn,", "tokens": ["Ich", "von", "mir", "sel\u00b7ber", "kann", "nicht", "un\u00b7ter\u00b7schie\u00b7den", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PPER", "ADV", "VMFIN", "PTKNEG", "VVPP", "VAINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mein Seyn vom Denken nicht, mein Denken nicht vom Seyn.", "tokens": ["Mein", "Seyn", "vom", "Den\u00b7ken", "nicht", ",", "mein", "Den\u00b7ken", "nicht", "vom", "Seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPRART", "NN", "PTKNEG", "$,", "PPOSAT", "NN", "PTKNEG", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Ich unterscheide mich, nicht mich von mir zu trennen,", "tokens": ["Ich", "un\u00b7ter\u00b7schei\u00b7de", "mich", ",", "nicht", "mich", "von", "mir", "zu", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PTKNEG", "PRF", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich unterscheide mich, als Eins mich zu erkennen.", "tokens": ["Ich", "un\u00b7ter\u00b7schei\u00b7de", "mich", ",", "als", "Eins", "mich", "zu", "er\u00b7ken\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KOUS", "NN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Dann wenn du eingesenkt dich hast in den Gedanken,", "tokens": ["Dann", "wenn", "du", "ein\u00b7ge\u00b7senkt", "dich", "hast", "in", "den", "Ge\u00b7dan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "VVFIN", "PPER", "VAFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Erheb dich auch daraus, und fleug ob allen Schranken.", "tokens": ["Er\u00b7heb", "dich", "auch", "da\u00b7raus", ",", "und", "fleug", "ob", "al\u00b7len", "Schran\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PAV", "$,", "KON", "NN", "KOUS", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Sag': Ich bin Ich! und wer wie ich sagt Ich bin Ich,", "tokens": ["Sag'", ":", "Ich", "bin", "Ich", "!", "und", "wer", "wie", "ich", "sagt", "Ich", "bin", "Ich", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "VAFIN", "PPER", "$.", "KON", "PWS", "KOKOM", "PPER", "VVFIN", "PPER", "VAFIN", "PPER", "$,"], "meter": "+-+-+-+-+-++", "measure": "unknown.measure.septa"}, "line.2": {"text": "Ist Ich wie ich, von ihm wie unterscheid' ich mich?", "tokens": ["Ist", "Ich", "wie", "ich", ",", "von", "ihm", "wie", "un\u00b7ter\u00b7scheid'", "ich", "mich", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "KOKOM", "PPER", "$,", "APPR", "PPER", "KOKOM", "VVFIN", "PPER", "PRF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Ich unterscheide mich, nicht mich von ihm zu trennen,", "tokens": ["Ich", "un\u00b7ter\u00b7schei\u00b7de", "mich", ",", "nicht", "mich", "von", "ihm", "zu", "tren\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PTKNEG", "PRF", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich unterscheide mich, als Eins uns zu erkennen.", "tokens": ["Ich", "un\u00b7ter\u00b7schei\u00b7de", "mich", ",", "als", "Eins", "uns", "zu", "er\u00b7ken\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KOUS", "NN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "So ist geschieden ungeschieden Ich vom Ich:", "tokens": ["So", "ist", "ge\u00b7schie\u00b7den", "un\u00b7ge\u00b7schie\u00b7den", "Ich", "vom", "Ich", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJD", "ADJA", "PPER", "APPRART", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Alle zusammen Eins, und jedes Eins f\u00fcr sich.", "tokens": ["Al\u00b7le", "zu\u00b7sam\u00b7men", "Eins", ",", "und", "je\u00b7des", "Eins", "f\u00fcr", "sich", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,", "KON", "PIAT", "NN", "APPR", "PRF", "$."], "meter": "---+-+-+-+-+", "measure": "unknown.measure.penta"}}, "stanza.9": {"line.1": {"text": "Ein Ganzes in sich selbst das Gr\u00f6ste wie das Kleinste,", "tokens": ["Ein", "Gan\u00b7zes", "in", "sich", "selbst", "das", "Gr\u00f6s\u00b7te", "wie", "das", "Kleins\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PRF", "ADV", "ART", "NN", "KOKOM", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und das Besonderste zugleich das Allgemeinste.", "tokens": ["Und", "das", "Be\u00b7son\u00b7ders\u00b7te", "zu\u00b7gleich", "das", "All\u00b7ge\u00b7meins\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Gott ist das Gro\u00dfe Ich, das selb sich seiend denkt,", "tokens": ["Gott", "ist", "das", "Gro\u00b7\u00dfe", "Ich", ",", "das", "selb", "sich", "sei\u00b7end", "denkt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "PPER", "$,", "PRELS", "ADJD", "PRF", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Sein Selbst in jeglichen Gedanken so versenkt,", "tokens": ["Sein", "Selbst", "in", "jeg\u00b7li\u00b7chen", "Ge\u00b7dan\u00b7ken", "so", "ver\u00b7senkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "APPR", "PIAT", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Da\u00df der Gedanke, der geworden \u00e4u\u00dferlich,", "tokens": ["Da\u00df", "der", "Ge\u00b7dan\u00b7ke", ",", "der", "ge\u00b7wor\u00b7den", "\u00e4u\u00b7\u00dfer\u00b7lich", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$,", "PRELS", "VAPP", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Nur wieder zu sich kommt, wenn er sagt Ich bin Ich;", "tokens": ["Nur", "wie\u00b7der", "zu", "sich", "kommt", ",", "wenn", "er", "sagt", "Ich", "bin", "Ich", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "PRF", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "PPER", "VAFIN", "PPER", "$."], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}}, "stanza.12": {"line.1": {"text": "Wenn du dich selber denkst als ewigen Gedanken", "tokens": ["Wenn", "du", "dich", "sel\u00b7ber", "denkst", "als", "e\u00b7wi\u00b7gen", "Ge\u00b7dan\u00b7ken"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "ADV", "VVFIN", "KOKOM", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Des ewig Denkenden, um ewig ihm zu danken.", "tokens": ["Des", "e\u00b7wig", "Den\u00b7ken\u00b7den", ",", "um", "e\u00b7wig", "ihm", "zu", "dan\u00b7ken", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "$,", "KOUI", "ADJD", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Darum nur Ich bin Ich sag' ewig, o Brahman,", "tokens": ["Da\u00b7rum", "nur", "Ich", "bin", "Ich", "sag'", "e\u00b7wig", ",", "o", "Brah\u00b7man", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PAV", "ADV", "PPER", "VAFIN", "PPER", "VVFIN", "ADJD", "$,", "FM", "NN", "$,"], "meter": "--+-+-+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Weil ewig Ich bin Ich dir Brahma sagt voran.", "tokens": ["Weil", "e\u00b7wig", "Ich", "bin", "Ich", "dir", "Brah\u00b7ma", "sagt", "vo\u00b7ran", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "PPER", "VAFIN", "PPER", "PPER", "NE", "VVFIN", "PTKVZ", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}}, "stanza.14": {"line.1": {"text": "Was sagt Bruwann Aham? Es saget: Sagend Ich", "tokens": ["Was", "sagt", "Bru\u00b7wann", "A\u00b7ham", "?", "Es", "sa\u00b7get", ":", "Sa\u00b7gend", "Ich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PWS", "VVFIN", "NE", "NE", "$.", "PPER", "VVFIN", "$.", "NN", "PPER"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und davon, o Brahman, gek\u00fcrzt nennt Brahma sich.", "tokens": ["Und", "da\u00b7von", ",", "o", "Brah\u00b7man", ",", "ge\u00b7k\u00fcrzt", "nennt", "Brah\u00b7ma", "sich", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "$,", "FM", "NN", "$,", "VVPP", "VVFIN", "NE", "PRF", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}}}}}