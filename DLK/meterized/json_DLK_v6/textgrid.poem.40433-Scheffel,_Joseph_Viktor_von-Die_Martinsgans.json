{"textgrid.poem.40433": {"metadata": {"author": {"name": "Scheffel, Joseph Viktor von", "birth": "N.A.", "death": "N.A."}, "title": "Die Martinsgans", "genre": "verse", "period": "N.A.", "pub_year": 1856, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Mensch ist ein Barbar von Natur,", "tokens": ["Der", "Mensch", "ist", "ein", "Bar\u00b7bar", "von", "Na\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Er achtet nicht im mindesten die Nebenkreatur,", "tokens": ["Er", "ach\u00b7tet", "nicht", "im", "min\u00b7des\u00b7ten", "die", "Ne\u00b7ben\u00b7kre\u00b7a\u00b7tur", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPRART", "ADJA", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.3": {"text": "Tut sieden sie und braten,", "tokens": ["Tut", "sie\u00b7den", "sie", "und", "bra\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "KON", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Verspeist sie mit Salaten,", "tokens": ["Ver\u00b7speist", "sie", "mit", "Sa\u00b7la\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Sch\u00fctt't Wein oben drauf aus g\u00fcldnem Gef\u00e4\u00df", "tokens": ["Sch\u00fctt't", "Wein", "o\u00b7ben", "drauf", "aus", "g\u00fcld\u00b7nem", "Ge\u00b7f\u00e4\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NN", "ADV", "PAV", "APPR", "ADJA", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Und nennt das gelehrt: Ern\u00e4hrungsproze\u00df.", "tokens": ["Und", "nennt", "das", "ge\u00b7lehrt", ":", "Er\u00b7n\u00e4h\u00b7rungs\u00b7pro\u00b7ze\u00df", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VVFIN", "PDS", "VVPP", "$.", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Mich gute Gans haben s' auch erwischt", "tokens": ["Mich", "gu\u00b7te", "Gans", "ha\u00b7ben", "s'", "auch", "er\u00b7wischt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADJA", "NN", "VAFIN", "NE", "ADV", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und allezeit gerupft und aufgetischt.", "tokens": ["Und", "al\u00b7le\u00b7zeit", "ge\u00b7rupft", "und", "auf\u00b7ge\u00b7tischt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Zum K\u00f6nige Gambrinus", "tokens": ["Zum", "K\u00f6\u00b7ni\u00b7ge", "Gam\u00b7bri\u00b7nus"], "token_info": ["word", "word", "word"], "pos": ["APPRART", "NN", "NE"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sprach einst schon Sankt Martinus:", "tokens": ["Sprach", "einst", "schon", "Sankt", "Mar\u00b7ti\u00b7nus", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "VVFIN", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "\u00bbdie Welt, edler Herr, ist nicht viel n\u00fctz,", "tokens": ["\u00bb", "die", "Welt", ",", "ed\u00b7ler", "Herr", ",", "ist", "nicht", "viel", "n\u00fctz", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "$,", "ADJA", "NN", "$,", "VAFIN", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-++-+-+-+", "measure": "unknown.measure.penta"}, "line.6": {"text": "Doch trefflich schmeckt zu Bier wie Wein ein Pfaffenschnitz.\u00ab", "tokens": ["Doch", "treff\u00b7lich", "schmeckt", "zu", "Bier", "wie", "Wein", "ein", "Pfaf\u00b7fen\u00b7schnitz", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADJD", "VVFIN", "APPR", "NN", "KOKOM", "NN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Der eilfte Novembris war der Tag,", "tokens": ["Der", "eilf\u00b7te", "No\u00b7vemb\u00b7ris", "war", "der", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Allwo er dieses Wort mit Nachdruck sprach;", "tokens": ["All\u00b7wo", "er", "die\u00b7ses", "Wort", "mit", "Nach\u00b7druck", "sprach", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PDAT", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Drum braten brave Leute", "tokens": ["Drum", "bra\u00b7ten", "bra\u00b7ve", "Leu\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die Martinsgans noch heute,", "tokens": ["Die", "Mar\u00b7tins\u00b7gans", "noch", "heu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "ADV", "ADV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ich armer Vogel, ist das mein Lohn,", "tokens": ["Ich", "ar\u00b7mer", "Vo\u00b7gel", ",", "ist", "das", "mein", "Lohn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NE", "$,", "VAFIN", "ART", "PPOSAT", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Da\u00df man mich tot verzehret auf Subskription?", "tokens": ["Da\u00df", "man", "mich", "tot", "ver\u00b7zeh\u00b7ret", "auf", "Sub\u00b7skrip\u00b7ti\u00b7on", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PRF", "ADJD", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Wie anders war's, da auf der Weid", "tokens": ["Wie", "an\u00b7ders", "wa\u00b7r's", ",", "da", "auf", "der", "Weid"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VAFIN", "$,", "KOUS", "APPR", "ART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Als G\u00e4nsulein ich prangte im Fl\u00fcgelkleid?!", "tokens": ["Als", "G\u00e4n\u00b7sul\u00b7ein", "ich", "prang\u00b7te", "im", "Fl\u00fc\u00b7gel\u00b7kleid", "?!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Auf ", "tokens": ["Auf"], "token_info": ["word"], "pos": ["APPR"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Und Aug' und Schnabel drehend", "tokens": ["Und", "Aug'", "und", "Schna\u00b7bel", "dre\u00b7hend"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NN", "CARD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Zum Liebsten, der just \u00fcber den Rhein", "tokens": ["Zum", "Liebs\u00b7ten", ",", "der", "just", "\u00fc\u00b7ber", "den", "Rhein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "$,", "PRELS", "ADV", "APPR", "ART", "NE"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.6": {"text": "In m\u00e4nnlicher Reife als G\u00e4nserich kam heim.", "tokens": ["In", "m\u00e4nn\u00b7li\u00b7cher", "Rei\u00b7fe", "als", "G\u00e4n\u00b7se\u00b7rich", "kam", "heim", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KOUS", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}}, "stanza.5": {"line.1": {"text": "O h\u00e4tt' ich nie gemu\u00dft in die Stadt,", "tokens": ["O", "h\u00e4tt'", "ich", "nie", "ge\u00b7mu\u00dft", "in", "die", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "ADV", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Wo niemals eine K\u00f6chin eine Bildung hat!", "tokens": ["Wo", "nie\u00b7mals", "ei\u00b7ne", "K\u00f6\u00b7chin", "ei\u00b7ne", "Bil\u00b7dung", "hat", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Sie lachte sehr gemeine", "tokens": ["Sie", "lach\u00b7te", "sehr", "ge\u00b7mei\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und pre\u00dft' mich an die Beine", "tokens": ["Und", "pre\u00dft'", "mich", "an", "die", "Bei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Und sprach: \u00bbOb's dich auch dr\u00fcckt und verkropft,", "tokens": ["Und", "sprach", ":", "\u00bb", "Ob's", "dich", "auch", "dr\u00fcckt", "und", "ver\u00b7kropft", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NE", "PPER", "ADV", "VVFIN", "KON", "VVPP", "$,"], "meter": "--+--+--+", "measure": "anapaest.tri.plus"}, "line.6": {"text": "Mit Welschkorn wirst du jetzt vollgestopft!\u00ab", "tokens": ["Mit", "Welschkorn", "wirst", "du", "jetzt", "voll\u00b7ge\u00b7stopft", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "So werd' ich schon bei lebender Zeit", "tokens": ["So", "werd'", "ich", "schon", "bei", "le\u00b7ben\u00b7der", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Zu Braten und Pasteten vorbereit't;", "tokens": ["Zu", "Bra\u00b7ten", "und", "Pas\u00b7te\u00b7ten", "vor\u00b7be\u00b7reit't", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Mein Geist geht sehr zur\u00fccke,", "tokens": ["Mein", "Geist", "geht", "sehr", "zu\u00b7r\u00fc\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die Leber nur wird dicke;", "tokens": ["Die", "Le\u00b7ber", "nur", "wird", "di\u00b7cke", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VAFIN", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Sie fragen nicht mehr: \u00bbIst sch\u00f6n ihr Gesicht?\u00ab", "tokens": ["Sie", "fra\u00b7gen", "nicht", "mehr", ":", "\u00bb", "Ist", "sch\u00f6n", "ihr", "Ge\u00b7sicht", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "$.", "$(", "VAFIN", "ADJD", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Sie fragen allein: \u00bbWie f\u00e4llt sie ins Gewicht?\u00ab", "tokens": ["Sie", "fra\u00b7gen", "al\u00b7lein", ":", "\u00bb", "Wie", "f\u00e4llt", "sie", "ins", "Ge\u00b7wicht", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$.", "$(", "PWAV", "VVFIN", "PPER", "APPRART", "NN", "$.", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "Ist das der Dank, da\u00df unsere Schar", "tokens": ["Ist", "das", "der", "Dank", ",", "da\u00df", "un\u00b7se\u00b7re", "Schar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "PDS", "ART", "NN", "$,", "KOUS", "PPOSAT", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Der Hauptstadt der Welt Erretterin einst war?", "tokens": ["Der", "Haupt\u00b7stadt", "der", "Welt", "Er\u00b7ret\u00b7te\u00b7rin", "einst", "war", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "NN", "ADV", "VAFIN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Von wegen Weinverkosten", "tokens": ["Von", "we\u00b7gen", "Wein\u00b7ver\u00b7kos\u00b7ten"], "token_info": ["word", "word", "word"], "pos": ["APPR", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Schlief alles auf den Posten,", "tokens": ["Schlief", "al\u00b7les", "auf", "den", "Pos\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ohn' unser tapfer Schnattern und Schrei'n", "tokens": ["Ohn'", "un\u00b7ser", "tap\u00b7fer", "Schnat\u00b7tern", "und", "Schrei'n"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.6": {"text": "H\u00e4tt' Rom schon anno Tubak franz\u00f6sisch m\u00fcssen sein.", "tokens": ["H\u00e4tt'", "Rom", "schon", "an\u00b7no", "Tu\u00b7bak", "fran\u00b7z\u00f6\u00b7sisch", "m\u00fcs\u00b7sen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADV", "ADV", "NN", "ADJD", "VMFIN", "VAINF", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.8": {"line.1": {"text": "Ihr schmausende Herrn, doch spart Euern Hohn,", "tokens": ["Ihr", "schmau\u00b7sen\u00b7de", "Herrn", ",", "doch", "spart", "Eu\u00b7ern", "Hohn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "ADV", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Wir retten nicht zum zweitenmal die Zivilisation:", "tokens": ["Wir", "ret\u00b7ten", "nicht", "zum", "zwei\u00b7ten\u00b7mal", "die", "Zi\u00b7vi\u00b7li\u00b7sa\u00b7ti\u00b7on", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPRART", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.3": {"text": "Und st\u00fcrmt am Kapitole", "tokens": ["Und", "st\u00fcrmt", "am", "Ka\u00b7pi\u00b7to\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Rheinwein, Bordeaux und Bowle,", "tokens": ["Rhein\u00b7wein", ",", "Bor\u00b7dea\u00b7ux", "und", "Bow\u00b7le", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Keine Gans wird Euch mehr warnen und kr\u00e4hn,", "tokens": ["Kei\u00b7ne", "Gans", "wird", "Euch", "mehr", "war\u00b7nen", "und", "kr\u00e4hn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "PPER", "ADV", "VVINF", "KON", "VVINF", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Doch jammernd werden morgen die Katzen vor Euch stehn.", "tokens": ["Doch", "jam\u00b7mernd", "wer\u00b7den", "mor\u00b7gen", "die", "Kat\u00b7zen", "vor", "Euch", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ADV", "ART", "NN", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}}}}}