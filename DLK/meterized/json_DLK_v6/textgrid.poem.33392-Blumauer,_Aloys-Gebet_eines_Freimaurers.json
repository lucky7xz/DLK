{"textgrid.poem.33392": {"metadata": {"author": {"name": "Blumauer, Aloys", "birth": "N.A.", "death": "N.A."}, "title": "Gebet eines Freimaurers", "genre": "verse", "period": "N.A.", "pub_year": 1776, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "O du, dessen Weisheit diesen weiten", "tokens": ["O", "du", ",", "des\u00b7sen", "Weis\u00b7heit", "die\u00b7sen", "wei\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NE", "$,", "PRELAT", "NN", "PDAT", "ADJA"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Weltenkreis aus Nichts hervorgebracht,", "tokens": ["Wel\u00b7ten\u00b7kreis", "aus", "Nichts", "her\u00b7vor\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIS", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Dessen St\u00e4rke ihn f\u00fcr Ewigkeiten,", "tokens": ["Des\u00b7sen", "St\u00e4r\u00b7ke", "ihn", "f\u00fcr", "E\u00b7wig\u00b7kei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "PPER", "APPR", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Dessen Liebe ihn so sch\u00f6n gemacht!", "tokens": ["Des\u00b7sen", "Lie\u00b7be", "ihn", "so", "sch\u00f6n", "ge\u00b7macht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "PPER", "ADV", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.2": {"line.1": {"text": "Du, den aller Erdenv\u00f6lker Zungen", "tokens": ["Du", ",", "den", "al\u00b7ler", "Er\u00b7den\u00b7v\u00f6l\u00b7ker", "Zun\u00b7gen"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "$,", "ART", "PIAT", "NN", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Tausendfach verschieden stets genannt,", "tokens": ["Tau\u00b7send\u00b7fach", "ver\u00b7schie\u00b7den", "stets", "ge\u00b7nannt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Den jedoch bei seinen Huldigungen", "tokens": ["Den", "je\u00b7doch", "bei", "sei\u00b7nen", "Hul\u00b7di\u00b7gun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Nie ein Volk auf Erden ganz verkannt!", "tokens": ["Nie", "ein", "Volk", "auf", "Er\u00b7den", "ganz", "ver\u00b7kannt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "APPR", "NN", "ADV", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Wesen, das nicht Zeit noch Raum umschr\u00e4nken,", "tokens": ["We\u00b7sen", ",", "das", "nicht", "Zeit", "noch", "Raum", "um\u00b7schr\u00e4n\u00b7ken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "PTKNEG", "NN", "ADV", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Das nie enden wird, und nie begann,", "tokens": ["Das", "nie", "en\u00b7den", "wird", ",", "und", "nie", "be\u00b7gann", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VVINF", "VAFIN", "$,", "KON", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das ich nicht in seiner Gr\u00f6sse denken,", "tokens": ["Das", "ich", "nicht", "in", "sei\u00b7ner", "Gr\u00f6s\u00b7se", "den\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPER", "PTKNEG", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Nur in seiner G\u00fcte lieben kann!", "tokens": ["Nur", "in", "sei\u00b7ner", "G\u00fc\u00b7te", "lie\u00b7ben", "kann", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Welchen Namen soll ein Mensch dir geben,", "tokens": ["Wel\u00b7chen", "Na\u00b7men", "soll", "ein", "Mensch", "dir", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "VMFIN", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Der dich nicht begreifet \u2013 ahndet nur?", "tokens": ["Der", "dich", "nicht", "be\u00b7grei\u00b7fet", "\u2013", "ahn\u00b7det", "nur", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "PPER", "PTKNEG", "VVFIN", "$(", "VVFIN", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Urkraft, Sch\u00f6pfer, oder Geist und Leben,", "tokens": ["Ur\u00b7kraft", ",", "Sch\u00f6p\u00b7fer", ",", "o\u00b7der", "Geist", "und", "Le\u00b7ben", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "KON", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Oder Ein's und Alles der Natur?", "tokens": ["O\u00b7der", "Ein's", "und", "Al\u00b7les", "der", "Na\u00b7tur", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "PIS", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.5": {"line.1": {"text": "Doch wie soll ein Wort dich fassen k\u00f6nnen,", "tokens": ["Doch", "wie", "soll", "ein", "Wort", "dich", "fas\u00b7sen", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "VMFIN", "ART", "NN", "PPER", "VVINF", "VMINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Den kein menschlicher Gedanke mi\u00dft!", "tokens": ["Den", "kein", "menschli\u00b7cher", "Ge\u00b7dan\u00b7ke", "mi\u00dft", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "NN", "VVFIN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Kein Gesch\u00f6pf auf Erden kann dich nennen,", "tokens": ["Kein", "Ge\u00b7sch\u00f6pf", "auf", "Er\u00b7den", "kann", "dich", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Du nur wei\u00dft allein es \u2013 wer du bist.", "tokens": ["Du", "nur", "wei\u00dft", "al\u00b7lein", "es", "\u2013", "wer", "du", "bist", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "ADV", "PPER", "$(", "PWS", "PPER", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.6": {"line.1": {"text": "Viele zwar der bl\u00f6den Menschen dachten", "tokens": ["Vie\u00b7le", "zwar", "der", "bl\u00f6\u00b7den", "Men\u00b7schen", "dach\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "ADV", "ART", "ADJA", "NN", "VVFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Dich in deiner Herrlichkeit zu seh'n,", "tokens": ["Dich", "in", "dei\u00b7ner", "Herr\u00b7lich\u00b7keit", "zu", "seh'n", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Wenn sie dich zu ihres Gleichen machten,", "tokens": ["Wenn", "sie", "dich", "zu", "ih\u00b7res", "Glei\u00b7chen", "mach\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Oder sich durch dich verg\u00f6tterten.", "tokens": ["O\u00b7der", "sich", "durch", "dich", "ver\u00b7g\u00f6t\u00b7ter\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PRF", "APPR", "PPER", "VVFIN", "$."], "meter": "+---+-+--", "measure": "dactylic.init"}}, "stanza.7": {"line.1": {"text": "Angethan mit ihren eig'nen Schw\u00e4chen,", "tokens": ["An\u00b7ge\u00b7than", "mit", "ih\u00b7ren", "eig'\u00b7nen", "Schw\u00e4\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Seh'n sie dich in ihrem stolzen Wahne", "tokens": ["Seh'n", "sie", "dich", "in", "ih\u00b7rem", "stol\u00b7zen", "Wah\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "PRF", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Blo\u00df bereuen, z\u00fcrnen, strafen, r\u00e4chen,", "tokens": ["Blo\u00df", "be\u00b7reu\u00b7en", ",", "z\u00fcr\u00b7nen", ",", "stra\u00b7fen", ",", "r\u00e4\u00b7chen", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "VVINF", "$,", "VVFIN", "$,", "VVFIN", "$,", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und seh'n nichts an dir, als den Tyrann;", "tokens": ["Und", "seh'n", "nichts", "an", "dir", ",", "als", "den", "Ty\u00b7rann", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "APPR", "PPER", "$,", "KOUS", "ART", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.8": {"line.1": {"text": "Glauben, da\u00df du all' die Millionen", "tokens": ["Glau\u00b7ben", ",", "da\u00df", "du", "all'", "die", "Mil\u00b7lion\u00b7en"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "KOUS", "PPER", "PIS", "ART", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Welten, nur sie zu zertr\u00fcmmern, schufst,", "tokens": ["Wel\u00b7ten", ",", "nur", "sie", "zu", "zer\u00b7tr\u00fcm\u00b7mern", ",", "schufst", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "ADV", "PPER", "PTKZU", "VVINF", "$,", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und noch t\u00e4glich ganze Nationen", "tokens": ["Und", "noch", "t\u00e4g\u00b7lich", "gan\u00b7ze", "Na\u00b7ti\u00b7o\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJD", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Blo\u00df zur ew'gen Qual in's Daseyn rufst;", "tokens": ["Blo\u00df", "zur", "ew'\u00b7gen", "Qual", "in's", "Da\u00b7seyn", "rufst", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.9": {"line.1": {"text": "Setzen durch ein ewiges Erbittern", "tokens": ["Set\u00b7zen", "durch", "ein", "e\u00b7wi\u00b7ges", "Er\u00b7bit\u00b7tern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Dich mit der Natur in Widerspruch,", "tokens": ["Dich", "mit", "der", "Na\u00b7tur", "in", "Wi\u00b7der\u00b7spruch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "H\u00f6ren deinen Zorn im Erdersch\u00fcttern,", "tokens": ["H\u00f6\u00b7ren", "dei\u00b7nen", "Zorn", "im", "Er\u00b7der\u00b7sch\u00fct\u00b7tern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "APPRART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und in Donnerwettern deinen Fluch.", "tokens": ["Und", "in", "Don\u00b7ner\u00b7wet\u00b7tern", "dei\u00b7nen", "Fluch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.10": {"line.1": {"text": "Ja sie glauben, da\u00df du nur zur S\u00fcnde", "tokens": ["Ja", "sie", "glau\u00b7ben", ",", "da\u00df", "du", "nur", "zur", "S\u00fcn\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "PPER", "VVFIN", "$,", "KOUS", "PPER", "ADV", "APPRART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Deines Menschen Herz so weich gemacht,", "tokens": ["Dei\u00b7nes", "Men\u00b7schen", "Herz", "so", "weich", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "ADV", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und, damit er nie die Wahrheit finde,", "tokens": ["Und", ",", "da\u00b7mit", "er", "nie", "die", "Wahr\u00b7heit", "fin\u00b7de", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PPER", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Den Verstand so hell ihm angefacht;", "tokens": ["Den", "Ver\u00b7stand", "so", "hell", "ihm", "an\u00b7ge\u00b7facht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "PPER", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.11": {"line.1": {"text": "W\u00e4hnen, da\u00df du blo\u00df des Widerstrebens", "tokens": ["W\u00e4h\u00b7nen", ",", "da\u00df", "du", "blo\u00df", "des", "Wi\u00b7der\u00b7stre\u00b7bens"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "KOUS", "PPER", "ADV", "ART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Wegen zum Genu\u00df den Menschen rufst,", "tokens": ["We\u00b7gen", "zum", "Ge\u00b7nu\u00df", "den", "Men\u00b7schen", "rufst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPRART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und die Rosen auf der Bahn des Lebens", "tokens": ["Und", "die", "Ro\u00b7sen", "auf", "der", "Bahn", "des", "Le\u00b7bens"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Nur der spitzen Dornen wegen schufst.", "tokens": ["Nur", "der", "spit\u00b7zen", "Dor\u00b7nen", "we\u00b7gen", "schufst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "APPR", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.12": {"line.1": {"text": "Welch ein Bild! \u2013 verzeih, was ich empfinde;", "tokens": ["Welch", "ein", "Bild", "!", "\u2013", "ver\u00b7zeih", ",", "was", "ich", "emp\u00b7fin\u00b7de", ";"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "ART", "NN", "$.", "$(", "VVIMP", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "(denn kein Zug von diesem Bild ist dein)", "tokens": ["(", "denn", "kein", "Zug", "von", "die\u00b7sem", "Bild", "ist", "dein", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "PIAT", "NN", "APPR", "PDAT", "NN", "VAFIN", "PPOSAT", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "So ein Gott, und wenn es bei mir st\u00fcnde,", "tokens": ["So", "ein", "Gott", ",", "und", "wenn", "es", "bei", "mir", "st\u00fcn\u00b7de", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,", "KON", "KOUS", "PPER", "APPR", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "M\u00f6cht' ich selbst als dein Gesch\u00f6pf nicht sein.", "tokens": ["M\u00f6cht'", "ich", "selbst", "als", "dein", "Ge\u00b7sch\u00f6pf", "nicht", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "KOUS", "PPOSAT", "NN", "PTKNEG", "VAINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.13": {"line.1": {"text": "Doch noch and're, die sich nicht getrauen", "tokens": ["Doch", "noch", "an\u00b7d'\u00b7re", ",", "die", "sich", "nicht", "ge\u00b7trau\u00b7en"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "PIS", "$,", "PRELS", "PRF", "PTKNEG", "VVPP"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dich, wie die, zu sich herabzuzieh'n,", "tokens": ["Dich", ",", "wie", "die", ",", "zu", "sich", "her\u00b7ab\u00b7zu\u00b7zieh'n", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PWAV", "ART", "$,", "APPR", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Glauben dann, dich durch und durch zu schauen,", "tokens": ["Glau\u00b7ben", "dann", ",", "dich", "durch", "und", "durch", "zu", "schau\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "PRF", "APPR", "KON", "APPR", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Wenn sie sich zu dir hinauf bem\u00fch'n;", "tokens": ["Wenn", "sie", "sich", "zu", "dir", "hin\u00b7auf", "be\u00b7m\u00fch'n", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPR", "PPER", "ADV", "VVINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.14": {"line.1": {"text": "Ringen \u00e4ngstlich von der schweren B\u00fcrde", "tokens": ["Rin\u00b7gen", "\u00e4ngst\u00b7lich", "von", "der", "schwe\u00b7ren", "B\u00fcr\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Dieser Menschlichkeit sich zu befrei'n,", "tokens": ["Die\u00b7ser", "Menschlich\u00b7keit", "sich", "zu", "be\u00b7frei'n", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "PRF", "PTKZU", "VVINF", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Und vergessen, da\u00df die h\u00f6chste W\u00fcrde", "tokens": ["Und", "ver\u00b7ges\u00b7sen", ",", "da\u00df", "die", "h\u00f6chs\u00b7te", "W\u00fcr\u00b7de"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVPP", "$,", "KOUS", "ART", "ADJA", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Eines Menschen sei \u2013 ein Mensch zu sein.", "tokens": ["Ei\u00b7nes", "Men\u00b7schen", "sei", "\u2013", "ein", "Mensch", "zu", "sein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$(", "ART", "NN", "PTKZU", "VAINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.15": {"line.1": {"text": "Blind f\u00fcr das, was ihnen in der N\u00e4he", "tokens": ["Blind", "f\u00fcr", "das", ",", "was", "ih\u00b7nen", "in", "der", "N\u00e4\u00b7he"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "PDS", "$,", "PWS", "PPER", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Die Natur in tausend Wundern zeigt,", "tokens": ["Die", "Na\u00b7tur", "in", "tau\u00b7send", "Wun\u00b7dern", "zeigt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "CARD", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Richten sie den Blick nach einer H\u00f6he,", "tokens": ["Rich\u00b7ten", "sie", "den", "Blick", "nach", "ei\u00b7ner", "H\u00f6\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Welche nie ein Menschenaug' erreicht.", "tokens": ["Wel\u00b7che", "nie", "ein", "Men\u00b7sche\u00b7naug'", "er\u00b7reicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.16": {"line.1": {"text": "Gleich den Riesen, w\u00e4hnen sie vermessen", "tokens": ["Gleich", "den", "Rie\u00b7sen", ",", "w\u00e4h\u00b7nen", "sie", "ver\u00b7mes\u00b7sen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "$,", "KOUS", "PPER", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Schon dir nah, mit dir vertraut zu sein,", "tokens": ["Schon", "dir", "nah", ",", "mit", "dir", "ver\u00b7traut", "zu", "sein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ADJD", "$,", "APPR", "PPER", "ADJD", "PTKZU", "VAINF", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Wollen sich mit deiner Gr\u00f6sse messen,", "tokens": ["Wol\u00b7len", "sich", "mit", "dei\u00b7ner", "Gr\u00f6s\u00b7se", "mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Ach! und sind \u2013 f\u00fcr diese Welt zu klein;", "tokens": ["Ach", "!", "und", "sind", "\u2013", "f\u00fcr", "die\u00b7se", "Welt", "zu", "klein", ";"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "KON", "VAFIN", "$(", "APPR", "PDAT", "NN", "PTKA", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.17": {"line.1": {"text": "Nennen hier auf Erden leben \u2013 schlafen,", "tokens": ["Nen\u00b7nen", "hier", "auf", "Er\u00b7den", "le\u00b7ben", "\u2013", "schla\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "NN", "VVINF", "$(", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Und den K\u00f6rper ihrer Seele Grab,", "tokens": ["Und", "den", "K\u00f6r\u00b7per", "ih\u00b7rer", "See\u00b7le", "Grab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und vergessen, da\u00df, der sie geschaffen,", "tokens": ["Und", "ver\u00b7ges\u00b7sen", ",", "da\u00df", ",", "der", "sie", "ge\u00b7schaf\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVPP", "$,", "KOUS", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Ihnen auch zur Arbeit \u2013 H\u00e4nde gab;", "tokens": ["Ih\u00b7nen", "auch", "zur", "Ar\u00b7beit", "\u2013", "H\u00e4n\u00b7de", "gab", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "ADV", "APPRART", "NN", "$(", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.18": {"line.1": {"text": "Streben deine Plane zu durchsp\u00e4hen,", "tokens": ["Stre\u00b7ben", "dei\u00b7ne", "Pla\u00b7ne", "zu", "durch\u00b7sp\u00e4\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Und zu seh'n dein g\u00f6ttlich Angesicht,", "tokens": ["Und", "zu", "seh'n", "dein", "g\u00f6tt\u00b7lich", "An\u00b7ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKZU", "VVFIN", "PPOSAT", "ADJD", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Ach, und kennen sich, und \u00fcbersehen", "tokens": ["Ach", ",", "und", "ken\u00b7nen", "sich", ",", "und", "\u00fc\u00b7ber\u00b7se\u00b7hen"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ITJ", "$,", "KON", "VVFIN", "PRF", "$,", "KON", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Selbst die Spanne ihres Lebens nicht.", "tokens": ["Selbst", "die", "Span\u00b7ne", "ih\u00b7res", "Le\u00b7bens", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PPOSAT", "NN", "PTKNEG", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.19": {"line.1": {"text": "D'rum, o Gott, bewahre vor dem Wahne", "tokens": ["D'\u00b7rum", ",", "o", "Gott", ",", "be\u00b7wah\u00b7re", "vor", "dem", "Wah\u00b7ne"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PAV", "$,", "FM", "NN", "$,", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Mich, der stolz sich bis zu dir erhebt,", "tokens": ["Mich", ",", "der", "stolz", "sich", "bis", "zu", "dir", "er\u00b7hebt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "ADJD", "PRF", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Lehre mich, wie man nach deinem Plane", "tokens": ["Leh\u00b7re", "mich", ",", "wie", "man", "nach", "dei\u00b7nem", "Pla\u00b7ne"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "PPER", "$,", "PWAV", "PIS", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Hier in diesem Erdenthale lebt.", "tokens": ["Hier", "in", "die\u00b7sem", "Er\u00b7dent\u00b7ha\u00b7le", "lebt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PDAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.20": {"line.1": {"text": "Nie, o Herr, wird sich mein Geist betr\u00fcben,", "tokens": ["Nie", ",", "o", "Herr", ",", "wird", "sich", "mein", "Geist", "be\u00b7tr\u00fc\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "FM", "NN", "$,", "VAFIN", "PRF", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Wenn er dir auch nie in's Antlitz schaut;", "tokens": ["Wenn", "er", "dir", "auch", "nie", "in's", "Ant\u00b7litz", "schaut", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Aber immer werd' ich jenen lieben,", "tokens": ["A\u00b7ber", "im\u00b7mer", "werd'", "ich", "je\u00b7nen", "lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "PDS", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Der mir diese sch\u00f6ne Welt gebaut.", "tokens": ["Der", "mir", "die\u00b7se", "sch\u00f6\u00b7ne", "Welt", "ge\u00b7baut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PDAT", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.21": {"line.1": {"text": "Stolz, o Herr, hat manchen meiner Br\u00fcder", "tokens": ["Stolz", ",", "o", "Herr", ",", "hat", "man\u00b7chen", "mei\u00b7ner", "Br\u00fc\u00b7der"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "FM", "NN", "$,", "VAFIN", "PIAT", "PPOSAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Hin nach h\u00f6hern Gegenden gek\u00f6rnt,", "tokens": ["Hin", "nach", "h\u00f6\u00b7hern", "Ge\u00b7gen\u00b7den", "ge\u00b7k\u00f6rnt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und der sch\u00f6nsten Menschenkette Glieder", "tokens": ["Und", "der", "sch\u00f6ns\u00b7ten", "Men\u00b7schen\u00b7ket\u00b7te", "Glie\u00b7der"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Von dem Pfade der Natur entfernt.", "tokens": ["Von", "dem", "Pfa\u00b7de", "der", "Na\u00b7tur", "ent\u00b7fernt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.22": {"line.1": {"text": "Viele wagten's, Wesen zu bezwingen,", "tokens": ["Vie\u00b7le", "wag\u00b7ten's", ",", "We\u00b7sen", "zu", "be\u00b7zwin\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Die ihr bl\u00f6des Auge gar nicht kennt,", "tokens": ["Die", "ihr", "bl\u00f6\u00b7des", "Au\u00b7ge", "gar", "nicht", "kennt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "ADJA", "NN", "ADV", "PTKNEG", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und die weite Kluft zu \u00fcberspringen,", "tokens": ["Und", "die", "wei\u00b7te", "Kluft", "zu", "\u00fc\u00b7bers\u00b7prin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Die den Menschen von den Geistern trennt.", "tokens": ["Die", "den", "Men\u00b7schen", "von", "den", "Geis\u00b7tern", "trennt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.23": {"line.1": {"text": "O la\u00df nie den Standort mich vergessen,", "tokens": ["O", "la\u00df", "nie", "den", "Stand\u00b7ort", "mich", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "ART", "NN", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Wo du mich als Menschen stelltest hin,", "tokens": ["Wo", "du", "mich", "als", "Men\u00b7schen", "stell\u00b7test", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "KOUS", "NN", "VVFIN", "PTKVZ", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und la\u00df nie mit einer Welt mich messen,", "tokens": ["Und", "la\u00df", "nie", "mit", "ei\u00b7ner", "Welt", "mich", "mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "ADV", "APPR", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Deren Glied ich nicht geworden bin.", "tokens": ["De\u00b7ren", "Glied", "ich", "nicht", "ge\u00b7wor\u00b7den", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PTKNEG", "VAPP", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.24": {"line.1": {"text": "Denn wie kann ich glauben, Herr! mir w\u00e4re", "tokens": ["Denn", "wie", "kann", "ich", "glau\u00b7ben", ",", "Herr", "!", "mir", "w\u00e4\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "PWAV", "VMFIN", "PPER", "VVINF", "$,", "NN", "$.", "PPER", "VAFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Eine Welt von Geistern unterthan,", "tokens": ["Ei\u00b7ne", "Welt", "von", "Geis\u00b7tern", "un\u00b7ter\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Da ich kaum den meinen in die Sph\u00e4re", "tokens": ["Da", "ich", "kaum", "den", "mei\u00b7nen", "in", "die", "Sph\u00e4\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ART", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Meiner Lebenspflichten bannen kann?", "tokens": ["Mei\u00b7ner", "Le\u00b7bens\u00b7pflich\u00b7ten", "ban\u00b7nen", "kann", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.25": {"line.1": {"text": "La\u00df auch nie als dein Gesch\u00f6pf mich w\u00e4hnen,", "tokens": ["La\u00df", "auch", "nie", "als", "dein", "Ge\u00b7sch\u00f6pf", "mich", "w\u00e4h\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "ADV", "KOKOM", "PPOSAT", "NN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Als bes\u00e4\u00df' ich deine Sch\u00f6pfungskraft,", "tokens": ["Als", "be\u00b7s\u00e4\u00df'", "ich", "dei\u00b7ne", "Sch\u00f6p\u00b7fungs\u00b7kraft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Die aus Erde, Blei und Eisensp\u00e4nen", "tokens": ["Die", "aus", "Er\u00b7de", ",", "Blei", "und", "Ei\u00b7sen\u00b7sp\u00e4\u00b7nen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "APPR", "NN", "$,", "NN", "KON", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Nach Belieben Klumpen Gold's sich schafft.", "tokens": ["Nach", "Be\u00b7lie\u00b7ben", "Klum\u00b7pen", "Gold's", "sich", "schafft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "NE", "PRF", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.26": {"line.1": {"text": "O es g\u00e4be Gold genug hienieden,", "tokens": ["O", "es", "g\u00e4\u00b7be", "Gold", "ge\u00b7nug", "hien\u00b7ie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "VVFIN", "NN", "ADV", "ADV", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Alle Menschen zu befriedigen,", "tokens": ["Al\u00b7le", "Men\u00b7schen", "zu", "be\u00b7frie\u00b7di\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "L\u00e4ge nicht, was Tausenden beschieden,", "tokens": ["L\u00e4\u00b7ge", "nicht", ",", "was", "Tau\u00b7sen\u00b7den", "be\u00b7schie\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "$,", "PWS", "NN", "VVPP", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Oft im Kasten eines Einzigen.", "tokens": ["Oft", "im", "Kas\u00b7ten", "ei\u00b7nes", "Ein\u00b7zi\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "ART", "NN", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.27": {"line.1": {"text": "Tausend Arme darben f\u00fcr den Reichen,", "tokens": ["Tau\u00b7send", "Ar\u00b7me", "dar\u00b7ben", "f\u00fcr", "den", "Rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "PAV", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Tausend hungern, da\u00df sich Einer n\u00e4hrt,", "tokens": ["Tau\u00b7send", "hun\u00b7gern", ",", "da\u00df", "sich", "Ei\u00b7ner", "n\u00e4hrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "VVFIN", "$,", "KOUS", "PRF", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und das all' durch Wohlthun auszugleichen,", "tokens": ["Und", "das", "all'", "durch", "Wohl\u00b7thun", "aus\u00b7zu\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PIS", "APPR", "NN", "VVIZU", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Diese Kunst ist eines Maurers werth.", "tokens": ["Die\u00b7se", "Kunst", "ist", "ei\u00b7nes", "Mau\u00b7rers", "werth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "VAFIN", "ART", "NN", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.28": {"line.1": {"text": "Aber, Herr, wenn unser Bund den Stempel", "tokens": ["A\u00b7ber", ",", "Herr", ",", "wenn", "un\u00b7ser", "Bund", "den", "Stem\u00b7pel"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "$,", "NN", "$,", "KOUS", "PPOSAT", "NN", "ART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Allgemeinen Wohlthuns je verliert,", "tokens": ["All\u00b7ge\u00b7mei\u00b7nen", "Wohl\u00b7thuns", "je", "ver\u00b7liert", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Wenn ein Vatikan aus unser'm Tempel,", "tokens": ["Wenn", "ein", "Va\u00b7ti\u00b7kan", "aus", "un\u00b7ser'm", "Tem\u00b7pel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und aus unser'm Schmuck ein M\u00f6nchskleid wird;", "tokens": ["Und", "aus", "un\u00b7ser'm", "Schmuck", "ein", "M\u00f6nchs\u00b7kleid", "wird", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "ART", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.29": {"line.1": {"text": "Wenn wir jemals einen Stein behauen,", "tokens": ["Wenn", "wir", "je\u00b7mals", "ei\u00b7nen", "Stein", "be\u00b7hau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Den nur Eigennutz zusammenh\u00e4lt;", "tokens": ["Den", "nur", "Ei\u00b7gen\u00b7nutz", "zu\u00b7sam\u00b7men\u00b7h\u00e4lt", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Wenn auf das Geb\u00e4ude, das wir bauen,", "tokens": ["Wenn", "auf", "das", "Ge\u00b7b\u00e4u\u00b7de", ",", "das", "wir", "bau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "VVINF", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Auch nur eine Menschenthr\u00e4ne f\u00e4llt;", "tokens": ["Auch", "nur", "ei\u00b7ne", "Men\u00b7schen\u00b7thr\u00e4\u00b7ne", "f\u00e4llt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.30": {"line.1": {"text": "O so hemme unsern Bau, verbreite", "tokens": ["O", "so", "hem\u00b7me", "un\u00b7sern", "Bau", ",", "ver\u00b7brei\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["NE", "ADV", "VVFIN", "PPOSAT", "NN", "$,", "VVFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Schnell Verwirrung \u00fcber unsern Sinn,", "tokens": ["Schnell", "Ver\u00b7wir\u00b7rung", "\u00fc\u00b7ber", "un\u00b7sern", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "La\u00df uns unbelohnt, besch\u00e4mt noch heute", "tokens": ["La\u00df", "uns", "un\u00b7be\u00b7lohnt", ",", "be\u00b7sch\u00e4mt", "noch", "heu\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVIMP", "PPER", "ADJD", "$,", "VVFIN", "ADV", "ADV"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Weg vom Baue dieses Babels zieh'n!", "tokens": ["Weg", "vom", "Bau\u00b7e", "die\u00b7ses", "Ba\u00b7bels", "zieh'n", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "PDAT", "NN", "VVINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.31": {"line.1": {"text": "Aber wenn wir nur auf deiner G\u00fcte", "tokens": ["A\u00b7ber", "wenn", "wir", "nur", "auf", "dei\u00b7ner", "G\u00fc\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "ADV", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Weisen Plan bei uns'rer Arbeit schau'n,", "tokens": ["Wei\u00b7sen", "Plan", "bei", "un\u00b7s'\u00b7rer", "Ar\u00b7beit", "schau'n", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Wenn wir jedem M\u00fcden eine H\u00fctte \u2013", "tokens": ["Wenn", "wir", "je\u00b7dem", "M\u00fc\u00b7den", "ei\u00b7ne", "H\u00fct\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "NN", "ART", "NN", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und der Tugend eine Freistatt bau'n;", "tokens": ["Und", "der", "Tu\u00b7gend", "ei\u00b7ne", "Freis\u00b7tatt", "bau'n", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.32": {"line.1": {"text": "Wenn wir uns bestreben hier auf Erden,", "tokens": ["Wenn", "wir", "uns", "be\u00b7stre\u00b7ben", "hier", "auf", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Da\u00df der Weg durch's Leben ebener,", "tokens": ["Da\u00df", "der", "Weg", "durch's", "Le\u00b7ben", "e\u00b7be\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPRART", "NN", "ADV", "$,"], "meter": "+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Minder m\u00fchsam seine Pfade werden,", "tokens": ["Min\u00b7der", "m\u00fch\u00b7sam", "sei\u00b7ne", "Pfa\u00b7de", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "PPOSAT", "NN", "VAINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und der schroffen Steine weniger;", "tokens": ["Und", "der", "schrof\u00b7fen", "Stei\u00b7ne", "we\u00b7ni\u00b7ger", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.33": {"line.1": {"text": "Wenn wir nur der Menschheit Wohl zu gr\u00fcnden", "tokens": ["Wenn", "wir", "nur", "der", "Menschheit", "Wohl", "zu", "gr\u00fcn\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ART", "NN", "ADV", "PTKZU", "VVINF"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Uns bem\u00fch'n nach deinem weisen Plan,", "tokens": ["Uns", "be\u00b7m\u00fch'n", "nach", "dei\u00b7nem", "wei\u00b7sen", "Plan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und den Lohn nur darin finden,", "tokens": ["Und", "den", "Lohn", "nur", "da\u00b7rin", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "PAV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df wir Gutes in der Welt gethan;", "tokens": ["Da\u00df", "wir", "Gu\u00b7tes", "in", "der", "Welt", "ge\u00b7than", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.34": {"line.1": {"text": "O, so gib, Allvater, unserm Bunde,", "tokens": ["O", ",", "so", "gib", ",", "All\u00b7va\u00b7ter", ",", "un\u00b7serm", "Bun\u00b7de", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "ADV", "VVIMP", "$,", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Gib ihm Wachsthum, Segen und Gedeih'n,", "tokens": ["Gib", "ihm", "Wach\u00b7sthum", ",", "Se\u00b7gen", "und", "Ge\u00b7deih'n", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "La\u00df uns hier auf diesem Erdenrunde", "tokens": ["La\u00df", "uns", "hier", "auf", "die\u00b7sem", "Er\u00b7den\u00b7run\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "ADV", "APPR", "PDAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Stets die Engel deiner Menschheit sein!", "tokens": ["Stets", "die", "En\u00b7gel", "dei\u00b7ner", "Menschheit", "sein", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PPOSAT", "NN", "VAINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}