{"textgrid.poem.55415": {"metadata": {"author": {"name": "Goethe, Johann Wolfgang", "birth": "N.A.", "death": "N.A."}, "title": "Zahme Xenien", "genre": "verse", "period": "N.A.", "pub_year": 1821, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wir sind vielleicht zu antik gewesen,", "tokens": ["Wir", "sind", "viel\u00b7leicht", "zu", "an\u00b7tik", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NN", "VAPP", "$,"], "meter": "---+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Nun wollen wir es moderner lesen.", "tokens": ["Nun", "wol\u00b7len", "wir", "es", "mo\u00b7der\u00b7ner", "le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "\u00bbsonst warst du so weit vom Prahlen entfernt,", "tokens": ["\u00bb", "sonst", "warst", "du", "so", "weit", "vom", "Prah\u00b7len", "ent\u00b7fernt", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "APPRART", "NN", "VVPP", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Wo hast du das Prahlen so grausam gelernt?\u00ab", "tokens": ["Wo", "hast", "du", "das", "Prah\u00b7len", "so", "grau\u00b7sam", "ge\u00b7lernt", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ART", "NN", "ADV", "ADJD", "VVPP", "$.", "$("], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.3": {"text": "Im Orient lernt ich das Prahlen.", "tokens": ["Im", "O\u00b7rient", "lernt", "ich", "das", "Prah\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Doch seit ich zur\u00fcck bin, im westlichen Land", "tokens": ["Doch", "seit", "ich", "zu\u00b7r\u00fcck", "bin", ",", "im", "west\u00b7li\u00b7chen", "Land"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "PTKVZ", "VAFIN", "$,", "APPRART", "ADJA", "NN"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.5": {"text": "Zu meiner Beruhigung find ich und fand", "tokens": ["Zu", "mei\u00b7ner", "Be\u00b7ru\u00b7hi\u00b7gung", "find", "ich", "und", "fand"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PPER", "KON", "VVFIN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Zu Hunderten Orientalen.", "tokens": ["Zu", "Hun\u00b7der\u00b7ten", "O\u00b7rien\u00b7ta\u00b7len", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "Und was die Menschen meinen,", "tokens": ["Und", "was", "die", "Men\u00b7schen", "mei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Das ist mir einerlei;", "tokens": ["Das", "ist", "mir", "ei\u00b7ner\u00b7lei", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "M\u00f6chte mich mir selbst vereinen,", "tokens": ["M\u00f6ch\u00b7te", "mich", "mir", "selbst", "ver\u00b7ei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Allein wir sind zu zwei;", "tokens": ["Al\u00b7lein", "wir", "sind", "zu", "zwei", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "APPR", "CARD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und im lebend'gen Treiben", "tokens": ["Und", "im", "le\u00b7ben\u00b7d'\u00b7gen", "Trei\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPRART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Sind wir ein Hier und Dort,", "tokens": ["Sind", "wir", "ein", "Hier", "und", "Dort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "ADV", "KON", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Das eine liebt zu bleiben,", "tokens": ["Das", "ei\u00b7ne", "liebt", "zu", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "Das andere m\u00f6chte fort;", "tokens": ["Das", "an\u00b7de\u00b7re", "m\u00f6ch\u00b7te", "fort", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VMFIN", "PTKVZ", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.9": {"text": "Doch zu dem Selbstverst\u00e4ndnis", "tokens": ["Doch", "zu", "dem", "Selbst\u00b7ver\u00b7st\u00e4nd\u00b7nis"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.10": {"text": "Ist auch wohl noch ein Rat:", "tokens": ["Ist", "auch", "wohl", "noch", "ein", "Rat", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADV", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Nach fr\u00f6hlichem Erkenntnis", "tokens": ["Nach", "fr\u00f6h\u00b7li\u00b7chem", "Er\u00b7kennt\u00b7nis"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.12": {"text": "Erfolge rasche Tat.", "tokens": ["Er\u00b7fol\u00b7ge", "ra\u00b7sche", "Tat", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Und wenn die Tat bisweilen", "tokens": ["Und", "wenn", "die", "Tat", "bis\u00b7wei\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "ART", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Ganz etwas anders bringt,", "tokens": ["Ganz", "et\u00b7was", "an\u00b7ders", "bringt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "So la\u00dft uns das ereilen,", "tokens": ["So", "la\u00dft", "uns", "das", "er\u00b7ei\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "PDS", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Was unverhofft gelingt.", "tokens": ["Was", "un\u00b7ver\u00b7hofft", "ge\u00b7lingt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Wie ihr denkt oder denken sollt,", "tokens": ["Wie", "ihr", "denkt", "o\u00b7der", "den\u00b7ken", "sollt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "KON", "VVINF", "VMFIN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Geht mich nichts an;", "tokens": ["Geht", "mich", "nichts", "an", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Was ihr Guten, ihr Besten wollt,", "tokens": ["Was", "ihr", "Gu\u00b7ten", ",", "ihr", "Bes\u00b7ten", "wollt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "VMFIN", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Hab ich zum Teil getan.", "tokens": ["Hab", "ich", "zum", "Teil", "ge\u00b7tan", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Viel \u00fcbrig bleibt zu tun,", "tokens": ["Viel", "\u00fcb\u00b7rig", "bleibt", "zu", "tun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVFIN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "M\u00f6ge nur keiner l\u00e4ssig ruhn! \u2013", "tokens": ["M\u00f6\u00b7ge", "nur", "kei\u00b7ner", "l\u00e4s\u00b7sig", "ruhn", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ADV", "PIS", "ADJD", "VVINF", "$.", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.7": {"text": "Was ich sag, ist Bekenntnis,", "tokens": ["Was", "ich", "sag", ",", "ist", "Be\u00b7kennt\u00b7nis", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "$,", "VAFIN", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "Zu meinem und eurem Verst\u00e4ndnis.", "tokens": ["Zu", "mei\u00b7nem", "und", "eu\u00b7rem", "Ver\u00b7st\u00e4nd\u00b7nis", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "KON", "PPOSAT", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.9": {"text": "Die Welt wird t\u00e4glich breiter und gr\u00f6\u00dfer,", "tokens": ["Die", "Welt", "wird", "t\u00e4g\u00b7lich", "brei\u00b7ter", "und", "gr\u00f6\u00b7\u00dfer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "So macht's denn auch vollkommner und besser!", "tokens": ["So", "macht's", "denn", "auch", "voll\u00b7komm\u00b7ner", "und", "bes\u00b7ser", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Besser sollt es hei\u00dfen und vollkommner;", "tokens": ["Bes\u00b7ser", "sollt", "es", "hei\u00b7\u00dfen", "und", "voll\u00b7komm\u00b7ner", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "VVINF", "KON", "ADJD", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.12": {"text": "So sei denn jeder ein Willkommner.", "tokens": ["So", "sei", "denn", "je\u00b7der", "ein", "Will\u00b7komm\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIAT", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wie das Gestirn,", "tokens": ["Wie", "das", "Ge\u00b7stirn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "Ohne Hast,", "tokens": ["Oh\u00b7ne", "Hast", ","], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NE", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Aber ohne Rast,", "tokens": ["A\u00b7ber", "oh\u00b7ne", "Rast", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Drehe sich jeder", "tokens": ["Dre\u00b7he", "sich", "je\u00b7der"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PRF", "PIAT"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Um die eigne Last.", "tokens": ["Um", "die", "eig\u00b7ne", "Last", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "ART", "ADJA", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.7": {"line.1": {"text": "Ich bin so guter Dinge,", "tokens": ["Ich", "bin", "so", "gu\u00b7ter", "Din\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "So heiter und rein,", "tokens": ["So", "hei\u00b7ter", "und", "rein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Und wenn ich einen Fehler beginge,", "tokens": ["Und", "wenn", "ich", "ei\u00b7nen", "Feh\u00b7ler", "be\u00b7gin\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "K\u00f6nnt's keiner sein.", "tokens": ["K\u00f6nnt's", "kei\u00b7ner", "sein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "PIS", "VAINF", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.8": {"line.1": {"text": "Ja, das ist das rechte Gleis,", "tokens": ["Ja", ",", "das", "ist", "das", "rech\u00b7te", "Gleis", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PDS", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df man nicht wei\u00df,", "tokens": ["Da\u00df", "man", "nicht", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "VVFIN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Was man denkt,", "tokens": ["Was", "man", "denkt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Wenn man denkt;", "tokens": ["Wenn", "man", "denkt", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Alles ist als wie geschenkt.", "tokens": ["Al\u00b7les", "ist", "als", "wie", "ge\u00b7schenkt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "KOUS", "KOKOM", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "\u00bbwarum man so manches leidet,", "tokens": ["\u00bb", "wa\u00b7rum", "man", "so", "man\u00b7ches", "lei\u00b7det", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "PIS", "ADV", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und zwar ohne S\u00fcnde? \u2013", "tokens": ["Und", "zwar", "oh\u00b7ne", "S\u00fcn\u00b7de", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "APPR", "NN", "$.", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Niemand gibt uns Geh\u00f6r.\u00ab", "tokens": ["Nie\u00b7mand", "gibt", "uns", "Ge\u00b7h\u00f6r", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NN", "$.", "$("], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.10": {"line.1": {"text": "Wie das T\u00e4tige scheidet,", "tokens": ["Wie", "das", "T\u00e4\u00b7ti\u00b7ge", "schei\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "Alles ist Pfr\u00fcnde,", "tokens": ["Al\u00b7les", "ist", "Pfr\u00fcn\u00b7de", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Und es lebt nichts mehr.", "tokens": ["Und", "es", "lebt", "nichts", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PIS", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.11": {"line.1": {"text": "\u00bbmanches k\u00f6nnen wir nicht verstehn.\u00ab", "tokens": ["\u00bb", "man\u00b7ches", "k\u00f6n\u00b7nen", "wir", "nicht", "ver\u00b7stehn", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PIS", "VMFIN", "PPER", "PTKNEG", "VVINF", "$.", "$("], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Lebt nur fort, es wird schon gehn.", "tokens": ["Lebt", "nur", "fort", ",", "es", "wird", "schon", "gehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKVZ", "$,", "PPER", "VAFIN", "ADV", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "\u00bbwie wei\u00dft du dich denn so zu fassen?\u00ab", "tokens": ["\u00bb", "wie", "wei\u00dft", "du", "dich", "denn", "so", "zu", "fas\u00b7sen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPER", "PRF", "ADV", "ADV", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was ich tadle, mu\u00df ich gelten lassen.", "tokens": ["Was", "ich", "tad\u00b7le", ",", "mu\u00df", "ich", "gel\u00b7ten", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "$,", "VMFIN", "PPER", "VVINF", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.13": {"line.1": {"text": "\u00bbbakis ist wieder auferstanden!\u00ab", "tokens": ["\u00bb", "ba\u00b7kis", "ist", "wie\u00b7der", "auf\u00b7er\u00b7stan\u00b7den", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VAFIN", "ADV", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ja, wie mir scheint, in allen Landen.", "tokens": ["Ja", ",", "wie", "mir", "scheint", ",", "in", "al\u00b7len", "Lan\u00b7den", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWAV", "PPER", "VVFIN", "$,", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00dcberall hat er mehr Gewicht", "tokens": ["\u00dc\u00b7be\u00b7rall", "hat", "er", "mehr", "Ge\u00b7wicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Als hier im kleinen Reimgedicht.", "tokens": ["Als", "hier", "im", "klei\u00b7nen", "Reim\u00b7ge\u00b7dicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Gott hat den Menschen gemacht", "tokens": ["Gott", "hat", "den", "Men\u00b7schen", "ge\u00b7macht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "ART", "NN", "VVPP"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.2": {"text": "Nach seinem Bilde;", "tokens": ["Nach", "sei\u00b7nem", "Bil\u00b7de", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Dann kam er selbst herab,", "tokens": ["Dann", "kam", "er", "selbst", "her\u00b7ab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Mensch, lieb und milde.", "tokens": ["Mensch", ",", "lieb", "und", "mil\u00b7de", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.15": {"line.1": {"text": "Barbaren hatten versucht,", "tokens": ["Bar\u00b7ba\u00b7ren", "hat\u00b7ten", "ver\u00b7sucht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "VVPP", "$,"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.2": {"text": "Sich G\u00f6tter zu machen;", "tokens": ["Sich", "G\u00f6t\u00b7ter", "zu", "ma\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Allein sie sahen verflucht,", "tokens": ["Al\u00b7lein", "sie", "sa\u00b7hen", "ver\u00b7flucht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "VVPP", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Garstiger als Drachen.", "tokens": ["Gars\u00b7ti\u00b7ger", "als", "Dra\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KOUS", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.16": {"line.1": {"text": "Wer wollte Schand und Spott", "tokens": ["Wer", "woll\u00b7te", "Schand", "und", "Spott"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "NN", "KON", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Nun weiter steuern?", "tokens": ["Nun", "wei\u00b7ter", "steu\u00b7ern", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Verwandelte sich Gott", "tokens": ["Ver\u00b7wan\u00b7del\u00b7te", "sich", "Gott"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PRF", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Zu Ungeheuern?", "tokens": ["Zu", "Un\u00b7ge\u00b7heu\u00b7ern", "?"], "token_info": ["word", "word", "punct"], "pos": ["APPR", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.17": {"line.1": {"text": "Und so will ich ein f\u00fcr allemal", "tokens": ["Und", "so", "will", "ich", "ein", "f\u00fcr", "al\u00b7le\u00b7mal"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VMFIN", "PPER", "ART", "APPR", "ADV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Keine Bestien in dem G\u00f6ttersaal!", "tokens": ["Kei\u00b7ne", "Be\u00b7sti\u00b7en", "in", "dem", "G\u00f6t\u00b7ter\u00b7saal", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "APPR", "ART", "NN", "$."], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}, "line.3": {"text": "Die leidigen Elefantenr\u00fcssel,", "tokens": ["Die", "lei\u00b7di\u00b7gen", "E\u00b7lef\u00b7an\u00b7ten\u00b7r\u00fcs\u00b7sel", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Das umgeschlungene Schlangengen\u00fcssel,", "tokens": ["Das", "um\u00b7ge\u00b7schlun\u00b7ge\u00b7ne", "Schlan\u00b7gen\u00b7ge\u00b7n\u00fcs\u00b7sel", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Tief Urschildkr\u00f6t' im Weltensumpf,", "tokens": ["Tief", "Ur\u00b7schild\u00b7kr\u00f6t'", "im", "Wel\u00b7ten\u00b7sumpf", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Viel K\u00f6nigsk\u00f6pf auf ", "tokens": ["Viel", "K\u00f6\u00b7nigs\u00b7k\u00f6pf", "auf"], "token_info": ["word", "word", "word"], "pos": ["PIAT", "NN", "APPR"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.7": {"text": "Die m\u00fcssen uns zur Verzweiflung bringen,", "tokens": ["Die", "m\u00fcs\u00b7sen", "uns", "zur", "Ver\u00b7zwei\u00b7flung", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Wird sie nicht reiner Ost verschlingen.", "tokens": ["Wird", "sie", "nicht", "rei\u00b7ner", "Ost", "ver\u00b7schlin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Der Ost hat sie schon l\u00e4ngst verschlungen:", "tokens": ["Der", "Ost", "hat", "sie", "schon", "l\u00e4ngst", "ver\u00b7schlun\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kalidas' und andere sind durchgedrungen;", "tokens": ["Ka\u00b7li\u00b7das'", "und", "an\u00b7de\u00b7re", "sind", "durch\u00b7ge\u00b7drun\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "PIS", "VAFIN", "VVPP", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Sie haben mit Dichterzierlichkeit", "tokens": ["Sie", "ha\u00b7ben", "mit", "Dich\u00b7ter\u00b7zier\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Von Pfaffen und Fratzen uns befreit.", "tokens": ["Von", "Pfaf\u00b7fen", "und", "Frat\u00b7zen", "uns", "be\u00b7freit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PPER", "VVPP", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "In Indien m\u00f6cht ich selber leben,", "tokens": ["In", "In\u00b7di\u00b7en", "m\u00f6cht", "ich", "sel\u00b7ber", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "H\u00e4tt es nur keine Steinhauer gegeben.", "tokens": ["H\u00e4tt", "es", "nur", "kei\u00b7ne", "Stein\u00b7hau\u00b7er", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PIAT", "NN", "VVPP", "$."], "meter": "+--+-++--+-", "measure": "iambic.penta.invert"}, "line.7": {"text": "Was will man denn vergn\u00fcglicher wissen!", "tokens": ["Was", "will", "man", "denn", "ver\u00b7gn\u00fcg\u00b7li\u00b7cher", "wis\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PIS", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Sakontala, Nala, die mu\u00df man k\u00fcssen,", "tokens": ["Sa\u00b7kon\u00b7ta\u00b7la", ",", "Na\u00b7la", ",", "die", "mu\u00df", "man", "k\u00fcs\u00b7sen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NE", "$,", "PDS", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Und Megha-Duta, den Wolkengesandten,", "tokens": ["Und", "Meg\u00b7ha\u00b7Duta", ",", "den", "Wol\u00b7ken\u00b7ge\u00b7sand\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "NE", "$,", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Wer schickt ihn nicht gerne zu Seelenverwandten!", "tokens": ["Wer", "schickt", "ihn", "nicht", "ger\u00b7ne", "zu", "See\u00b7len\u00b7ver\u00b7wand\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PTKNEG", "ADV", "APPR", "NN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}}, "stanza.19": {"line.1": {"text": "\u00bbwillst du, was doch Genesene preisen,", "tokens": ["\u00bb", "willst", "du", ",", "was", "doch", "Ge\u00b7ne\u00b7se\u00b7ne", "prei\u00b7sen", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPER", "$,", "PRELS", "ADV", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Das Eisen und handhabende Weisen", "tokens": ["Das", "Ei\u00b7sen", "und", "hand\u00b7ha\u00b7ben\u00b7de", "Wei\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "ADJA", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "So ganz entschieden fliehen und hassen?\u00ab", "tokens": ["So", "ganz", "ent\u00b7schie\u00b7den", "flie\u00b7hen", "und", "has\u00b7sen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADV", "ADJD", "VVINF", "KON", "VVINF", "$.", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da Gott mir h\u00f6here Menschheit g\u00f6nnte,", "tokens": ["Da", "Gott", "mir", "h\u00f6\u00b7he\u00b7re", "Menschheit", "g\u00f6nn\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PPER", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Mag ich die t\u00e4ppischen Elemente", "tokens": ["Mag", "ich", "die", "t\u00e4p\u00b7pi\u00b7schen", "E\u00b7le\u00b7men\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.6": {"text": "Nicht verkehrt auf mich wirken lassen.", "tokens": ["Nicht", "ver\u00b7kehrt", "auf", "mich", "wir\u00b7ken", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "APPR", "PRF", "VVINF", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.20": {"line.1": {"text": "Als h\u00e4tte, da w\u00e4r ich sehr erstaunt,", "tokens": ["Als", "h\u00e4t\u00b7te", ",", "da", "w\u00e4r", "ich", "sehr", "er\u00b7staunt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "$,", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der Nabel mir was ins Ohr geraunt,", "tokens": ["Der", "Na\u00b7bel", "mir", "was", "ins", "Ohr", "ge\u00b7raunt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "PIS", "APPRART", "NN", "VVPP", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ein Rad zu schlagen, auf 'm Kopf zu stehn,", "tokens": ["Ein", "Rad", "zu", "schla\u00b7gen", ",", "auf", "'m", "Kopf", "zu", "stehn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,", "APPR", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Das mag f\u00fcr lustige Jungen gehn;", "tokens": ["Das", "mag", "f\u00fcr", "lus\u00b7ti\u00b7ge", "Jun\u00b7gen", "gehn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Wir aber lassen es wohl beim alten,", "tokens": ["Wir", "a\u00b7ber", "las\u00b7sen", "es", "wohl", "beim", "al\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "PPER", "ADV", "APPRART", "ADJA", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Den Kopf wo m\u00f6glich oben zu halten.", "tokens": ["Den", "Kopf", "wo", "m\u00f6g\u00b7lich", "o\u00b7ben", "zu", "hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PWAV", "ADJD", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.21": {"line.1": {"text": "Die Deutschen sind ein gut Geschlecht,", "tokens": ["Die", "Deut\u00b7schen", "sind", "ein", "gut", "Ge\u00b7schlecht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJD", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Ein jeder sagt: \u00bbWill nur, was recht;", "tokens": ["Ein", "je\u00b7der", "sagt", ":", "\u00bb", "Will", "nur", ",", "was", "recht", ";"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "$.", "$(", "VMFIN", "ADV", "$,", "PRELS", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Recht aber soll vorz\u00fcglich hei\u00dfen,", "tokens": ["Recht", "a\u00b7ber", "soll", "vor\u00b7z\u00fcg\u00b7lich", "hei\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VMFIN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Was ich und meine Gevattern preisen;", "tokens": ["Was", "ich", "und", "mei\u00b7ne", "Ge\u00b7vat\u00b7tern", "prei\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Das \u00fcbrige ist ein weitl\u00e4ufig Ding,", "tokens": ["Das", "\u00fcb\u00b7ri\u00b7ge", "ist", "ein", "weit\u00b7l\u00e4u\u00b7fig", "Ding", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Das sch\u00e4tz ich lieber gleich gering.\u00ab", "tokens": ["Das", "sch\u00e4tz", "ich", "lie\u00b7ber", "gleich", "ge\u00b7ring", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Ich habe gar nichts gegen die Menge;", "tokens": ["Ich", "ha\u00b7be", "gar", "nichts", "ge\u00b7gen", "die", "Men\u00b7ge", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIS", "APPR", "ART", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Doch kommt sie einmal ins Gedr\u00e4nge,", "tokens": ["Doch", "kommt", "sie", "ein\u00b7mal", "ins", "Ge\u00b7dr\u00e4n\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "So ruft sie, um den Teufel zu bannen,", "tokens": ["So", "ruft", "sie", ",", "um", "den", "Teu\u00b7fel", "zu", "ban\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUI", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Gewi\u00df die Schelme, die Tyrannen.", "tokens": ["Ge\u00b7wi\u00df", "die", "Schel\u00b7me", ",", "die", "Ty\u00b7ran\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Seit sechzig Jahren seh ich gr\u00f6blich irren", "tokens": ["Seit", "sech\u00b7zig", "Jah\u00b7ren", "seh", "ich", "gr\u00f6b\u00b7lich", "ir\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "NN", "VVFIN", "PPER", "ADJD", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und irre derb mit drein;", "tokens": ["Und", "ir\u00b7re", "derb", "mit", "drein", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "APPR", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da Labyrinthe nun das Labyrinth verwirren,", "tokens": ["Da", "La\u00b7by\u00b7rin\u00b7the", "nun", "das", "La\u00b7by\u00b7rinth", "ver\u00b7wir\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wo soll euch Ariadne sein?", "tokens": ["Wo", "soll", "euch", "A\u00b7riad\u00b7ne", "sein", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "NN", "VAINF", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.24": {"line.1": {"text": "\u00bbwie weit soll das noch gehn!", "tokens": ["\u00bb", "wie", "weit", "soll", "das", "noch", "gehn", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "ADJD", "VMFIN", "PDS", "ADV", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Du f\u00e4llst gar oft ins Abstruse,", "tokens": ["Du", "f\u00e4llst", "gar", "oft", "ins", "Abst\u00b7ru\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir k\u00f6nnen dich nicht verstehn.\u00ab", "tokens": ["Wir", "k\u00f6n\u00b7nen", "dich", "nicht", "ver\u00b7stehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Deshalb tu ich Bu\u00dfe;", "tokens": ["Des\u00b7halb", "tu", "ich", "Bu\u00b7\u00dfe", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Das geh\u00f6rt zu den S\u00fcnden.", "tokens": ["Das", "ge\u00b7h\u00f6rt", "zu", "den", "S\u00fcn\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.6": {"text": "Seht mich an als Propheten!", "tokens": ["Seht", "mich", "an", "als", "Pro\u00b7phe\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKVZ", "KOKOM", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.7": {"text": "Viel Denken, mehr Empfinden", "tokens": ["Viel", "Den\u00b7ken", ",", "mehr", "Emp\u00b7fin\u00b7den"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PIAT", "NN", "$,", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "Und wenig Reden.", "tokens": ["Und", "we\u00b7nig", "Re\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.25": {"line.1": {"text": "Was ich sagen wollt,", "tokens": ["Was", "ich", "sa\u00b7gen", "wollt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Verbietet mir keine Zensur!", "tokens": ["Ver\u00b7bie\u00b7tet", "mir", "kei\u00b7ne", "Zen\u00b7sur", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Sagt verst\u00e4ndig immer nur,", "tokens": ["Sagt", "ver\u00b7st\u00e4n\u00b7dig", "im\u00b7mer", "nur", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ADV", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Was jedem frommt,", "tokens": ["Was", "je\u00b7dem", "frommt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Was ihr und andere sollt;", "tokens": ["Was", "ihr", "und", "an\u00b7de\u00b7re", "sollt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "KON", "PIS", "VMFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Da kommt,", "tokens": ["Da", "kommt", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "Ich versichr' euch, so viel zur Sprache,", "tokens": ["Ich", "ver\u00b7sichr'", "euch", ",", "so", "viel", "zur", "Spra\u00b7che", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.8": {"text": "Was uns besch\u00e4ftigt auf lange Tage.", "tokens": ["Was", "uns", "be\u00b7sch\u00e4f\u00b7tigt", "auf", "lan\u00b7ge", "Ta\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.26": {"line.1": {"text": "O Freiheit s\u00fc\u00df der Presse!", "tokens": ["O", "Frei\u00b7heit", "s\u00fc\u00df", "der", "Pres\u00b7se", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Nun sind wir endlich froh;", "tokens": ["Nun", "sind", "wir", "end\u00b7lich", "froh", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie pocht von Messe zu Messe", "tokens": ["Sie", "pocht", "von", "Mes\u00b7se", "zu", "Mes\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "APPR", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "In dulci jubilo.", "tokens": ["In", "dul\u00b7ci", "ju\u00b7bi\u00b7lo", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NE", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Kommt, la\u00dft uns alles drucken", "tokens": ["Kommt", ",", "la\u00dft", "uns", "al\u00b7les", "dru\u00b7cken"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "VVIMP", "PPER", "PIS", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Und walten f\u00fcr und f\u00fcr;", "tokens": ["Und", "wal\u00b7ten", "f\u00fcr", "und", "f\u00fcr", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "KON", "APPR", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Nur sollte keiner mucken,", "tokens": ["Nur", "soll\u00b7te", "kei\u00b7ner", "mu\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "Der nicht so denkt wie wir.", "tokens": ["Der", "nicht", "so", "denkt", "wie", "wir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "ADV", "VVFIN", "KOKOM", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.27": {"line.1": {"text": "Was euch die heilige Pre\u00dffreiheit", "tokens": ["Was", "euch", "die", "hei\u00b7li\u00b7ge", "Pre\u00df\u00b7frei\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+--++-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "F\u00fcr Frommen, Vorteil und Fr\u00fcchte beut?", "tokens": ["F\u00fcr", "From\u00b7men", ",", "Vor\u00b7teil", "und", "Fr\u00fcch\u00b7te", "beut", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Davon habt ihr gewisse Erscheinung:", "tokens": ["Da\u00b7von", "habt", "ihr", "ge\u00b7wis\u00b7se", "Er\u00b7schei\u00b7nung", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.4": {"text": "Tiefe Verachtung \u00f6ffentlicher Meinung.", "tokens": ["Tie\u00b7fe", "Ver\u00b7ach\u00b7tung", "\u00f6f\u00b7fent\u00b7li\u00b7cher", "Mei\u00b7nung", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADJA", "NN", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}}, "stanza.28": {"line.1": {"text": "Nicht jeder kann alles ertragen:", "tokens": ["Nicht", "je\u00b7der", "kann", "al\u00b7les", "er\u00b7tra\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIS", "VMFIN", "PIS", "VVINF", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der weicht diesem, der jenem aus;", "tokens": ["Der", "weicht", "die\u00b7sem", ",", "der", "je\u00b7nem", "aus", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PDAT", "$,", "PRELS", "PDAT", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Warum soll ich nicht sagen:", "tokens": ["Wa\u00b7rum", "soll", "ich", "nicht", "sa\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die indischen G\u00f6tzen, die sind mir ein Graus?", "tokens": ["Die", "in\u00b7di\u00b7schen", "G\u00f6t\u00b7zen", ",", "die", "sind", "mir", "ein", "Graus", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.29": {"line.1": {"text": "Nichts schmerzlicher kann den Menschen geschehn,", "tokens": ["Nichts", "schmerz\u00b7li\u00b7cher", "kann", "den", "Men\u00b7schen", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VMFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Als das Absurde verk\u00f6rpert zu sehn.", "tokens": ["Als", "das", "Ab\u00b7sur\u00b7de", "ver\u00b7k\u00f6r\u00b7pert", "zu", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVPP", "PTKZU", "VVINF", "$."], "meter": "+-+---+--+", "measure": "iambic.tetra.chol"}}, "stanza.30": {"line.1": {"text": "Dummes Zeug kann man viel reden,", "tokens": ["Dum\u00b7mes", "Zeug", "kann", "man", "viel", "re\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "Kann es auch schreiben,", "tokens": ["Kann", "es", "auch", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Wird weder Leib noch Seele t\u00f6ten,", "tokens": ["Wird", "we\u00b7der", "Leib", "noch", "See\u00b7le", "t\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KON", "NN", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es wird alles beim alten bleiben.", "tokens": ["Es", "wird", "al\u00b7les", "beim", "al\u00b7ten", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "APPRART", "ADJA", "VVINF", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}}, "stanza.31": {"line.1": {"text": "Dummes aber, vors Auge gestellt,", "tokens": ["Dum\u00b7mes", "a\u00b7ber", ",", "vors", "Au\u00b7ge", "ge\u00b7stellt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "$,", "APPRART", "NN", "VVPP", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Hat ein magisches Recht;", "tokens": ["Hat", "ein", "ma\u00b7gi\u00b7sches", "Recht", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Weil es die Sinne gefesselt h\u00e4lt,", "tokens": ["Weil", "es", "die", "Sin\u00b7ne", "ge\u00b7fes\u00b7selt", "h\u00e4lt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "VVFIN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Bleibt der Geist ein Knecht.", "tokens": ["Bleibt", "der", "Geist", "ein", "Knecht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.32": {"line.1": {"text": "Auch diese will ich nicht verschonen,", "tokens": ["Auch", "die\u00b7se", "will", "ich", "nicht", "ver\u00b7scho\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die tollen H\u00f6hl'-Exkavationen,", "tokens": ["Die", "tol\u00b7len", "H\u00f6hl'\u00b7Ex\u00b7ka\u00b7va\u00b7ti\u00b7o\u00b7nen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das d\u00fcstre Troglodytengew\u00fchl,", "tokens": ["Das", "d\u00fcst\u00b7re", "Tro\u00b7glo\u00b7dy\u00b7ten\u00b7ge\u00b7w\u00fchl", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Mit Schnauz und R\u00fcssel ein albern Spiel;", "tokens": ["Mit", "Schnauz", "und", "R\u00fcs\u00b7sel", "ein", "al\u00b7bern", "Spiel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Verr\u00fcckte Zieratbrauerei,", "tokens": ["Ver\u00b7r\u00fcck\u00b7te", "Zie\u00b7rat\u00b7brau\u00b7e\u00b7rei", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Es ist eine saubre Bauerei.", "tokens": ["Es", "ist", "ei\u00b7ne", "saub\u00b7re", "Bau\u00b7e\u00b7rei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "----+-+-+", "measure": "unknown.measure.tri"}, "line.7": {"text": "Nehme sie niemand zum Exempel,", "tokens": ["Neh\u00b7me", "sie", "nie\u00b7mand", "zum", "Ex\u00b7em\u00b7pel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "APPRART", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.8": {"text": "Die Elefanten- und Fratzentempel.", "tokens": ["Die", "E\u00b7le\u00b7fan\u00b7ten", "und", "Frat\u00b7zen\u00b7tem\u00b7pel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "TRUNC", "KON", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Mit heiligen Grillen treiben sie Spott,", "tokens": ["Mit", "hei\u00b7li\u00b7gen", "Gril\u00b7len", "trei\u00b7ben", "sie", "Spott", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Man f\u00fchlt weder Natur noch Gott.", "tokens": ["Man", "f\u00fchlt", "we\u00b7der", "Na\u00b7tur", "noch", "Gott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KON", "NN", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "Auf ewig hab ich sie vertrieben,", "tokens": ["Auf", "e\u00b7wig", "hab", "ich", "sie", "ver\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "VAFIN", "PPER", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Vielk\u00f6pfige G\u00f6tter trifft mein Bann,", "tokens": ["Viel\u00b7k\u00f6p\u00b7fi\u00b7ge", "G\u00f6t\u00b7ter", "trifft", "mein", "Bann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "So Wischnu, Kama, Brahma, Schiven,", "tokens": ["So", "Wischnu", ",", "Ka\u00b7ma", ",", "Brah\u00b7ma", ",", "Schi\u00b7ven", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "NE", "$,", "NN", "$,", "NE", "$,", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Sogar den Affen Hannemann.", "tokens": ["So\u00b7gar", "den", "Af\u00b7fen", "Han\u00b7ne\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nun soll am Nil ich mir gefallen,", "tokens": ["Nun", "soll", "am", "Nil", "ich", "mir", "ge\u00b7fal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "APPRART", "NN", "PPER", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Hundsk\u00f6pfige G\u00f6tter hei\u00dfen gro\u00df:", "tokens": ["Hunds\u00b7k\u00f6p\u00b7fi\u00b7ge", "G\u00f6t\u00b7ter", "hei\u00b7\u00dfen", "gro\u00df", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.7": {"text": "O w\u00e4r ich doch aus meinen Hallen", "tokens": ["O", "w\u00e4r", "ich", "doch", "aus", "mei\u00b7nen", "Hal\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Auch Isis und Osiris los!", "tokens": ["Auch", "I\u00b7sis", "und", "O\u00b7si\u00b7ris", "los", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "KON", "NE", "PTKVZ", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.34": {"line.1": {"text": "Ihr guten Dichter ihr,", "tokens": ["Ihr", "gu\u00b7ten", "Dich\u00b7ter", "ihr", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "PPER", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Seid nur in Zeiten zahm!", "tokens": ["Seid", "nur", "in", "Zei\u00b7ten", "zahm", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie machen Shakespeare", "tokens": ["Sie", "ma\u00b7chen", "Sha\u00b7ke\u00b7spe\u00b7a\u00b7re"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "NE"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Auch noch am Ende lahm.", "tokens": ["Auch", "noch", "am", "En\u00b7de", "lahm", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.35": {"line.1": {"text": "Im Auslegen seid frisch und munter!", "tokens": ["Im", "Aus\u00b7le\u00b7gen", "seid", "frisch", "und", "mun\u00b7ter", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "ADJD", "KON", "ADJD", "$."], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Legt ihr's nicht aus, so legt was unter.", "tokens": ["Legt", "ih\u00b7r's", "nicht", "aus", ",", "so", "legt", "was", "un\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "PTKVZ", "$,", "ADV", "VVFIN", "PIS", "APPR", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.36": {"line.1": {"text": "Was dem einen widerf\u00e4hrt,", "tokens": ["Was", "dem", "ei\u00b7nen", "wi\u00b7der\u00b7f\u00e4hrt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Widerf\u00e4hrt dem andern;", "tokens": ["Wi\u00b7der\u00b7f\u00e4hrt", "dem", "an\u00b7dern", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Niemand w\u00e4re so gelehrt,", "tokens": ["Nie\u00b7mand", "w\u00e4\u00b7re", "so", "ge\u00b7lehrt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Der nicht sollte wandern,", "tokens": ["Der", "nicht", "soll\u00b7te", "wan\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "VMFIN", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "Und ein armer Teufel kommt", "tokens": ["Und", "ein", "ar\u00b7mer", "Teu\u00b7fel", "kommt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Auch von Stell zu Stelle,", "tokens": ["Auch", "von", "Stell", "zu", "Stel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.7": {"text": "Frauen wissen, was ihm frommt,", "tokens": ["Frau\u00b7en", "wis\u00b7sen", ",", "was", "ihm", "frommt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "$,", "PWS", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Welle folgt der Welle.", "tokens": ["Wel\u00b7le", "folgt", "der", "Wel\u00b7le", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.37": {"line.1": {"text": "\u00bbich zieh ins Feld!", "tokens": ["\u00bb", "ich", "zieh", "ins", "Feld", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Wie macht's der Held?\u00ab", "tokens": ["Wie", "macht's", "der", "Held", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Vor der Schlacht hochherzig,", "tokens": ["Vor", "der", "Schlacht", "hoch\u00b7her\u00b7zig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJD", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Ist sie gewannen, barmherzig,", "tokens": ["Ist", "sie", "ge\u00b7wan\u00b7nen", ",", "barm\u00b7her\u00b7zig", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "PPER", "VVPP", "$,", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mit h\u00fcbschen Kindern liebherzig;", "tokens": ["Mit", "h\u00fcb\u00b7schen", "Kin\u00b7dern", "lieb\u00b7her\u00b7zig", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "W\u00e4r ich Soldat,", "tokens": ["W\u00e4r", "ich", "Sol\u00b7dat", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Das w\u00e4r mein Rat.", "tokens": ["Das", "w\u00e4r", "mein", "Rat", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.38": {"line.1": {"text": "\u00bbgib eine Norm zur B\u00fcrgerf\u00fchrung!\u00ab", "tokens": ["\u00bb", "gib", "ei\u00b7ne", "Norm", "zur", "B\u00fcr\u00b7ger\u00b7f\u00fch\u00b7rung", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "ART", "NN", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hienieden,", "tokens": ["Hien\u00b7ie\u00b7den", ","], "token_info": ["word", "punct"], "pos": ["NE", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Im Frieden,", "tokens": ["Im", "Frie\u00b7den", ","], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Kehre jeder vor seiner T\u00fcre;", "tokens": ["Keh\u00b7re", "je\u00b7der", "vor", "sei\u00b7ner", "T\u00fc\u00b7re", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Bekriegt,", "tokens": ["Be\u00b7kriegt", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.6": {"text": "Besiegt,", "tokens": ["Be\u00b7siegt", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "Vertrage man sich mit der Einquartierung.", "tokens": ["Ver\u00b7tra\u00b7ge", "man", "sich", "mit", "der", "Ein\u00b7quar\u00b7tie\u00b7rung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PRF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.39": {"line.1": {"text": "Wenn der J\u00fcngling absurd ist,", "tokens": ["Wenn", "der", "J\u00fcng\u00b7ling", "ab\u00b7surd", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "NE", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "F\u00e4llt er dar\u00fcber in lange Pein;", "tokens": ["F\u00e4llt", "er", "da\u00b7r\u00fc\u00b7ber", "in", "lan\u00b7ge", "Pein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PAV", "APPR", "ADJA", "NN", "$."], "meter": "+-+---+-+", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Der Alte soll nicht absurd sein,", "tokens": ["Der", "Al\u00b7te", "soll", "nicht", "ab\u00b7surd", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PTKNEG", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Weil das Leben ihm kurz ist.", "tokens": ["Weil", "das", "Le\u00b7ben", "ihm", "kurz", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "ADJD", "VAFIN", "$."], "meter": "+-+-+++", "measure": "unknown.measure.penta"}}, "stanza.40": {"line.1": {"text": "\u00bbwas hast du uns absurd genannt!", "tokens": ["\u00bb", "was", "hast", "du", "uns", "ab\u00b7surd", "ge\u00b7nannt", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "PPER", "PRF", "ADJD", "VVPP", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Absurd allein ist der Pedant.\u00ab", "tokens": ["Ab\u00b7surd", "al\u00b7lein", "ist", "der", "Pe\u00b7dant", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "ADV", "VAFIN", "ART", "NN", "$.", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.41": {"line.1": {"text": "Will ich euch aber Pedanten benennen,", "tokens": ["Will", "ich", "euch", "a\u00b7ber", "Pe\u00b7dan\u00b7ten", "be\u00b7nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da mu\u00df ich mich erst besinnen k\u00f6nnen.", "tokens": ["Da", "mu\u00df", "ich", "mich", "erst", "be\u00b7sin\u00b7nen", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "ADV", "VVINF", "VMINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.42": {"line.1": {"text": "Titius, Cajus, die wohl Bekannten! \u2013", "tokens": ["Ti\u00b7tius", ",", "Ca\u00b7jus", ",", "die", "wohl", "Be\u00b7kann\u00b7ten", "!", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["NE", "$,", "NE", "$,", "PRELS", "ADV", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Doch wenn ich's recht beim Licht besah,", "tokens": ["Doch", "wenn", "ich's", "recht", "beim", "Licht", "be\u00b7sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIS", "ADJD", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Einer steht dem andern so nah,", "tokens": ["Ei\u00b7ner", "steht", "dem", "an\u00b7dern", "so", "nah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ART", "ADJA", "ADV", "ADJD", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Am Ende sind wir alle Pedanten.", "tokens": ["Am", "En\u00b7de", "sind", "wir", "al\u00b7le", "Pe\u00b7dan\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "PIAT", "NN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.43": {"line.1": {"text": "Das mach ich mir denn zum reichen Gewinn,", "tokens": ["Das", "mach", "ich", "mir", "denn", "zum", "rei\u00b7chen", "Ge\u00b7winn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PPER", "ADV", "APPRART", "ADJA", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da\u00df ich getrost ein Pedante bin.", "tokens": ["Da\u00df", "ich", "ge\u00b7trost", "ein", "Pe\u00b7dan\u00b7te", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ART", "NN", "VAFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.44": {"line.1": {"text": "Tust deine Sache und tust sie recht,", "tokens": ["Tust", "dei\u00b7ne", "Sa\u00b7che", "und", "tust", "sie", "recht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "KON", "VVFIN", "PPER", "ADJD", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Halt fest und ehre deinen Orden;", "tokens": ["Halt", "fest", "und", "eh\u00b7re", "dei\u00b7nen", "Or\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKVZ", "KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "H\u00e4ltst du aber die andern f\u00fcr schlecht,", "tokens": ["H\u00e4ltst", "du", "a\u00b7ber", "die", "an\u00b7dern", "f\u00fcr", "schlecht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "ADJA", "APPR", "ADJD", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "So bist du selbst ein Pedant geworden.", "tokens": ["So", "bist", "du", "selbst", "ein", "Pe\u00b7dant", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ART", "NN", "VAPP", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.45": {"line.1": {"text": "Wie einer denkt, ist einerlei,", "tokens": ["Wie", "ei\u00b7ner", "denkt", ",", "ist", "ei\u00b7ner\u00b7lei", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VVFIN", "$,", "VAFIN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was einer tut, ist zweierlei;", "tokens": ["Was", "ei\u00b7ner", "tut", ",", "ist", "zwei\u00b7er\u00b7lei", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "$,", "VAFIN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Macht er's gut, so ist es recht,", "tokens": ["Macht", "er's", "gut", ",", "so", "ist", "es", "recht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "ADJD", "$,", "ADV", "VAFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ger\u00e4t es nicht, so bleibt es schlecht.", "tokens": ["Ge\u00b7r\u00e4t", "es", "nicht", ",", "so", "bleibt", "es", "schlecht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "$,", "ADV", "VVFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.46": {"line.1": {"text": "Von Jahren zu Jahren", "tokens": ["Von", "Jah\u00b7ren", "zu", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "APPR", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Mu\u00df man viel Fremdes erfahren;", "tokens": ["Mu\u00df", "man", "viel", "Frem\u00b7des", "er\u00b7fah\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "PIAT", "NN", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.3": {"text": "Du trachte, wie du lebst und leibst,", "tokens": ["Du", "trach\u00b7te", ",", "wie", "du", "lebst", "und", "leibst", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df du nur immer derselbe bleibst.", "tokens": ["Da\u00df", "du", "nur", "im\u00b7mer", "der\u00b7sel\u00b7be", "bleibst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "PDAT", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.47": {"line.1": {"text": "Wenn ich kennte den Weg des Herrn,", "tokens": ["Wenn", "ich", "kenn\u00b7te", "den", "Weg", "des", "Herrn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Ich ging' ihn wahrhaftig gar zu gern;", "tokens": ["Ich", "ging'", "ihn", "wahr\u00b7haf\u00b7tig", "gar", "zu", "gern", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "ADV", "PTKA", "ADV", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "F\u00fchrte man mich in der Wahrheit Haus,", "tokens": ["F\u00fchr\u00b7te", "man", "mich", "in", "der", "Wahr\u00b7heit", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PRF", "APPR", "ART", "NN", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Bei Gott! ich ging' nicht wieder heraus.", "tokens": ["Bei", "Gott", "!", "ich", "ging'", "nicht", "wie\u00b7der", "he\u00b7raus", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$.", "PPER", "VVFIN", "PTKNEG", "ADV", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.48": {"line.1": {"text": "\u00bbsei deinen Worten Lob und Ehre,", "tokens": ["\u00bb", "sei", "dei\u00b7nen", "Wor\u00b7ten", "Lob", "und", "Eh\u00b7re", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "PPOSAT", "NN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wir sehn, da\u00df du ein Erfahrner bist.\u00ab", "tokens": ["Wir", "sehn", ",", "da\u00df", "du", "ein", "Er\u00b7fahr\u00b7ner", "bist", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN", "VAFIN", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Sieht aus, als wenn es von gestern w\u00e4re,", "tokens": ["Sieht", "aus", ",", "als", "wenn", "es", "von", "ge\u00b7stern", "w\u00e4\u00b7re", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$,", "KOKOM", "KOUS", "PPER", "APPR", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Weil es von heut ist.", "tokens": ["Weil", "es", "von", "heut", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADV", "VAFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.49": {"line.1": {"text": "Das Beste m\u00f6cht ich euch vertrauen:", "tokens": ["Das", "Bes\u00b7te", "m\u00f6cht", "ich", "euch", "ver\u00b7trau\u00b7en", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sollt erst in eignen Spiegel schauen.", "tokens": ["Sollt", "erst", "in", "eig\u00b7nen", "Spie\u00b7gel", "schau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Seid ihr, wie sch\u00f6n geputzte Braut,", "tokens": ["Seid", "ihr", ",", "wie", "sch\u00f6n", "ge\u00b7putz\u00b7te", "Braut", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PPER", "$,", "PWAV", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bei diesem Anblick froh geblieben,", "tokens": ["Bei", "die\u00b7sem", "An\u00b7blick", "froh", "ge\u00b7blie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Fragt: ob ihr alles, was ihr schaut,", "tokens": ["Fragt", ":", "ob", "ihr", "al\u00b7les", ",", "was", "ihr", "schaut", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "KOUS", "PPER", "PIS", "$,", "PWS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit redlichem Gesicht m\u00f6gt lieben.", "tokens": ["Mit", "red\u00b7li\u00b7chem", "Ge\u00b7sicht", "m\u00f6gt", "lie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.51": {"line.1": {"text": "Habt ihr gelogen in Wort und Schrift,", "tokens": ["Habt", "ihr", "ge\u00b7lo\u00b7gen", "in", "Wort", "und", "Schrift", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "VVPP", "APPR", "NN", "KON", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Andern ist es und euch ein Gift.", "tokens": ["An\u00b7dern", "ist", "es", "und", "euch", "ein", "Gift", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "VAFIN", "PPER", "KON", "PPER", "ART", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.52": {"line.1": {"text": "X hat sich nie des Wahren beflissen,", "tokens": ["X", "hat", "sich", "nie", "des", "Wah\u00b7ren", "be\u00b7flis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["XY", "VAFIN", "PRF", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Im Widerspruche fand er's;", "tokens": ["Im", "Wi\u00b7der\u00b7spru\u00b7che", "fand", "er's", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Nun glaubt er alles besser zu wissen", "tokens": ["Nun", "glaubt", "er", "al\u00b7les", "bes\u00b7ser", "zu", "wis\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PIS", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und wei\u00df es nur anders.", "tokens": ["Und", "wei\u00df", "es", "nur", "an\u00b7ders", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADV", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.53": {"line.1": {"text": "\u00bbdu hast ", "tokens": ["\u00bb", "du", "hast"], "token_info": ["punct", "word", "word"], "pos": ["$(", "PPER", "VAFIN"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Doch das zu sagen ist klein.", "tokens": ["Doch", "das", "zu", "sa\u00b7gen", "ist", "klein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "PTKZU", "VVINF", "VAFIN", "ADJD", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Habe ", "tokens": ["Ha\u00b7be"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.54": {"line.1": {"text": "Da kommen sie von verschiedenen Seiten,", "tokens": ["Da", "kom\u00b7men", "sie", "von", "ver\u00b7schie\u00b7de\u00b7nen", "Sei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Nord, Ost, S\u00fcd, West und anderen Weiten,", "tokens": ["Nord", ",", "Ost", ",", "S\u00fcd", ",", "West", "und", "an\u00b7de\u00b7ren", "Wei\u00b7ten", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,", "NN", "KON", "ADJA", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Und klagen diesen und jenen an,", "tokens": ["Und", "kla\u00b7gen", "die\u00b7sen", "und", "je\u00b7nen", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PDAT", "KON", "PDS", "PTKVZ", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Er habe nicht ihren Willen getan!", "tokens": ["Er", "ha\u00b7be", "nicht", "ih\u00b7ren", "Wil\u00b7len", "ge\u00b7tan", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und was sie dann nicht gelten lassen,", "tokens": ["Und", "was", "sie", "dann", "nicht", "gel\u00b7ten", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "PTKNEG", "VVINF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Das sollen die \u00fcbrigen gleichfalls hassen.", "tokens": ["Das", "sol\u00b7len", "die", "\u00fcb\u00b7ri\u00b7gen", "gleich\u00b7falls", "has\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "ADJA", "ADV", "VVINF", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Warum ich aber mich Alter betr\u00fcbe?", "tokens": ["Wa\u00b7rum", "ich", "a\u00b7ber", "mich", "Al\u00b7ter", "be\u00b7tr\u00fc\u00b7be", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PPER", "NN", "VVFIN", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Da\u00df man nicht liebt \u2013 was ich liebe.", "tokens": ["Da\u00df", "man", "nicht", "liebt", "\u2013", "was", "ich", "lie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "VVFIN", "$(", "PWS", "PPER", "VVFIN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.55": {"line.1": {"text": "Und doch bleibt was Liebes immer,", "tokens": ["Und", "doch", "bleibt", "was", "Lie\u00b7bes", "im\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "NN", "ADV", "$,"], "meter": "+-+-+-++", "measure": "unknown.measure.penta"}, "line.2": {"text": "So im Reden, so im Denken;", "tokens": ["So", "im", "Re\u00b7den", ",", "so", "im", "Den\u00b7ken", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$,", "ADV", "APPRART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie wir sch\u00f6ne Frauenzimmer", "tokens": ["Wie", "wir", "sch\u00f6\u00b7ne", "Frau\u00b7en\u00b7zim\u00b7mer"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mehr als garstige beschenken.", "tokens": ["Mehr", "als", "gars\u00b7ti\u00b7ge", "be\u00b7schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "KOKOM", "VVFIN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.56": {"line.1": {"text": "Bleibt so etwas, dem wir huld'gen,", "tokens": ["Bleibt", "so", "et\u00b7was", ",", "dem", "wir", "huld'\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PIS", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wenn wir's auch nicht recht begreifen;", "tokens": ["Wenn", "wir's", "auch", "nicht", "recht", "be\u00b7grei\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "PTKNEG", "ADJD", "VVINF", "$."], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Wir erkennen, wir entschuld'gen,", "tokens": ["Wir", "er\u00b7ken\u00b7nen", ",", "wir", "ent\u00b7schuld'\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "M\u00f6gen nicht zur Seite weichen.", "tokens": ["M\u00f6\u00b7gen", "nicht", "zur", "Sei\u00b7te", "wei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PTKNEG", "APPRART", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.57": {"line.1": {"text": "\u00bbsagt! wie k\u00f6nnten wir das Wahre,", "tokens": ["\u00bb", "sagt", "!", "wie", "k\u00f6nn\u00b7ten", "wir", "das", "Wah\u00b7re", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$.", "PWAV", "VMFIN", "PPER", "ART", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Denn es ist uns ungelegen,", "tokens": ["Denn", "es", "ist", "uns", "un\u00b7ge\u00b7le\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Niederlegen auf die Bahre,", "tokens": ["Nie\u00b7der\u00b7le\u00b7gen", "auf", "die", "Bah\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df es nie sich m\u00f6chte regen?\u00ab", "tokens": ["Da\u00df", "es", "nie", "sich", "m\u00f6ch\u00b7te", "re\u00b7gen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADV", "PRF", "VMFIN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.58": {"line.1": {"text": "Diese M\u00fche wird nicht gro\u00df sein", "tokens": ["Die\u00b7se", "M\u00fc\u00b7he", "wird", "nicht", "gro\u00df", "sein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDAT", "NN", "VAFIN", "PTKNEG", "ADJD", "VAINF"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Kultivierten deutschen Orten;", "tokens": ["Kul\u00b7ti\u00b7vier\u00b7ten", "deut\u00b7schen", "Or\u00b7ten", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wollt ihr es auf ewig los sein,", "tokens": ["Wollt", "ihr", "es", "auf", "e\u00b7wig", "los", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PPER", "APPR", "ADJD", "ADJD", "VAINF", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "So erstickt es nur mit Worten.", "tokens": ["So", "er\u00b7stickt", "es", "nur", "mit", "Wor\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.59": {"line.1": {"text": "Immer mu\u00df man wiederholen:", "tokens": ["Im\u00b7mer", "mu\u00df", "man", "wie\u00b7der\u00b7ho\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wenn ich diesen, jenen kr\u00e4nke,", "tokens": ["Wenn", "ich", "die\u00b7sen", ",", "je\u00b7nen", "kr\u00e4n\u00b7ke", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "$,", "PDS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Kr\u00e4nk auch er mich unverhohlen.", "tokens": ["Kr\u00e4nk", "auch", "er", "mich", "un\u00b7ver\u00b7hoh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "PPER", "PRF", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.60": {"line.1": {"text": "St\u00f6ret ja \u2013 mir sagt's die Zeitung \u2013", "tokens": ["St\u00f6\u00b7ret", "ja", "\u2013", "mir", "sagt's", "die", "Zei\u00b7tung", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$(", "PPER", "VVFIN", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Unverletzten w\u00fcrd'gen Ortes", "tokens": ["Un\u00b7ver\u00b7letz\u00b7ten", "w\u00fcrd'\u00b7gen", "Or\u00b7tes"], "token_info": ["word", "word", "word"], "pos": ["NN", "VVFIN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Dieser jenem, heft'gen Wortes,", "tokens": ["Die\u00b7ser", "je\u00b7nem", ",", "heft'\u00b7gen", "Wor\u00b7tes", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PDAT", "PDAT", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die beliebige Bereitung.", "tokens": ["Die", "be\u00b7lie\u00b7bi\u00b7ge", "Be\u00b7rei\u00b7tung", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.61": {"line.1": {"text": "Was der eine will bereiten,", "tokens": ["Was", "der", "ei\u00b7ne", "will", "be\u00b7rei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "PIS", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Einem andern will's nicht gelten;", "tokens": ["Ei\u00b7nem", "an\u00b7dern", "will's", "nicht", "gel\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "H\u00fcben, dr\u00fcben mu\u00df man schelten:", "tokens": ["H\u00fc\u00b7ben", ",", "dr\u00fc\u00b7ben", "mu\u00df", "man", "schel\u00b7ten", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADV", "VMFIN", "PIS", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Das ist nun der Geist der Zeiten.", "tokens": ["Das", "ist", "nun", "der", "Geist", "der", "Zei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ART", "NN", "ART", "NN", "$."], "meter": "----+-+-", "measure": "unknown.measure.di"}}, "stanza.62": {"line.1": {"text": "L\u00e4\u00dft mich das Alter im Stich?", "tokens": ["L\u00e4\u00dft", "mich", "das", "Al\u00b7ter", "im", "Stich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Bin ich wieder ein Kind?", "tokens": ["Bin", "ich", "wie\u00b7der", "ein", "Kind", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Ich wei\u00df nicht, ob ich", "tokens": ["Ich", "wei\u00df", "nicht", ",", "ob", "ich"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "KOUS", "PPER"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Oder die andern verr\u00fcckt sind.", "tokens": ["O\u00b7der", "die", "an\u00b7dern", "ver\u00b7r\u00fcckt", "sind", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "ADJD", "VAFIN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.63": {"line.1": {"text": "\u00bbsag nur, warum du in manchem Falle", "tokens": ["\u00bb", "sag", "nur", ",", "wa\u00b7rum", "du", "in", "man\u00b7chem", "Fal\u00b7le"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "VVIMP", "ADV", "$,", "PWAV", "PPER", "APPR", "PIAT", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "So ganz untr\u00f6stlich bist?\u00ab", "tokens": ["So", "ganz", "un\u00b7tr\u00f6st\u00b7lich", "bist", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADV", "ADJD", "VAFIN", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die Menschen bem\u00fchen sich alle", "tokens": ["Die", "Men\u00b7schen", "be\u00b7m\u00fc\u00b7hen", "sich", "al\u00b7le"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PRF", "PIAT"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Umzutun, was getan ist.", "tokens": ["Um\u00b7zu\u00b7tun", ",", "was", "ge\u00b7tan", "ist", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PRELS", "VVPP", "VAFIN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.64": {"line.1": {"text": "\u00bbund wenn was umzutun w\u00e4re,", "tokens": ["\u00bb", "und", "wenn", "was", "um\u00b7zu\u00b7tun", "w\u00e4\u00b7re", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "KOUS", "PIS", "VVIZU", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Das w\u00fcrde wohl auch getan;", "tokens": ["Das", "w\u00fcr\u00b7de", "wohl", "auch", "ge\u00b7tan", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Ich frage dich bei Wort und Ehre,", "tokens": ["Ich", "fra\u00b7ge", "dich", "bei", "Wort", "und", "Eh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wo fangen wir's an?\u00ab", "tokens": ["Wo", "fan\u00b7gen", "wir's", "an", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "PTKVZ", "$.", "$("], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.65": {"line.1": {"text": "Umst\u00fclpen f\u00fchrt nicht ins Weite;", "tokens": ["Um\u00b7st\u00fcl\u00b7pen", "f\u00fchrt", "nicht", "ins", "Wei\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PTKNEG", "APPRART", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wir kehren frank und froh", "tokens": ["Wir", "keh\u00b7ren", "frank", "und", "froh"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Den Strumpf auf die linke Seite", "tokens": ["Den", "Strumpf", "auf", "die", "lin\u00b7ke", "Sei\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und tragen ihn so.", "tokens": ["Und", "tra\u00b7gen", "ihn", "so", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.66": {"line.1": {"text": "Und sollen das Falsche sie umtun,", "tokens": ["Und", "sol\u00b7len", "das", "Fal\u00b7sche", "sie", "um\u00b7tun", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ART", "NN", "PPER", "VVINF", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "So fangen sie wieder von vornen an;", "tokens": ["So", "fan\u00b7gen", "sie", "wie\u00b7der", "von", "vor\u00b7nen", "an", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "ADV", "PTKVZ", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Sie lassen immer das Wahre ruhn", "tokens": ["Sie", "las\u00b7sen", "im\u00b7mer", "das", "Wah\u00b7re", "ruhn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "VVINF"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und meinen, mit Falschem w\u00e4r's auch getan.", "tokens": ["Und", "mei\u00b7nen", ",", "mit", "Fal\u00b7schem", "w\u00e4r's", "auch", "ge\u00b7tan", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "APPR", "NN", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.67": {"line.1": {"text": "Da steht man denn von neuem still,", "tokens": ["Da", "steht", "man", "denn", "von", "neu\u00b7em", "still", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "APPR", "ADJA", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Warum das auch nicht gehen will.", "tokens": ["Wa\u00b7rum", "das", "auch", "nicht", "ge\u00b7hen", "will", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PDS", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.68": {"line.1": {"text": "Niemand mu\u00df herein rennen", "tokens": ["Nie\u00b7mand", "mu\u00df", "her\u00b7ein", "ren\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "PTKVZ", "VVFIN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "Auch mit den besten Gaben;", "tokens": ["Auch", "mit", "den", "bes\u00b7ten", "Ga\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sollen's die Deutschen mit Dank erkennen,", "tokens": ["Sol\u00b7len's", "die", "Deut\u00b7schen", "mit", "Dank", "er\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "So wollen sie Zeit haben.", "tokens": ["So", "wol\u00b7len", "sie", "Zeit", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "NN", "VAFIN", "$."], "meter": "-+--++-", "measure": "iambic.tri.relaxed"}}, "stanza.69": {"line.1": {"text": "Das T\u00fcchtige, und wenn auch falsch,", "tokens": ["Das", "T\u00fcch\u00b7ti\u00b7ge", ",", "und", "wenn", "auch", "falsch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KON", "KOUS", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wirkt Tag f\u00fcr Tag, von Haus zu Haus;", "tokens": ["Wirkt", "Tag", "f\u00fcr", "Tag", ",", "von", "Haus", "zu", "Haus", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "APPR", "NN", "$,", "APPR", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das T\u00fcchtige, wenn's wahrhaft ist,", "tokens": ["Das", "T\u00fcch\u00b7ti\u00b7ge", ",", "wenn's", "wahr\u00b7haft", "ist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUS", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wirkt \u00fcber alle Zeiten hinaus.", "tokens": ["Wirkt", "\u00fc\u00b7ber", "al\u00b7le", "Zei\u00b7ten", "hin\u00b7aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}}}}