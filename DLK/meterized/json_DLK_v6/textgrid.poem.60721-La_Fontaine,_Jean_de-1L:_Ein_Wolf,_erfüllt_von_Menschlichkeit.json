{"textgrid.poem.60721": {"metadata": {"author": {"name": "La Fontaine, Jean de", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein Wolf, erf\u00fcllt von Menschlichkeit", "genre": "verse", "period": "N.A.", "pub_year": 1658, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Wolf, erf\u00fcllt von Menschlichkeit", "tokens": ["Ein", "Wolf", ",", "er\u00b7f\u00fcllt", "von", "Menschlich\u00b7keit"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NE", "$,", "VVPP", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "(wenn's solche gibt in dieser Welt),", "tokens": ["(", "wenn's", "sol\u00b7che", "gibt", "in", "die\u00b7ser", "Welt", ")", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KOUS", "PIS", "VVFIN", "APPR", "PDAT", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat einst ob seiner Grausamkeit,", "tokens": ["Hat", "einst", "ob", "sei\u00b7ner", "Grau\u00b7sam\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "KOUS", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Obwohl bei ihm Notwendigkeit,", "tokens": ["Ob\u00b7wohl", "bei", "ihm", "Not\u00b7wen\u00b7dig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PPER", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Ernste Betrachtung angestellt.", "tokens": ["Erns\u00b7te", "Be\u00b7trach\u00b7tung", "an\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVPP", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "\u00bbich bin geha\u00dft\u00ab, sprach er; \u00bbvon wem? von allen!", "tokens": ["\u00bb", "ich", "bin", "ge\u00b7ha\u00dft", "\u00ab", ",", "sprach", "er", ";", "\u00bb", "von", "wem", "?", "von", "al\u00b7len", "!"], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "VVPP", "$(", "$,", "VVFIN", "PPER", "$.", "$(", "APPR", "PWS", "$.", "APPR", "PIAT", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Der Wolf ist allen der gemeine Feind.", "tokens": ["Der", "Wolf", "ist", "al\u00b7len", "der", "ge\u00b7mei\u00b7ne", "Feind", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "PIAT", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Er sei dem Untergang verfallen!", "tokens": ["Er", "sei", "dem", "Un\u00b7ter\u00b7gang", "ver\u00b7fal\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "So schreien J\u00e4ger, Bauer, Hund vereint.", "tokens": ["So", "schrei\u00b7en", "J\u00e4\u00b7ger", ",", "Bau\u00b7er", ",", "Hund", "ver\u00b7eint", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "$,", "NN", "$,", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Jupiter schwindelt's droben vom Geschrei.", "tokens": ["Ju\u00b7pi\u00b7ter", "schwin\u00b7delt's", "dro\u00b7ben", "vom", "Ge\u00b7schrei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.11": {"text": "Bereits ist England ganz von W\u00f6lfen frei,", "tokens": ["Be\u00b7reits", "ist", "En\u00b7gland", "ganz", "von", "W\u00f6l\u00b7fen", "frei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "ADV", "APPR", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Man hatte Preise auf uns ausgesetzt.", "tokens": ["Man", "hat\u00b7te", "Prei\u00b7se", "auf", "uns", "aus\u00b7ge\u00b7setzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "NN", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Kein Junker, der uns nicht zu Tode hetzt,", "tokens": ["Kein", "Jun\u00b7ker", ",", "der", "uns", "nicht", "zu", "To\u00b7de", "hetzt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "$,", "PRELS", "PPER", "PTKNEG", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Und keine Mutter, die nicht ihrem Kind", "tokens": ["Und", "kei\u00b7ne", "Mut\u00b7ter", ",", "die", "nicht", "ih\u00b7rem", "Kind"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "$,", "PRELS", "PTKNEG", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "Best\u00e4ndig mit dem b\u00f6sen Wolfe droht.", "tokens": ["Be\u00b7st\u00e4n\u00b7dig", "mit", "dem", "b\u00f6\u00b7sen", "Wol\u00b7fe", "droht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Das alles f\u00fcr ein r\u00e4udig Rind,", "tokens": ["Das", "al\u00b7les", "f\u00fcr", "ein", "r\u00e4u\u00b7dig", "Rind", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "APPR", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "F\u00fcr ein verirrtes Schaf, f\u00fcr einen dreisten Hund,", "tokens": ["F\u00fcr", "ein", "ver\u00b7irr\u00b7tes", "Schaf", ",", "f\u00fcr", "ei\u00b7nen", "dreis\u00b7ten", "Hund", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Die ich verzehrt aus eigner Not.", "tokens": ["Die", "ich", "ver\u00b7zehrt", "aus", "eig\u00b7ner", "Not", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVPP", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "Nun gut, beseitigen wir der Feindschaft Grund,", "tokens": ["Nun", "gut", ",", "be\u00b7sei\u00b7ti\u00b7gen", "wir", "der", "Feind\u00b7schaft", "Grund", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "VVFIN", "PPER", "ART", "NN", "NN", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.20": {"text": "Verschonen wir fortan lebendiges Brot", "tokens": ["Ver\u00b7scho\u00b7nen", "wir", "for\u00b7tan", "le\u00b7ben\u00b7di\u00b7ges", "Brot"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "ADJA", "NN"], "meter": "-+---+-+--+", "measure": "iambic.tetra.chol"}, "line.21": {"text": "Und sterben eher Hungers. Essen wir allein", "tokens": ["Und", "ster\u00b7ben", "e\u00b7her", "Hun\u00b7gers", ".", "Es\u00b7sen", "wir", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "NN", "$.", "NN", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Noch Gras. Das scheint mir nicht so schrecklich schwer", "tokens": ["Noch", "Gras", ".", "Das", "scheint", "mir", "nicht", "so", "schreck\u00b7lich", "schwer"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "$.", "PDS", "VVFIN", "PPER", "PTKNEG", "ADV", "ADJD", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Wie jener grimme Ha\u00df von gro\u00df und klein.\u00ab", "tokens": ["Wie", "je\u00b7ner", "grim\u00b7me", "Ha\u00df", "von", "gro\u00df", "und", "klein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PDAT", "ADJA", "NN", "APPR", "ADJD", "KON", "ADJD", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Indem er also sprach, gewahrte er,", "tokens": ["In\u00b7dem", "er", "al\u00b7so", "sprach", ",", "ge\u00b7wahr\u00b7te", "er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVFIN", "$,", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Wie Hirten ein am Spie\u00df gebratnes Lamm verzehrten.", "tokens": ["Wie", "Hir\u00b7ten", "ein", "am", "Spie\u00df", "ge\u00b7brat\u00b7nes", "Lamm", "ver\u00b7zehr\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "ART", "APPRART", "NN", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Da sprach er: \u00bbOh, ich schelte mein Begehr", "tokens": ["Da", "sprach", "er", ":", "\u00bb", "Oh", ",", "ich", "schel\u00b7te", "mein", "Be\u00b7gehr"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$.", "$(", "ITJ", "$,", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Nach Blut der gleichen Sippe, ihre H\u00fcter hier", "tokens": ["Nach", "Blut", "der", "glei\u00b7chen", "Sip\u00b7pe", ",", "ih\u00b7re", "H\u00fc\u00b7ter", "hier"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "ART", "ADJA", "NN", "$,", "PPOSAT", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Erquicken sich daran mit ihren Hunden.", "tokens": ["Er\u00b7qui\u00b7cken", "sich", "da\u00b7ran", "mit", "ih\u00b7ren", "Hun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "PAV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Den G\u00f6ttern Dank, da\u00df sie mich schnell belehrten!", "tokens": ["Den", "G\u00f6t\u00b7tern", "Dank", ",", "da\u00df", "sie", "mich", "schnell", "be\u00b7lehr\u00b7ten", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "KOUS", "PPER", "PRF", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Wahrlich, ich w\u00e4r das l\u00e4cherlichste Tier!", "tokens": ["Wahr\u00b7lich", ",", "ich", "w\u00e4r", "das", "l\u00e4\u00b7cher\u00b7lichs\u00b7te", "Tier", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Nein, mir soll weiterhin das L\u00e4mmchen munden,", "tokens": ["Nein", ",", "mir", "soll", "wei\u00b7ter\u00b7hin", "das", "L\u00e4mm\u00b7chen", "mun\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VMFIN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Selbst ungebraten, auch die Mutter, die es s\u00e4ugt,", "tokens": ["Selbst", "un\u00b7ge\u00b7bra\u00b7ten", ",", "auch", "die", "Mut\u00b7ter", ",", "die", "es", "s\u00e4ugt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "$,", "ADV", "ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Sowie der Vater, der's gezeugt!\u00ab", "tokens": ["So\u00b7wie", "der", "Va\u00b7ter", ",", "der's", "ge\u00b7zeugt", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "$,", "PRELS", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der Wolf sprach recht. Begehn wir je ein Fest,", "tokens": ["Der", "Wolf", "sprach", "recht", ".", "Be\u00b7gehn", "wir", "je", "ein", "Fest", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "ADJD", "$.", "VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "F\u00fcr das nicht manch Getier sein Leben l\u00e4\u00dft?", "tokens": ["F\u00fcr", "das", "nicht", "manch", "Ge\u00b7tier", "sein", "Le\u00b7ben", "l\u00e4\u00dft", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDS", "PTKNEG", "PIAT", "NN", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-++-+-+-+", "measure": "zehnsilber"}, "line.3": {"text": "Und wollen doch den Tieren nur gew\u00e4hren,", "tokens": ["Und", "wol\u00b7len", "doch", "den", "Tie\u00b7ren", "nur", "ge\u00b7w\u00e4h\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df sie sich von der Kost des goldnen Alters n\u00e4hren?", "tokens": ["Da\u00df", "sie", "sich", "von", "der", "Kost", "des", "gold\u00b7nen", "Al\u00b7ters", "n\u00e4h\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPR", "ART", "NN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ihr Hirten, h\u00f6rt und wi\u00dft,", "tokens": ["Ihr", "Hir\u00b7ten", ",", "h\u00f6rt", "und", "wi\u00dft", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Man kann dem Wolf nur unrecht geben", "tokens": ["Man", "kann", "dem", "Wolf", "nur", "un\u00b7recht", "ge\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "ART", "NE", "ADV", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Da, wo er nicht der St\u00e4rkere ist.", "tokens": ["Da", ",", "wo", "er", "nicht", "der", "St\u00e4r\u00b7ke\u00b7re", "ist", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "PPER", "PTKNEG", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.8": {"text": "Soll er als Eremit denn leben?", "tokens": ["Soll", "er", "als", "E\u00b7re\u00b7mit", "denn", "le\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "KOUS", "NN", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}