{"textgrid.poem.64802": {"metadata": {"author": {"name": "K\u00e4stner, Abraham Gotthelf", "birth": "N.A.", "death": "N.A."}, "title": "10. Ob eine Gesellschaft, die Sprache zu verbessern, durch \u00f6ffentliches Ansehn m\u00fcsse berechtigt werden", "genre": "verse", "period": "N.A.", "pub_year": 1741, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ihr Deutsche, die ihr euch f\u00fcr Deutschlands Ruhm vereinigt,", "tokens": ["Ihr", "Deut\u00b7sche", ",", "die", "ihr", "euch", "f\u00fcr", "Deutschlands", "Ruhm", "ver\u00b7ei\u00b7nigt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "PPER", "PRF", "APPR", "NE", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ihr, die ihr unsern Witz und unsre Mundart reinigt,", "tokens": ["Ihr", ",", "die", "ihr", "un\u00b7sern", "Witz", "und", "uns\u00b7re", "Mund\u00b7art", "rei\u00b7nigt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PRELS", "PPER", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Mit Zweifel, der die Lust in engen Schranken h\u00e4lt,", "tokens": ["Mit", "Zwei\u00b7fel", ",", "der", "die", "Lust", "in", "en\u00b7gen", "Schran\u00b7ken", "h\u00e4lt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PRELS", "ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Erblick' ich diesen Tag, der mich zu euch gesellt;", "tokens": ["Er\u00b7blick'", "ich", "die\u00b7sen", "Tag", ",", "der", "mich", "zu", "euch", "ge\u00b7sellt", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PDAT", "NN", "$,", "PRELS", "PRF", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Vielleicht, da\u00df meine Kraft durch eure H\u00fclfe steiget,", "tokens": ["Viel\u00b7leicht", ",", "da\u00df", "mei\u00b7ne", "Kraft", "durch", "eu\u00b7re", "H\u00fcl\u00b7fe", "stei\u00b7get", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PPOSAT", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Vielleicht, da\u00df neben euch sich meine Schw\u00e4che zeiget.", "tokens": ["Viel\u00b7leicht", ",", "da\u00df", "ne\u00b7ben", "euch", "sich", "mei\u00b7ne", "Schw\u00e4\u00b7che", "zei\u00b7get", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "APPR", "PPER", "PRF", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ich wag' es, nehmt von mir nur Flei\u00df und Eifer an,", "tokens": ["Ich", "wag'", "es", ",", "nehmt", "von", "mir", "nur", "Flei\u00df", "und", "Ei\u00b7fer", "an", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "VVFIN", "APPR", "PPER", "ADV", "NN", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Wo ich durch Witz und Geist mich nicht erheben kann.", "tokens": ["Wo", "ich", "durch", "Witz", "und", "Geist", "mich", "nicht", "er\u00b7he\u00b7ben", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "NN", "KON", "NN", "PPER", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Ist's euer Eifer doch, den ich an euch gesch\u00e4tzet,", "tokens": ["Ist's", "eu\u00b7er", "Ei\u00b7fer", "doch", ",", "den", "ich", "an", "euch", "ge\u00b7sch\u00e4t\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "NN", "ADV", "$,", "PRELS", "PPER", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-++-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Noch mehr, als euer Witz, so sehr er mich erg\u00f6tzet:", "tokens": ["Noch", "mehr", ",", "als", "eu\u00b7er", "Witz", ",", "so", "sehr", "er", "mich", "er\u00b7g\u00f6t\u00b7zet", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "PPOSAT", "NN", "$,", "ADV", "ADV", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihr liebt das Vaterland, nur dadurch brennt der Flei\u00df,", "tokens": ["Ihr", "liebt", "das", "Va\u00b7ter\u00b7land", ",", "nur", "da\u00b7durch", "brennt", "der", "Flei\u00df", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ADV", "PAV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der noch der Klugen Lob statt alles Lohnes wei\u00df;", "tokens": ["Der", "noch", "der", "Klu\u00b7gen", "Lob", "statt", "al\u00b7les", "Loh\u00b7nes", "wei\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "ADJA", "NN", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Zu gl\u00fccklich, sollt' ihn nur dies Lob allein vergelten,", "tokens": ["Zu", "gl\u00fcck\u00b7lich", ",", "sollt'", "ihn", "nur", "dies", "Lob", "al\u00b7lein", "ver\u00b7gel\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "$,", "VMFIN", "PPER", "ADV", "PDS", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und ihn nicht jeder Thor, der euch nicht kennet, schelten.", "tokens": ["Und", "ihn", "nicht", "je\u00b7der", "Thor", ",", "der", "euch", "nicht", "ken\u00b7net", ",", "schel\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PPER", "PTKNEG", "PIAT", "NN", "$,", "PRELS", "PPER", "PTKNEG", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ihr Deutsche, die ihr euch von deutschem Sinn entfernt,", "tokens": ["Ihr", "Deut\u00b7sche", ",", "die", "ihr", "euch", "von", "deut\u00b7schem", "Sinn", "ent\u00b7fernt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "PPER", "PRF", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und fremde Thorheit nur von fremden V\u00f6lkern lernt,", "tokens": ["Und", "frem\u00b7de", "Thor\u00b7heit", "nur", "von", "frem\u00b7den", "V\u00f6l\u00b7kern", "lernt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Den Satz noch nicht erkennt, den sie so deutlich zeigen:", "tokens": ["Den", "Satz", "noch", "nicht", "er\u00b7kennt", ",", "den", "sie", "so", "deut\u00b7lich", "zei\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "PTKNEG", "VVFIN", "$,", "PRELS", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Es steigt des Landes Ruhm, wenn Witz und Mundart steigen.", "tokens": ["Es", "steigt", "des", "Lan\u00b7des", "Ruhm", ",", "wenn", "Witz", "und", "Mund\u00b7art", "stei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "NN", "$,", "KOUS", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "An euch bekehrt man nichts. Es sey euch immer recht,", "tokens": ["An", "euch", "be\u00b7kehrt", "man", "nichts", ".", "Es", "sey", "euch", "im\u00b7mer", "recht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVFIN", "PIS", "PIS", "$.", "PPER", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Da\u00df ihr mit Fremden sch\u00f6n, mit uns barbarisch sprecht:", "tokens": ["Da\u00df", "ihr", "mit", "Frem\u00b7den", "sch\u00f6n", ",", "mit", "uns", "bar\u00b7ba\u00b7risch", "sprecht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "ADJD", "$,", "APPR", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Darf eure Thaten doch kein deutscher ", "tokens": ["Darf", "eu\u00b7re", "Tha\u00b7ten", "doch", "kein", "deut\u00b7scher"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPOSAT", "NN", "ADV", "PIAT", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Werd' ich ein ", "tokens": ["Werd'", "ich", "ein"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PPER", "ART"], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "\u00bbwie aber? Ist dein Flei\u00df, Gesellschaft, nennenswerth?", "tokens": ["\u00bb", "wie", "a\u00b7ber", "?", "Ist", "dein", "Flei\u00df", ",", "Ge\u00b7sell\u00b7schaft", ",", "nen\u00b7nens\u00b7werth", "?"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["$(", "KOKOM", "ADV", "$.", "VAFIN", "PPOSAT", "NN", "$,", "NN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dein scharfer Richterspruch, der Das f\u00fcr falsch erkl\u00e4rt,", "tokens": ["Dein", "schar\u00b7fer", "Rich\u00b7ter\u00b7spruch", ",", "der", "Das", "f\u00fcr", "falsch", "er\u00b7kl\u00e4rt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "PRELS", "PDS", "APPR", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und Jenes richtig hei\u00dft, was ist es, das er n\u00fctzet,", "tokens": ["Und", "Je\u00b7nes", "rich\u00b7tig", "hei\u00dft", ",", "was", "ist", "es", ",", "das", "er", "n\u00fct\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "ADJD", "VVFIN", "$,", "PWS", "VAFIN", "PPER", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wenn ihn dein Eigensinn statt alles Ansehns st\u00fctzet?", "tokens": ["Wenn", "ihn", "dein", "Ei\u00b7gen\u00b7sinn", "statt", "al\u00b7les", "An\u00b7sehns", "st\u00fct\u00b7zet", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "APPR", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So redet Der", "tokens": ["So", "re\u00b7det", "Der"], "token_info": ["word", "word", "word"], "pos": ["ADV", "VVFIN", "ART"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "Jetzt Frankreichs Fehler zeigt, jetzt Deutschlands M\u00e4ngel weist,", "tokens": ["Jetzt", "Fran\u00b7kreichs", "Feh\u00b7ler", "zeigt", ",", "jetzt", "Deutschlands", "M\u00e4n\u00b7gel", "weist", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NN", "VVFIN", "$,", "ADV", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Gut, wenn er sich zuvor genugsam unterrichtet,", "tokens": ["Gut", ",", "wenn", "er", "sich", "zu\u00b7vor", "ge\u00b7nug\u00b7sam", "un\u00b7ter\u00b7rich\u00b7tet", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "KOUS", "PPER", "PRF", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Schlecht, wenn er \u00fcbereilt und ohne Kenntni\u00df richtet.", "tokens": ["Schlecht", ",", "wenn", "er", "\u00fc\u00b7be\u00b7reilt", "und", "oh\u00b7ne", "Kennt\u00b7ni\u00df", "rich\u00b7tet", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KOUS", "PPER", "VVPP", "KON", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Dies widerf\u00e4hret ihm, wenn er der Sprache lacht,", "tokens": ["Dies", "wi\u00b7der\u00b7f\u00e4h\u00b7ret", "ihm", ",", "wenn", "er", "der", "Spra\u00b7che", "lacht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "$,", "KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wo ", "tokens": ["Wo"], "token_info": ["word"], "pos": ["PWAV"], "meter": "+", "measure": "single.up"}, "line.11": {"text": "Und ", "tokens": ["Und"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.12": {"text": "Und ein verachtet Lied f\u00fcr ", "tokens": ["Und", "ein", "ver\u00b7ach\u00b7tet", "Lied", "f\u00fcr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ART", "VVFIN", "NN", "APPR"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.13": {"text": "Und, als verst\u00fcnd er deutsch, es ohne Zittern wagt,", "tokens": ["Und", ",", "als", "ver\u00b7st\u00fcnd", "er", "deutsch", ",", "es", "oh\u00b7ne", "Zit\u00b7tern", "wagt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "ADJD", "PPER", "ADJD", "$,", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Aus ", "tokens": ["Aus"], "token_info": ["word"], "pos": ["APPR"], "meter": "-", "measure": "single.down"}}, "stanza.5": {"line.1": {"text": "Der ist es, der an euch die eitle M\u00fche tadelt,", "tokens": ["Der", "ist", "es", ",", "der", "an", "euch", "die", "eit\u00b7le", "M\u00fc\u00b7he", "ta\u00b7delt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "$,", "PRELS", "APPR", "PPER", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df ihr, die kein Gesetz zu deutschen Richtern adelt,", "tokens": ["Da\u00df", "ihr", ",", "die", "kein", "Ge\u00b7setz", "zu", "deut\u00b7schen", "Rich\u00b7tern", "a\u00b7delt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "PRELS", "PIAT", "NN", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nur k\u00fchn auf eigne Kraft, ganz Deutschland Regeln gebt,", "tokens": ["Nur", "k\u00fchn", "auf", "eig\u00b7ne", "Kraft", ",", "ganz", "Deutschland", "Re\u00b7geln", "gebt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ADJA", "NN", "$,", "ADV", "NE", "NN", "VVFIN", "$,"], "meter": "-+-+-+-++-+", "measure": "unknown.measure.hexa"}, "line.4": {"text": "Da, wie er sicher wei\u00df, ganz Deutschland widerstrebt.", "tokens": ["Da", ",", "wie", "er", "si\u00b7cher", "wei\u00df", ",", "ganz", "Deutschland", "wi\u00b7der\u00b7strebt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "PPER", "ADJD", "VVFIN", "$,", "ADV", "NE", "VVFIN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Er selbst gestehet dies. Nun wundre man sich nicht.", "tokens": ["Er", "selbst", "ge\u00b7ste\u00b7het", "dies", ".", "Nun", "wund\u00b7re", "man", "sich", "nicht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "PDS", "$.", "ADV", "VVFIN", "PIS", "PRF", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wenn er, so wohl gelehrt, ein weises Urtheil spricht.", "tokens": ["Wenn", "er", ",", "so", "wohl", "ge\u00b7lehrt", ",", "ein", "wei\u00b7ses", "Ur\u00b7theil", "spricht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "ADV", "ADV", "VVPP", "$,", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "\u00bbdie deutsche Zierlichkeit ist nicht den ", "tokens": ["\u00bb", "die", "deut\u00b7sche", "Zier\u00b7lich\u00b7keit", "ist", "nicht", "den"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ART", "ADJA", "NN", "VAFIN", "PTKNEG", "ART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Der freyen V\u00f6lker Zahl, die Deutschlands Weite hegt,", "tokens": ["Der", "frey\u00b7en", "V\u00f6l\u00b7ker", "Zahl", ",", "die", "Deutschlands", "Wei\u00b7te", "hegt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Wird keiner Sprachkunst Joch von ", "tokens": ["Wird", "kei\u00b7ner", "Sprach\u00b7kunst", "Joch", "von"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN", "NN", "APPR"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "So wird dadurch ein Volk als Oberherr verehret,", "tokens": ["So", "wird", "da\u00b7durch", "ein", "Volk", "als", "O\u00b7ber\u00b7herr", "ver\u00b7eh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PAV", "ART", "NN", "KOUS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df es ein gleiches Volk der Sprache Sch\u00f6nheit lehret?", "tokens": ["Da\u00df", "es", "ein", "glei\u00b7ches", "Volk", "der", "Spra\u00b7che", "Sch\u00f6n\u00b7heit", "leh\u00b7ret", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wenn ", "tokens": ["Wenn"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Was ging B\u00f6otien an seiner Freyheit ab?", "tokens": ["Was", "ging", "B\u00f6o\u00b7ti\u00b7en", "an", "sei\u00b7ner", "Frey\u00b7heit", "ab", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "NE", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.9": {"line.1": {"text": "Ja, soll das Deutsch des ", "tokens": ["Ja", ",", "soll", "das", "Deutsch", "des"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "VMFIN", "ART", "NN", "ART"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "So wird Der ohne Streit auch seinen Zweck erreichen,", "tokens": ["So", "wird", "Der", "oh\u00b7ne", "Streit", "auch", "sei\u00b7nen", "Zweck", "er\u00b7rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "APPR", "NN", "ADV", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der nach der Sprache Glanz, die Frankreichs Ruhm erh\u00f6ht,", "tokens": ["Der", "nach", "der", "Spra\u00b7che", "Glanz", ",", "die", "Fran\u00b7kreichs", "Ruhm", "er\u00b7h\u00f6ht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "NN", "$,", "PRELS", "NE", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Zum ", "tokens": ["Zum"], "token_info": ["word"], "pos": ["APPRART"], "meter": "+", "measure": "single.up"}}, "stanza.10": {"line.1": {"text": "\u00bbda, wo der K\u00f6nig selbst der Sprache Richter setzet,", "tokens": ["\u00bb", "da", ",", "wo", "der", "K\u00f6\u00b7nig", "selbst", "der", "Spra\u00b7che", "Rich\u00b7ter", "set\u00b7zet", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "PWAV", "ART", "NN", "ADV", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da wird durchs ganze Land ihr Urtheil hochgesch\u00e4tzet.\u00ab", "tokens": ["Da", "wird", "durchs", "gan\u00b7ze", "Land", "ihr", "Ur\u00b7theil", "hoch\u00b7ge\u00b7sch\u00e4t\u00b7zet", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "APPRART", "ADJA", "NN", "PPOSAT", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Dies ist der k\u00fchne Satz, den kein Beweis besch\u00fctzt,", "tokens": ["Dies", "ist", "der", "k\u00fch\u00b7ne", "Satz", ",", "den", "kein", "Be\u00b7weis", "be\u00b7sch\u00fctzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,", "PRELS", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Den selbst das Beyspiel f\u00e4llt, durch welches er ihn st\u00fctzt.", "tokens": ["Den", "selbst", "das", "Bey\u00b7spiel", "f\u00e4llt", ",", "durch", "wel\u00b7ches", "er", "ihn", "st\u00fctzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "VVFIN", "$,", "APPR", "PRELS", "PPER", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Gebeut man auch dem Ohr in drohenden Gesetzen?", "tokens": ["Ge\u00b7beut", "man", "auch", "dem", "Ohr", "in", "dro\u00b7hen\u00b7den", "Ge\u00b7set\u00b7zen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "ADV", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Nein, zwar die Sprache steigt, wo der Monarch sie liebt,", "tokens": ["Nein", ",", "zwar", "die", "Spra\u00b7che", "steigt", ",", "wo", "der", "Mon\u00b7arch", "sie", "liebt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADV", "ART", "NN", "VVFIN", "$,", "PWAV", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Weil man durch's ganze Land des Hofes Sitten \u00fcbt,", "tokens": ["Weil", "man", "durch's", "gan\u00b7ze", "Land", "des", "Ho\u00b7fes", "Sit\u00b7ten", "\u00fcbt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPRART", "ADJA", "NN", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Allein, will er sein Volk des Ausdrucks Sch\u00f6nheit lehren,", "tokens": ["Al\u00b7lein", ",", "will", "er", "sein", "Volk", "des", "Aus\u00b7drucks", "Sch\u00f6n\u00b7heit", "leh\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "VMFIN", "PPER", "PPOSAT", "NN", "ART", "NN", "NN", "VVINF", "$,"], "meter": "-++--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Wird man sein Beyspiel nur, nicht sein Befehlen ehren.", "tokens": ["Wird", "man", "sein", "Bey\u00b7spiel", "nur", ",", "nicht", "sein", "Be\u00b7feh\u00b7len", "eh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PPOSAT", "NN", "ADV", "$,", "PTKNEG", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der Sprachkunst Quellen sind: Brauch, Ursprung Aehnlichkeit;", "tokens": ["Der", "Sprach\u00b7kunst", "Quel\u00b7len", "sind", ":", "Brauch", ",", "Ur\u00b7sprung", "A\u00b7ehn\u00b7lich\u00b7keit", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "$.", "NN", "$,", "NN", "NN", "$."], "meter": "-+-+--++--+-+", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "Was der Gelehrte schreibt, nicht was der F\u00fcrst gebeut.", "tokens": ["Was", "der", "Ge\u00b7lehr\u00b7te", "schreibt", ",", "nicht", "was", "der", "F\u00fcrst", "ge\u00b7beut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "VVFIN", "$,", "PTKNEG", "PWS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Der F\u00fcrst, der Lehrer setzt, macht sie zugleich nicht t\u00fcchtig;", "tokens": ["Der", "F\u00fcrst", ",", "der", "Leh\u00b7rer", "setzt", ",", "macht", "sie", "zu\u00b7gleich", "nicht", "t\u00fcch\u00b7tig", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ihr Ausspruch wird dadurch nur bey dem P\u00f6bel wichtig.", "tokens": ["Ihr", "Aus\u00b7spruch", "wird", "da\u00b7durch", "nur", "bey", "dem", "P\u00f6\u00b7bel", "wich\u00b7tig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PAV", "ADV", "APPR", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Oft trotzet tr\u00e4ger Stolz auf ein erschlichnes Amt,", "tokens": ["Oft", "trot\u00b7zet", "tr\u00e4\u00b7ger", "Stolz", "auf", "ein", "er\u00b7schlich\u00b7nes", "Amt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Doch geht der Flei\u00df ihm vor, der sich nur selbst entflammt.", "tokens": ["Doch", "geht", "der", "Flei\u00df", "ihm", "vor", ",", "der", "sich", "nur", "selbst", "ent\u00b7flammt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PPER", "PTKVZ", "$,", "PRELS", "PRF", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Mu\u00df man die ", "tokens": ["Mu\u00df", "man", "die"], "token_info": ["word", "word", "word"], "pos": ["VMFIN", "PIS", "ART"], "meter": "+--", "measure": "dactylic.init"}, "line.2": {"text": "So, wie den K\u00f6nig selbst, der sie gesetzt, verehren?", "tokens": ["So", ",", "wie", "den", "K\u00f6\u00b7nig", "selbst", ",", "der", "sie", "ge\u00b7setzt", ",", "ver\u00b7eh\u00b7ren", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ART", "NN", "ADV", "$,", "PRELS", "PPER", "VVPP", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Hat dieses Ansehn wohl ein ", "tokens": ["Hat", "die\u00b7ses", "An\u00b7sehn", "wohl", "ein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PDAT", "NN", "ADV", "ART"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Wenn die Gesellschaft hier, dort er mit Frankreich stand?", "tokens": ["Wenn", "die", "Ge\u00b7sell\u00b7schaft", "hier", ",", "dort", "er", "mit", "Fran\u00b7kreich", "stand", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "$,", "ADV", "PPER", "APPR", "NE", "VVFIN", "$."], "meter": "---+-++--+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Durft ihren W\u00f6rterbau kein ", "tokens": ["Durft", "ih\u00b7ren", "W\u00f6r\u00b7ter\u00b7bau", "kein"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "PIAT"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Und sie kein ", "tokens": ["Und", "sie", "kein"], "token_info": ["word", "word", "word"], "pos": ["KON", "PPER", "PIAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Doch ist man jetzt vielleicht nicht wie vor Zeiten k\u00fchn;", "tokens": ["Doch", "ist", "man", "jetzt", "viel\u00b7leicht", "nicht", "wie", "vor", "Zei\u00b7ten", "k\u00fchn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIS", "ADV", "ADV", "PTKNEG", "KOKOM", "APPR", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Denn wer jetzt denken will, mu\u00df Frankreich \u00f6fters fliehn.", "tokens": ["Denn", "wer", "jetzt", "den\u00b7ken", "will", ",", "mu\u00df", "Fran\u00b7kreich", "\u00f6f\u00b7ters", "fliehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "VVINF", "VMFIN", "$,", "VMFIN", "NE", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Vielleicht mu\u00df man sich jetzt nur nach den ", "tokens": ["Viel\u00b7leicht", "mu\u00df", "man", "sich", "jetzt", "nur", "nach", "den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "PRF", "ADV", "ADV", "APPR", "ART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Und, schreibt man nicht wie sie, nach ", "tokens": ["Und", ",", "schreibt", "man", "nicht", "wie", "sie", ",", "nach"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["KON", "$,", "VVFIN", "PIS", "PTKNEG", "PWAV", "PPER", "$,", "APPR"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.14": {"line.1": {"text": "Gef\u00fchl, und nicht Gebot, regiert des Deutschen Ohr;", "tokens": ["Ge\u00b7f\u00fchl", ",", "und", "nicht", "Ge\u00b7bot", ",", "re\u00b7giert", "des", "Deut\u00b7schen", "Ohr", ";"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "PTKNEG", "NN", "$,", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Er zieht, was ", "tokens": ["Er", "zieht", ",", "was"], "token_info": ["word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "PWS"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Nicht, weil ein ", "tokens": ["Nicht", ",", "weil", "ein"], "token_info": ["word", "punct", "word", "word"], "pos": ["PTKNEG", "$,", "KOUS", "ART"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Nein, nur weil er doch mehr durch Richtigkeit erg\u00f6tzet.", "tokens": ["Nein", ",", "nur", "weil", "er", "doch", "mehr", "durch", "Rich\u00b7tig\u00b7keit", "er\u00b7g\u00f6t\u00b7zet", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ADV", "KOUS", "PPER", "ADV", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Lehrt unser ", "tokens": ["Lehrt", "un\u00b7ser"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Noch weiter, als ", "tokens": ["Noch", "wei\u00b7ter", ",", "als"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADV", "ADV", "$,", "KOUS"], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Selbst, wo der ", "tokens": ["Selbst", ",", "wo", "der"], "token_info": ["word", "punct", "word", "word"], "pos": ["ADV", "$,", "PWAV", "ART"], "meter": "+--", "measure": "dactylic.init"}, "line.8": {"text": "Selbst, wo um ", "tokens": ["Selbst", ",", "wo", "um"], "token_info": ["word", "punct", "word", "word"], "pos": ["ADV", "$,", "PWAV", "KOUI"], "meter": "+-+", "measure": "trochaic.di"}, "line.9": {"text": "Kennt zwar das freye Volk der Deutschen Herrschaft nicht;", "tokens": ["Kennt", "zwar", "das", "frey\u00b7e", "Volk", "der", "Deut\u00b7schen", "Herr\u00b7schaft", "nicht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Doch uns gehorchet es, und red't, wie ", "tokens": ["Doch", "uns", "ge\u00b7hor\u00b7chet", "es", ",", "und", "red't", ",", "wie"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["KON", "PPER", "VVFIN", "PPER", "$,", "KON", "VVFIN", "$,", "PWAV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Gelehrte, fahret fort die Mundart auszubessern,", "tokens": ["Ge\u00b7lehr\u00b7te", ",", "fah\u00b7ret", "fort", "die", "Mund\u00b7art", "aus\u00b7zu\u00b7bes\u00b7sern", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "PTKVZ", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df ihr nur selbst euch hebt, mu\u00df euren Werth vergr\u00f6\u00dfern.", "tokens": ["Da\u00df", "ihr", "nur", "selbst", "euch", "hebt", ",", "mu\u00df", "eu\u00b7ren", "Werth", "ver\u00b7gr\u00f6\u00b7\u00dfern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "PPER", "VVFIN", "$,", "VMFIN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "La\u00dft nicht den Eifer nach, der Deutschlands Ruhm vermehrt:", "tokens": ["La\u00dft", "nicht", "den", "Ei\u00b7fer", "nach", ",", "der", "Deutschlands", "Ruhm", "ver\u00b7mehrt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKNEG", "ART", "NN", "PTKVZ", "$,", "ART", "NN", "NN", "VVPP", "$."], "meter": "---+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hofft keinen schlechtern Lohn, als da\u00df euch Deutschland ehrt.", "tokens": ["Hofft", "kei\u00b7nen", "schlech\u00b7tern", "Lohn", ",", "als", "da\u00df", "euch", "Deutschland", "ehrt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "ADJA", "NN", "$,", "KOKOM", "KOUS", "PPER", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}