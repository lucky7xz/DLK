{"textgrid.poem.52910": {"metadata": {"author": {"name": "Dingelstedt, Franz von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ihr macht mich irr durch das Gekr\u00e4chz", "genre": "verse", "period": "N.A.", "pub_year": 1847, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ihr macht mich irr durch das Gekr\u00e4chz", "tokens": ["Ihr", "macht", "mich", "irr", "durch", "das", "Ge\u00b7kr\u00e4chz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von Russen und Franzosen;", "tokens": ["Von", "Rus\u00b7sen", "und", "Fran\u00b7zo\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbkonservativer\u00ab hei\u00dft es rechts,", "tokens": ["\u00bb", "kon\u00b7ser\u00b7va\u00b7ti\u00b7ver", "\u00ab", "hei\u00dft", "es", "rechts", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "NE", "$(", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Und links hei\u00dft's \u00bbOhne-Hosen\u00ab.", "tokens": ["Und", "links", "hei\u00dft's", "\u00bb", "Oh\u00b7ne\u00b7Ho\u00b7sen", "\u00ab", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["KON", "ADV", "NE", "$(", "NN", "$(", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "\u00bbwas ist des Deutschen Vaterland?\u00ab", "tokens": ["\u00bb", "was", "ist", "des", "Deut\u00b7schen", "Va\u00b7ter\u00b7land", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VAFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So singt Ihr alle Tage,", "tokens": ["So", "singt", "Ihr", "al\u00b7le", "Ta\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Doch weder Rhein- noch Donaustrand", "tokens": ["Doch", "we\u00b7der", "Rhein", "noch", "Do\u00b7naus\u00b7trand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "KON", "TRUNC", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Antworten auf die Frage.", "tokens": ["Ant\u00b7wor\u00b7ten", "auf", "die", "Fra\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Wenn einer: \u00bbLippe-Detmold\u00ab spricht, \u2013", "tokens": ["Wenn", "ei\u00b7ner", ":", "\u00bb", "Lip\u00b7pe\u00b7Det\u00b7mold", "\u00ab", "spricht", ",", "\u2013"], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["KOUS", "PIS", "$.", "$(", "NE", "$(", "VVFIN", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hui, Partikularismus!", "tokens": ["Hui", ",", "Par\u00b7ti\u00b7ku\u00b7la\u00b7ris\u00b7mus", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["FM", "$,", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und ha\u00dft er die Pariser nicht, \u2013", "tokens": ["Und", "ha\u00dft", "er", "die", "Pa\u00b7ri\u00b7ser", "nicht", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "PTKNEG", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Pfui, Kosmopolitismus!", "tokens": ["Pfui", ",", "Kos\u00b7mo\u00b7po\u00b7li\u00b7tis\u00b7mus", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Das Vaterland ist immer so,", "tokens": ["Das", "Va\u00b7ter\u00b7land", "ist", "im\u00b7mer", "so", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie's passend wird befunden,", "tokens": ["Wie's", "pas\u00b7send", "wird", "be\u00b7fun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VVPP", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bald Klein-Sedez, bald Folio,", "tokens": ["Bald", "Klein\u00b7Se\u00b7dez", ",", "bald", "Fo\u00b7lio", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "NN", "$,", "ADV", "NE", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Doch immerdar \u2013 gebunden!", "tokens": ["Doch", "im\u00b7mer\u00b7dar", "\u2013", "ge\u00b7bun\u00b7den", "!"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["KON", "ADV", "$(", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Auflagen und den Druck versehn", "tokens": ["Auf\u00b7la\u00b7gen", "und", "den", "Druck", "ver\u00b7sehn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "KON", "ART", "NN", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gern selbst die gro\u00dfen Herren,", "tokens": ["Gern", "selbst", "die", "gro\u00b7\u00dfen", "Her\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und die nicht so wie andre stehn,", "tokens": ["Und", "die", "nicht", "so", "wie", "and\u00b7re", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PTKNEG", "ADV", "KOKOM", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Lettern l\u00e4\u00dft man \u2013 sperren.", "tokens": ["Die", "Let\u00b7tern", "l\u00e4\u00dft", "man", "\u2013", "sper\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIS", "$(", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "F\u00fcrwahr, ein komischer Roman!", "tokens": ["F\u00fcr\u00b7wahr", ",", "ein", "ko\u00b7mi\u00b7scher", "Ro\u00b7man", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wie w\u00e4r's, wenn wir's versuchten,", "tokens": ["Wie", "w\u00e4r's", ",", "wenn", "wir's", "ver\u00b7such\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "$,", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und b\u00e4nden statt in Corduan", "tokens": ["Und", "b\u00e4n\u00b7den", "statt", "in", "Cor\u00b7du\u00b7an"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "APPR", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In Klammern ihn und Juchten?!", "tokens": ["In", "Klam\u00b7mern", "ihn", "und", "Juch\u00b7ten", "?!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PPER", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}