{"textgrid.poem.64796": {"metadata": {"author": {"name": "K\u00e4stner, Abraham Gotthelf", "birth": "N.A.", "death": "N.A."}, "title": "2. Gedanken \u00fcber die Verbindlichkeit der Dichter, allen Lesern deutlich zu seyn", "genre": "verse", "period": "N.A.", "pub_year": 1759, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Dich, Freund, reizt muntrer Witz, so wie erhabnes Wissen,", "tokens": ["Dich", ",", "Freund", ",", "reizt", "mun\u00b7trer", "Witz", ",", "so", "wie", "er\u00b7hab\u00b7nes", "Wis\u00b7sen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "NN", "$,", "VVFIN", "ADJA", "NN", "$,", "ADV", "KOKOM", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Du denkst bey ", "tokens": ["Du", "denkst", "bey"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR"], "meter": "--+", "measure": "anapaest.init"}, "line.3": {"text": "Sprich, ", "tokens": ["Sprich", ","], "token_info": ["word", "punct"], "pos": ["VVIMP", "$,"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Gemeiner Leser Schwarm sich nie entziehen soll?", "tokens": ["Ge\u00b7mei\u00b7ner", "Le\u00b7ser", "Schwarm", "sich", "nie", "ent\u00b7zie\u00b7hen", "soll", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "PRF", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Sprich, ob es strafbar ist, nicht Allen deutlich bleiben,", "tokens": ["Sprich", ",", "ob", "es", "straf\u00b7bar", "ist", ",", "nicht", "Al\u00b7len", "deut\u00b7lich", "blei\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "$,", "KOUS", "PPER", "ADJD", "VAFIN", "$,", "PTKNEG", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Manch Lied den Sch\u00f6nen weyhn, und Manches Weisen schreiben?", "tokens": ["Manch", "Lied", "den", "Sch\u00f6\u00b7nen", "weyhn", ",", "und", "Man\u00b7ches", "Wei\u00b7sen", "schrei\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "NN", "VVINF", "$,", "KON", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Den Reimer sch\u00fctz' ich nicht, der, was er dunkel denkt,", "tokens": ["Den", "Rei\u00b7mer", "sch\u00fctz'", "ich", "nicht", ",", "der", ",", "was", "er", "dun\u00b7kel", "denkt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PTKNEG", "$,", "PRELS", "$,", "PWS", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Zu seiner Leser Qual in dunklern Ausdruck senkt.", "tokens": ["Zu", "sei\u00b7ner", "Le\u00b7ser", "Qual", "in", "dunk\u00b7lern", "Aus\u00b7druck", "senkt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Mir wird er deutlich seyn, w\u00e4hlt er sich auch zum Muster", "tokens": ["Mir", "wird", "er", "deut\u00b7lich", "seyn", ",", "w\u00e4hlt", "er", "sich", "auch", "zum", "Mus\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "VAINF", "$,", "VVFIN", "PPER", "PRF", "ADV", "APPRART", "NN"], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Den Ruhm Lusatiens, den ", "tokens": ["Den", "Ruhm", "Lu\u00b7sa\u00b7ti\u00b7ens", ",", "den"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "NE", "$,", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Mir sagt ein jeder Ort, der Manchem M\u00fche macht,", "tokens": ["Mir", "sagt", "ein", "je\u00b7der", "Ort", ",", "der", "Man\u00b7chem", "M\u00fc\u00b7he", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "PIAT", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Dies war des Autors Sinn; er hatte nichts gedacht.", "tokens": ["Dies", "war", "des", "Au\u00b7tors", "Sinn", ";", "er", "hat\u00b7te", "nichts", "ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "NN", "$.", "PPER", "VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ein Andrer kennt vielleicht der Weisheit \u00e4u\u00dfre Schalen,", "tokens": ["Ein", "A\u00b7ndrer", "kennt", "viel\u00b7leicht", "der", "Weis\u00b7heit", "\u00e4u\u00df\u00b7re", "Scha\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und gleichwohl soll sein Vers mit hohem Wissen prahlen;", "tokens": ["Und", "gleich\u00b7wohl", "soll", "sein", "Vers", "mit", "ho\u00b7hem", "Wis\u00b7sen", "prah\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PPOSAT", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Drum f\u00fchrt er, was er sagt, in Dampf und Nebel ein.", "tokens": ["Drum", "f\u00fchrt", "er", ",", "was", "er", "sagt", ",", "in", "Dampf", "und", "Ne\u00b7bel", "ein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "$,", "PWS", "PPER", "VVFIN", "$,", "APPR", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Dr\u00fcckt es nur deutlich aus, so wird nichts Schlechters seyn;", "tokens": ["Dr\u00fcckt", "es", "nur", "deut\u00b7lich", "aus", ",", "so", "wird", "nichts", "Schlech\u00b7ters", "seyn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$,", "ADV", "VAFIN", "PIS", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "So wie uns manchen Satz, den jedes Kind erkennet,", "tokens": ["So", "wie", "uns", "man\u00b7chen", "Satz", ",", "den", "je\u00b7des", "Kind", "er\u00b7ken\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "PPER", "PIAT", "NN", "$,", "ART", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Der Thelematolog in dunkeln W\u00f6rtern nennet.", "tokens": ["Der", "The\u00b7le\u00b7ma\u00b7to\u00b7log", "in", "dun\u00b7keln", "W\u00f6r\u00b7tern", "nen\u00b7net", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Mich reizet nur ein Lied von tiefem Denken voll,", "tokens": ["Mich", "rei\u00b7zet", "nur", "ein", "Lied", "von", "tie\u00b7fem", "Den\u00b7ken", "voll", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN", "APPR", "ADJA", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Gemacht, da\u00df man es mehr als einmal lesen soll:", "tokens": ["Ge\u00b7macht", ",", "da\u00df", "man", "es", "mehr", "als", "ein\u00b7mal", "le\u00b7sen", "soll", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KOUS", "PIS", "PPER", "PIAT", "KOKOM", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nicht, das durch Dunkelheit des Einfalls Armuth decket,", "tokens": ["Nicht", ",", "das", "durch", "Dun\u00b7kel\u00b7heit", "des", "Ein\u00b7falls", "Ar\u00b7muth", "de\u00b7cket", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "$,", "PRELS", "APPR", "NN", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Nicht, das mit Flei\u00dfe nur, was man schon wei\u00df, verstecket.", "tokens": ["Nicht", ",", "das", "mit", "Flei\u00b7\u00dfe", "nur", ",", "was", "man", "schon", "wei\u00df", ",", "ver\u00b7ste\u00b7cket", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PTKNEG", "$,", "PRELS", "APPR", "NN", "ADV", "$,", "PRELS", "PIS", "ADV", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "O nein, ein solches Lied, das hohe Wahrheit singt,", "tokens": ["O", "nein", ",", "ein", "sol\u00b7ches", "Lied", ",", "das", "ho\u00b7he", "Wahr\u00b7heit", "singt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKANT", "$,", "ART", "PIAT", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die st\u00e4rker in den Sinn durch k\u00fchnen Ausdruck dringt,", "tokens": ["Die", "st\u00e4r\u00b7ker", "in", "den", "Sinn", "durch", "k\u00fch\u00b7nen", "Aus\u00b7druck", "dringt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "APPR", "ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Das man von neuem liest, und neue Sch\u00f6nheit findet,", "tokens": ["Das", "man", "von", "neu\u00b7em", "liest", ",", "und", "neu\u00b7e", "Sch\u00f6n\u00b7heit", "fin\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "APPR", "ADJA", "VVFIN", "$,", "KON", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Und den zu reichen Schatz stets gr\u00e4bet, nie ergr\u00fcndet", "tokens": ["Und", "den", "zu", "rei\u00b7chen", "Schatz", "stets", "gr\u00e4\u00b7bet", ",", "nie", "er\u00b7gr\u00fcn\u00b7det"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "ART", "PTKZU", "ADJA", "NN", "ADV", "VVFIN", "$,", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Wie, wenn durch unrein Pech das Feuer lodernd dringt,", "tokens": ["Wie", ",", "wenn", "durch", "un\u00b7rein", "Pech", "das", "Feu\u00b7er", "lo\u00b7dernd", "dringt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "KOUS", "APPR", "PPOSAT", "NN", "ART", "NN", "VVPP", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der Flamme schwaches Licht in dicken Dampf versinkt:", "tokens": ["Der", "Flam\u00b7me", "schwa\u00b7ches", "Licht", "in", "di\u00b7cken", "Dampf", "ver\u00b7sinkt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wird Der, den Stolz und Wahn f\u00fcr gr\u00fcndlich Wissen f\u00fcllen,", "tokens": ["Wird", "Der", ",", "den", "Stolz", "und", "Wahn", "f\u00fcr", "gr\u00fcnd\u00b7lich", "Wis\u00b7sen", "f\u00fcl\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "$,", "ART", "NN", "KON", "NN", "APPR", "ADJD", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Oft den gemeinsten Satz in dunkle Pracht verh\u00fcllen.", "tokens": ["Oft", "den", "ge\u00b7meins\u00b7ten", "Satz", "in", "dunk\u00b7le", "Pracht", "ver\u00b7h\u00fcl\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch wie wenn heitre Gluth aus wei\u00dfem Wachse strahlt,", "tokens": ["Doch", "wie", "wenn", "heit\u00b7re", "Gluth", "aus", "wei\u00b7\u00dfem", "Wach\u00b7se", "strahlt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOKOM", "KOUS", "ADJA", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Sich deutlich und belebt das Bild im Auge malt:", "tokens": ["Sich", "deut\u00b7lich", "und", "be\u00b7lebt", "das", "Bild", "im", "Au\u00b7ge", "malt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "KON", "VVFIN", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Wird des Gelehrten Werk mit Deutlichkeit erg\u00f6tzen;", "tokens": ["Wird", "des", "Ge\u00b7lehr\u00b7ten", "Werk", "mit", "Deut\u00b7lich\u00b7keit", "er\u00b7g\u00f6t\u00b7zen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "NN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Nur Augen bl\u00f6der Art kann selbst sein Glanz verletzen.", "tokens": ["Nur", "Au\u00b7gen", "bl\u00f6\u00b7der", "Art", "kann", "selbst", "sein", "Glanz", "ver\u00b7let\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "ADJA", "NN", "VMFIN", "ADV", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Ein Sch\u00fcler, der bereits das Octaedrum kennt,", "tokens": ["Ein", "Sch\u00fc\u00b7ler", ",", "der", "be\u00b7reits", "das", "Oc\u00b7taed\u00b7rum", "kennt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Des Zirkels Umfang mi\u00dft, die Logarithmen nennt,", "tokens": ["Des", "Zir\u00b7kels", "Um\u00b7fang", "mi\u00dft", ",", "die", "Lo\u00b7ga\u00b7rith\u00b7men", "nennt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Erblickt des Briten Werk, das alle Weisen ehren,", "tokens": ["Er\u00b7blickt", "des", "Bri\u00b7ten", "Werk", ",", "das", "al\u00b7le", "Wei\u00b7sen", "eh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "NN", "$,", "PRELS", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Er liest, versteht es nicht, schm\u00e4ht ", "tokens": ["Er", "liest", ",", "ver\u00b7steht", "es", "nicht", ",", "schm\u00e4ht"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "VVFIN", "PPER", "PTKNEG", "$,", "VVFIN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Ein M\u00e4dchen, die den Werth der Hochzeitlieder sch\u00e4tzt,", "tokens": ["Ein", "M\u00e4d\u00b7chen", ",", "die", "den", "Werth", "der", "Hoch\u00b7zeit\u00b7lie\u00b7der", "sch\u00e4tzt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die, (so gelehrt ist sie!) selbst ", "tokens": ["Die", ",", "(", "so", "ge\u00b7lehrt", "ist", "sie", "!", ")", "selbst"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word"], "pos": ["ART", "$,", "$(", "ADV", "VVPP", "VAFIN", "PPER", "$.", "$(", "ADV"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.7": {"text": "Will ", "tokens": ["Will"], "token_info": ["word"], "pos": ["VMFIN"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Sie liest, versteht es nicht, schm\u00e4ht ", "tokens": ["Sie", "liest", ",", "ver\u00b7steht", "es", "nicht", ",", "schm\u00e4ht"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "VVFIN", "PPER", "PTKNEG", "$,", "VVFIN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.9": {"text": "Mit Rechte schm\u00e4hte sie, br\u00e4cht', um ihr Herz bem\u00fcht,", "tokens": ["Mit", "Rech\u00b7te", "schm\u00e4h\u00b7te", "sie", ",", "br\u00e4cht'", ",", "um", "ihr", "Herz", "be\u00b7m\u00fcht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "$,", "VVFIN", "$,", "KOUI", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.10": {"text": "Der Stutzer, den sie liebt, ihr ein so schweres Lied:", "tokens": ["Der", "Stut\u00b7zer", ",", "den", "sie", "liebt", ",", "ihr", "ein", "so", "schwe\u00b7res", "Lied", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,", "PPER", "ART", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Was schilt sie ", "tokens": ["Was", "schilt", "sie"], "token_info": ["word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER"], "meter": "-+-", "measure": "amphibrach.single"}, "line.12": {"text": "Das, st\u00e4rker an Vernunft, des Liedes Reiz durchdrungen?", "tokens": ["Das", ",", "st\u00e4r\u00b7ker", "an", "Ver\u00b7nunft", ",", "des", "Lie\u00b7des", "Reiz", "durch\u00b7drun\u00b7gen", "?"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$,", "ADJD", "APPR", "NN", "$,", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Der Leser, dem man schreibt, bestimmt des Autors Pflicht:", "tokens": ["Der", "Le\u00b7ser", ",", "dem", "man", "schreibt", ",", "be\u00b7stimmt", "des", "Au\u00b7tors", "Pflicht", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PIS", "VVFIN", "$,", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wenn ", "tokens": ["Wenn"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Sagt, was den Dichter zwingt, nur Lesern ohne Denken,", "tokens": ["Sagt", ",", "was", "den", "Dich\u00b7ter", "zwingt", ",", "nur", "Le\u00b7sern", "oh\u00b7ne", "Den\u00b7ken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PRELS", "ART", "NN", "VVFIN", "$,", "ADV", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Ein Lied, das h\u00f6her strebt, best\u00e4ndig zuzusenken?", "tokens": ["Ein", "Lied", ",", "das", "h\u00f6\u00b7her", "strebt", ",", "be\u00b7st\u00e4n\u00b7dig", "zu\u00b7zu\u00b7sen\u00b7ken", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADJD", "VVFIN", "$,", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ungl\u00fccklich, wenn ihn nur die Dichtergluth entflammt,", "tokens": ["Un\u00b7gl\u00fcck\u00b7lich", ",", "wenn", "ihn", "nur", "die", "Dich\u00b7ter\u00b7gluth", "ent\u00b7flammt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "KOUS", "PPER", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Da\u00df ihn ein harter Spruch zum P\u00f6bel hin verdammt!", "tokens": ["Da\u00df", "ihn", "ein", "har\u00b7ter", "Spruch", "zum", "P\u00f6\u00b7bel", "hin", "ver\u00b7dammt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPRART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Ja, spricht man: denn es soll der Dichtkunst weise Lehren,", "tokens": ["Ja", ",", "spricht", "man", ":", "denn", "es", "soll", "der", "Dicht\u00b7kunst", "wei\u00b7se", "Leh\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "VVFIN", "PIS", "$.", "KON", "PPER", "VMFIN", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Zu seiner Besserung der Ungelehrte h\u00f6ren:", "tokens": ["Zu", "sei\u00b7ner", "Bes\u00b7se\u00b7rung", "der", "Un\u00b7ge\u00b7lehr\u00b7te", "h\u00f6\u00b7ren", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So hat, da ", "tokens": ["So", "hat", ",", "da"], "token_info": ["word", "word", "punct", "word"], "pos": ["ADV", "VAFIN", "$,", "KOUS"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Der Menschheit ersten Trieb der rohe Mensch gef\u00fchlt;", "tokens": ["Der", "Menschheit", "ers\u00b7ten", "Trieb", "der", "ro\u00b7he", "Mensch", "ge\u00b7f\u00fchlt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "--+-+-+-+-+", "measure": "anapaest.init"}, "line.5": {"text": "So hat den Deutschen einst des Barden Lied erhitzet,", "tokens": ["So", "hat", "den", "Deut\u00b7schen", "einst", "des", "Bar\u00b7den", "Lied", "er\u00b7hit\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn auf der Freyheit Feind sein siegreich Schwerdt geblitzet.", "tokens": ["Wenn", "auf", "der", "Frey\u00b7heit", "Feind", "sein", "sieg\u00b7reich", "Schwerdt", "ge\u00b7blit\u00b7zet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "NN", "PPOSAT", "ADJD", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Doch wie? verstand ein Geist, im Denken unbem\u00fcht,", "tokens": ["Doch", "wie", "?", "ver\u00b7stand", "ein", "Geist", ",", "im", "Den\u00b7ken", "un\u00b7be\u00b7m\u00fcht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "$.", "VVFIN", "ART", "NN", "$,", "APPRART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In allem Wissen fremd, der ersten Dichter Lied?", "tokens": ["In", "al\u00b7lem", "Wis\u00b7sen", "fremd", ",", "der", "ers\u00b7ten", "Dich\u00b7ter", "Lied", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "NN", "ADJD", "$,", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Den Bau der gro\u00dfen Welt, das g\u00f6ttliche Geschlechte,", "tokens": ["Den", "Bau", "der", "gro\u00b7\u00dfen", "Welt", ",", "das", "g\u00f6tt\u00b7li\u00b7che", "Ge\u00b7schlech\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Die Wunder alter Zeit, der Menschen Amt und Rechte,", "tokens": ["Die", "Wun\u00b7der", "al\u00b7ter", "Zeit", ",", "der", "Men\u00b7schen", "Amt", "und", "Rech\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,", "ART", "NN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Dies hat die erste Welt von ihnen angeh\u00f6rt.", "tokens": ["Dies", "hat", "die", "ers\u00b7te", "Welt", "von", "ih\u00b7nen", "an\u00b7ge\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wer lernt jetzt halb so viel, und d\u00fcnkt sich nicht gelehrt?", "tokens": ["Wer", "lernt", "jetzt", "halb", "so", "viel", ",", "und", "d\u00fcnkt", "sich", "nicht", "ge\u00b7lehrt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ADJD", "ADV", "ADV", "$,", "KON", "VVFIN", "PRF", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Doch, Dichter, prahlet nur mit eurer Ahnen Thaten,", "tokens": ["Doch", ",", "Dich\u00b7ter", ",", "prah\u00b7let", "nur", "mit", "eu\u00b7rer", "Ah\u00b7nen", "Tha\u00b7ten", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "NN", "$,", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "La\u00dft Wahn und Eitelkeit erfinden, nicht errathen;", "tokens": ["La\u00dft", "Wahn", "und", "Ei\u00b7tel\u00b7keit", "er\u00b7fin\u00b7den", ",", "nicht", "er\u00b7ra\u00b7then", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVIMP", "NN", "KON", "NN", "VVINF", "$,", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Sagt, was ihr Lied vollbracht, und was es nie vollbracht,", "tokens": ["Sagt", ",", "was", "ihr", "Lied", "voll\u00b7bracht", ",", "und", "was", "es", "nie", "voll\u00b7bracht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,", "KON", "PWS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und was Vernunft nur kann, das sucht in seiner Macht.", "tokens": ["Und", "was", "Ver\u00b7nunft", "nur", "kann", ",", "das", "sucht", "in", "sei\u00b7ner", "Macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "NN", "ADV", "VMFIN", "$,", "PDS", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Es sey, da\u00df ", "tokens": ["Es", "sey", ",", "da\u00df"], "token_info": ["word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "$,", "KOUS"], "meter": "-+-", "measure": "amphibrach.single"}, "line.6": {"text": "Ward denn durch jedes Lied der Tugend Reich vergr\u00f6\u00dfert?", "tokens": ["Ward", "denn", "durch", "je\u00b7des", "Lied", "der", "Tu\u00b7gend", "Reich", "ver\u00b7gr\u00f6\u00b7\u00dfert", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "PIAT", "NN", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Erregte ", "tokens": ["Er\u00b7reg\u00b7te"], "token_info": ["word"], "pos": ["NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.8": {"text": "Empfand nicht mancher Thor oft ihren Witz und Muth?", "tokens": ["Emp\u00b7fand", "nicht", "man\u00b7cher", "Thor", "oft", "ih\u00b7ren", "Witz", "und", "Muth", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "PIAT", "NN", "ADV", "PPOSAT", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Und konnt', eh' ", "tokens": ["Und", "konnt'", ",", "eh'"], "token_info": ["word", "word", "punct", "word"], "pos": ["KON", "VMFIN", "$,", "KOUS"], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Kein Lied der Sch\u00e4ferinn die Spr\u00f6digkeit benehmen?", "tokens": ["Kein", "Lied", "der", "Sch\u00e4\u00b7fe\u00b7rinn", "die", "Spr\u00f6\u00b7dig\u00b7keit", "be\u00b7neh\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Vor Zeiten gab ", "tokens": ["Vor", "Zei\u00b7ten", "gab"], "token_info": ["word", "word", "word"], "pos": ["APPR", "NN", "VVFIN"], "meter": "-+-+", "measure": "iambic.di"}, "line.12": {"text": "Doch ein ", "tokens": ["Doch", "ein"], "token_info": ["word", "word"], "pos": ["KON", "ART"], "meter": "-+", "measure": "iambic.single"}, "line.13": {"text": "Gleich neben dem ", "tokens": ["Gleich", "ne\u00b7ben", "dem"], "token_info": ["word", "word", "word"], "pos": ["ADV", "APPR", "ART"], "meter": "+-+-", "measure": "trochaic.di"}, "line.14": {"text": "Der lehrt den ", "tokens": ["Der", "lehrt", "den"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.15": {"text": "Wenn aber ja dein Werk so Manche lehren soll:", "tokens": ["Wenn", "a\u00b7ber", "ja", "dein", "Werk", "so", "Man\u00b7che", "leh\u00b7ren", "soll", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "PPOSAT", "NN", "ADV", "PIS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "So sey es auch zugleich vom Reiz f\u00fcr Weise voll,", "tokens": ["So", "sey", "es", "auch", "zu\u00b7gleich", "vom", "Reiz", "f\u00fcr", "Wei\u00b7se", "voll", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "APPRART", "NN", "APPR", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "So wie ein Altarblatt, mit kunsterf\u00fcllten Z\u00fcgen,", "tokens": ["So", "wie", "ein", "Al\u00b7tar\u00b7blatt", ",", "mit", "kuns\u00b7ter\u00b7f\u00fcll\u00b7ten", "Z\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ART", "NN", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Des Volkes Andacht mehrt, und Kenner kann vergn\u00fcgen,", "tokens": ["Des", "Vol\u00b7kes", "An\u00b7dacht", "mehrt", ",", "und", "Ken\u00b7ner", "kann", "ver\u00b7gn\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "$,", "KON", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Kein bunt Marienbild, vom Holzschnitt abgedr\u00fcckt,", "tokens": ["Kein", "bunt", "Ma\u00b7ri\u00b7en\u00b7bild", ",", "vom", "Holz\u00b7schnitt", "ab\u00b7ge\u00b7dr\u00fcckt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJD", "NN", "$,", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Das Kinder nur erg\u00f6tzt, und Bauerstuben schm\u00fcckt.", "tokens": ["Das", "Kin\u00b7der", "nur", "er\u00b7g\u00f6tzt", ",", "und", "Bau\u00b7er\u00b7stu\u00b7ben", "schm\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVPP", "$,", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Es folgt nicht, da\u00df kein Lied mit Nutzen Lust verbindet,", "tokens": ["Es", "folgt", "nicht", ",", "da\u00df", "kein", "Lied", "mit", "Nut\u00b7zen", "Lust", "ver\u00b7bin\u00b7det", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "KOUS", "PIAT", "NN", "APPR", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Wo der gemeinste Geist nicht jeden Satz empfindet;", "tokens": ["Wo", "der", "ge\u00b7meins\u00b7te", "Geist", "nicht", "je\u00b7den", "Satz", "emp\u00b7fin\u00b7det", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJA", "NN", "PTKNEG", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Genug, trifft er f\u00fcr sich da gute Lehren an,", "tokens": ["Ge\u00b7nug", ",", "trifft", "er", "f\u00fcr", "sich", "da", "gu\u00b7te", "Leh\u00b7ren", "an", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "VVFIN", "PPER", "APPR", "PRF", "ADV", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Wo Manches ihm zu hoch, Gelehrte r\u00fchren kann.", "tokens": ["Wo", "Man\u00b7ches", "ihm", "zu", "hoch", ",", "Ge\u00b7lehr\u00b7te", "r\u00fch\u00b7ren", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "PPER", "PTKA", "ADJD", "$,", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Doch Niemand zieht vielleicht den Dichter ganz zur Erden:", "tokens": ["Doch", "Nie\u00b7mand", "zieht", "viel\u00b7leicht", "den", "Dich\u00b7ter", "ganz", "zur", "Er\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADV", "ART", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Er soll kein Lehrer nicht des schlechten P\u00f6bels werden;", "tokens": ["Er", "soll", "kein", "Leh\u00b7rer", "nicht", "des", "schlech\u00b7ten", "P\u00f6\u00b7bels", "wer\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIAT", "NN", "PTKNEG", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Man will nicht, da\u00df sein Lied ein Weib zum Weinen zwingt,", "tokens": ["Man", "will", "nicht", ",", "da\u00df", "sein", "Lied", "ein", "Weib", "zum", "Wei\u00b7nen", "zwingt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PTKNEG", "$,", "KOUS", "PPOSAT", "NN", "ART", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wenn es am Petersthor ein deutscher ", "tokens": ["Wenn", "es", "am", "Pe\u00b7ter\u00b7sthor", "ein", "deut\u00b7scher"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPRART", "NN", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Genug, bem\u00fcht er sich, f\u00fcr einen Sinn zu spielen,", "tokens": ["Ge\u00b7nug", ",", "be\u00b7m\u00fcht", "er", "sich", ",", "f\u00fcr", "ei\u00b7nen", "Sinn", "zu", "spie\u00b7len", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "VVFIN", "PPER", "PRF", "$,", "APPR", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der richtig denken kann, und z\u00e4rtlich wei\u00df zu f\u00fchlen.", "tokens": ["Der", "rich\u00b7tig", "den\u00b7ken", "kann", ",", "und", "z\u00e4rt\u00b7lich", "wei\u00df", "zu", "f\u00fch\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVINF", "VMFIN", "$,", "KON", "ADJD", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Gut, doch wofern ihr nur f\u00fcr solche Seelen schreibt,", "tokens": ["Gut", ",", "doch", "wo\u00b7fern", "ihr", "nur", "f\u00fcr", "sol\u00b7che", "See\u00b7len", "schreibt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "ADV", "KOUS", "PPER", "ADV", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Sagt, wo der Dichtkunst Zweck, das Unterrichten, bleibt?", "tokens": ["Sagt", ",", "wo", "der", "Dicht\u00b7kunst", "Zweck", ",", "das", "Un\u00b7ter\u00b7rich\u00b7ten", ",", "bleibt", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "PWAV", "ART", "NN", "NN", "$,", "ART", "NN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Was braucht's, da\u00df sie von euch die Lebensregeln h\u00f6ren,", "tokens": ["Was", "braucht's", ",", "da\u00df", "sie", "von", "euch", "die", "Le\u00b7bens\u00b7re\u00b7geln", "h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "PPER", "APPR", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Die ihnen eigner Witz, Flei\u00df und Erziehung lehren?", "tokens": ["Die", "ih\u00b7nen", "eig\u00b7ner", "Witz", ",", "Flei\u00df", "und", "Er\u00b7zie\u00b7hung", "leh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADJA", "NN", "$,", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-++--+-+-", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "Als nur, damit ein Satz, den eure Kunst geschm\u00fcckt,", "tokens": ["Als", "nur", ",", "da\u00b7mit", "ein", "Satz", ",", "den", "eu\u00b7re", "Kunst", "ge\u00b7schm\u00fcckt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "$,", "KOUS", "ART", "NN", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Zwar den Verstand nicht lehrt, doch in das Herz sich dr\u00fcckt.", "tokens": ["Zwar", "den", "Ver\u00b7stand", "nicht", "lehrt", ",", "doch", "in", "das", "Herz", "sich", "dr\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PTKNEG", "VVFIN", "$,", "ADV", "APPR", "ART", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Auch das geb' ich euch zu, doch selbst aus diesen Seelen,", "tokens": ["Auch", "das", "geb'", "ich", "euch", "zu", ",", "doch", "selbst", "aus", "die\u00b7sen", "See\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VVFIN", "PPER", "PPER", "PTKVZ", "$,", "ADV", "ADV", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die ihr vom P\u00f6bel trennt, werd' ich von neuem w\u00e4hlen.", "tokens": ["Die", "ihr", "vom", "P\u00f6\u00b7bel", "trennt", ",", "werd'", "ich", "von", "neu\u00b7em", "w\u00e4h\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPRART", "NN", "VVFIN", "$,", "VAFIN", "PPER", "APPR", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "D\u00fcrft ihr nicht euren Vers gleich jedem B\u00fcrger weyhn:", "tokens": ["D\u00fcrft", "ihr", "nicht", "eu\u00b7ren", "Vers", "gleich", "je\u00b7dem", "B\u00fcr\u00b7ger", "weyhn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "PPOSAT", "NN", "ADV", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "So darf auch meiner nicht f\u00fcr jeden Leser seyn.", "tokens": ["So", "darf", "auch", "mei\u00b7ner", "nicht", "f\u00fcr", "je\u00b7den", "Le\u00b7ser", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ADV", "PPOSAT", "PTKNEG", "APPR", "PIAT", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ihr irrt, wofern ihr glaubt, frey von gelehrten S\u00e4tzen", "tokens": ["Ihr", "irrt", ",", "wo\u00b7fern", "ihr", "glaubt", ",", "frey", "von", "ge\u00b7lehr\u00b7ten", "S\u00e4t\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$,", "ADJD", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Werd' eure Deutlichkeit auch Alle gleich erg\u00f6tzen.", "tokens": ["Werd'", "eu\u00b7re", "Deut\u00b7lich\u00b7keit", "auch", "Al\u00b7le", "gleich", "er\u00b7g\u00f6t\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "F\u00fcr Leser mancher Art sind ", "tokens": ["F\u00fcr", "Le\u00b7ser", "man\u00b7cher", "Art", "sind"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "PIAT", "NN", "VAFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "F\u00fcr Leonoren das, und das f\u00fcr den Eugen.", "tokens": ["F\u00fcr", "Le\u00b7o\u00b7no\u00b7ren", "das", ",", "und", "das", "f\u00fcr", "den", "Eu\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PDS", "$,", "KON", "PDS", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Ihr straft es, wenn man singt, nur Weise zu vergn\u00fcgen:", "tokens": ["Ihr", "straft", "es", ",", "wenn", "man", "singt", ",", "nur", "Wei\u00b7se", "zu", "ver\u00b7gn\u00fc\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KOUS", "PIS", "VVFIN", "$,", "ADV", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "So straft auch, wenn man singt, nur Sch\u00f6nen zu besiegen.", "tokens": ["So", "straft", "auch", ",", "wenn", "man", "singt", ",", "nur", "Sch\u00f6\u00b7nen", "zu", "be\u00b7sie\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "$,", "KOUS", "PIS", "VVFIN", "$,", "ADV", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Euch mi\u00dff\u00e4llt, wenn mein Vers von ", "tokens": ["Euch", "mi\u00df\u00b7f\u00e4llt", ",", "wenn", "mein", "Vers", "von"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPOSAT", "NN", "APPR"], "meter": "+-+-++-", "measure": "unknown.measure.tetra"}, "line.12": {"text": "So braucht im Trauerspiel Geschicht' und Fabel nicht.", "tokens": ["So", "braucht", "im", "Trau\u00b7er\u00b7spiel", "Ge\u00b7schicht'", "und", "Fa\u00b7bel", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "NN", "KON", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Soll ein Gelehrter nur vor euren Schauplatz gehen:", "tokens": ["Soll", "ein", "Ge\u00b7lehr\u00b7ter", "nur", "vor", "eu\u00b7ren", "Schau\u00b7platz", "ge\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "So sey auch Der gelehrt, der will mein Lied verstehen.", "tokens": ["So", "sey", "auch", "Der", "ge\u00b7lehrt", ",", "der", "will", "mein", "Lied", "ver\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ART", "VVPP", "$,", "PRELS", "VMFIN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Den Leser w\u00e4hl' ich mir; sagt, ob ich strafbar bin?", "tokens": ["Den", "Le\u00b7ser", "w\u00e4hl'", "ich", "mir", ";", "sagt", ",", "ob", "ich", "straf\u00b7bar", "bin", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PPER", "$.", "VVFIN", "$,", "KOUS", "PPER", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Hat jeder eurer Zunft doch gleichen Eigensinn.", "tokens": ["Hat", "je\u00b7der", "eu\u00b7rer", "Zunft", "doch", "glei\u00b7chen", "Ei\u00b7gen\u00b7sinn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PPOSAT", "NN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Der, der die Sch\u00e4ferinn mit Lied und Einfalt zieret,", "tokens": ["Der", ",", "der", "die", "Sch\u00e4\u00b7fe\u00b7rinn", "mit", "Lied", "und", "Ein\u00b7falt", "zie\u00b7ret", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PRELS", "ART", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Was fragt er, ob ihr Bild den Philosophen r\u00fchret?", "tokens": ["Was", "fragt", "er", ",", "ob", "ihr", "Bild", "den", "Phi\u00b7lo\u00b7so\u00b7phen", "r\u00fch\u00b7ret", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$,", "KOUS", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Ein Andrer singt entz\u00fcckt von seiner Chloris Ku\u00df:", "tokens": ["Ein", "A\u00b7ndrer", "singt", "ent\u00b7z\u00fcckt", "von", "sei\u00b7ner", "Chlo\u00b7ris", "Ku\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "VVPP", "APPR", "PPOSAT", "NE", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Ich bin nicht so entz\u00fcckt, und les ihn mit Verdru\u00df.", "tokens": ["Ich", "bin", "nicht", "so", "ent\u00b7z\u00fcckt", ",", "und", "les", "ihn", "mit", "Ver\u00b7dru\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADV", "VVPP", "$,", "KON", "PIS", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Wie, soll der Dichter stets sich Stutzern \u00e4hnlich zeigen?", "tokens": ["Wie", ",", "soll", "der", "Dich\u00b7ter", "stets", "sich", "Stut\u00b7zern", "\u00e4hn\u00b7lich", "zei\u00b7gen", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "VMFIN", "ART", "NN", "VVFIN", "PRF", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Bey M\u00e4dchen witzig seyn, bey Klugen aber schweigen?", "tokens": ["Bey", "M\u00e4d\u00b7chen", "wit\u00b7zig", "seyn", ",", "bey", "Klu\u00b7gen", "a\u00b7ber", "schwei\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "VAINF", "$,", "APPR", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Doch geh' ich nicht zu weit? Wer ist es, der es schilt,", "tokens": ["Doch", "geh'", "ich", "nicht", "zu", "weit", "?", "Wer", "ist", "es", ",", "der", "es", "schilt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "PTKA", "ADJD", "$.", "PWS", "VAFIN", "PPER", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wenn Kunst und Wissenschaft erhabne Lieder f\u00fcllt?", "tokens": ["Wenn", "Kunst", "und", "Wis\u00b7sen\u00b7schaft", "er\u00b7hab\u00b7ne", "Lie\u00b7der", "f\u00fcllt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nur das verbietet man, da\u00df tiefer S\u00e4tze Menge,", "tokens": ["Nur", "das", "ver\u00b7bie\u00b7tet", "man", ",", "da\u00df", "tie\u00b7fer", "S\u00e4t\u00b7ze", "Men\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VVFIN", "PIS", "$,", "KOUS", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Zu dunkel ausgedr\u00fcckt, im schweren Vers sich dr\u00e4nge.", "tokens": ["Zu", "dun\u00b7kel", "aus\u00b7ge\u00b7dr\u00fcckt", ",", "im", "schwe\u00b7ren", "Vers", "sich", "dr\u00e4n\u00b7ge", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VVPP", "$,", "APPRART", "ADJA", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Gut, theilt den Einfall gleich in zwanzig Zeilen ein;", "tokens": ["Gut", ",", "theilt", "den", "Ein\u00b7fall", "gleich", "in", "zwan\u00b7zig", "Zei\u00b7len", "ein", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "VVFIN", "ART", "NN", "ADV", "APPR", "CARD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Nur merkt, ihr werdet matt, und doch nicht deutlich seyn.", "tokens": ["Nur", "merkt", ",", "ihr", "wer\u00b7det", "matt", ",", "und", "doch", "nicht", "deut\u00b7lich", "seyn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "PPER", "VAFIN", "ADJD", "$,", "KON", "ADV", "PTKNEG", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Vergebens, da\u00df man Dem, dem alle Kenntni\u00df fehlet,", "tokens": ["Ver\u00b7ge\u00b7bens", ",", "da\u00df", "man", "Dem", ",", "dem", "al\u00b7le", "Kennt\u00b7ni\u00df", "feh\u00b7let", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PIS", "PDS", "$,", "PRELS", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "So, wie ein Lehrer thut, Satz und Beweis erz\u00e4hlet.", "tokens": ["So", ",", "wie", "ein", "Leh\u00b7rer", "thut", ",", "Satz", "und", "Be\u00b7weis", "er\u00b7z\u00e4h\u00b7let", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ART", "NN", "VVFIN", "$,", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Nicht Alles fa\u00dft der Vers; und wenn er Alles fa\u00dft,", "tokens": ["Nicht", "Al\u00b7les", "fa\u00dft", "der", "Vers", ";", "und", "wenn", "er", "Al\u00b7les", "fa\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIS", "VVFIN", "ART", "NN", "$.", "KON", "KOUS", "PPER", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "So wird die Deutlichkeit dem Leser selbst zur Last.", "tokens": ["So", "wird", "die", "Deut\u00b7lich\u00b7keit", "dem", "Le\u00b7ser", "selbst", "zur", "Last", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ART", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Mit ekelem Geschw\u00e4tz wird uns der Dichter plagen,", "tokens": ["Mit", "e\u00b7ke\u00b7lem", "Ge\u00b7schw\u00e4tz", "wird", "uns", "der", "Dich\u00b7ter", "pla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Der uns nichts denken l\u00e4\u00dft, und Alles strebt zu sagen;", "tokens": ["Der", "uns", "nichts", "den\u00b7ken", "l\u00e4\u00dft", ",", "und", "Al\u00b7les", "strebt", "zu", "sa\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PIS", "VVINF", "VVFIN", "$,", "KON", "PIS", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Doch ist ein Mittel hier: auch Der gef\u00e4llt uns nicht,", "tokens": ["Doch", "ist", "ein", "Mit\u00b7tel", "hier", ":", "auch", "Der", "ge\u00b7f\u00e4llt", "uns", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "ADV", "$.", "ADV", "ART", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Der nicht genug uns sagt, und wie Orakel spricht.", "tokens": ["Der", "nicht", "ge\u00b7nug", "uns", "sagt", ",", "und", "wie", "O\u00b7ra\u00b7kel", "spricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "ADV", "PPER", "VVFIN", "$,", "KON", "PWAV", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Die freche Buhlerinn, die mehr giebt, als verg\u00f6nnet,", "tokens": ["Die", "fre\u00b7che", "Buh\u00b7le\u00b7rinn", ",", "die", "mehr", "giebt", ",", "als", "ver\u00b7g\u00f6n\u00b7net", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "ADV", "VVFIN", "$,", "KOUS", "VVPP", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Die Spr\u00f6de, die uns kaum mit halbem Blicke kennet,", "tokens": ["Die", "Spr\u00f6\u00b7de", ",", "die", "uns", "kaum", "mit", "hal\u00b7bem", "Bli\u00b7cke", "ken\u00b7net", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Sind beyde reizungsleer", "tokens": ["Sind", "bey\u00b7de", "rei\u00b7zungs\u00b7leer"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PIAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.18": {"text": "Wo muntre Sittsamkeit bem\u00fcht und auch vergn\u00fcgt.", "tokens": ["Wo", "mun\u00b7tre", "Sitt\u00b7sam\u00b7keit", "be\u00b7m\u00fcht", "und", "auch", "ver\u00b7gn\u00fcgt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "VVPP", "KON", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Doch hei\u00dft die Sch\u00f6ne nicht durch eitlen Stolz verblendet,", "tokens": ["Doch", "hei\u00dft", "die", "Sch\u00f6\u00b7ne", "nicht", "durch", "eit\u00b7len", "Stolz", "ver\u00b7blen\u00b7det", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKNEG", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Die Jedem unverdient nicht ihre Gunst verschwendet:", "tokens": ["Die", "Je\u00b7dem", "un\u00b7ver\u00b7di\u00b7ent", "nicht", "ih\u00b7re", "Gunst", "ver\u00b7schwen\u00b7det", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "PTKNEG", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.21": {"text": "So wi\u00dft auch, da\u00df ihr oft ein Lied als dunkel schm\u00e4ht,", "tokens": ["So", "wi\u00dft", "auch", ",", "da\u00df", "ihr", "oft", "ein", "Lied", "als", "dun\u00b7kel", "schm\u00e4ht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "$,", "KOUS", "PPER", "ADV", "ART", "NN", "KOKOM", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Und denket nicht daran, da\u00df ihr nur bl\u00f6de seht.", "tokens": ["Und", "den\u00b7ket", "nicht", "da\u00b7ran", ",", "da\u00df", "ihr", "nur", "bl\u00f6\u00b7de", "seht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "PAV", "$,", "KOUS", "PPER", "ADV", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Ihr sprecht, das sey nicht Lust, was uns mit Denken qu\u00e4let,", "tokens": ["Ihr", "sprecht", ",", "das", "sey", "nicht", "Lust", ",", "was", "uns", "mit", "Den\u00b7ken", "qu\u00e4\u00b7let", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PDS", "VAFIN", "PTKNEG", "NN", "$,", "PRELS", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "So h\u00f6rt zu guter letzt noch was mein Vers erz\u00e4hlet.", "tokens": ["So", "h\u00f6rt", "zu", "gu\u00b7ter", "letzt", "noch", "was", "mein", "Vers", "er\u00b7z\u00e4h\u00b7let", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "ADJA", "ADV", "ADV", "PWS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Sonst, als den Deutschen noch kein seiner Witz vergn\u00fcgt,", "tokens": ["Sonst", ",", "als", "den", "Deut\u00b7schen", "noch", "kein", "sei\u00b7ner", "Witz", "ver\u00b7gn\u00fcgt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "ADV", "PIAT", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und nur sein redlich Herz mit tapfrer Faust gesiegt,", "tokens": ["Und", "nur", "sein", "red\u00b7lich", "Herz", "mit", "tapf\u00b7rer", "Faust", "ge\u00b7siegt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPOSAT", "ADJD", "NN", "APPR", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Gebraucht' er sich, die Zeit erg\u00f6tzend zu verlieren,", "tokens": ["Ge\u00b7braucht'", "er", "sich", ",", "die", "Zeit", "er\u00b7g\u00f6t\u00b7zend", "zu", "ver\u00b7lie\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "$,", "ART", "NN", "VVPP", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der bunten Heere schon, von streitenden Papieren.", "tokens": ["Der", "bun\u00b7ten", "Hee\u00b7re", "schon", ",", "von", "strei\u00b7ten\u00b7den", "Pa\u00b7pie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "$,", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Vier gleiche Haufen sinds. Des Schicksals Eigensinn", "tokens": ["Vier", "glei\u00b7che", "Hau\u00b7fen", "sinds", ".", "Des", "Schick\u00b7sals", "Ei\u00b7gen\u00b7sinn"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["CARD", "ADJA", "NN", "VAFIN", "$.", "ART", "NN", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Giebt einem M\u00e4chtigern der Andern Leben hin.", "tokens": ["Giebt", "ei\u00b7nem", "M\u00e4ch\u00b7ti\u00b7gern", "der", "An\u00b7dern", "Le\u00b7ben", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Vor seiner Sieben mu\u00df zu oft ein Taus erbleichen,", "tokens": ["Vor", "sei\u00b7ner", "Sie\u00b7ben", "mu\u00df", "zu", "oft", "ein", "Taus", "er\u00b7blei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VMFIN", "APPR", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Doch wird ihr bald darauf nicht eine Sieben weichen.", "tokens": ["Doch", "wird", "ihr", "bald", "da\u00b7rauf", "nicht", "ei\u00b7ne", "Sie\u00b7ben", "wei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "PAV", "PTKNEG", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Die Stunden k\u00fcrzten sich mit Spielen mancher Art;", "tokens": ["Die", "Stun\u00b7den", "k\u00fcrz\u00b7ten", "sich", "mit", "Spie\u00b7len", "man\u00b7cher", "Art", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "APPR", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Da hoffte man ein Gl\u00fcck, das F\u00fcrst und Ober paart", "tokens": ["Da", "hoff\u00b7te", "man", "ein", "Gl\u00fcck", ",", "das", "F\u00fcrst", "und", "O\u00b7ber", "paart"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "ART", "NN", "$,", "ART", "NN", "KON", "NN", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Auch ging man Wetten ein, wo stets die Hand verspielte,", "tokens": ["Auch", "ging", "man", "Wet\u00b7ten", "ein", ",", "wo", "stets", "die", "Hand", "ver\u00b7spiel\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "NN", "PTKVZ", "$,", "PWAV", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Die Bl\u00e4tter eines Rangs in mindrer Anzahl hielte,", "tokens": ["Die", "Bl\u00e4t\u00b7ter", "ei\u00b7nes", "Rangs", "in", "mind\u00b7rer", "An\u00b7zahl", "hiel\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Bot einem Spieler Trotz, der zu verwegen war,", "tokens": ["Bot", "ei\u00b7nem", "Spie\u00b7ler", "Trotz", ",", "der", "zu", "ver\u00b7we\u00b7gen", "war", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "NN", "$,", "PRELS", "PTKZU", "VVINF", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Und setzte sich ihm gleich in Hoffnung und Gefahr.", "tokens": ["Und", "setz\u00b7te", "sich", "ihm", "gleich", "in", "Hoff\u00b7nung", "und", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PPER", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Drauf, als die Rohigkeit von Deutschland sich entfernte,", "tokens": ["Drauf", ",", "als", "die", "Ro\u00b7hig\u00b7keit", "von", "Deutschland", "sich", "ent\u00b7fern\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "KOUS", "ART", "NN", "APPR", "NE", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Und man der Fremden Kunst und fremde Thorheit lernte,", "tokens": ["Und", "man", "der", "Frem\u00b7den", "Kunst", "und", "frem\u00b7de", "Thor\u00b7heit", "lern\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "ART", "NN", "NN", "KON", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Ward auch der Zeitvertreib, den Spanien erdacht,", "tokens": ["Ward", "auch", "der", "Zeit\u00b7ver\u00b7treib", ",", "den", "Spa\u00b7ni\u00b7en", "er\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "$,", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Und Frankreich ausgeputzt, bey uns bekannt gemacht.", "tokens": ["Und", "Fran\u00b7kreich", "aus\u00b7ge\u00b7putzt", ",", "bey", "uns", "be\u00b7kannt", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVPP", "$,", "APPR", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Dem Tutti wich der Martsch, der Sequens Matadoren;", "tokens": ["Dem", "Tut\u00b7ti", "wich", "der", "Martsch", ",", "der", "Se\u00b7quens", "Ma\u00b7ta\u00b7do\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Vom Spieler ward a Tout, kein Trumpf vom Gl\u00fcck erkoren.", "tokens": ["Vom", "Spie\u00b7ler", "ward", "a", "Tout", ",", "kein", "Trumpf", "vom", "Gl\u00fcck", "er\u00b7ko\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "NE", "NE", "$,", "PIAT", "NN", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Voll Ordnung war das Spiel, nur war sie mehr versteckt,", "tokens": ["Voll", "Ord\u00b7nung", "war", "das", "Spiel", ",", "nur", "war", "sie", "mehr", "ver\u00b7steckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "VAFIN", "ART", "NN", "$,", "ADV", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Voll Regeln, deren Zahl gemeine Seelen schreckt:", "tokens": ["Voll", "Re\u00b7geln", ",", "de\u00b7ren", "Zahl", "ge\u00b7mei\u00b7ne", "See\u00b7len", "schreckt", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "$,", "PRELAT", "NN", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Und wem zum Denken sonst Geduld und St\u00e4rke fehlte,", "tokens": ["Und", "wem", "zum", "Den\u00b7ken", "sonst", "Ge\u00b7duld", "und", "St\u00e4r\u00b7ke", "fehl\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPRART", "NN", "ADV", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Der ward ein ", "tokens": ["Der", "ward", "ein"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VAFIN", "ART"], "meter": "+-+", "measure": "trochaic.di"}, "line.25": {"text": "Wie h\u00f6rte man dabey die schw\u00e4chern Geister schreyn:", "tokens": ["Wie", "h\u00f6r\u00b7te", "man", "da\u00b7bey", "die", "schw\u00e4\u00b7chern", "Geis\u00b7ter", "schreyn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "PAV", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Was so viel M\u00fche macht, kann kein Vergn\u00fcgen seyn!", "tokens": ["Was", "so", "viel", "M\u00fc\u00b7he", "macht", ",", "kann", "kein", "Ver\u00b7gn\u00fc\u00b7gen", "seyn", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "PIAT", "NN", "VVFIN", "$,", "VMFIN", "PIAT", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Umsonst, die Sch\u00f6nen selbst gew\u00f6hnten sich zu denken,", "tokens": ["Um\u00b7sonst", ",", "die", "Sch\u00f6\u00b7nen", "selbst", "ge\u00b7w\u00f6hn\u00b7ten", "sich", "zu", "den\u00b7ken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ART", "NN", "ADV", "VVFIN", "PRF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Der Wenzel und das Taus flohn endlich in die Schenken.", "tokens": ["Der", "Wen\u00b7zel", "und", "das", "Taus", "flohn", "end\u00b7lich", "in", "die", "Schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}