{"textgrid.poem.56164": {"metadata": {"author": {"name": "Rilke, Rainer Maria", "birth": "N.A.", "death": "N.A."}, "title": "23.", "genre": "verse", "period": "N.A.", "pub_year": 1900, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Rufe mich zu jener deiner Stunden,", "tokens": ["Ru\u00b7fe", "mich", "zu", "je\u00b7ner", "dei\u00b7ner", "Stun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PDAT", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "die dir unaufh\u00f6rlich widersteht:", "tokens": ["die", "dir", "un\u00b7auf\u00b7h\u00f6r\u00b7lich", "wi\u00b7der\u00b7steht", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "flehend nah wie das Gesicht von Hunden,", "tokens": ["fle\u00b7hend", "nah", "wie", "das", "Ge\u00b7sicht", "von", "Hun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "KOKOM", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "aber immer wieder weggedreht,", "tokens": ["a\u00b7ber", "im\u00b7mer", "wie\u00b7der", "weg\u00b7ge\u00b7dreht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.2": {"line.1": {"text": "wenn du meinst, sie endlich zu erfassen.", "tokens": ["wenn", "du", "meinst", ",", "sie", "end\u00b7lich", "zu", "er\u00b7fas\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "$,", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "So Entzognes ist am meisten dein.", "tokens": ["So", "Ent\u00b7zog\u00b7nes", "ist", "am", "meis\u00b7ten", "dein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VAFIN", "PTKA", "PIS", "PPOSAT", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Wir sind frei. Wir wurden dort entlassen,", "tokens": ["Wir", "sind", "frei", ".", "Wir", "wur\u00b7den", "dort", "ent\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$.", "PPER", "VAFIN", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "wo wir meinten, erst begr\u00fc\u00dft zu sein.", "tokens": ["wo", "wir", "mein\u00b7ten", ",", "erst", "be\u00b7gr\u00fc\u00dft", "zu", "sein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "ADV", "VVPP", "PTKZU", "VAINF", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Bang verlangen wir nach einem Halte,", "tokens": ["Bang", "ver\u00b7lan\u00b7gen", "wir", "nach", "ei\u00b7nem", "Hal\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "wir zu Jungen manchmal f\u00fcr das Alte", "tokens": ["wir", "zu", "Jun\u00b7gen", "manch\u00b7mal", "f\u00fcr", "das", "Al\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "NN", "ADV", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "und zu alt f\u00fcr das, was niemals war.", "tokens": ["und", "zu", "alt", "f\u00fcr", "das", ",", "was", "nie\u00b7mals", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PTKA", "ADJD", "APPR", "PDS", "$,", "PRELS", "ADV", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Wir, gerecht nur, wo wir dennoch preisen,", "tokens": ["Wir", ",", "ge\u00b7recht", "nur", ",", "wo", "wir", "den\u00b7noch", "prei\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "ADJD", "ADV", "$,", "PWAV", "PPER", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "weil wir, ach, der Ast sind und das Eisen", "tokens": ["weil", "wir", ",", "ach", ",", "der", "Ast", "sind", "und", "das", "Ei\u00b7sen"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "ITJ", "$,", "ART", "NN", "VAFIN", "KON", "ART", "NN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "und das S\u00fc\u00dfe reifender Gefahr.", "tokens": ["und", "das", "S\u00fc\u00b7\u00dfe", "rei\u00b7fen\u00b7der", "Ge\u00b7fahr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}}}}