{"textgrid.poem.54192": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Worauf man in Europa stolz ist", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Dieser Erdteil ist stolz auf sich, und er kann auch stolz auf sich sein. Man ist stolz in Europa:", "tokens": ["Die\u00b7ser", "Erd\u00b7teil", "ist", "stolz", "auf", "sich", ",", "und", "er", "kann", "auch", "stolz", "auf", "sich", "sein", ".", "Man", "ist", "stolz", "in", "Eu\u00b7ro\u00b7pa", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "VAFIN", "ADJD", "APPR", "PRF", "$,", "KON", "PPER", "VMFIN", "ADV", "ADJD", "APPR", "PRF", "VAINF", "$.", "PIS", "VAFIN", "ADJD", "APPR", "NE", "$."], "meter": "+-+-+-+-+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.2": {"text": "Deutscher zu sein.", "tokens": ["Deut\u00b7scher", "zu", "sein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PAV", "PTKZU", "VAINF", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Franzose zu sein.", "tokens": ["Fran\u00b7zo\u00b7se", "zu", "sein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VAINF", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Engl\u00e4nder zu sein.", "tokens": ["En\u00b7gl\u00e4n\u00b7der", "zu", "sein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VAINF", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "Kein Deutscher zu sein.", "tokens": ["Kein", "Deut\u00b7scher", "zu", "sein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PTKZU", "VAINF", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "Kein Franzose zu sein.", "tokens": ["Kein", "Fran\u00b7zo\u00b7se", "zu", "sein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PTKZU", "VAINF", "$."], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.7": {"text": "Kein Engl\u00e4nder zu sein.", "tokens": ["Kein", "En\u00b7gl\u00e4n\u00b7der", "zu", "sein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PTKZU", "VAINF", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.8": {"text": "An der Spitze der 3. Kompanie zu stehn.", "tokens": ["An", "der", "Spit\u00b7ze", "der", "3.", "Kom\u00b7pa\u00b7nie", "zu", "stehn", "."], "token_info": ["word", "word", "word", "word", "ordinal", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.9": {"text": "Eine deutsche Mutter zu sein. Am deutschen Rhein zu stehn. Und \u00fcberhaupt.", "tokens": ["Ei\u00b7ne", "deut\u00b7sche", "Mut\u00b7ter", "zu", "sein", ".", "Am", "deut\u00b7schen", "Rhein", "zu", "stehn", ".", "Und", "\u00fc\u00b7ber\u00b7haupt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VAINF", "$.", "APPRART", "ADJA", "NE", "PTKZU", "VVINF", "$.", "KON", "ADV", "$."], "meter": "+-+-+--+-+-+-+-+-+", "measure": "trochaic.octa.plus.relaxed"}, "line.10": {"text": "Ein Autogramm von Otto Geb\u00fchr zu besitzen.", "tokens": ["Ein", "Au\u00b7to\u00b7gramm", "von", "Ot\u00b7to", "Ge\u00b7b\u00fchr", "zu", "be\u00b7sit\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.11": {"text": "Eine Fahne zu haben. Ein Kriegsschiff zu sein. (\u00bbDas stolze Kriegsschiff . . . \u00ab)", "tokens": ["Ei\u00b7ne", "Fah\u00b7ne", "zu", "ha\u00b7ben", ".", "Ein", "Kriegs\u00b7schiff", "zu", "sein", ".", "(", "\u00bb", "Das", "stol\u00b7ze", "Kriegs\u00b7schiff", ".", ".", ".", "\u00ab", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct", "word", "word", "word", "punct", "punct", "punct", "punct", "punct"], "pos": ["ART", "NN", "PTKZU", "VAINF", "$.", "ART", "NN", "PTKZU", "VAINF", "$.", "$(", "$(", "ART", "ADJA", "NN", "$.", "$.", "$.", "$(", "$("], "meter": "+-+--+-+-+-+-+--+", "measure": "trochaic.octa.plus.relaxed"}, "line.12": {"text": "Im Kriege Proviantamtsverwalterstellvertreter gewesen zu sein.", "tokens": ["Im", "Krie\u00b7ge", "Pro\u00b7vi\u00b7an\u00b7tamts\u00b7ver\u00b7wal\u00b7ter\u00b7stell\u00b7ver\u00b7tre\u00b7ter", "ge\u00b7we\u00b7sen", "zu", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "VAPP", "PTKZU", "VAINF", "$."], "meter": "-+-+-+--+-+-+--+--+", "measure": "iambic.octa.plus.relaxed"}, "line.13": {"text": "B\u00fcrgermeister von Eistadt a. d. Dotter zu sein.", "tokens": ["B\u00fcr\u00b7ger\u00b7meis\u00b7ter", "von", "Eis\u00b7tadt", "a.", "d.", "Dot\u00b7ter", "zu", "sein", "."], "token_info": ["word", "word", "word", "abbreviation", "abbreviation", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "APPR", "ART", "NN", "PTKZU", "VAINF", "$."], "meter": "+-+--+-+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Als deutscher Sozialdemokrat Schlimmeres verh\u00fctet zu haben.", "tokens": ["Als", "deut\u00b7scher", "So\u00b7zi\u00b7al\u00b7de\u00b7mo\u00b7krat", "Schlim\u00b7me\u00b7res", "ver\u00b7h\u00fc\u00b7tet", "zu", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "NN", "VVPP", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+-+--+-", "measure": "iambic.octa.plus.relaxed"}}, "stanza.3": {"line.1": {"text": "Deutscher zu sein. Das hatten wir schon. Ein j\u00fcdischer Mann sagte einmal:", "tokens": ["Deut\u00b7scher", "zu", "sein", ".", "Das", "hat\u00b7ten", "wir", "schon", ".", "Ein", "j\u00fc\u00b7di\u00b7scher", "Mann", "sag\u00b7te", "ein\u00b7mal", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PTKZU", "VAINF", "$.", "PDS", "VAFIN", "PPER", "ADV", "$.", "ART", "ADJA", "NN", "VVFIN", "ADV", "$."], "meter": "+--+-+---+---+---+", "measure": "iambic.hexa.invert"}}}}}