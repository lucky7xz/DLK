{"dta.poem.4364": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Ueberzeugendes Exempel.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Nachdem ich \u00f6fters, mit Betr\u00fcben, woher die Menschen", "tokens": ["Nach\u00b7dem", "ich", "\u00f6f\u00b7ters", ",", "mit", "Be\u00b7tr\u00fc\u00b7ben", ",", "wo\u00b7her", "die", "Men\u00b7schen"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "$,", "APPR", "NN", "$,", "PWAV", "ART", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "doch so blind,", "tokens": ["doch", "so", "blind", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Den Sch\u00f6pfer, in den Creaturen, zu sehn und zu bewundern,", "tokens": ["Den", "Sch\u00f6p\u00b7fer", ",", "in", "den", "Crea\u00b7tu\u00b7ren", ",", "zu", "sehn", "und", "zu", "be\u00b7wun\u00b7dern", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "APPR", "ART", "NN", "$,", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+--+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "sind,", "tokens": ["sind", ","], "token_info": ["word", "punct"], "pos": ["VAFIN", "$,"], "meter": "-", "measure": "single.down"}, "line.5": {"text": "Betrachtet, \u00fcberlegt, erwogen,", "tokens": ["Be\u00b7trach\u00b7tet", ",", "\u00fc\u00b7ber\u00b7legt", ",", "er\u00b7wo\u00b7gen", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "$,", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und es, mit bitterer Betr\u00fcbni\u00df, im mehreren Betracht", "tokens": ["Und", "es", ",", "mit", "bit\u00b7te\u00b7rer", "Be\u00b7tr\u00fcb\u00b7ni\u00df", ",", "im", "meh\u00b7re\u00b7ren", "Be\u00b7tracht"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPER", "$,", "APPR", "ADJA", "NN", "$,", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.7": {"text": "gezogen;", "tokens": ["ge\u00b7zo\u00b7gen", ";"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.8": {"text": "So fand ich bey dem grossen Newton j\u00fcngst etwas, so es mir", "tokens": ["So", "fand", "ich", "bey", "dem", "gros\u00b7sen", "New\u00b7ton", "j\u00fcngst", "et\u00b7was", ",", "so", "es", "mir"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN", "ADV", "PIS", "$,", "ADV", "PPER", "PPER"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.9": {"text": "erkl\u00e4rte;", "tokens": ["er\u00b7kl\u00e4r\u00b7te", ";"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.10": {"text": "Indem er, da\u00df man sonder Kunst und ohn\u2019 Vernunft", "tokens": ["In\u00b7dem", "er", ",", "da\u00df", "man", "son\u00b7der", "Kunst", "und", "ohn'", "Ver\u00b7nunft"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "KOUS", "PIS", "ADJA", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "nicht sehe, lehrte.", "tokens": ["nicht", "se\u00b7he", ",", "lehr\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "$,", "VVFIN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.12": {"text": "Er schreibet: Da\u00df ein Blindgebohrner, den ein vern\u00fcnft\u2019ger", "tokens": ["Er", "schrei\u00b7bet", ":", "Da\u00df", "ein", "Blind\u00b7ge\u00b7bohr\u00b7ner", ",", "den", "ein", "ver\u00b7n\u00fcnft'\u00b7ger"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$.", "KOUS", "ART", "NN", "$,", "PRELS", "ART", "ADJA"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.13": {"text": "Arzt geheilt,", "tokens": ["Arzt", "ge\u00b7heilt", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.14": {"text": "Nachdem er ihm den Brauch der Augen, durch Kunst, und", "tokens": ["Nach\u00b7dem", "er", "ihm", "den", "Brauch", "der", "Au\u00b7gen", ",", "durch", "Kunst", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["KOUS", "PPER", "PPER", "ART", "NN", "ART", "NN", "$,", "APPR", "NN", "$,", "KON"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.15": {"text": "gl\u00fccklich mitgetheilt,", "tokens": ["gl\u00fcck\u00b7lich", "mit\u00b7ge\u00b7theilt", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVPP", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.16": {"text": "Zwar C\u00f6rper gleich erblicken k\u00f6nnen, doch, ob er selbe in", "tokens": ["Zwar", "C\u00f6r\u00b7per", "gleich", "er\u00b7bli\u00b7cken", "k\u00f6n\u00b7nen", ",", "doch", ",", "ob", "er", "sel\u00b7be", "in"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "NE", "ADV", "VVINF", "VMINF", "$,", "ADV", "$,", "KOUS", "PPER", "VVFIN", "APPR"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.17": {"text": "der N\u00e4he,", "tokens": ["der", "N\u00e4\u00b7he", ","], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.18": {"text": "Wie oder in der Ferne, sehe,", "tokens": ["Wie", "o\u00b7der", "in", "der", "Fer\u00b7ne", ",", "se\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "KON", "APPR", "ART", "NN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Ob selbe gro\u00df, ob selbe klein, das h\u00e4tte sich von ihm nicht", "tokens": ["Ob", "sel\u00b7be", "gro\u00df", ",", "ob", "sel\u00b7be", "klein", ",", "das", "h\u00e4t\u00b7te", "sich", "von", "ihm", "nicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADJA", "ADJD", "$,", "KOUS", "ADJA", "ADJD", "$,", "PDS", "VAFIN", "PRF", "APPR", "PPER", "PTKNEG"], "meter": "-+-+-+-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.20": {"text": "fassen,", "tokens": ["fas\u00b7sen", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.21": {"text": "Und sonder Prob\u2019, Erfahrung, Zeit, durchaus nicht unter-", "tokens": ["Und", "son\u00b7der", "Prob'", ",", "Er\u00b7fah\u00b7rung", ",", "Zeit", ",", "durc\u00b7haus", "nicht", "un\u00b7ter"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "$,", "NN", "$,", "NN", "$,", "ADV", "PTKNEG", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "scheiden lassen.", "tokens": ["schei\u00b7den", "las\u00b7sen", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.23": {"text": "Bis er, nach vieler Zeit und M\u00fch\u2019, zu dieser Wahrheit", "tokens": ["Bis", "er", ",", "nach", "vie\u00b7ler", "Zeit", "und", "M\u00fch'", ",", "zu", "die\u00b7ser", "Wahr\u00b7heit"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "APPR", "PIAT", "NN", "KON", "NN", "$,", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "erst gekommen,", "tokens": ["erst", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.25": {"text": "Und, durch ein aufmerksames Denken, der Dinge Zustand", "tokens": ["Und", ",", "durch", "ein", "auf\u00b7merk\u00b7sa\u00b7mes", "Den\u00b7ken", ",", "der", "Din\u00b7ge", "Zu\u00b7stand"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "APPR", "ART", "ADJA", "NN", "$,", "ART", "NN", "NN"], "meter": "-+--+--+--+--+", "measure": "amphibrach.penta.plus"}, "line.26": {"text": "erst vernommen.", "tokens": ["erst", "ver\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVPP", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.2": {"line.1": {"text": "Die\u00df ist ein unerh\u00f6rt Exempel, dem nicht kann wider-", "tokens": ["Die\u00df", "ist", "ein", "un\u00b7er\u00b7h\u00f6rt", "Ex\u00b7em\u00b7pel", ",", "dem", "nicht", "kann", "wi\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "ADJD", "NN", "$,", "PRELS", "PTKNEG", "VMFIN", "TRUNC"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "sprochen werden,", "tokens": ["spro\u00b7chen", "wer\u00b7den", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "VAINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Und ein Beweis, da\u00df, wo wir nicht, auf gleiche Weis\u2019,", "tokens": ["Und", "ein", "Be\u00b7weis", ",", "da\u00df", ",", "wo", "wir", "nicht", ",", "auf", "glei\u00b7che", "Weis'", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "$,", "KOUS", "$,", "PWAV", "PPER", "PTKNEG", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "auf dieser Erden", "tokens": ["auf", "die\u00b7ser", "Er\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PDAT", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Die Sinnen besser zu gebrauchen, vern\u00fcnftig sehn und", "tokens": ["Die", "Sin\u00b7nen", "bes\u00b7ser", "zu", "ge\u00b7brau\u00b7chen", ",", "ver\u00b7n\u00fcnf\u00b7tig", "sehn", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "PTKZU", "VVINF", "$,", "ADJD", "VVINF", "KON"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "h\u00f6ren lernen,", "tokens": ["h\u00f6\u00b7ren", "ler\u00b7nen", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "(das hei\u00dft: bey unsern Sinnen mehr, als wir gewohnt,", "tokens": ["(", "das", "hei\u00dft", ":", "bey", "un\u00b7sern", "Sin\u00b7nen", "mehr", ",", "als", "wir", "ge\u00b7wohnt", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VVFIN", "$.", "APPR", "PPOSAT", "NN", "ADV", "$,", "KOUS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "ein Ueberlegen", "tokens": ["ein", "Ue\u00b7ber\u00b7le\u00b7gen"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Zu f\u00fcgen, und mit unserm Geist das, was wir sehen, zu", "tokens": ["Zu", "f\u00fc\u00b7gen", ",", "und", "mit", "un\u00b7serm", "Geist", "das", ",", "was", "wir", "se\u00b7hen", ",", "zu"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["PTKZU", "VVINF", "$,", "KON", "APPR", "PPOSAT", "NN", "PDS", "$,", "PRELS", "PPER", "VVINF", "$,", "APPR"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.10": {"text": "erwegen)", "tokens": ["er\u00b7we\u00b7gen", ")"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.11": {"text": "Wir von dem Wege zu der Wahrheit unwiederbringlich", "tokens": ["Wir", "von", "dem", "We\u00b7ge", "zu", "der", "Wahr\u00b7heit", "un\u00b7wie\u00b7der\u00b7bring\u00b7lich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "APPR", "ART", "NN", "APPR", "ART", "NN", "ADJD"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "uns entfernen.", "tokens": ["uns", "ent\u00b7fer\u00b7nen", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "Wie aber kann, so wie wir leben, es doch mit uns wohl", "tokens": ["Wie", "a\u00b7ber", "kann", ",", "so", "wie", "wir", "le\u00b7ben", ",", "es", "doch", "mit", "uns", "wohl"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VMFIN", "$,", "ADV", "KOKOM", "PPER", "VVFIN", "$,", "PPER", "ADV", "APPR", "PPER", "ADV"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.2": {"text": "anders gehn,", "tokens": ["an\u00b7ders", "gehn", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Da wir oft, ohne Sehen, denken, noch \u00f6fters, sonder", "tokens": ["Da", "wir", "oft", ",", "oh\u00b7ne", "Se\u00b7hen", ",", "den\u00b7ken", ",", "noch", "\u00f6f\u00b7ters", ",", "son\u00b7der"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "punct", "word"], "pos": ["KOUS", "PPER", "ADV", "$,", "KOUI", "NN", "$,", "VVINF", "$,", "ADV", "ADV", "$,", "KON"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Denken, sehn!", "tokens": ["Den\u00b7ken", ",", "sehn", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVINF", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Und es doch mehr als allzu wahr, was ein vern\u00fcnft\u2019ger", "tokens": ["Und", "es", "doch", "mehr", "als", "all\u00b7zu", "wahr", ",", "was", "ein", "ver\u00b7n\u00fcnft'\u00b7ger"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPER", "ADV", "PIS", "KOKOM", "PTKA", "ADJD", "$,", "PRELS", "ART", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Heyde spricht:", "tokens": ["Hey\u00b7de", "spricht", ":"], "token_info": ["word", "word", "punct"], "pos": ["NE", "VVFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "\u201ces seh\u2019n und h\u00f6ren Aug- und Ohren, mit allen ihren Kr\u00e4f-", "tokens": ["\u201c", "es", "seh'n", "und", "h\u00f6\u00b7ren", "Aug", "und", "Oh\u00b7ren", ",", "mit", "al\u00b7len", "ih\u00b7ren", "Kr\u00e4f"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VVINF", "KON", "VVINF", "TRUNC", "KON", "NN", "$,", "APPR", "PIAT", "PPOSAT", "NN"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.8": {"text": "ten, nicht,", "tokens": ["ten", ",", "nicht", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["FM", "$,", "PTKNEG", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.9": {"text": "\u201ewenn von dem Geist, indem wir seh\u2019n und h\u00f6ren, etwas", "tokens": ["\u201e", "wenn", "von", "dem", "Geist", ",", "in\u00b7dem", "wir", "seh'n", "und", "h\u00f6\u00b7ren", ",", "et\u00b7was"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["$(", "KOUS", "APPR", "ART", "NN", "$,", "KOUS", "PPER", "VVINF", "KON", "VVINF", "$,", "ADV"], "meter": "+--+-+-+-+---", "measure": "iambic.penta.invert"}, "line.10": {"text": "sonst geschicht.", "tokens": ["sonst", "ge\u00b7schicht", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVPP", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Willt du nun in der Sinnen Schule ein bald geschickter", "tokens": ["Willt", "du", "nun", "in", "der", "Sin\u00b7nen", "Schu\u00b7le", "ein", "bald", "ge\u00b7schick\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADV", "APPR", "ART", "NN", "NN", "ART", "ADV", "ADJA"], "meter": "-----+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.12": {"text": "Sch\u00fcler seyn,", "tokens": ["Sch\u00fc\u00b7ler", "seyn", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VAINF", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.13": {"text": "Und wei\u00dft nicht, ob und wie du doch, in so viel ungez\u00e4hl-", "tokens": ["Und", "wei\u00dft", "nicht", ",", "ob", "und", "wie", "du", "doch", ",", "in", "so", "viel", "un\u00b7ge\u00b7z\u00e4hl"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "KOUS", "KON", "PWAV", "PPER", "ADV", "$,", "APPR", "ADV", "ADV", "TRUNC"], "meter": "-+-+-+-+---+-+", "measure": "unknown.measure.hexa"}, "line.14": {"text": "ten Sachen,", "tokens": ["ten", "Sa\u00b7chen", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.15": {"text": "Auf eine andre be\u00dfre Art zu sehen, sollst den Anfang", "tokens": ["Auf", "ei\u00b7ne", "and\u00b7re", "be\u00df\u00b7re", "Art", "zu", "se\u00b7hen", ",", "sollst", "den", "An\u00b7fang"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "PTKZU", "VVINF", "$,", "VMFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.16": {"text": "machen;", "tokens": ["ma\u00b7chen", ";"], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.17": {"text": "So nimm die erste Lection, und zwar zu Anfangs nur allein:", "tokens": ["So", "nimm", "die", "ers\u00b7te", "Lec\u00b7ti\u00b7on", ",", "und", "zwar", "zu", "An\u00b7fangs", "nur", "al\u00b7lein", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ART", "ADJA", "NN", "$,", "KON", "ADV", "APPR", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}, "line.18": {"text": "\u201etheil\u2019alles, was du siehst, nur erst in Farben und Figu-", "tokens": ["\u201e", "theil'\u00b7al\u00b7les", ",", "was", "du", "siehst", ",", "nur", "erst", "in", "Far\u00b7ben", "und", "Fi\u00b7gu"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADJA", "$,", "PWS", "PPER", "VVFIN", "$,", "ADV", "ADV", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.19": {"text": "ren ein.", "tokens": ["ren", "ein", "."], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$."], "meter": "-+", "measure": "iambic.single"}}}}}