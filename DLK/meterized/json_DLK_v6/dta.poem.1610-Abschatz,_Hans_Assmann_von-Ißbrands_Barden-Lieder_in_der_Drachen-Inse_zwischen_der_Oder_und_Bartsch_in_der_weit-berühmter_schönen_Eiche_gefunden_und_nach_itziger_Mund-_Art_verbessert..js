{"dta.poem.1610": {"metadata": {"author": {"name": "Abschatz, Hans Assmann von", "birth": "N.A.", "death": "N.A."}, "title": "I\u00dfbrands Barden-Lieder/ in der Drachen-Inse  \n zwischen der Oder und Bartsch in der weit-ber\u00fchmter  \n  sch\u00f6nen Eiche gefunden/ und nach itziger Mund-  \n Art verbessert.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1704", "urn": "urn:nbn:de:kobv:b4-200905199889", "language": ["de:0.99"], "booktitle": "Abschatz, Hans Assmann von: Poetische Ubersetzungen und Gedichte. Leipzig, 1704."}, "poem": {"stanza.1": {"line.1": {"text": "H\u00f6r spate Nach-Welt an/ was dir von Alten Helden/ ", "tokens": ["H\u00f6r", "spa\u00b7te", "Nach\u00b7Welt", "an", "/", "was", "dir", "von", "Al\u00b7ten", "Hel\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "PTKVZ", "$(", "PWS", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Quaden-Land gekennt/ mein Barden-Lied soll melden", "tokens": ["Die", "Qua\u00b7den\u00b7Land", "ge\u00b7kennt", "/", "mein", "Bar\u00b7den\u00b7Lied", "soll", "mel\u00b7den"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVPP", "$(", "PPOSAT", "NN", "VMFIN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich schlie\u00df in diesen Stamm die Rinden-B\u00fccher ein/", "tokens": ["Ich", "schlie\u00df", "in", "die\u00b7sen", "Stamm", "die", "Rin\u00b7den\u00b7B\u00fc\u00b7cher", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PDAT", "NN", "ART", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Damit sie ihres Ruhms ein Zeugni\u00df k\u00f6nnen seyn.", "tokens": ["Da\u00b7mit", "sie", "ih\u00b7res", "Ruhms", "ein", "Zeug\u00b7ni\u00df", "k\u00f6n\u00b7nen", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "ART", "NN", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Wenn Gallien dem Rhein die F\u00e4ssel wird bereiten/", "tokens": ["Wenn", "Gal\u00b7li\u00b7en", "dem", "Rhein", "die", "F\u00e4s\u00b7sel", "wird", "be\u00b7rei\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ART", "NE", "ART", "NN", "VAFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wenn L wird wider L um Deutschlands Freyheit streiten;", "tokens": ["Wenn", "L", "wird", "wi\u00b7der", "L", "um", "Deutschlands", "Frey\u00b7heit", "strei\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VAFIN", "APPR", "NE", "APPR", "NE", "NN", "VVFIN", "$."], "meter": "-+-+-+-++-+-", "measure": "unknown.measure.hexa"}, "line.3": {"text": "Wenn Hermans neuer Ruhm aus Grufft und Moder bricht/", "tokens": ["Wenn", "Her\u00b7mans", "neu\u00b7er", "Ruhm", "aus", "Grufft", "und", "Mo\u00b7der", "bricht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ADJA", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "K\u00f6mmt diese meine Schrifft auch wieder an das Licht.", "tokens": ["K\u00f6mmt", "die\u00b7se", "mei\u00b7ne", "Schrifft", "auch", "wie\u00b7der", "an", "das", "Licht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PDS", "PPOSAT", "NN", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ein Ph\u00f6nix wird alsdenn aus meiner Asch entstehen/", "tokens": ["Ein", "Ph\u00f6\u00b7nix", "wird", "als\u00b7denn", "aus", "mei\u00b7ner", "Asch", "ent\u00b7ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und deutscher Ahnen Prei\u00df bi\u00df an die Stern erh\u00f6hen;", "tokens": ["Und", "deut\u00b7scher", "Ah\u00b7nen", "Prei\u00df", "bi\u00df", "an", "die", "Stern", "er\u00b7h\u00f6\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "NN", "APPR", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da\u00df nun manch k\u00fchnes Blutt gantz unvergessen sey/", "tokens": ["Da\u00df", "nun", "manch", "k\u00fch\u00b7nes", "Blutt", "gantz", "un\u00b7ver\u00b7ges\u00b7sen", "sey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "ADJA", "NN", "ADV", "ADJD", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Legt ihm I\u00dfbrandes Hand hier di\u00df Verzeichnis bey.", "tokens": ["Legt", "ihm", "I\u00df\u00b7bran\u00b7des", "Hand", "hier", "di\u00df", "Ver\u00b7zeich\u00b7nis", "bey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJA", "NN", "ADV", "PDS", "NN", "PTKVZ", "$."], "meter": "--++-+-+-+-+", "measure": "anapaest.init"}}, "stanza.4": {"line.1": {"text": "Klingt dir mein rauher Reim nicht wohl in linden Ohren/", "tokens": ["Klingt", "dir", "mein", "rau\u00b7her", "Reim", "nicht", "wohl", "in", "lin\u00b7den", "Oh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "ADJA", "NN", "PTKNEG", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Er ist auff gr\u00fcner Heyd in Schwei\u00df und Staub gebohren/", "tokens": ["Er", "ist", "auff", "gr\u00fc\u00b7ner", "Heyd", "in", "Schwei\u00df", "und", "Staub", "ge\u00b7boh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ADJA", "NN", "APPR", "NN", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Bircke war mein Buch/ mein Griffel war ein Schwerdt/", "tokens": ["Die", "Bir\u00b7cke", "war", "mein", "Buch", "/", "mein", "Grif\u00b7fel", "war", "ein", "Schwerdt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "NN", "$(", "PPOSAT", "NN", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der Helden Thaten seyn dergleichen Feder werth.", "tokens": ["Der", "Hel\u00b7den", "Tha\u00b7ten", "seyn", "derg\u00b7lei\u00b7chen", "Fe\u00b7der", "werth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAINF", "PIS", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Als Varus unser Land zu Diensten wolte zwingen/", "tokens": ["Als", "Va\u00b7rus", "un\u00b7ser", "Land", "zu", "Diens\u00b7ten", "wol\u00b7te", "zwin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PPOSAT", "NN", "APPR", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Augustus \u00fcber uns Triumph-Geth\u00f6ne singen/", "tokens": ["Au\u00b7gus\u00b7tus", "\u00fc\u00b7ber", "uns", "Tri\u00b7um\u00b7ph\u00b7Get\u00b7h\u00f6\u00b7ne", "sin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "PPER", "NN", "VVINF", "$("], "meter": "+--+--+-+-+-+-", "measure": "elegiambus"}, "line.3": {"text": "So wachte Hermans Mutt nebst andern F\u00fcrsten auff/", "tokens": ["So", "wach\u00b7te", "Her\u00b7mans", "Mutt", "nebst", "an\u00b7dern", "F\u00fcrs\u00b7ten", "auff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NE", "NE", "APPR", "ADJA", "NN", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und hemmte seinen Sieg/ verbeugte seinen Lauff.", "tokens": ["Und", "hemm\u00b7te", "sei\u00b7nen", "Sieg", "/", "ver\u00b7beug\u00b7te", "sei\u00b7nen", "Lauff", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "$(", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Die Adler haben hier die Fl\u00fcgel sencken m\u00fcssen/", "tokens": ["Die", "Ad\u00b7ler", "ha\u00b7ben", "hier", "die", "Fl\u00fc\u00b7gel", "sen\u00b7cken", "m\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ART", "NN", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es trat Cheruscens Pferd den R\u00f6mschen Wolff mit F\u00fcssen/", "tokens": ["Es", "trat", "Che\u00b7rus\u00b7cens", "Pferd", "den", "R\u00f6m\u00b7schen", "Wolff", "mit", "F\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "NN", "ART", "NN", "NE", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Den Drachen ward der Durst gel\u00f6scht mit eignem Blutt/", "tokens": ["Den", "Dra\u00b7chen", "ward", "der", "Durst", "ge\u00b7l\u00f6scht", "mit", "eig\u00b7nem", "Blutt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "VVPP", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der stoltze Feldherr selbst vergieng in Zweiffel-Mutt.", "tokens": ["Der", "stolt\u00b7ze", "Feld\u00b7herr", "selbst", "ver\u00b7gieng", "in", "Zweif\u00b7fel\u00b7Mutt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "ns war der Feind an List und Waffen \u00fcberlegen/", "tokens": ["ns", "war", "der", "Feind", "an", "List", "und", "Waf\u00b7fen", "\u00fc\u00b7berl\u00b7e\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Doch unerschrockner Mutt ist beste Wehr und Degen.", "tokens": ["Doch", "un\u00b7er\u00b7schrock\u00b7ner", "Mutt", "ist", "bes\u00b7te", "Wehr", "und", "De\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAFIN", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wir st\u00fcrmten unverzagt zu ihten Fahnen ein/", "tokens": ["Wir", "st\u00fcrm\u00b7ten", "un\u00b7ver\u00b7zagt", "zu", "ih\u00b7ten", "Fah\u00b7nen", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ADJA", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Es muste wohl-gesiegt und frey gestorben seyn.", "tokens": ["Es", "mus\u00b7te", "wohl\u00b7ge\u00b7siegt", "und", "frey", "ge\u00b7stor\u00b7ben", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "KON", "ADJD", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Wie Hermundur/ Cherusc/ und Chanz und Catte rungen/", "tokens": ["Wie", "Her\u00b7mun\u00b7dur", "/", "Che\u00b7rusc", "/", "und", "Chanz", "und", "Cat\u00b7te", "run\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$(", "NE", "$(", "KON", "NN", "KON", "NE", "VVFIN", "$("], "meter": "-+---+-+-+-+-", "measure": "dactylic.init"}, "line.2": {"text": "at jedes Volckes Bard und Landsmann abgesungen;", "tokens": ["at", "je\u00b7des", "Vol\u00b7ckes", "Bard", "und", "Lands\u00b7mann", "ab\u00b7ge\u00b7sun\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "NE", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "ch habe mir zum Theil der Quaden Lob erw\u00e4hlt/", "tokens": ["ch", "ha\u00b7be", "mir", "zum", "Theil", "der", "Qua\u00b7den", "Lob", "er\u00b7w\u00e4hlt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "APPRART", "NN", "ART", "NN", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "nd was der Lygier in seine Gr\u00e4ntzen z\u00e4hlt.", "tokens": ["nd", "was", "der", "Ly\u00b7gi\u00b7er", "in", "sei\u00b7ne", "Gr\u00e4nt\u00b7zen", "z\u00e4hlt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "ein Falcke hat sich so in hoher Lufft geschwungen/", "tokens": ["ein", "Fal\u00b7cke", "hat", "sich", "so", "in", "ho\u00b7her", "Lufft", "ge\u00b7schwun\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PRF", "ADV", "APPR", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "ein Donnerstral so bald den Tannen-Baum durchdrungen/", "tokens": ["ein", "Don\u00b7ner\u00b7stral", "so", "bald", "den", "Tan\u00b7nen\u00b7Baum", "durch\u00b7drun\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "ein auffgeschwellter Flu\u00df/ vom Ufer ungehemmt/", "tokens": ["ein", "auff\u00b7ge\u00b7schwell\u00b7ter", "Flu\u00df", "/", "vom", "U\u00b7fer", "un\u00b7ge\u00b7hemmt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "APPRART", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "o bald das nahe Land ers\u00e4ufft und \u00fcberschwemmt/", "tokens": ["o", "bald", "das", "na\u00b7he", "Land", "er\u00b7s\u00e4ufft", "und", "\u00fc\u00b7bersc\u00b7hwemmt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM", "ADV", "ART", "ADJA", "NN", "VVFIN", "KON", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "ls eine k\u00fchne Schaar/ in diesem Land entsprossen/", "tokens": ["ls", "ei\u00b7ne", "k\u00fch\u00b7ne", "Schaar", "/", "in", "die\u00b7sem", "Land", "ent\u00b7spros\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$(", "APPR", "PDAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "o Elb und Weichsel quillt/ izt Hermans Bundsgenossen", "tokens": ["o", "Elb", "und", "Weich\u00b7sel", "quillt", "/", "izt", "Her\u00b7mans", "Bunds\u00b7ge\u00b7nos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["FM", "NE", "KON", "NN", "VVFIN", "$(", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Auff Varus Schaaren dringt/ sticht/ hauet/ wirfft und rennt/", "tokens": ["Auff", "Va\u00b7rus", "Schaa\u00b7ren", "dringt", "/", "sticht", "/", "hau\u00b7et", "/", "wirfft", "und", "rennt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "VVFIN", "$(", "VVFIN", "$(", "VVFIN", "$(", "VVFIN", "KON", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Durch Spisse/ Schild und Schwerdt/ die ersten Glieder tren\u0303t.", "tokens": ["Durch", "Spis\u00b7se", "/", "Schild", "und", "Schwerdt", "/", "die", "ers\u00b7ten", "Glie\u00b7der", "tre\u00f1t", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "NN", "KON", "NN", "$(", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Zwar der und jener f\u00e4llt/ doch keiner ungerochen.", "tokens": ["Zwar", "der", "und", "je\u00b7ner", "f\u00e4llt", "/", "doch", "kei\u00b7ner", "un\u00b7ge\u00b7ro\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "KON", "PDS", "VVFIN", "$(", "ADV", "PIAT", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ans Weichen denckt man nicht/ bi\u00df da\u00df man durchgebrochen/", "tokens": ["Ans", "Wei\u00b7chen", "denckt", "man", "nicht", "/", "bi\u00df", "da\u00df", "man", "durch\u00b7ge\u00b7bro\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "PTKNEG", "$(", "APPR", "KOUS", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und wenn ein neues Heer f\u00fcr Augen fertig steht/", "tokens": ["Und", "wenn", "ein", "neu\u00b7es", "Heer", "f\u00fcr", "Au\u00b7gen", "fer\u00b7tig", "steht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "NN", "APPR", "NN", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Mit neu-gefa\u00dftem Mutt ihm in die Seite geht.", "tokens": ["Mit", "neu\u00b7ge\u00b7fa\u00df\u00b7tem", "Mutt", "ihm", "in", "die", "Sei\u00b7te", "geht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PPER", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Wer also f\u00e4llt/ f\u00e4llt nicht; er steiget zu den Sternen/", "tokens": ["Wer", "al\u00b7so", "f\u00e4llt", "/", "f\u00e4llt", "nicht", ";", "er", "stei\u00b7get", "zu", "den", "Ster\u00b7nen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVFIN", "$(", "VVFIN", "PTKNEG", "$.", "PPER", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Du kanst noch/ wer er war/ aus diesen Reimen lernen:", "tokens": ["Du", "kanst", "noch", "/", "wer", "er", "war", "/", "aus", "die\u00b7sen", "Rei\u00b7men", "ler\u00b7nen", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "$(", "PWS", "PPER", "VAFIN", "$(", "APPR", "PDAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Axleben/ Busewey/ C\u00f6lln/                                 Eicke/ Doberschiz/                             ", "tokens": ["Ax\u00b7le\u00b7ben", "/", "Bu\u00b7se\u00b7wey", "/", "C\u00f6lln", "/", "Ei\u00b7cke", "/", "Do\u00b7ber\u00b7schiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Faust/ Gr\u00fcnberg/ Jorniz/ Hund/ Keul/ Lucke/                                 Motschelniz/", "tokens": ["Faust", "/", "Gr\u00fcn\u00b7berg", "/", "Jor\u00b7niz", "/", "Hund", "/", "Keul", "/", "Lu\u00b7cke", "/", "Mot\u00b7schel\u00b7niz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nez/ Oppel/ Prizelwiz/ Neinbaben/ Reder/ Stibiz/", "tokens": ["Nez", "/", "Op\u00b7pel", "/", "Pri\u00b7zel\u00b7wiz", "/", "Nein\u00b7ba\u00b7ben", "/", "Re\u00b7der", "/", "Sti\u00b7biz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NN", "$(", "NN", "$("], "meter": "-+-+-+---+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "Schaffgotsche/ Sommerfeld/ Sch\u00f6n Eiche/ Stange/                                 Schlibiz/", "tokens": ["Schaff\u00b7got\u00b7sche", "/", "Som\u00b7mer\u00b7feld", "/", "Sch\u00f6n", "Ei\u00b7che", "/", "Stan\u00b7ge", "/", "Schli\u00b7biz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "NE", "NN", "$(", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Trach/ Unruh/ Waldau/ Wuntsch/ Zirn/ Zetriz/ Zabeltiz/", "tokens": ["Trach", "/", "Un\u00b7ruh", "/", "Wal\u00b7dau", "/", "Wunt\u00b7sch", "/", "Zirn", "/", "Ze\u00b7triz", "/", "Za\u00b7bel\u00b7tiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NN", "$(", "NE", "$(", "NE", "$("], "meter": "-+-++-+-+-+-+", "measure": "unknown.measure.septa"}, "line.6": {"text": "Rotkirche/ Falckenhain/ Unw\u00fcrde/ Nadelwiz/", "tokens": ["Rot\u00b7kir\u00b7che", "/", "Fal\u00b7cken\u00b7hain", "/", "Un\u00b7w\u00fcr\u00b7de", "/", "Na\u00b7del\u00b7wiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NN", "$(", "NE", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.7": {"text": "Lo\u00df/ Zedliz/ Wenzky/ Lest/ Dachs/ Spiller/ Hoberg/                                 Tschischwiz", "tokens": ["Lo\u00df", "/", "Zed\u00b7liz", "/", "Wenz\u00b7ky", "/", "Lest", "/", "Dachs", "/", "Spil\u00b7ler", "/", "Ho\u00b7berg", "/", "Tschischwiz"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Braun/ Nostiz/ Nastelwiz/ Klinkowsky/ Hase/                                 Stischwiz/", "tokens": ["Braun", "/", "Nos\u00b7tiz", "/", "Nas\u00b7tel\u00b7wiz", "/", "Klin\u00b7kows\u00b7ky", "/", "Ha\u00b7se", "/", "Stischwiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Buchwinckel/ Rotenburg/ Fel\u00df/ Oppersdorff/ Malzan/", "tokens": ["Buch\u00b7win\u00b7ckel", "/", "Ro\u00b7ten\u00b7burg", "/", "Fel\u00df", "/", "Op\u00b7pers\u00b7dorff", "/", "Mal\u00b7zan", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Paczin\u00dfky/ Schreibersdorff/ Skor/ Bischoffsheim/ Bibran/", "tokens": ["Pac\u00b7zin\u00df\u00b7ky", "/", "Schrei\u00b7bers\u00b7dorff", "/", "Skor", "/", "Bi\u00b7schoffs\u00b7heim", "/", "Bi\u00b7bran", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Sehr/ Logau/ Schweinichen/ Schkopp/ Tschamer/                                 Posadowsky", "tokens": ["Sehr", "/", "Lo\u00b7gau", "/", "Schwei\u00b7ni\u00b7chen", "/", "Sch\u00b7kopp", "/", "Tscha\u00b7mer", "/", "Po\u00b7sa\u00b7dows\u00b7ky"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["ADV", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NE"], "meter": "-+--+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "Borschniz/ Sebottendorff/ Mohl/ Pakisch/                                 Stabelowsky/", "tokens": ["Bor\u00b7schniz", "/", "Se\u00b7bot\u00b7ten\u00b7dorff", "/", "Mohl", "/", "Pa\u00b7kisch", "/", "Sta\u00b7be\u00b7lows\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$("], "meter": "+--+--+--+--+", "measure": "dactylic.penta"}, "line.13": {"text": "Kanz/ Lemberg/ Regensburg/ Stentsch/ Seidliz/                                 Kreischelwiz/", "tokens": ["Kanz", "/", "Lem\u00b7berg", "/", "Re\u00b7gens\u00b7burg", "/", "Stentsch", "/", "Seid\u00b7liz", "/", "Krei\u00b7schel\u00b7wiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Kalckreuter/ Franckenberg/ Kottlinsky/ Nibelschiz/", "tokens": ["Kalck\u00b7reu\u00b7ter", "/", "Fran\u00b7cken\u00b7berg", "/", "Kott\u00b7lins\u00b7ky", "/", "Ni\u00b7bel\u00b7schiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+---+-+-+", "measure": "unknown.measure.penta"}, "line.15": {"text": "Pfeil/ Panwiz/ Langenau/ Vee\u00df/ D\u00f6bschiz/ Sturm/                                 Latowsky/", "tokens": ["Pfeil", "/", "Pan\u00b7wiz", "/", "Lan\u00b7ge\u00b7nau", "/", "Vee\u00df", "/", "D\u00f6b\u00b7schiz", "/", "Sturm", "/", "La\u00b7tows\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.16": {"text": "Horn/ Thader/ Brockendorff/ Skal/ Sigrot/ Ozerowsky/", "tokens": ["Horn", "/", "Tha\u00b7der", "/", "Bro\u00b7cken\u00b7dorff", "/", "Skal", "/", "Sig\u00b7rot", "/", "O\u00b7ze\u00b7rows\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NE", "$("], "meter": "+--+--++-+--+", "measure": "dactylic.di.plus"}, "line.17": {"text": "Rohr/ Nisemeuschel/ Pusch/ Petsch/ Bedau/                                 Landescron.", "tokens": ["Rohr", "/", "Ni\u00b7se\u00b7meu\u00b7schel", "/", "Pusch", "/", "Petsch", "/", "Be\u00b7dau", "/", "Lan\u00b7de\u00b7scron", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NN", "$."], "meter": "+--+-++-++-+", "measure": "iambic.septa.invert"}, "line.18": {"text": "Filz/ Uchtriz/ Reideburg/ Kuhl/ Victor/ Schelion/", "tokens": ["Filz", "/", "Ucht\u00b7riz", "/", "Rei\u00b7de\u00b7burg", "/", "Kuhl", "/", "Vic\u00b7tor", "/", "Sche\u00b7li\u00b7on", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Marcklowsky/ Schellendorff/ Mehwalde/ Wirsewinzky/", "tokens": ["Mar\u00b7ck\u00b7lows\u00b7ky", "/", "Schel\u00b7len\u00b7dorff", "/", "Meh\u00b7wal\u00b7de", "/", "Wir\u00b7se\u00b7winz\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.20": {"text": "La\u00dfota/ Gelhorn/ Axt/ Damm/ Ebersbach/ Stwolinsky/", "tokens": ["La\u00b7\u00dfo\u00b7ta", "/", "Gel\u00b7horn", "/", "Axt", "/", "Damm", "/", "E\u00b7bers\u00b7bach", "/", "St\u00b7wo\u00b7lins\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "+--+-+-+--+--+", "measure": "iambic.hexa.invert"}, "line.21": {"text": "Taur/ Popschiz/ Rechenberg/ Danbrofsky/ Schlichting/ Dyhr/", "tokens": ["Taur", "/", "Pop\u00b7schiz", "/", "Re\u00b7chen\u00b7berg", "/", "Dan\u00b7brofs\u00b7ky", "/", "Schlich\u00b7ting", "/", "Dyhr", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Keltsch/ Kitliz/ Reichenbach/ Salz/ Peterswalde/ Schir/", "tokens": ["Keltsch", "/", "Kit\u00b7liz", "/", "Rei\u00b7chen\u00b7bach", "/", "Salz", "/", "Pe\u00b7ters\u00b7wal\u00b7de", "/", "Schir", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Plach/ Mauschwiz/ Winterfeld/ Hock/ Eicholz/ W\u00fcrben/                                     Dol-", "tokens": ["Plach", "/", "Mausc\u00b7hwiz", "/", "Win\u00b7ter\u00b7feld", "/", "Hock", "/", "Eic\u00b7holz", "/", "W\u00fcr\u00b7ben", "/", "Dol"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NE", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NN", "$(", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Stosch/ Abschaz/ Knobelsdorff/ Sack/ Reibniz/                                 Wise/ Rol-", "tokens": ["Stosch", "/", "Ab\u00b7schaz", "/", "Kno\u00b7bels\u00b7dorff", "/", "Sack", "/", "Reib\u00b7niz", "/", "Wi\u00b7se", "/", "Rol"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["ADJD", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Mosch/ Heyde/ Falckenberg/ Brockt/ Schindel/                                 Jarotschin/", "tokens": ["Mosch", "/", "Hey\u00b7de", "/", "Fal\u00b7cken\u00b7berg", "/", "Brockt", "/", "Schin\u00b7del", "/", "Ja\u00b7rot\u00b7schin", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Branchtsch/ Tschirnhau\u00df/ Portugall/ Koppsch/                                 Profe/ Zirotin/", "tokens": ["Branchtsch", "/", "Tschirn\u00b7hau\u00df", "/", "Por\u00b7tu\u00b7gall", "/", "Koppsch", "/", "Pro\u00b7fe", "/", "Zi\u00b7ro\u00b7tin", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NN", "$(", "NE", "$("], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.27": {"text": "Bock/ Glaubiz/ Widebach/ Droschk/ Aulig/ Poser/                                 Promniz/", "tokens": ["Bock", "/", "Glau\u00b7biz", "/", "Wi\u00b7de\u00b7bach", "/", "Droschk", "/", "Au\u00b7lig", "/", "Po\u00b7ser", "/", "Prom\u00b7niz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Verg/ Borwiz/ Rau\u00dfendorff/ Schweinz/ Seefeld/ Schleu\u00dfer/                             ", "tokens": ["Verg", "/", "Bor\u00b7wiz", "/", "Rau\u00b7\u00dfen\u00b7dorff", "/", "Schweinz", "/", "See\u00b7feld", "/", "Schleu\u00b7\u00dfer", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NN", "$(", "NN", "$(", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.29": {"text": "Schenck/ Obisch/ Mesenau/ Leitsch/                                 Lestwiz/ Kreidelwiz/", "tokens": ["Schenck", "/", "O\u00b7bisch", "/", "Me\u00b7se\u00b7nau", "/", "Leitsch", "/", "Lest\u00b7wiz", "/", "Krei\u00b7del\u00b7wiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Sitsch/ M\u00fchlheim/ Talkenberg/ Odersky/                                 Keseliz/", "tokens": ["Sit\u00b7sch", "/", "M\u00fchl\u00b7heim", "/", "Tal\u00b7ken\u00b7berg", "/", "O\u00b7ders\u00b7ky", "/", "Ke\u00b7se\u00b7liz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+--+--+--+-+", "measure": "amphibrach.tetra.plus"}, "line.31": {"text": "Sterz/ Schwobsdorff/ Schwudiger/ Mettsch/ Oderwolff/                                     Ka-", "tokens": ["Sterz", "/", "Schwobs\u00b7dorff", "/", "Schwu\u00b7di\u00b7ger", "/", "Mettsch", "/", "O\u00b7der\u00b7wolff", "/", "Ka"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["NN", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "TRUNC"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.32": {"text": "Streit/ Raschwiz/ Frogelwiz/ Aff/ Elbel/ Morawizky/", "tokens": ["Streit", "/", "Rasc\u00b7hwiz", "/", "Fro\u00b7gel\u00b7wiz", "/", "Aff", "/", "El\u00b7bel", "/", "Mo\u00b7ra\u00b7wiz\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.33": {"text": "Senz/ Baruth/ Biberstein/ Tschesch/ Ulbersdorff/                                 Ratschin/", "tokens": ["Senz", "/", "Ba\u00b7ru\u00b7th", "/", "Bi\u00b7bers\u00b7tein", "/", "Tschesch", "/", "Ul\u00b7bers\u00b7dorff", "/", "Rat\u00b7schin", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NN", "$("], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.34": {"text": "Nimz/ Bohrau/ Koschenbar/ Skribensky/ Nimtsch/                                 Gaschin.", "tokens": ["Nimz", "/", "Bo\u00b7hrau", "/", "Ko\u00b7schen\u00b7bar", "/", "Skri\u00b7bens\u00b7ky", "/", "Nimtsch", "/", "Ga\u00b7schin", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NE", "$(", "NN", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.35": {"text": "Saur/ Schwettlig/ Schneckenhau\u00df/ Schimonsky/ Sedenizky/", "tokens": ["Saur", "/", "Schwett\u00b7lig", "/", "Schne\u00b7cken\u00b7hau\u00df", "/", "Schi\u00b7mons\u00b7ky", "/", "Se\u00b7de\u00b7niz\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "ADJD", "$(", "NN", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+--+--+--+", "measure": "iambic.penta.relaxed"}, "line.36": {"text": "Bornst\u00e4tt/ Dre\u00dfk/ Adelsbach/ Gfug/ Hohendorff/                                 Kochtizky/", "tokens": ["Born\u00b7st\u00e4tt", "/", "Dre\u00dfk", "/", "A\u00b7dels\u00b7bach", "/", "Gfug", "/", "Ho\u00b7hen\u00b7dorff", "/", "Koch\u00b7tiz\u00b7ky", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NE", "$(", "NE", "$("], "meter": "+--+-+-+--+-+", "measure": "iambic.hexa.invert"}, "line.37": {"text": "Bludowsky/ Biberitsch/ Burghau\u00df/ Ungnade/ Stolz/", "tokens": ["Blu\u00b7dows\u00b7ky", "/", "Bi\u00b7be\u00b7ritsch", "/", "Burg\u00b7hau\u00df", "/", "Ung\u00b7na\u00b7de", "/", "Stolz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NN", "$("], "meter": "+--+--+--+-+", "measure": "dactylic.tri.plus"}, "line.38": {"text": "Grodizky/ Gregersdorff/ Gurezky/ L\u00f6ben/ Schmolz.", "tokens": ["Gro\u00b7diz\u00b7ky", "/", "Gre\u00b7gers\u00b7dorff", "/", "Gu\u00b7rez\u00b7ky", "/", "L\u00f6\u00b7ben", "/", "Schmolz", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "NN", "$(", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.39": {"text": "Postolzky/ K\u00f6ckeriz/ Sternberg/ auch Leubel/                                 Kottwiz/", "tokens": ["Pos\u00b7tolz\u00b7ky", "/", "K\u00f6\u00b7cke\u00b7riz", "/", "Stern\u00b7berg", "/", "auch", "Leu\u00b7bel", "/", "Kott\u00b7wiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NE", "$(", "ADV", "NN", "$(", "NE", "$("], "meter": "-+-+--+--+--+", "measure": "iambic.penta.relaxed"}, "line.40": {"text": "Larsch/ Kunhein/ Stadion/ mit Gersdorff/ Rime/ Rottwiz/", "tokens": ["Larsch", "/", "Kun\u00b7hein", "/", "Sta\u00b7di\u00b7on", "/", "mit", "Gers\u00b7dorff", "/", "Ri\u00b7me", "/", "Rott\u00b7wiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NN", "$(", "APPR", "NE", "$(", "NN", "$(", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Viltsch/ Krekwiz/ Pogrel/ Bucht/ N\u00e4f/ Haugwiz/ und                                 Zigan/", "tokens": ["Viltsch", "/", "Krek\u00b7wiz", "/", "Pog\u00b7rel", "/", "Bucht", "/", "N\u00e4f", "/", "Haug\u00b7wiz", "/", "und", "Zi\u00b7gan", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$(", "NN", "$(", "NE", "$(", "NN", "$(", "NN", "$(", "NN", "$(", "KON", "NE", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.42": {"text": "(verzeiht mir/ wenn der Reim nicht alle fassen                                 kan.)", "tokens": ["(", "ver\u00b7zeiht", "mir", "/", "wenn", "der", "Reim", "nicht", "al\u00b7le", "fas\u00b7sen", "kan", ".", ")"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "PPER", "$(", "KOUS", "ART", "NN", "PTKNEG", "PIS", "VVINF", "VMFIN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "Salsch/ Sunek/ Studeniz/ dann Lidlau/ Strzela/ Pritwiz/", "tokens": ["Sal\u00b7sch", "/", "Su\u00b7nek", "/", "Stu\u00b7de\u00b7niz", "/", "dann", "Lid\u00b7lau", "/", "Str\u00b7ze\u00b7la", "/", "Prit\u00b7wiz", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "NN", "$(", "ADV", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.44": {"text": "Auch Tschirschke/ Koschelig/ nebst                                 Warnsdorff/ Wirbsky/", "tokens": ["Auch", "Tschirschke", "/", "Ko\u00b7sche\u00b7lig", "/", "nebst", "Warns\u00b7dorff", "/", "Wirbs\u00b7ky", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "NE", "$(", "NE", "$(", "APPR", "NE", "$(", "NE", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.45": {"text": "Di\u00df ist der Helden-Stamm/ so viel ich mich                                 besonnen/", "tokens": ["Di\u00df", "ist", "der", "Hel\u00b7den\u00b7Stamm", "/", "so", "viel", "ich", "mich", "be\u00b7son\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$(", "ADV", "ADV", "PPER", "PRF", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.46": {"text": "Die nicht den schlechtsten Theil der                                 grossen Schlacht ge-", "tokens": ["Die", "nicht", "den", "schlechts\u00b7ten", "Theil", "der", "gros\u00b7sen", "Schlacht", "ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PTKNEG", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.14": {"line.1": {"text": "Ich wei\u00df wohl/ wenn di\u00df Lied das Licht auffs neue sieht/", "tokens": ["Ich", "wei\u00df", "wohl", "/", "wenn", "di\u00df", "Lied", "das", "Licht", "auffs", "neu\u00b7e", "sieht", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$(", "KOUS", "PDS", "NN", "ART", "NN", "APPRART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df mancher edler Stock f\u00fcrl\u00e4ngst wird seyn verbl\u00fcht/", "tokens": ["Da\u00df", "man\u00b7cher", "ed\u00b7ler", "Stock", "f\u00fcr\u00b7l\u00e4ngst", "wird", "seyn", "ver\u00b7bl\u00fcht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "ADJA", "NN", "VVFIN", "VAFIN", "PPOSAT", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Jedoch erlangt er hier durch mich ein neues Leben:", "tokens": ["Je\u00b7doch", "er\u00b7langt", "er", "hier", "durch", "mich", "ein", "neu\u00b7es", "Le\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Den Andern aber soll di\u00df Zeugni\u00df Anla\u00df geben/", "tokens": ["Den", "An\u00b7dern", "a\u00b7ber", "soll", "di\u00df", "Zeug\u00b7ni\u00df", "An\u00b7la\u00df", "ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADV", "VMFIN", "PDS", "NN", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wenn sie nicht Mi\u00dfbrutt wolln von Falck und Adler seyn/", "tokens": ["Wenn", "sie", "nicht", "Mi\u00df\u00b7brutt", "wolln", "von", "Falck", "und", "Ad\u00b7ler", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "NN", "VMFIN", "APPR", "NN", "KON", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Durch Tugend sich auffs neu dabey zu schreiben ein.", "tokens": ["Durch", "Tu\u00b7gend", "sich", "auffs", "neu", "da\u00b7bey", "zu", "schrei\u00b7ben", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PRF", "APPRART", "ADJD", "PAV", "PTKZU", "VVINF", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}