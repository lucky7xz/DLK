{"dta.poem.9303": {"metadata": {"author": {"name": "Weise, Christian", "birth": "N.A.", "death": "N.A."}, "title": "Das f\u00fcnffte Gespr\u00e4ch.", "genre": "Lyrik; Drama; Prosa", "period": "N.A.", "pub_year": "1701", "urn": "urn:nbn:de:kobv:b4-25043-0", "language": ["de:0.99"], "booktitle": "Weise, Christian: \u00dcberfl\u00fc\u00dfige Gedancken Der gr\u00fcnenden jugend. Leipzig, 1701."}, "poem": {"stanza.1": {"line.1": {"text": "Vexiert die jungfer braut/", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sie hat es wol verdienet/", "tokens": ["Sie", "hat", "es", "wol", "ver\u00b7die\u00b7net", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Denn sie hat sich erk\u00fchnet", "tokens": ["Denn", "sie", "hat", "sich", "er\u00b7k\u00fch\u00b7net"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "PRF", "VVFIN"], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Bey einem junggesellen", "tokens": ["Bey", "ei\u00b7nem", "jung\u00b7ge\u00b7sel\u00b7len"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Sich gestern einzustellen", "tokens": ["Sich", "ge\u00b7stern", "ein\u00b7zu\u00b7stel\u00b7len"], "token_info": ["word", "word", "word"], "pos": ["PRF", "ADV", "VVINF"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "2. Vexiert die jungfer braut/", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Denn sie ist mit verlangen", "tokens": ["Denn", "sie", "ist", "mit", "ver\u00b7lan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "APPR", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Mit ihm zu bette gangen/", "tokens": ["Mit", "ihm", "zu", "bet\u00b7te", "gan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PTKZU", "VVFIN", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und hat bey ihm geschlaffen/", "tokens": ["Und", "hat", "bey", "ihm", "ge\u00b7schlaf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "PPER", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Drum m\u00fcssen wir sie straffen/", "tokens": ["Drum", "m\u00fcs\u00b7sen", "wir", "sie", "straf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "PPER", "VVFIN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "3. Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Und h\u00f6rt nicht auf zu fragen/", "tokens": ["Und", "h\u00f6rt", "nicht", "auf", "zu", "fra\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "APPR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sie mu\u00df nun alles sagen/", "tokens": ["Sie", "mu\u00df", "nun", "al\u00b7les", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PIS", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Wie sich die sache reimet/", "tokens": ["Wie", "sich", "die", "sa\u00b7che", "rei\u00b7met", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Und ob ihr was getr\u00e4umet/", "tokens": ["Und", "ob", "ihr", "was", "ge\u00b7tr\u00e4u\u00b7met", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PIS", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "4. Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Ob sie des nachbars wegen", "tokens": ["Ob", "sie", "des", "nach\u00b7bars", "we\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Auch heinte wol gelegen?", "tokens": ["Auch", "hein\u00b7te", "wol", "ge\u00b7le\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ob sie das tuch zerrissen?", "tokens": ["Ob", "sie", "das", "tuch", "zer\u00b7ris\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ob sie ein floch gebissen?", "tokens": ["Ob", "sie", "ein", "floch", "ge\u00b7bis\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJD", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "5. Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Es habens auch die alten", "tokens": ["Es", "ha\u00b7bens", "auch", "die", "al\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Vor diesem so gehalten/", "tokens": ["Vor", "die\u00b7sem", "so", "ge\u00b7hal\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADV", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da\u00df wir es bester massen/", "tokens": ["Da\u00df", "wir", "es", "bes\u00b7ter", "mas\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADJD", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Beyn alten l\u00f6chern lassen/", "tokens": ["Beyn", "al\u00b7ten", "l\u00f6\u00b7chern", "las\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VVINF", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "6. Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Es werden ihre kinder", "tokens": ["Es", "wer\u00b7den", "ih\u00b7re", "kin\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Schiers k\u00fcnftig viel ges\u00fcnder/", "tokens": ["Schiers", "k\u00fcnf\u00b7tig", "viel", "ge\u00b7s\u00fcn\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ADV", "ADJD", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Wenn wir mit schertz und lachen", "tokens": ["Wenn", "wir", "mit", "schertz", "und", "la\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ADJD", "KON", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Ein wenig possen machen/", "tokens": ["Ein", "we\u00b7nig", "pos\u00b7sen", "ma\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVINF", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "7. Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Und wenn sie an geberden", "tokens": ["Und", "wenn", "sie", "an", "ge\u00b7ber\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "APPR", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Will etwas b\u00f6se werden/", "tokens": ["Will", "et\u00b7was", "b\u00f6\u00b7se", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ADJD", "VAINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "So last sie immer schertzen/", "tokens": ["So", "last", "sie", "im\u00b7mer", "schert\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Sie meynt es nicht von hertzen.", "tokens": ["Sie", "meynt", "es", "nicht", "von", "hert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "8. Vexiert die jungfer braut/", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Und w\u00fcnschet ihr gel\u00fccke", "tokens": ["Und", "w\u00fcn\u00b7schet", "ihr", "ge\u00b7l\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Zum k\u00fcnfftgen meister-st\u00fccke/", "tokens": ["Zum", "k\u00fcnfft\u00b7gen", "meis\u00b7ter\u00b7st\u00fc\u00b7cke", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und zum gevatter-kuchen/", "tokens": ["Und", "zum", "ge\u00b7vat\u00b7ter\u00b7ku\u00b7chen", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Da\u00df wir sie bald besuchen.", "tokens": ["Da\u00df", "wir", "sie", "bald", "be\u00b7su\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Vexiert die jungfer braut.", "tokens": ["Ve\u00b7xiert", "die", "jung\u00b7fer", "braut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}