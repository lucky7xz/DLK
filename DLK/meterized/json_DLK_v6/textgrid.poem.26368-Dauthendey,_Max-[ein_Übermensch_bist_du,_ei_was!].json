{"textgrid.poem.26368": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "[ein \u00dcbermensch bist du, ei was!]", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbein \u00dcbermensch bist du, ei was!", "tokens": ["\u00bb", "ein", "\u00dc\u00b7ber\u00b7mensch", "bist", "du", ",", "ei", "was", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "PPER", "$,", "ITJ", "PWS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ach, sage mir, wie macht man das?\u00ab \u2013", "tokens": ["Ach", ",", "sa\u00b7ge", "mir", ",", "wie", "macht", "man", "das", "?", "\u00ab", "\u2013"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ITJ", "$,", "VVFIN", "PPER", "$,", "PWAV", "VVFIN", "PIS", "PDS", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "\u00bbmein Lieber, das ist gar nicht schwer,", "tokens": ["\u00bb", "mein", "Lie\u00b7ber", ",", "das", "ist", "gar", "nicht", "schwer", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "PDS", "VAFIN", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Man ist einfach nicht menschlich mehr.", "tokens": ["Man", "ist", "ein\u00b7fach", "nicht", "menschlich", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "PTKNEG", "ADJD", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Bist du von dir steif \u00fcberzeugt,", "tokens": ["Bist", "du", "von", "dir", "steif", "\u00fc\u00b7berz\u00b7eugt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es jeden andern auch so deucht.", "tokens": ["Es", "je\u00b7den", "an\u00b7dern", "auch", "so", "deucht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "PIS", "ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Nie danke, wenn man dir was gibt,", "tokens": ["Nie", "dan\u00b7ke", ",", "wenn", "man", "dir", "was", "gibt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "PIS", "PPER", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nimm einfach, weil es dir beliebt;", "tokens": ["Nimm", "ein\u00b7fach", ",", "weil", "es", "dir", "be\u00b7liebt", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "$,", "KOUS", "PPER", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Denn Dank ist eine Knechtaktion.", "tokens": ["Denn", "Dank", "ist", "ei\u00b7ne", "Knech\u00b7tak\u00b7ti\u00b7on", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ART", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Du nimmst, und das sei andern Lohn.", "tokens": ["Du", "nimmst", ",", "und", "das", "sei", "an\u00b7dern", "Lohn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "PDS", "VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Und Achtung sollst du niemals suchen,", "tokens": ["Und", "Ach\u00b7tung", "sollst", "du", "nie\u00b7mals", "su\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VMFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die ganze Menschheit sei dir Kuchen.", "tokens": ["Die", "gan\u00b7ze", "Menschheit", "sei", "dir", "Ku\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "PPER", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.7": {"line.1": {"text": "Geld kennt man nicht, weil's zu viel gibt,", "tokens": ["Geld", "kennt", "man", "nicht", ",", "weil's", "zu", "viel", "gibt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "PTKNEG", "$,", "KOUS", "PTKA", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und weil es jeder weiterschiebt.", "tokens": ["Und", "weil", "es", "je\u00b7der", "wei\u00b7ter\u00b7schiebt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Mit Schulden sollst du alles zahlen,", "tokens": ["Mit", "Schul\u00b7den", "sollst", "du", "al\u00b7les", "zah\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VMFIN", "PPER", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das wird dir auch viel leichter fallen.", "tokens": ["Das", "wird", "dir", "auch", "viel", "leich\u00b7ter", "fal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Man spreche immer nur von sich,", "tokens": ["Man", "spre\u00b7che", "im\u00b7mer", "nur", "von", "sich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "ADV", "APPR", "PRF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und alle denken dann an dich,", "tokens": ["Und", "al\u00b7le", "den\u00b7ken", "dann", "an", "dich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADV", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Denn du allein sollst weiterleben,", "tokens": ["Denn", "du", "al\u00b7lein", "sollst", "wei\u00b7ter\u00b7le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADV", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Weil das dem \u00dcbermensch gegeben.\u00ab \u2013", "tokens": ["Weil", "das", "dem", "\u00dc\u00b7ber\u00b7mensch", "ge\u00b7ge\u00b7ben", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KOUS", "PDS", "ART", "NN", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "\u00bbgar manches h\u00e4tt' ich einzuwenden,", "tokens": ["\u00bb", "gar", "man\u00b7ches", "h\u00e4tt'", "ich", "ein\u00b7zu\u00b7wen\u00b7den", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "PIS", "VAFIN", "PPER", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sind \u00dcbermenschen nicht zu pf\u00e4nden? \u2013", "tokens": ["Sind", "\u00dc\u00b7ber\u00b7men\u00b7schen", "nicht", "zu", "pf\u00e4n\u00b7den", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "NN", "PTKNEG", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Nicht leicht, da sie nicht alles haben,", "tokens": ["Nicht", "leicht", ",", "da", "sie", "nicht", "al\u00b7les", "ha\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$,", "KOUS", "PPER", "PTKNEG", "PIS", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn Glanz, den lieben nur die Raben.\u00ab", "tokens": ["Denn", "Glanz", ",", "den", "lie\u00b7ben", "nur", "die", "Ra\u00b7ben", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "NN", "$,", "ART", "ADJA", "ADV", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "\u00bbwie ist's mit Lieben, Rauchen, Trinken? \u2013", "tokens": ["\u00bb", "wie", "ist's", "mit", "Lie\u00b7ben", ",", "Rau\u00b7chen", ",", "Trin\u00b7ken", "?", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "KOKOM", "VAFIN", "APPR", "ADJA", "$,", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das sollst du, bis die Knochen stinken.\u00ab", "tokens": ["Das", "sollst", "du", ",", "bis", "die", "Kno\u00b7chen", "stin\u00b7ken", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VMFIN", "PPER", "$,", "KOUS", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "\u00bbdies letztre scheint mir, taugt etwas,", "tokens": ["\u00bb", "dies", "letz\u00b7tre", "scheint", "mir", ",", "taugt", "et\u00b7was", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "PDS", "PIS", "VVFIN", "PPER", "$,", "VVFIN", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich werde \u00dcbermensch zum Spa\u00df.\u00ab", "tokens": ["Ich", "wer\u00b7de", "\u00dc\u00b7ber\u00b7mensch", "zum", "Spa\u00df", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "NN", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Man sieht, die Gro\u00dfstadt macht geweckt,", "tokens": ["Man", "sieht", ",", "die", "Gro\u00df\u00b7stadt", "macht", "ge\u00b7weckt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "ART", "NN", "VVFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich hatte einen Freund entdeckt,", "tokens": ["Ich", "hat\u00b7te", "ei\u00b7nen", "Freund", "ent\u00b7deckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Den \u00dcbermenschen Balduin", "tokens": ["Den", "\u00dc\u00b7ber\u00b7men\u00b7schen", "Bal\u00b7du\u00b7in"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Tom C\u00e4sar Christian P.T. Stien.", "tokens": ["Tom", "C\u00e4\u00b7sar", "Chris\u00b7ti\u00b7an", "P.", "T.", "Sti\u00b7en", "."], "token_info": ["word", "word", "word", "abbreviation", "abbreviation", "word", "punct"], "pos": ["NE", "NE", "NE", "NE", "NE", "NE", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.17": {"line.1": {"text": "Gar gern erz\u00e4hlte er von Dingen,", "tokens": ["Gar", "gern", "er\u00b7z\u00e4hl\u00b7te", "er", "von", "Din\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die zwischen Erd' und Himmel hingen.", "tokens": ["Die", "zwi\u00b7schen", "Erd'", "und", "Him\u00b7mel", "hin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Und \u00dcbermensch war er von Herzen,", "tokens": ["Und", "\u00dc\u00b7ber\u00b7mensch", "war", "er", "von", "Her\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "PPER", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich wurd' es auch, doch mehr mir Schmerzen.", "tokens": ["Ich", "wurd'", "es", "auch", ",", "doch", "mehr", "mir", "Schmer\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "$,", "ADV", "ADV", "PPER", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Auch \u00dcbermenschen f\u00e4llt beim Wein", "tokens": ["Auch", "\u00dc\u00b7ber\u00b7men\u00b7schen", "f\u00e4llt", "beim", "Wein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Des Lebens hohe Seite ein,", "tokens": ["Des", "Le\u00b7bens", "ho\u00b7he", "Sei\u00b7te", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Vom Weibe spricht man viel Gespr\u00e4che,", "tokens": ["Vom", "Wei\u00b7be", "spricht", "man", "viel", "Ge\u00b7spr\u00e4\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und h\u00f6her w\u00e4chst des Weines Zeche.", "tokens": ["Und", "h\u00f6\u00b7her", "w\u00e4chst", "des", "Wei\u00b7nes", "Ze\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "P.T. meinte, ich sei verloren", "tokens": ["P.", "T.", "mein\u00b7te", ",", "ich", "sei", "ver\u00b7lo\u00b7ren"], "token_info": ["abbreviation", "abbreviation", "word", "punct", "word", "word", "word"], "pos": ["NE", "NE", "VVFIN", "$,", "PPER", "VAFIN", "VVPP"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Und nicht als \u00dcbermensch geboren,", "tokens": ["Und", "nicht", "als", "\u00dc\u00b7ber\u00b7mensch", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "KOUS", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Wenn ich vom Weibe H\u00f6h'res wollte,", "tokens": ["Wenn", "ich", "vom", "Wei\u00b7be", "H\u00f6h'\u00b7res", "woll\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "NN", "VMFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als wie sie sein und bleiben sollte.", "tokens": ["Als", "wie", "sie", "sein", "und", "blei\u00b7ben", "soll\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOKOM", "PPER", "VAINF", "KON", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "\u00bbhohes bei Frauen gibt es nicht,", "tokens": ["\u00bb", "ho\u00b7hes", "bei", "Frau\u00b7en", "gibt", "es", "nicht", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "APPR", "NN", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Als da\u00df sie mal Franz\u00f6sisch spricht,", "tokens": ["Als", "da\u00df", "sie", "mal", "Fran\u00b7z\u00f6\u00b7sisch", "spricht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "ADV", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Nimm nicht der Frau die Proportion,", "tokens": ["Nimm", "nicht", "der", "Frau", "die", "Pro\u00b7por\u00b7ti\u00b7on", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKNEG", "ART", "NN", "ART", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Die Frau wirkt leer im h\u00f6hern Ton.", "tokens": ["Die", "Frau", "wirkt", "leer", "im", "h\u00f6\u00b7hern", "Ton", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Heut tut sich jede gleich beschweren,", "tokens": ["Heut", "tut", "sich", "je\u00b7de", "gleich", "be\u00b7schwe\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "PIAT", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Soll sie im Jahr einmal geb\u00e4ren,", "tokens": ["Soll", "sie", "im", "Jahr", "ein\u00b7mal", "ge\u00b7b\u00e4\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPRART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Sie wirft sich kalt auf das Gehirn,", "tokens": ["Sie", "wirft", "sich", "kalt", "auf", "das", "Ge\u00b7hirn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Statt Busen hat sie nur noch Stirn,", "tokens": ["Statt", "Bu\u00b7sen", "hat", "sie", "nur", "noch", "Stirn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "PPER", "ADV", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Zu laut wird sie f\u00fcr heut geboren", "tokens": ["Zu", "laut", "wird", "sie", "f\u00fcr", "heut", "ge\u00b7bo\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKA", "ADJD", "VAFIN", "PPER", "APPR", "ADV", "VVPP"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Und ist oft ein Geschrei den Ohren.", "tokens": ["Und", "ist", "oft", "ein", "Ge\u00b7schrei", "den", "Oh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "P.T. verhalf mir auf die Sohlen,", "tokens": ["P.", "T.", "ver\u00b7half", "mir", "auf", "die", "Soh\u00b7len", ","], "token_info": ["abbreviation", "abbreviation", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Vers\u00e4umtes schleunigst nachzuholen,", "tokens": ["Ver\u00b7s\u00e4um\u00b7tes", "schleu\u00b7nigst", "nach\u00b7zu\u00b7ho\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Als ich ihm n\u00e4mlich eingestand,", "tokens": ["Als", "ich", "ihm", "n\u00e4m\u00b7lich", "ein\u00b7ge\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Weib sei mir noch unbekannt.\u00ab", "tokens": ["Das", "Weib", "sei", "mir", "noch", "un\u00b7be\u00b7kannt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "\u00bbdas Weib,\u00ab sagt' er \u00bbman bring' es her!", "tokens": ["\u00bb", "das", "Weib", ",", "\u00ab", "sagt'", "er", "\u00bb", "man", "bring'", "es", "her", "!"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "$,", "$(", "VVFIN", "PPER", "$(", "PIS", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "W\u00fcnschest du eins oder gleich mehr?", "tokens": ["W\u00fcn\u00b7schest", "du", "eins", "o\u00b7der", "gleich", "mehr", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIS", "KON", "ADV", "ADV", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.31": {"line.1": {"text": "Ach,\u00ab f\u00fcgt er zu, \u00bbdu bist noch sch\u00fcchtern,", "tokens": ["Ach", ",", "\u00ab", "f\u00fcgt", "er", "zu", ",", "\u00bb", "du", "bist", "noch", "sch\u00fcch\u00b7tern", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "$(", "VVFIN", "PPER", "PTKVZ", "$,", "$(", "PPER", "VAFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann macht dich wohl schon eine n\u00fcchtern.\u00ab", "tokens": ["Dann", "macht", "dich", "wohl", "schon", "ei\u00b7ne", "n\u00fcch\u00b7tern", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ADV", "ART", "ADJD", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Ja, wenn du willst, so hol' ich sie,", "tokens": ["Ja", ",", "wenn", "du", "willst", ",", "so", "hol'", "ich", "sie", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KOUS", "PPER", "VMFIN", "$,", "ADV", "VVFIN", "PPER", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie steht gleich unten ", "tokens": ["Sie", "steht", "gleich", "un\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV"], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.33": {"line.1": {"text": "Und damit eilt' er fort, der Gute.", "tokens": ["Und", "da\u00b7mit", "eilt'", "er", "fort", ",", "der", "Gu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "PTKVZ", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich wartete auf meiner Bude.", "tokens": ["Ich", "war\u00b7te\u00b7te", "auf", "mei\u00b7ner", "Bu\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.34": {"line.1": {"text": "Ach, dachte ich, wie soll das werden,", "tokens": ["Ach", ",", "dach\u00b7te", "ich", ",", "wie", "soll", "das", "wer\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "VVFIN", "PPER", "$,", "PWAV", "VMFIN", "PDS", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dein Freund macht sich zu viel Beschwerden,", "tokens": ["Dein", "Freund", "macht", "sich", "zu", "viel", "Be\u00b7schwer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PRF", "APPR", "PIAT", "NN", "$,"], "meter": "-+---+-+-", "measure": "dactylic.init"}}, "stanza.35": {"line.1": {"text": "Doch \u00dcbermenschen sind wir beide,", "tokens": ["Doch", "\u00dc\u00b7ber\u00b7men\u00b7schen", "sind", "wir", "bei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "PPER", "PIS", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und deshalb macht es ihm wohl Freude.", "tokens": ["Und", "des\u00b7halb", "macht", "es", "ihm", "wohl", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "PPER", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.36": {"line.1": {"text": "Es war ein \u00dcbersommerabend,", "tokens": ["Es", "war", "ein", "\u00dc\u00b7ber\u00b7som\u00b7mer\u00b7a\u00b7bend", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und nicht einmal die Spree war labend.", "tokens": ["Und", "nicht", "ein\u00b7mal", "die", "Spree", "war", "la\u00b7bend", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ADV", "ART", "NE", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "Ich dacht' an Vater und an Schwester", "tokens": ["Ich", "dacht'", "an", "Va\u00b7ter", "und", "an", "Schwes\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und an die lieben Heimatrester.", "tokens": ["Und", "an", "die", "lie\u00b7ben", "Hei\u00b7mat\u00b7res\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Mein Herz hatte K\u00fcrbisgewicht", "tokens": ["Mein", "Herz", "hat\u00b7te", "K\u00fcr\u00b7bis\u00b7ge\u00b7wicht"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Und seufzte: Ach Gott, k\u00e4m' sie nicht!", "tokens": ["Und", "seufz\u00b7te", ":", "Ach", "Gott", ",", "k\u00e4m'", "sie", "nicht", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "ITJ", "NN", "$,", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Ich l\u00f6schte Lamp' und Kerzen aus", "tokens": ["Ich", "l\u00f6schte", "Lamp'", "und", "Ker\u00b7zen", "aus"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "KON", "NN", "APPR"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Und tat, als w\u00e4r' ich nie zu Haus.", "tokens": ["Und", "tat", ",", "als", "w\u00e4r'", "ich", "nie", "zu", "Haus", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOKOM", "VAFIN", "PPER", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.40": {"line.1": {"text": "Vielleicht bleibt sie mir dann vom Hals,", "tokens": ["Viel\u00b7leicht", "bleibt", "sie", "mir", "dann", "vom", "Hals", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und alles andre ebenfalls.", "tokens": ["Und", "al\u00b7les", "and\u00b7re", "e\u00b7ben\u00b7falls", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PIS", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Ich schwur: Ich la\u00df sie nicht herein,", "tokens": ["Ich", "schwur", ":", "Ich", "la\u00df", "sie", "nicht", "her\u00b7ein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VVFIN", "PPER", "PTKNEG", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dies Zimmer ist doch, denk' ich, mein.", "tokens": ["Dies", "Zim\u00b7mer", "ist", "doch", ",", "denk'", "ich", ",", "mein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "ADV", "$,", "VVFIN", "PPER", "$,", "PPOSAT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Am liebsten wollt' ich mich verstecken,", "tokens": ["Am", "liebs\u00b7ten", "wollt'", "ich", "mich", "ver\u00b7ste\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "VMFIN", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Tauchte den Kopf ins Wasserbecken,", "tokens": ["Tauch\u00b7te", "den", "Kopf", "ins", "Was\u00b7ser\u00b7be\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPRART", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.43": {"line.1": {"text": "Doch mu\u00dft' ich bald wieder heraus,", "tokens": ["Doch", "mu\u00dft'", "ich", "bald", "wie\u00b7der", "he\u00b7raus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADV", "ADV", "PTKVZ", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Ich f\u00fchlte mich nicht ganz zu Haus.", "tokens": ["Ich", "f\u00fchl\u00b7te", "mich", "nicht", "ganz", "zu", "Haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.44": {"line.1": {"text": "Nichts hilft, dacht' ich, ich sage: Ja.", "tokens": ["Nichts", "hilft", ",", "dacht'", "ich", ",", "ich", "sa\u00b7ge", ":", "Ja", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PIS", "VVFIN", "$,", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "$.", "PTKANT", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Da stand sie in persona da,", "tokens": ["Da", "stand", "sie", "in", "per\u00b7so\u00b7na", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NE", "ADV", "$,"], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}}, "stanza.45": {"line.1": {"text": "Das Weib! O, das war viel, mein Gott!", "tokens": ["Das", "Weib", "!", "O", ",", "das", "war", "viel", ",", "mein", "Gott", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "NE", "$,", "PDS", "VAFIN", "ADV", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mir war's die erste Nacht in Rot.", "tokens": ["Mir", "wa\u00b7r's", "die", "ers\u00b7te", "Nacht", "in", "Rot", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "APPR", "NN", "$."], "meter": "--+-+-+-+", "measure": "anapaest.init"}}, "stanza.46": {"line.1": {"text": "Hatt' ich zwei Br\u00fcste je gef\u00fchlt?", "tokens": ["Hatt'", "ich", "zwei", "Br\u00fcs\u00b7te", "je", "ge\u00b7f\u00fchlt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "CARD", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nie wu\u00dfte ich, da\u00df Hitze k\u00fchlt,", "tokens": ["Nie", "wu\u00df\u00b7te", "ich", ",", "da\u00df", "Hit\u00b7ze", "k\u00fchlt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Mein Herz war eine Kanonade", "tokens": ["Mein", "Herz", "war", "ei\u00b7ne", "Ka\u00b7no\u00b7na\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und schlug durch alle Breitegrade.", "tokens": ["Und", "schlug", "durch", "al\u00b7le", "Brei\u00b7te\u00b7gra\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Wo war ich denn so lang gewesen?", "tokens": ["Wo", "war", "ich", "denn", "so", "lang", "ge\u00b7we\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und warum lernte man denn Lesen?", "tokens": ["Und", "wa\u00b7rum", "lern\u00b7te", "man", "denn", "Le\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "VVFIN", "PIS", "ADV", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Wenn's Leben doch, als Weib genommen,", "tokens": ["Wenn's", "Le\u00b7ben", "doch", ",", "als", "Weib", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "$,", "KOUS", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In allen Lagen s\u00fc\u00df vollkommen.", "tokens": ["In", "al\u00b7len", "La\u00b7gen", "s\u00fc\u00df", "voll\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Und K\u00fcsse sind ja reich erfunden,", "tokens": ["Und", "K\u00fcs\u00b7se", "sind", "ja", "reich", "er\u00b7fun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Steigend wie an der Uhr die Stunden.", "tokens": ["Stei\u00b7gend", "wie", "an", "der", "Uhr", "die", "Stun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.51": {"line.1": {"text": "Ich f\u00fchlte, da\u00df die Liebesnacht", "tokens": ["Ich", "f\u00fchl\u00b7te", ",", "da\u00df", "die", "Lie\u00b7bes\u00b7nacht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Noch vor dem Sch\u00f6pfungstag gemacht.", "tokens": ["Noch", "vor", "dem", "Sch\u00f6p\u00b7fungs\u00b7tag", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.52": {"line.1": {"text": "Sie ist es, die auf dieser Welt", "tokens": ["Sie", "ist", "es", ",", "die", "auf", "die\u00b7ser", "Welt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "$,", "PRELS", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erde und Mensch zusammenh\u00e4lt.", "tokens": ["Er\u00b7de", "und", "Mensch", "zu\u00b7sam\u00b7men\u00b7h\u00e4lt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.53": {"line.1": {"text": "Warum erf\u00e4hrt man das so sp\u00e4t,", "tokens": ["Wa\u00b7rum", "er\u00b7f\u00e4hrt", "man", "das", "so", "sp\u00e4t", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "ART", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was n\u00e4chtlich k\u00f6stlich vor sich geht?", "tokens": ["Was", "n\u00e4cht\u00b7lich", "k\u00f6st\u00b7lich", "vor", "sich", "geht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADJD", "APPR", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.54": {"line.1": {"text": "Wie kann es Krieg und Schauder geben,", "tokens": ["Wie", "kann", "es", "Krieg", "und", "Schau\u00b7der", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da doch die sch\u00f6nsten Frauen leben?", "tokens": ["Da", "doch", "die", "sch\u00f6ns\u00b7ten", "Frau\u00b7en", "le\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.55": {"line.1": {"text": "Was braucht ein Volk noch Religionen,", "tokens": ["Was", "braucht", "ein", "Volk", "noch", "Re\u00b7li\u00b7gi\u00b7o\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ART", "NN", "ADV", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn Mann und Weib im Himmel wohnen?", "tokens": ["Wenn", "Mann", "und", "Weib", "im", "Him\u00b7mel", "woh\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.56": {"line.1": {"text": "Nie schien mir eine Nacht so klar,", "tokens": ["Nie", "schien", "mir", "ei\u00b7ne", "Nacht", "so", "klar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Jetzt wu\u00dft' ich doch, weshalb ich war.", "tokens": ["Jetzt", "wu\u00dft'", "ich", "doch", ",", "we\u00b7shalb", "ich", "war", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PWAV", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.57": {"line.1": {"text": "Als Knab' war stets mein Bettgebet:", "tokens": ["Als", "Knab'", "war", "stets", "mein", "Bett\u00b7ge\u00b7bet", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VAFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gott gib, da\u00df ich nicht sterben t\u00e4t,", "tokens": ["Gott", "gib", ",", "da\u00df", "ich", "nicht", "ster\u00b7ben", "t\u00e4t", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVIMP", "$,", "KOUS", "PPER", "PTKNEG", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.58": {"line.1": {"text": "Eh nicht mein Blut einmal erfuhr", "tokens": ["Eh", "nicht", "mein", "Blut", "ein\u00b7mal", "er\u00b7fuhr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "PTKNEG", "PPOSAT", "NN", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Des Weibes Liebe in Natur.", "tokens": ["Des", "Wei\u00b7bes", "Lie\u00b7be", "in", "Na\u00b7tur", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.59": {"line.1": {"text": "Ich kann nicht gleich davon aufh\u00f6ren,", "tokens": ["Ich", "kann", "nicht", "gleich", "da\u00b7von", "auf\u00b7h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "ADV", "PAV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich mu\u00df noch etwas weiterschw\u00f6ren.", "tokens": ["Ich", "mu\u00df", "noch", "et\u00b7was", "wei\u00b7ter\u00b7schw\u00f6\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.60": {"line.1": {"text": "So hei\u00df mir nie ein Mantel war,", "tokens": ["So", "hei\u00df", "mir", "nie", "ein", "Man\u00b7tel", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "ADV", "ART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie in der Nacht des Weibes Haar,", "tokens": ["Wie", "in", "der", "Nacht", "des", "Wei\u00b7bes", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "ART", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.61": {"line.1": {"text": "Und K\u00fcsse lehrte sie mich viel,", "tokens": ["Und", "K\u00fcs\u00b7se", "lehr\u00b7te", "sie", "mich", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "PPER", "PRF", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Pointen bei dem Liebesspiel.", "tokens": ["Poin\u00b7ten", "bei", "dem", "Lie\u00b7bes\u00b7spiel", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.62": {"line.1": {"text": "Gelehriger kein Sch\u00fcler war", "tokens": ["Ge\u00b7leh\u00b7ri\u00b7ger", "kein", "Sch\u00fc\u00b7ler", "war"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PIAT", "NN", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als in der Nacht der Balthasar.", "tokens": ["Als", "in", "der", "Nacht", "der", "Balt\u00b7ha\u00b7sar", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.63": {"line.1": {"text": "Am Morgen wu\u00dft' ich gar nicht mehr,", "tokens": ["Am", "Mor\u00b7gen", "wu\u00dft'", "ich", "gar", "nicht", "mehr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ADV", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ob ich in meiner Haut noch w\u00e4r.", "tokens": ["Ob", "ich", "in", "mei\u00b7ner", "Haut", "noch", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPOSAT", "NN", "ADV", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.64": {"line.1": {"text": "Ich sagte mir: wie ich es seh,", "tokens": ["Ich", "sag\u00b7te", "mir", ":", "wie", "ich", "es", "seh", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$.", "PWAV", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Liegt ja Berlin noch an der Spree.", "tokens": ["Liegt", "ja", "Ber\u00b7lin", "noch", "an", "der", "Spree", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "NE", "ADV", "APPR", "ART", "NE", "$."], "meter": "+-++-+-+", "measure": "unknown.measure.penta"}}, "stanza.65": {"line.1": {"text": "Zum Spiegel trat ich dann schnell hin,", "tokens": ["Zum", "Spie\u00b7gel", "trat", "ich", "dann", "schnell", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil ich so gerne eitel bin,", "tokens": ["Weil", "ich", "so", "ger\u00b7ne", "ei\u00b7tel", "bin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.66": {"line.1": {"text": "Und sagte: \u00bbEi, da sieh mal an,", "tokens": ["Und", "sag\u00b7te", ":", "\u00bb", "Ei", ",", "da", "sieh", "mal", "an", ","], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "NN", "$,", "KOUS", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da drin steht Balthasar, der Mann.", "tokens": ["Da", "drin", "steht", "Balt\u00b7ha\u00b7sar", ",", "der", "Mann", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "NE", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.67": {"line.1": {"text": "Ich hoffe, da\u00df wir Freunde bleiben,", "tokens": ["Ich", "hof\u00b7fe", ",", "da\u00df", "wir", "Freun\u00b7de", "blei\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "M\u00e4nnlich sind wir, nicht zu beschreiben.", "tokens": ["M\u00e4nn\u00b7lich", "sind", "wir", ",", "nicht", "zu", "be\u00b7schrei\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "$,", "PTKNEG", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.68": {"line.1": {"text": "Rechne dem Vater hoch es an,", "tokens": ["Rech\u00b7ne", "dem", "Va\u00b7ter", "hoch", "es", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ADJD", "PPER", "PTKVZ", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Da\u00df er mich auf die Welt getan,", "tokens": ["Da\u00df", "er", "mich", "auf", "die", "Welt", "ge\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.69": {"line.1": {"text": "Und auch der Mutter in dem Grab", "tokens": ["Und", "auch", "der", "Mut\u00b7ter", "in", "dem", "Grab"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Send' ich mehr als den Dank hinab.\u00ab", "tokens": ["Send'", "ich", "mehr", "als", "den", "Dank", "hin\u00b7ab", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "KOKOM", "ART", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.70": {"line.1": {"text": "Und als mich dann Berlin begr\u00fc\u00dfte,", "tokens": ["Und", "als", "mich", "dann", "Ber\u00b7lin", "be\u00b7gr\u00fc\u00df\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "NE", "VVFIN", "$,"], "meter": "--+-++-+-", "measure": "anapaest.init"}, "line.2": {"text": "Kr\u00e4nkt's mich, da\u00df es nicht jeder w\u00fc\u00dfte.", "tokens": ["Kr\u00e4nkt's", "mich", ",", "da\u00df", "es", "nicht", "je\u00b7der", "w\u00fc\u00df\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "$,", "KOUS", "PPER", "PTKNEG", "PIS", "VVFIN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.71": {"line.1": {"text": "Die Menschen ich ganz anders sah,", "tokens": ["Die", "Men\u00b7schen", "ich", "ganz", "an\u00b7ders", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Man wu\u00dfte jetzt, was nachts geschah.", "tokens": ["Man", "wu\u00df\u00b7te", "jetzt", ",", "was", "nachts", "ge\u00b7schah", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "$,", "PRELS", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.72": {"line.1": {"text": "Ich fand, man macht zu wenig draus,", "tokens": ["Ich", "fand", ",", "man", "macht", "zu", "we\u00b7nig", "draus", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PIS", "VVFIN", "PTKA", "PIS", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Menschheit sah undankbar aus.", "tokens": ["Die", "Menschheit", "sah", "un\u00b7dank\u00b7bar", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.73": {"line.1": {"text": "Ich sah die Sonne kr\u00e4ftig an,", "tokens": ["Ich", "sah", "die", "Son\u00b7ne", "kr\u00e4f\u00b7tig", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchlte mich als \u00dcbermann.", "tokens": ["Und", "f\u00fchl\u00b7te", "mich", "als", "\u00dc\u00b7ber\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "KOUS", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}