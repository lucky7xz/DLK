{"textgrid.poem.53734": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Nur die Ruhe . . .", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn diese d\u00fcrren Bambusrohre bl\u00fchen,", "tokens": ["Wenn", "die\u00b7se", "d\u00fcr\u00b7ren", "Bam\u00b7bus\u00b7roh\u00b7re", "bl\u00fc\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und wenn die Schwalben in den Bienen nisten,", "tokens": ["Und", "wenn", "die", "Schwal\u00b7ben", "in", "den", "Bie\u00b7nen", "nis\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wenn dieser Stein das Schwimmen je erlernt \u2013", "tokens": ["Wenn", "die\u00b7ser", "Stein", "das", "Schwim\u00b7men", "je", "er\u00b7lernt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDAT", "NN", "ART", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Mags m\u00f6glich sein, vielleicht, da\u00df ich dich liebe!", "tokens": ["Mags", "m\u00f6g\u00b7lich", "sein", ",", "viel\u00b7leicht", ",", "da\u00df", "ich", "dich", "lie\u00b7be", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAINF", "$,", "ADV", "$,", "KOUS", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Wenn ein Beamter einmal h\u00f6flich ist,", "tokens": ["Wenn", "ein", "Be\u00b7am\u00b7ter", "ein\u00b7mal", "h\u00f6f\u00b7lich", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "und wenn die Deutschen keine Titel tragen,", "tokens": ["und", "wenn", "die", "Deut\u00b7schen", "kei\u00b7ne", "Ti\u00b7tel", "tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "wenn Redakteure dicke Gelder kriegen \u2013", "tokens": ["wenn", "Re\u00b7dak\u00b7teu\u00b7re", "di\u00b7cke", "Gel\u00b7der", "krie\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "mags m\u00f6glich sein, vielleicht, da\u00df ich dich liebe!", "tokens": ["mags", "m\u00f6g\u00b7lich", "sein", ",", "viel\u00b7leicht", ",", "da\u00df", "ich", "dich", "lie\u00b7be", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAINF", "$,", "ADV", "$,", "KOUS", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Wenn Kinohelden nicht in Fr\u00e4cken weinen,", "tokens": ["Wenn", "Ki\u00b7no\u00b7hel\u00b7den", "nicht", "in", "Fr\u00e4\u00b7cken", "wei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PTKNEG", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "und wenn die Edschmids nicht mit Weibern protzen,", "tokens": ["und", "wenn", "die", "E\u00b7dschmids", "nicht", "mit", "Wei\u00b7bern", "prot\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "PTKNEG", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "wenn Paul Cassirer immer billiger wird \u2013", "tokens": ["wenn", "Paul", "Cas\u00b7si\u00b7rer", "im\u00b7mer", "bil\u00b7li\u00b7ger", "wird", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "ADV", "ADJD", "VAFIN", "$("], "meter": "-+---+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "mags m\u00f6glich sein, vielleicht, da\u00df ich dich liebe!", "tokens": ["mags", "m\u00f6g\u00b7lich", "sein", ",", "viel\u00b7leicht", ",", "da\u00df", "ich", "dich", "lie\u00b7be", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAINF", "$,", "ADV", "$,", "KOUS", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Wenn unsre Richter wie die Menschen richten,", "tokens": ["Wenn", "uns\u00b7re", "Rich\u00b7ter", "wie", "die", "Men\u00b7schen", "rich\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "KOKOM", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "und wenn Herr Ge\u00dfler merkt, was um ihn vorgeht,", "tokens": ["und", "wenn", "Herr", "Ge\u00df\u00b7ler", "merkt", ",", "was", "um", "ihn", "vor\u00b7geht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "NN", "NE", "VVFIN", "$,", "PRELS", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "wenn Ebert sich besinnt, was er gewesen \u2013", "tokens": ["wenn", "E\u00b7bert", "sich", "be\u00b7sinnt", ",", "was", "er", "ge\u00b7we\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "PRF", "ADJD", "$,", "PWS", "PPER", "VAPP", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "mags m\u00f6glich sein, vielleicht, da\u00df ich dich liebe!", "tokens": ["mags", "m\u00f6g\u00b7lich", "sein", ",", "viel\u00b7leicht", ",", "da\u00df", "ich", "dich", "lie\u00b7be", "!"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAINF", "$,", "ADV", "$,", "KOUS", "PPER", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Mags m\u00f6glich sein, vielleicht, da\u00df ich dich liebe . . .", "tokens": ["Mags", "m\u00f6g\u00b7lich", "sein", ",", "viel\u00b7leicht", ",", "da\u00df", "ich", "dich", "lie\u00b7be", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADV", "ADJD", "VAINF", "$,", "ADV", "$,", "KOUS", "PPER", "PRF", "VVFIN", "$.", "$.", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Doch bis das Land den Anschlu\u00df wiederfindet", "tokens": ["Doch", "bis", "das", "Land", "den", "An\u00b7schlu\u00df", "wie\u00b7der\u00b7fin\u00b7det"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "an Welt, Vernunft und an die Kontinente \u2013:", "tokens": ["an", "Welt", ",", "Ver\u00b7nunft", "und", "an", "die", "Kon\u00b7ti\u00b7nen\u00b7te", "\u2013", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "APPR", "ART", "NN", "$(", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "H\u00e4lt Stresemann noch manche wilde Rede,", "tokens": ["H\u00e4lt", "Stre\u00b7se\u00b7mann", "noch", "man\u00b7che", "wil\u00b7de", "Re\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "f\u00e4llt um die SPD noch manches Mal,", "tokens": ["f\u00e4llt", "um", "die", "SpD", "noch", "man\u00b7ches", "Mal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "ADV", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "verliert so manche Jungfrau manche Sachen,", "tokens": ["ver\u00b7liert", "so", "man\u00b7che", "Jung\u00b7frau", "man\u00b7che", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PIAT", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "br\u00fcllt mancher deutsche Mann: \u00bbWir waren nicht schuld \u2013!\u00ab", "tokens": ["br\u00fcllt", "man\u00b7cher", "deut\u00b7sche", "Mann", ":", "\u00bb", "Wir", "wa\u00b7ren", "nicht", "schuld", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["VVFIN", "PIAT", "ADJA", "NN", "$.", "$(", "PPER", "VAFIN", "PTKNEG", "ADJD", "$(", "$.", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.5": {"text": "Drum hab geliebtes Wesen, bis Ermatten", "tokens": ["Drum", "hab", "ge\u00b7lieb\u00b7tes", "We\u00b7sen", ",", "bis", "Er\u00b7mat\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["PAV", "VAFIN", "ADJA", "NN", "$,", "KOUS", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "erfolgt, das, was die preu\u00dfischen Soldaten hatten:", "tokens": ["er\u00b7folgt", ",", "das", ",", "was", "die", "preu\u00b7\u00dfi\u00b7schen", "Sol\u00b7da\u00b7ten", "hat\u00b7ten", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PDS", "$,", "PRELS", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Geduld.", "tokens": ["Ge\u00b7duld", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "-+", "measure": "iambic.single"}}}}}