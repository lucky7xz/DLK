{"textgrid.poem.52913": {"metadata": {"author": {"name": "Dingelstedt, Franz von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Zur Zeit des Sturmes, so wie heute,", "genre": "verse", "period": "N.A.", "pub_year": 1847, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Zur Zeit des Sturmes, so wie heute,", "tokens": ["Zur", "Zeit", "des", "Stur\u00b7mes", ",", "so", "wie", "heu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,", "ADV", "KOKOM", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da sondert sich vom Korn die Spreu,", "tokens": ["Da", "son\u00b7dert", "sich", "vom", "Korn", "die", "Spreu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da lernt man kennen seine Leute,", "tokens": ["Da", "lernt", "man", "ken\u00b7nen", "sei\u00b7ne", "Leu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die meisten falsch, nur wenig' treu.", "tokens": ["Die", "meis\u00b7ten", "falsch", ",", "nur", "we\u00b7nig'", "treu", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADJD", "$,", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Auch du hast in den letzten Jahren,", "tokens": ["Auch", "du", "hast", "in", "den", "letz\u00b7ten", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Hart heimgesuchtes deutsches Land,", "tokens": ["Hart", "heim\u00b7ge\u00b7such\u00b7tes", "deut\u00b7sches", "Land", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Manch schmerzlichen Verlust erfahren", "tokens": ["Manch", "schmerz\u00b7li\u00b7chen", "Ver\u00b7lust", "er\u00b7fah\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["PIAT", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und eingeb\u00fc\u00dft manch wackre Hand.", "tokens": ["Und", "ein\u00b7ge\u00b7b\u00fc\u00dft", "manch", "wack\u00b7re", "Hand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Doch stehn als einsam letzte St\u00fctzen", "tokens": ["Doch", "stehn", "als", "ein\u00b7sam", "letz\u00b7te", "St\u00fct\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "KOKOM", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Noch viele M\u00e4nner stark und fest,", "tokens": ["Noch", "vie\u00b7le", "M\u00e4n\u00b7ner", "stark", "und", "fest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "ADJD", "KON", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die, unerm\u00fcdet dir zu n\u00fctzen,", "tokens": ["Die", ",", "un\u00b7er\u00b7m\u00fc\u00b7det", "dir", "zu", "n\u00fct\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "ADJD", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ausharren bis zum einst'gen Rest,", "tokens": ["Aus\u00b7har\u00b7ren", "bis", "zum", "einst'\u00b7gen", "Rest", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Die treu dem abgelegten Eide", "tokens": ["Die", "treu", "dem", "ab\u00b7ge\u00b7leg\u00b7ten", "Ei\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verfechten dein geweihtes Recht,", "tokens": ["Ver\u00b7fech\u00b7ten", "dein", "ge\u00b7weih\u00b7tes", "Recht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und hoch ob allem Ha\u00df und Neide", "tokens": ["Und", "hoch", "ob", "al\u00b7lem", "Ha\u00df", "und", "Nei\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "KOUS", "PIS", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Fortstreben, ein Hero'ngeschlecht.", "tokens": ["Fort\u00b7stre\u00b7ben", ",", "ein", "He\u00b7ro'\u00b7nge\u00b7schlecht", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Das ist der Deutschen wahre Einheit,", "tokens": ["Das", "ist", "der", "Deut\u00b7schen", "wah\u00b7re", "Ein\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das ihres Volkes bester Halt,", "tokens": ["Das", "ih\u00b7res", "Vol\u00b7kes", "bes\u00b7ter", "Halt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "M\u00e4nner von strenger Sitten-Reinheit", "tokens": ["M\u00e4n\u00b7ner", "von", "stren\u00b7ger", "Sit\u00b7ten\u00b7Rein\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Und von Gesinnungs-Allgewalt.", "tokens": ["Und", "von", "Ge\u00b7sin\u00b7nungs\u00b7All\u00b7ge\u00b7walt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Sie wissen einer kaum vom andern,", "tokens": ["Sie", "wis\u00b7sen", "ei\u00b7ner", "kaum", "vom", "an\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADV", "APPRART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie stehn vereinzelt, unbekannt,", "tokens": ["Sie", "stehn", "ver\u00b7ein\u00b7zelt", ",", "un\u00b7be\u00b7kannt", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "VVPP", "$,", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch ihre Wort' und Werke wandern,", "tokens": ["Doch", "ih\u00b7re", "Wort'", "und", "Wer\u00b7ke", "wan\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Elektrisch z\u00fcndend, durch das Land.", "tokens": ["E\u00b7lekt\u00b7risch", "z\u00fcn\u00b7dend", ",", "durch", "das", "Land", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Me\u00dft sie nicht an der Kr\u00e4mer-Elle,", "tokens": ["Me\u00dft", "sie", "nicht", "an", "der", "Kr\u00e4\u00b7mer\u00b7El\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Lobt und verdammt sie nicht zumal,", "tokens": ["Lobt", "und", "ver\u00b7dammt", "sie", "nicht", "zu\u00b7mal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "PPER", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nennt sie nicht Konstitutionelle,", "tokens": ["Nennt", "sie", "nicht", "Kons\u00b7ti\u00b7tu\u00b7ti\u00b7o\u00b7nel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "NN", "$,"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Noch minder ultra-liberal!", "tokens": ["Noch", "min\u00b7der", "ul\u00b7tra\u00b7li\u00b7be\u00b7ral", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}}, "stanza.8": {"line.1": {"text": "Es sind nur eben deutsche Herzen,", "tokens": ["Es", "sind", "nur", "e\u00b7ben", "deut\u00b7sche", "Her\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den' nichts fehlt, als ein deutscher Arm,", "tokens": ["Den'", "nichts", "fehlt", ",", "als", "ein", "deut\u00b7scher", "Arm", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PIS", "VVFIN", "$,", "KOUS", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "F\u00fcr eines Volkes Wohl und Schmerzen", "tokens": ["F\u00fcr", "ei\u00b7nes", "Vol\u00b7kes", "Wohl", "und", "Schmer\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADV", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "In einer k\u00fchlen Zeit noch warm;", "tokens": ["In", "ei\u00b7ner", "k\u00fch\u00b7len", "Zeit", "noch", "warm", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Und sind nur eben deutsche Geister,", "tokens": ["Und", "sind", "nur", "e\u00b7ben", "deut\u00b7sche", "Geis\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "F\u00fcr Freiheit, Recht und Licht entbrannt,", "tokens": ["F\u00fcr", "Frei\u00b7heit", ",", "Recht", "und", "Licht", "ent\u00b7brannt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die in der Wahrheit ihren Meister,", "tokens": ["Die", "in", "der", "Wahr\u00b7heit", "ih\u00b7ren", "Meis\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Tyrannen in der L\u00fcg' erkannt.", "tokens": ["Ty\u00b7ran\u00b7nen", "in", "der", "L\u00fcg'", "er\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Seht dort im S\u00fcden, hier im Norden,", "tokens": ["Seht", "dort", "im", "S\u00fc\u00b7den", ",", "hier", "im", "Nor\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPRART", "NN", "$,", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zerstreut wie Sterne, stehn sie da,", "tokens": ["Zer\u00b7streut", "wie", "Ster\u00b7ne", ",", "stehn", "sie", "da", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOKOM", "NN", "$,", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Noch ist die Nacht nicht Tag geworden,", "tokens": ["Noch", "ist", "die", "Nacht", "nicht", "Tag", "ge\u00b7wor\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PTKNEG", "NN", "VAPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Allein, allein \u2013 der Morgen nah!", "tokens": ["Al\u00b7lein", ",", "al\u00b7lein", "\u2013", "der", "Mor\u00b7gen", "nah", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADV", "$(", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Wer zweifelt, da\u00df es tagen werde,", "tokens": ["Wer", "zwei\u00b7felt", ",", "da\u00df", "es", "ta\u00b7gen", "wer\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "PPER", "VVINF", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der schaue sich die Sterne an;", "tokens": ["Der", "schau\u00b7e", "sich", "die", "Ster\u00b7ne", "an", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Apostel f\u00fcr die deutsche Erde", "tokens": ["A\u00b7pos\u00b7tel", "f\u00fcr", "die", "deut\u00b7sche", "Er\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ist ja ein jeder solcher Mann!", "tokens": ["Ist", "ja", "ein", "je\u00b7der", "sol\u00b7cher", "Mann", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "PIAT", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Auf, auf! Ein Chor zu ihrem Preise,", "tokens": ["Auf", ",", "auf", "!", "Ein", "Chor", "zu", "ih\u00b7rem", "Prei\u00b7se", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$,", "PTKVZ", "$.", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Flistern und kein P\u00f6belschrein,", "tokens": ["Kein", "Flis\u00b7tern", "und", "kein", "P\u00f6\u00b7bel\u00b7schrein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Nicht nach der Marseillaise Weise", "tokens": ["Nicht", "nach", "der", "Mar\u00b7seil\u00b7laise", "Wei\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PTKNEG", "APPR", "ART", "NN", "NN"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Und auch nicht nach dem Lied vom Rhein!", "tokens": ["Und", "auch", "nicht", "nach", "dem", "Lied", "vom", "Rhein", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "APPR", "ART", "NN", "APPRART", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Ein Hoch, das sie uns nicht verwehren,", "tokens": ["Ein", "Hoch", ",", "das", "sie", "uns", "nicht", "ver\u00b7weh\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Darin kein Namen und kein Stand,", "tokens": ["Da\u00b7rin", "kein", "Na\u00b7men", "und", "kein", "Stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIAT", "NN", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den Helden nah und fern zu Ehren,", "tokens": ["Den", "Hel\u00b7den", "nah", "und", "fern", "zu", "Eh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "KON", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Den letzten f\u00fcr ihr Vaterland!", "tokens": ["Den", "letz\u00b7ten", "f\u00fcr", "ihr", "Va\u00b7ter\u00b7land", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}