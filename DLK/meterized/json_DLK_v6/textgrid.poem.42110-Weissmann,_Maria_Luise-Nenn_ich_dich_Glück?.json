{"textgrid.poem.42110": {"metadata": {"author": {"name": "Weissmann, Maria Luise", "birth": "N.A.", "death": "N.A."}, "title": "Nenn ich dich Gl\u00fcck?", "genre": "verse", "period": "N.A.", "pub_year": 1914, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Nenn ich dich Gl\u00fcck? Entsetzen? nenn ich dich", "tokens": ["Nenn", "ich", "dich", "Gl\u00fcck", "?", "Ent\u00b7set\u00b7zen", "?", "nenn", "ich", "dich"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "NN", "$.", "NN", "$.", "VVFIN", "PPER", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Heil oder Folter? Ich wei\u00df keinen Namen", "tokens": ["Heil", "o\u00b7der", "Fol\u00b7ter", "?", "Ich", "wei\u00df", "kei\u00b7nen", "Na\u00b7men"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "KON", "NN", "$.", "PPER", "VVFIN", "PIAT", "NN"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Zu fassen dich; ich f\u00fcgte keinen Rahmen", "tokens": ["Zu", "fas\u00b7sen", "dich", ";", "ich", "f\u00fcg\u00b7te", "kei\u00b7nen", "Rah\u00b7men"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "PPER", "$.", "PPER", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Um dich, daraus dein Bild nicht l\u00f6ste sich", "tokens": ["Um", "dich", ",", "da\u00b7raus", "dein", "Bild", "nicht", "l\u00f6s\u00b7te", "sich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["KOUI", "PPER", "$,", "PAV", "PPOSAT", "NN", "PTKNEG", "VVFIN", "PRF"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Und schritt davon.", "tokens": ["Und", "schritt", "da\u00b7von", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PAV", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "Ich habe dich zu halten", "tokens": ["Ich", "ha\u00b7be", "dich", "zu", "hal\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "PTKZU", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.7": {"text": "Vermocht in keiner einzigen Gestalt.", "tokens": ["Ver\u00b7mocht", "in", "kei\u00b7ner", "ein\u00b7zi\u00b7gen", "Ge\u00b7stalt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Ich griff mit einer innigen Gewalt", "tokens": ["Ich", "griff", "mit", "ei\u00b7ner", "in\u00b7ni\u00b7gen", "Ge\u00b7walt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Und was ich griff, l\u00e4chelte schon gespalten...", "tokens": ["Und", "was", "ich", "griff", ",", "l\u00e4\u00b7chel\u00b7te", "schon", "ge\u00b7spal\u00b7ten", "..."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$,", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Du bist so weise dich stets zu entwinden", "tokens": ["Du", "bist", "so", "wei\u00b7se", "dich", "stets", "zu", "ent\u00b7win\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "VVFIN", "PPER", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Aus meinen Worten, meinem Blick, der Hand,", "tokens": ["Aus", "mei\u00b7nen", "Wor\u00b7ten", ",", "mei\u00b7nem", "Blick", ",", "der", "Hand", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Da\u00df ich schon oft vermeinte dich zu finden,", "tokens": ["Da\u00df", "ich", "schon", "oft", "ver\u00b7mein\u00b7te", "dich", "zu", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVFIN", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wenn ich fand", "tokens": ["Wenn", "ich", "fand"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "PPER", "VVFIN"], "meter": "+-+", "measure": "trochaic.di"}}}}}