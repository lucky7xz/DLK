{"textgrid.poem.32531": {"metadata": {"author": {"name": "Lessing, Gotthold Ephraim", "birth": "N.A.", "death": "N.A."}, "title": "XiI. Der Eremit", "genre": "verse", "period": "N.A.", "pub_year": 1755, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Im Walde nah bei einer Stadt,", "tokens": ["Im", "Wal\u00b7de", "nah", "bei", "ei\u00b7ner", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die man mir nicht genennet hat,", "tokens": ["Die", "man", "mir", "nicht", "ge\u00b7nen\u00b7net", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PPER", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Lie\u00df einst ein seltenes Gefieder,", "tokens": ["Lie\u00df", "einst", "ein", "sel\u00b7te\u00b7nes", "Ge\u00b7fie\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein junger Eremit sich nieder.", "tokens": ["Ein", "jun\u00b7ger", "E\u00b7re\u00b7mit", "sich", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PRF", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "\u00bbin einer Stadt, denkt Applikant,", "tokens": ["\u00bb", "in", "ei\u00b7ner", "Stadt", ",", "denkt", "Ap\u00b7pli\u00b7kant", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "APPR", "ART", "NN", "$,", "VVFIN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die man ihm nicht genannt?", "tokens": ["Die", "man", "ihm", "nicht", "ge\u00b7nannt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Was mu\u00df er wohl f\u00fcr eine meinen?", "tokens": ["Was", "mu\u00df", "er", "wohl", "f\u00fcr", "ei\u00b7ne", "mei\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "ADV", "APPR", "ART", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bei nahe sollte mir es scheinen,", "tokens": ["Bei", "na\u00b7he", "soll\u00b7te", "mir", "es", "schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "VMFIN", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Da\u00df die, \u2013 nein die \u2013 gemeinet w\u00e4r.\u00ab", "tokens": ["Da\u00df", "die", ",", "\u2013", "nein", "die", "\u2013", "ge\u00b7mei\u00b7net", "w\u00e4r", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["KOUS", "ART", "$,", "$(", "PTKANT", "ART", "$(", "VVPP", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kurz Applikant denkt hin und her,", "tokens": ["Kurz", "Ap\u00b7pli\u00b7kant", "denkt", "hin", "und", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "VVFIN", "PTKVZ", "KON", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Und schlie\u00dft, noch eh er mich gelesen,", "tokens": ["Und", "schlie\u00dft", ",", "noch", "eh", "er", "mich", "ge\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ADV", "KOUS", "PPER", "PRF", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Es sei gewi\u00df Berlin gewesen.", "tokens": ["Es", "sei", "ge\u00b7wi\u00df", "Ber\u00b7lin", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NE", "VAPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "\u00bbberlin? Ja, ja, das sieht man bald;", "tokens": ["\u00bb", "ber\u00b7lin", "?", "Ja", ",", "ja", ",", "das", "sieht", "man", "bald", ";"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$.", "PTKANT", "$,", "PTKANT", "$,", "PDS", "VVFIN", "PIS", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn bei Berlin ist ja ein Wald.\u00ab", "tokens": ["Denn", "bei", "Ber\u00b7lin", "ist", "ja", "ein", "Wald", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "NE", "VAFIN", "ADV", "ART", "NN", "$.", "$("], "meter": "+-++-+-+", "measure": "unknown.measure.penta"}}, "stanza.4": {"line.1": {"text": "Der Schlu\u00df ist stark, bei meiner Ehre:", "tokens": ["Der", "Schlu\u00df", "ist", "stark", ",", "bei", "mei\u00b7ner", "Eh\u00b7re", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich dachte nicht, da\u00df es so deutlich w\u00e4re.", "tokens": ["Ich", "dach\u00b7te", "nicht", ",", "da\u00df", "es", "so", "deut\u00b7lich", "w\u00e4\u00b7re", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Der Wald pa\u00dft herrlich auf Berlin,", "tokens": ["Der", "Wald", "pa\u00dft", "herr\u00b7lich", "auf", "Ber\u00b7lin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ohn' ihn beim Haar' herbei zu ziehn.", "tokens": ["Ohn'", "ihn", "beim", "Haa\u00b7r'", "her\u00b7bei", "zu", "ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPRART", "NN", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "Und ob das \u00fcbrige wird passen,", "tokens": ["Und", "ob", "das", "\u00fcb\u00b7ri\u00b7ge", "wird", "pas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "ADJA", "VAFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Will ich dem Leser \u00fcberlassen.", "tokens": ["Will", "ich", "dem", "Le\u00b7ser", "\u00fc\u00b7ber\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Auf griechisch wei\u00df ich, wie sie hie\u00df;", "tokens": ["Auf", "grie\u00b7chisch", "wei\u00df", "ich", ",", "wie", "sie", "hie\u00df", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "VVFIN", "PPER", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Doch wer verstehts? Kerapolis.", "tokens": ["Doch", "wer", "ver\u00b7stehts", "?", "Ke\u00b7ra\u00b7po\u00b7lis", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "$.", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Hier, nahe bei Kerapolis,", "tokens": ["Hier", ",", "na\u00b7he", "bei", "Ke\u00b7ra\u00b7po\u00b7lis", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADJD", "APPR", "NE", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Wars, wo ein junger Eremite,", "tokens": ["Wars", ",", "wo", "ein", "jun\u00b7ger", "E\u00b7re\u00b7mi\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PWAV", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "In einer kleinen leeren H\u00fctte,", "tokens": ["In", "ei\u00b7ner", "klei\u00b7nen", "lee\u00b7ren", "H\u00fct\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Im dicksten Wald sich niederlie\u00df.", "tokens": ["Im", "dicks\u00b7ten", "Wald", "sich", "nie\u00b7der\u00b7lie\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Was je ein Eremit getan,", "tokens": ["Was", "je", "ein", "E\u00b7re\u00b7mit", "ge\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Fing er mit gr\u00f6\u00dftem Eifer an.", "tokens": ["Fing", "er", "mit", "gr\u00f6\u00df\u00b7tem", "Ei\u00b7fer", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Er betete, er sang, er schrie,", "tokens": ["Er", "be\u00b7te\u00b7te", ",", "er", "sang", ",", "er", "schrie", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Des Tags, des Nachts, und sp\u00e4t und fr\u00fch.", "tokens": ["Des", "Tags", ",", "des", "Nachts", ",", "und", "sp\u00e4t", "und", "fr\u00fch", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADV", "$,", "KON", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Er a\u00df kein Fleisch, er trank nicht Wein,", "tokens": ["Er", "a\u00df", "kein", "Fleisch", ",", "er", "trank", "nicht", "Wein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "NN", "$,", "PPER", "VVFIN", "PTKNEG", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Lie\u00df Wurzeln seine Nahrung sein,", "tokens": ["Lie\u00df", "Wur\u00b7zeln", "sei\u00b7ne", "Nah\u00b7rung", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "PPOSAT", "NN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Und seinen Trank das helle Wasser;", "tokens": ["Und", "sei\u00b7nen", "Trank", "das", "hel\u00b7le", "Was\u00b7ser", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "Bei allem Appetit kein Prasser.", "tokens": ["Bei", "al\u00b7lem", "Ap\u00b7pe\u00b7tit", "kein", "Pras\u00b7ser", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Er gei\u00dfelte sich bis aufs Blut,", "tokens": ["Er", "gei\u00b7\u00dfel\u00b7te", "sich", "bis", "aufs", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "APPRART", "NN", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.14": {"text": "Und wu\u00dfte wie das Wachen tut.", "tokens": ["Und", "wu\u00df\u00b7te", "wie", "das", "Wa\u00b7chen", "tut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "KOKOM", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Er fastete wohl ganze Tage,", "tokens": ["Er", "fas\u00b7te\u00b7te", "wohl", "gan\u00b7ze", "Ta\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Und blieb auf Einem Fu\u00dfe stehn;", "tokens": ["Und", "blieb", "auf", "Ei\u00b7nem", "Fu\u00b7\u00dfe", "stehn", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Und machte sich rechtschaffne Plage,", "tokens": ["Und", "mach\u00b7te", "sich", "recht\u00b7schaff\u00b7ne", "Pla\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "In Himmel m\u00fchsam einzugehn.", "tokens": ["In", "Him\u00b7mel", "m\u00fch\u00b7sam", "ein\u00b7zu\u00b7gehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJD", "VVIZU", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "Was Wunder also, da\u00df gar bald", "tokens": ["Was", "Wun\u00b7der", "al\u00b7so", ",", "da\u00df", "gar", "bald"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "NN", "ADV", "$,", "KOUS", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Vom jungen Heiligen im Wald", "tokens": ["Vom", "jun\u00b7gen", "Hei\u00b7li\u00b7gen", "im", "Wald"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "Der Ruf bis in die Stadt erschallt?", "tokens": ["Der", "Ruf", "bis", "in", "die", "Stadt", "er\u00b7schallt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Die erste, die aus dieser Stadt", "tokens": ["Die", "ers\u00b7te", ",", "die", "aus", "die\u00b7ser", "Stadt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "$,", "PRELS", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu ihm die heil'ge Wallfahrt tat,", "tokens": ["Zu", "ihm", "die", "heil'\u00b7ge", "Wall\u00b7fahrt", "tat", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "War ein betagtes Weib.", "tokens": ["War", "ein", "be\u00b7tag\u00b7tes", "Weib", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Auf Kr\u00fccken, zitternd, kam sie an,", "tokens": ["Auf", "Kr\u00fc\u00b7cken", ",", "zit\u00b7ternd", ",", "kam", "sie", "an", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "VVPP", "$,", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und fand den wilden Gottesmann,", "tokens": ["Und", "fand", "den", "wil\u00b7den", "Got\u00b7tes\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Der sie von weitem kommen sahe,", "tokens": ["Der", "sie", "von", "wei\u00b7tem", "kom\u00b7men", "sa\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PIS", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Dem h\u00f6lzern Kreuze knieend nahe.", "tokens": ["Dem", "h\u00f6l\u00b7zern", "Kreu\u00b7ze", "kni\u00b7e\u00b7end", "na\u00b7he", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "PTKVZ", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Je n\u00e4her sie ihm k\u00f6mmt, je mehr", "tokens": ["Je", "n\u00e4\u00b7her", "sie", "ihm", "k\u00f6mmt", ",", "je", "mehr"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "PPER", "PPER", "VVFIN", "$,", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Schl\u00e4gt er die Brust, und weint, und winselt er,", "tokens": ["Schl\u00e4gt", "er", "die", "Brust", ",", "und", "weint", ",", "und", "win\u00b7selt", "er", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "$,", "KON", "VVFIN", "$,", "KON", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.10": {"text": "Und wie es sich f\u00fcr einen Heil'gen schicket,", "tokens": ["Und", "wie", "es", "sich", "f\u00fcr", "ei\u00b7nen", "Heil'\u00b7gen", "schi\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "PRF", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Erblickt sie nicht, ob er sie gleich erblicket.", "tokens": ["Er\u00b7blickt", "sie", "nicht", ",", "ob", "er", "sie", "gleich", "er\u00b7bli\u00b7cket", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "$,", "KOUS", "PPER", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Bis er zuletzt vom Knieen matt,", "tokens": ["Bis", "er", "zu\u00b7letzt", "vom", "Kni\u00b7e\u00b7en", "matt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.13": {"text": "Und heiliger Verstellung satt,", "tokens": ["Und", "hei\u00b7li\u00b7ger", "Ver\u00b7stel\u00b7lung", "satt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Vom Fasten, Kreuz'gen, Klosterleben,", "tokens": ["Vom", "Fas\u00b7ten", ",", "Kreuz'\u00b7gen", ",", "Klos\u00b7ter\u00b7le\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPRART", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Marienbildern, Opfergeben,", "tokens": ["Ma\u00b7ri\u00b7en\u00b7bil\u00b7dern", ",", "Op\u00b7fer\u00b7ge\u00b7ben", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Von Beichte, Salbung, Seelenmessen,", "tokens": ["Von", "Beich\u00b7te", ",", "Sal\u00b7bung", ",", "See\u00b7len\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Ohn' das Verm\u00e4chtnis zu vergessen,", "tokens": ["Ohn'", "das", "Ver\u00b7m\u00e4cht\u00b7nis", "zu", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "Von Rosenkr\u00e4nzen mit ihr redte,", "tokens": ["Von", "Ro\u00b7sen\u00b7kr\u00e4n\u00b7zen", "mit", "ihr", "red\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Und das so oratorisch sagt,", "tokens": ["Und", "das", "so", "o\u00b7ra\u00b7to\u00b7risch", "sagt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Da\u00df sie erb\u00e4rmlich weint und klagt,", "tokens": ["Da\u00df", "sie", "er\u00b7b\u00e4rm\u00b7lich", "weint", "und", "klagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "Als ob er sie gepr\u00fcgelt h\u00e4tte.", "tokens": ["Als", "ob", "er", "sie", "ge\u00b7pr\u00fc\u00b7gelt", "h\u00e4t\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "Zum Schlu\u00df bricht sie von seiner H\u00fctte,", "tokens": ["Zum", "Schlu\u00df", "bricht", "sie", "von", "sei\u00b7ner", "H\u00fct\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.23": {"text": "Wozu der saure Eremite", "tokens": ["Wo\u00b7zu", "der", "sau\u00b7re", "E\u00b7re\u00b7mi\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.24": {"text": "Mit Not ihr die Erlaubnis gab,", "tokens": ["Mit", "Not", "ihr", "die", "Er\u00b7laub\u00b7nis", "gab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.25": {"text": "Sich einen heil'gen Splitter ab,", "tokens": ["Sich", "ei\u00b7nen", "heil'\u00b7gen", "Split\u00b7ter", "ab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ART", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.26": {"text": "Den sie bek\u00fcsset und belecket,", "tokens": ["Den", "sie", "be\u00b7k\u00fcs\u00b7set", "und", "be\u00b7le\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.27": {"text": "Und in den welken Busen stecket.", "tokens": ["Und", "in", "den", "wel\u00b7ken", "Bu\u00b7sen", "ste\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.28": {"text": "Mit diesem Schatz von Heiligkeit", "tokens": ["Mit", "die\u00b7sem", "Schatz", "von", "Hei\u00b7lig\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.29": {"text": "Kehrt sie zur\u00fcck begnadigt und erfreut,", "tokens": ["Kehrt", "sie", "zu\u00b7r\u00fcck", "be\u00b7gna\u00b7digt", "und", "er\u00b7freut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PTKVZ", "VVPP", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.30": {"text": "Und l\u00e4\u00dft daheim die fr\u00f6mmsten Frauen", "tokens": ["Und", "l\u00e4\u00dft", "da\u00b7heim", "die", "fr\u00f6mms\u00b7ten", "Frau\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.31": {"text": "Ihn k\u00fcssen, andre nur beschauen.", "tokens": ["Ihn", "k\u00fcs\u00b7sen", ",", "and\u00b7re", "nur", "be\u00b7schau\u00b7en", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.32": {"text": "Sie ging zugleich von Haus zu Haus,", "tokens": ["Sie", "ging", "zu\u00b7gleich", "von", "Haus", "zu", "Haus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.33": {"text": "Und rief auf allen Gassen aus:", "tokens": ["Und", "rief", "auf", "al\u00b7len", "Gas\u00b7sen", "aus", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.34": {"text": "\u00bbder ist verloren und verflucht,", "tokens": ["\u00bb", "der", "ist", "ver\u00b7lo\u00b7ren", "und", "ver\u00b7flucht", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "VAFIN", "VVPP", "KON", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.35": {"text": "Der unsern Eremiten nicht besucht!\u00ab", "tokens": ["Der", "un\u00b7sern", "E\u00b7re\u00b7mi\u00b7ten", "nicht", "be\u00b7sucht", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "PPOSAT", "NN", "PTKNEG", "VVPP", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.36": {"text": "Und brachte hundert Gr\u00fcnde bei,", "tokens": ["Und", "brach\u00b7te", "hun\u00b7dert", "Gr\u00fcn\u00b7de", "bei", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "CARD", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.37": {"text": "Warum es sonderlich den Weibern n\u00fctzlich sei.", "tokens": ["Wa\u00b7rum", "es", "son\u00b7der\u00b7lich", "den", "Wei\u00b7bern", "n\u00fctz\u00b7lich", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADJD", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Ein altes Weib kann Eindruck machen;", "tokens": ["Ein", "al\u00b7tes", "Weib", "kann", "Ein\u00b7druck", "ma\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zum Weinen bei der Frau, und bei dem Mann zum Lachen.", "tokens": ["Zum", "Wei\u00b7nen", "bei", "der", "Frau", ",", "und", "bei", "dem", "Mann", "zum", "La\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "ART", "NN", "$,", "KON", "APPR", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Zwar ist der Satz nicht allgemein;", "tokens": ["Zwar", "ist", "der", "Satz", "nicht", "all\u00b7ge\u00b7mein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auch M\u00e4nner k\u00f6nnen Weiber sein.", "tokens": ["Auch", "M\u00e4n\u00b7ner", "k\u00f6n\u00b7nen", "Wei\u00b7ber", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VMFIN", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Doch diesmal waren sie es nicht.", "tokens": ["Doch", "dies\u00b7mal", "wa\u00b7ren", "sie", "es", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Weiber schienen nur erpicht,", "tokens": ["Die", "Wei\u00b7ber", "schie\u00b7nen", "nur", "er\u00b7picht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Den teuern Waldseraph zu sehen.", "tokens": ["Den", "teu\u00b7ern", "Wald\u00b7se\u00b7raph", "zu", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die M\u00e4nner aber? \u2013 wehrtens nicht,", "tokens": ["Die", "M\u00e4n\u00b7ner", "a\u00b7ber", "?", "\u2013", "wehr\u00b7tens", "nicht", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$.", "$(", "ADV", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Und lie\u00dfen ihre Weiber gehen.", "tokens": ["Und", "lie\u00b7\u00dfen", "ih\u00b7re", "Wei\u00b7ber", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Die H\u00e4\u00dflichen und Sch\u00f6nen,", "tokens": ["Die", "H\u00e4\u00df\u00b7li\u00b7chen", "und", "Sch\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.11": {"text": "Die \u00e4ltesten und j\u00fcngsten Frauen,", "tokens": ["Die", "\u00e4l\u00b7tes\u00b7ten", "und", "j\u00fcng\u00b7sten", "Frau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "Das arme wie das reiche Weib, \u2013", "tokens": ["Das", "ar\u00b7me", "wie", "das", "rei\u00b7che", "Weib", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "KOKOM", "ART", "ADJA", "NN", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Kurz jede ging, sich zu erbauen,", "tokens": ["Kurz", "je\u00b7de", "ging", ",", "sich", "zu", "er\u00b7bau\u00b7en", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "PIAT", "VVFIN", "$,", "PRF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Und jede fand erw\u00fcnschten Zeitvertreib.", "tokens": ["Und", "je\u00b7de", "fand", "er\u00b7w\u00fcnschten", "Zeit\u00b7ver\u00b7treib", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "\u00bbwas? Zeitvertreib, wo man erbauen will?", "tokens": ["\u00bb", "was", "?", "Zeit\u00b7ver\u00b7treib", ",", "wo", "man", "er\u00b7bau\u00b7en", "will", "?"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "$.", "NN", "$,", "PWAV", "PIS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Was soll der Widerspruch bedeuten?\u00ab", "tokens": ["Was", "soll", "der", "Wi\u00b7der\u00b7spruch", "be\u00b7deu\u00b7ten", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Widerspruch? Das w\u00e4re viel!", "tokens": ["Ein", "Wi\u00b7der\u00b7spruch", "?", "Das", "w\u00e4\u00b7re", "viel", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PDS", "VAFIN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bber sprach ja sonst von lauter Seligkeiten!\u00ab \u2013", "tokens": ["\u00bb", "er", "sprach", "ja", "sonst", "von", "lau\u00b7ter", "Se\u00b7lig\u00b7kei\u00b7ten", "!", "\u00ab", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ADV", "ADV", "APPR", "PIAT", "NN", "$.", "$(", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "O! davon sprach er noch, nur mit dem Unterscheide:", "tokens": ["O", "!", "da\u00b7von", "sprach", "er", "noch", ",", "nur", "mit", "dem", "Un\u00b7ter\u00b7schei\u00b7de", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PAV", "VVFIN", "PPER", "ADV", "$,", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Mit Alten sprach er stets von Tod und Eitelkeit,", "tokens": ["Mit", "Al\u00b7ten", "sprach", "er", "stets", "von", "Tod", "und", "Ei\u00b7tel\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "ADV", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Mit Armen von des Himmels Freude,", "tokens": ["Mit", "Ar\u00b7men", "von", "des", "Him\u00b7mels", "Freu\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Mit H\u00e4\u00dflichen von Ehrbarkeit,", "tokens": ["Mit", "H\u00e4\u00df\u00b7li\u00b7chen", "von", "Ehr\u00b7bar\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Nur mit den Sch\u00f6nen allezeit", "tokens": ["Nur", "mit", "den", "Sch\u00f6\u00b7nen", "al\u00b7le\u00b7zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "NN", "ADV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Vom ersten jeder Christentriebe.", "tokens": ["Vom", "ers\u00b7ten", "je\u00b7der", "Chris\u00b7ten\u00b7trie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Was ist das? Wer mich fragt, kann der ein Christ wohl sein?", "tokens": ["Was", "ist", "das", "?", "Wer", "mich", "fragt", ",", "kann", "der", "ein", "Christ", "wohl", "sein", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PDS", "$.", "PWS", "PPER", "VVFIN", "$,", "VMFIN", "ART", "ART", "NN", "ADV", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Denn jeder Christ k\u00f6mmt damit \u00fcberein,", "tokens": ["Denn", "je\u00b7der", "Christ", "k\u00f6mmt", "da\u00b7mit", "\u00fc\u00b7be\u00b7re\u00b7in", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "PAV", "PTKVZ", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.13": {"text": "Es sei die liebe Liebe.", "tokens": ["Es", "sei", "die", "lie\u00b7be", "Lie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Der Eremit war jung; das hab' ich schon gesagt.", "tokens": ["Der", "E\u00b7re\u00b7mit", "war", "jung", ";", "das", "hab'", "ich", "schon", "ge\u00b7sagt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$.", "PDS", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Doch sch\u00f6n? Wer nach der Sch\u00f6nheit fragt,", "tokens": ["Doch", "sch\u00f6n", "?", "Wer", "nach", "der", "Sch\u00f6n\u00b7heit", "fragt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$.", "PWS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der mag ihn hier besehn.", "tokens": ["Der", "mag", "ihn", "hier", "be\u00b7sehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Genug, den Weibern war er sch\u00f6n.", "tokens": ["Ge\u00b7nug", ",", "den", "Wei\u00b7bern", "war", "er", "sch\u00f6n", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ART", "NN", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ein starker, frischer, junger Kerl,", "tokens": ["Ein", "star\u00b7ker", ",", "fri\u00b7scher", ",", "jun\u00b7ger", "Kerl", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "ADJA", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Nicht dicke wie ein Fa\u00df, nicht hager wie ein Querl \u2013", "tokens": ["Nicht", "di\u00b7cke", "wie", "ein", "Fa\u00df", ",", "nicht", "ha\u00b7ger", "wie", "ein", "Querl", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJA", "KOKOM", "ART", "NN", "$,", "PTKNEG", "ADJD", "KOKOM", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "\u00bbnun, nun, aus seiner Kost ist jenes leicht zu schlie\u00dfen.\u00ab", "tokens": ["\u00bb", "nun", ",", "nun", ",", "aus", "sei\u00b7ner", "Kost", "ist", "je\u00b7nes", "leicht", "zu", "schlie\u00b7\u00dfen", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "$,", "ADV", "$,", "APPR", "PPOSAT", "NN", "VAFIN", "PDS", "ADJD", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Doch sollte man auch wissen,", "tokens": ["Doch", "soll\u00b7te", "man", "auch", "wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Da\u00df Gott dem, den er liebt,", "tokens": ["Da\u00df", "Gott", "dem", ",", "den", "er", "liebt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "Zu Steinen wohl Gedeihen gibt;", "tokens": ["Zu", "Stei\u00b7nen", "wohl", "Ge\u00b7dei\u00b7hen", "gibt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Und das ist doch kein fett Gerichte!", "tokens": ["Und", "das", "ist", "doch", "kein", "fett", "Ge\u00b7rich\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ADV", "PIAT", "ADJD", "NN", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.12": {"text": "Ein br\u00e4unlich m\u00e4nnliches Gesichte,", "tokens": ["Ein", "br\u00e4un\u00b7lich", "m\u00e4nn\u00b7li\u00b7ches", "Ge\u00b7sich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Nicht allzu klein, nicht allzu gro\u00df,", "tokens": ["Nicht", "all\u00b7zu", "klein", ",", "nicht", "all\u00b7zu", "gro\u00df", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PTKA", "ADJD", "$,", "PTKNEG", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Das sich im dichten Barte schlo\u00df;", "tokens": ["Das", "sich", "im", "dich\u00b7ten", "Bar\u00b7te", "schlo\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PRF", "APPRART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Die Blicke wild, doch sonder Anmut nicht;", "tokens": ["Die", "Bli\u00b7cke", "wild", ",", "doch", "son\u00b7der", "An\u00b7mut", "nicht", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "$,", "ADV", "ADJA", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.16": {"text": "Die Nase lang, wie man die Kaisernasen dicht't.", "tokens": ["Die", "Na\u00b7se", "lang", ",", "wie", "man", "die", "Kai\u00b7ser\u00b7na\u00b7sen", "dicht'", "t."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "abbreviation"], "pos": ["ART", "NN", "ADJD", "$,", "PWAV", "PIS", "ART", "NN", "VVFIN", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Das ungebundne Haar flo\u00df straubicht um das Haupt;", "tokens": ["Das", "un\u00b7ge\u00b7bund\u00b7ne", "Haar", "flo\u00df", "strau\u00b7bicht", "um", "das", "Haupt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Und wesentlichre Sch\u00f6nheitsst\u00fccke", "tokens": ["Und", "we\u00b7sent\u00b7lich\u00b7re", "Sch\u00f6n\u00b7heits\u00b7st\u00fc\u00b7cke"], "token_info": ["word", "word", "word"], "pos": ["KON", "VVFIN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Hat der zerri\u00dfne Rock dem Blicke", "tokens": ["Hat", "der", "zer\u00b7ri\u00df\u00b7ne", "Rock", "dem", "Bli\u00b7cke"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "ADJA", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Nicht ganz entdeckt, nicht ganz geraubt.", "tokens": ["Nicht", "ganz", "ent\u00b7deckt", ",", "nicht", "ganz", "ge\u00b7raubt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "VVPP", "$,", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "Der Waden nur noch zu gedenken:", "tokens": ["Der", "Wa\u00b7den", "nur", "noch", "zu", "ge\u00b7den\u00b7ken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "Sie waren gro\u00df, und hart wie Stein.", "tokens": ["Sie", "wa\u00b7ren", "gro\u00df", ",", "und", "hart", "wie", "Stein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "KON", "ADJD", "KOKOM", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Das sollen, wie man sagt, nicht schlimme Zeichen sein;", "tokens": ["Das", "sol\u00b7len", ",", "wie", "man", "sagt", ",", "nicht", "schlim\u00b7me", "Zei\u00b7chen", "sein", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "$,", "PWAV", "PIS", "VVFIN", "$,", "PTKNEG", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Allein den Grund wird man mir schenken.", "tokens": ["Al\u00b7lein", "den", "Grund", "wird", "man", "mir", "schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "PIS", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Nun wahrlich, so ein Kerl kann Weiber l\u00fcstern machen.", "tokens": ["Nun", "wahr\u00b7lich", ",", "so", "ein", "Kerl", "kann", "Wei\u00b7ber", "l\u00fcs\u00b7tern", "ma\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "ADV", "ART", "NN", "VMFIN", "NN", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich sag' es nicht f\u00fcr mich; es sind geschehne Sachen.", "tokens": ["Ich", "sag'", "es", "nicht", "f\u00fcr", "mich", ";", "es", "sind", "ge\u00b7scheh\u00b7ne", "Sa\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "APPR", "PPER", "$.", "PPER", "VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "\u00bbgeschehne Sachen? was?", "tokens": ["\u00bb", "ge\u00b7scheh\u00b7ne", "Sa\u00b7chen", "?", "was", "?"], "token_info": ["punct", "word", "word", "punct", "word", "punct"], "pos": ["$(", "ADJA", "NN", "$.", "PWS", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "So ist man gar zur Tat gekommen?\u00ab", "tokens": ["So", "ist", "man", "gar", "zur", "Tat", "ge\u00b7kom\u00b7men", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "PIS", "ADV", "APPRART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Mein lieber Simplex, fragt sich das?", "tokens": ["Mein", "lie\u00b7ber", "Sim\u00b7plex", ",", "fragt", "sich", "das", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADV", "NE", "$,", "VVFIN", "PRF", "PDS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Weswegen h\u00e4tt' er denn die Predigt unternommen?", "tokens": ["Wes\u00b7we\u00b7gen", "h\u00e4tt'", "er", "denn", "die", "Pre\u00b7digt", "un\u00b7ter\u00b7nom\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die s\u00fc\u00dfe Lehre s\u00fc\u00dfer Triebe?", "tokens": ["Die", "s\u00fc\u00b7\u00dfe", "Leh\u00b7re", "s\u00fc\u00b7\u00dfer", "Trie\u00b7be", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die Liebe heischet Gegenliebe,", "tokens": ["Die", "Lie\u00b7be", "hei\u00b7schet", "Ge\u00b7gen\u00b7lie\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Und wer ihr Priester ist, verdienet keinen Ha\u00df.", "tokens": ["Und", "wer", "ihr", "Pries\u00b7ter", "ist", ",", "ver\u00b7die\u00b7net", "kei\u00b7nen", "Ha\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPOSAT", "NN", "VAFIN", "$,", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "O Andacht, mu\u00dft du doch so manche S\u00fcnde decken!", "tokens": ["O", "An\u00b7dacht", ",", "mu\u00dft", "du", "doch", "so", "man\u00b7che", "S\u00fcn\u00b7de", "de\u00b7cken", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "VMFIN", "PPER", "ADV", "ADV", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Zwar die Moral ist hier zu scharf,", "tokens": ["Zwar", "die", "Mo\u00b7ral", "ist", "hier", "zu", "scharf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAFIN", "ADV", "PTKA", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Weil mancher Mensch sich nicht bespiegeln darf,", "tokens": ["Weil", "man\u00b7cher", "Mensch", "sich", "nicht", "be\u00b7spie\u00b7geln", "darf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "PRF", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Aus Furcht, er m\u00f6chte vor sich selbst erschrecken.", "tokens": ["Aus", "Furcht", ",", "er", "m\u00f6ch\u00b7te", "vor", "sich", "selbst", "er\u00b7schre\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "PPER", "VMFIN", "APPR", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "Drum will ich nur mit meinen Lehren", "tokens": ["Drum", "will", "ich", "nur", "mit", "mei\u00b7nen", "Leh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VMFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Ganz still nach Hause wieder kehren.", "tokens": ["Ganz", "still", "nach", "Hau\u00b7se", "wie\u00b7der", "keh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "K\u00f6mmt mir einmal der Einfall ein,", "tokens": ["K\u00f6mmt", "mir", "ein\u00b7mal", "der", "Ein\u00b7fall", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Und ein Verleger will f\u00fcr mich so gn\u00e4dig sein,", "tokens": ["Und", "ein", "Ver\u00b7le\u00b7ger", "will", "f\u00fcr", "mich", "so", "gn\u00e4\u00b7dig", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VMFIN", "APPR", "PPER", "ADV", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Mich in gro\u00df Quart in Druck zu nehmen;", "tokens": ["Mich", "in", "gro\u00df", "Quart", "in", "Druck", "zu", "neh\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ADJD", "NN", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "So k\u00f6nnt' ich mich vielleicht bequemen,", "tokens": ["So", "k\u00f6nnt'", "ich", "mich", "viel\u00b7leicht", "be\u00b7que\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "ADV", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Mit hundert englischen Moralen,", "tokens": ["Mit", "hun\u00b7dert", "eng\u00b7li\u00b7schen", "Mo\u00b7ra\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.21": {"text": "Die ich im Laden sah, zu prahlen,", "tokens": ["Die", "ich", "im", "La\u00b7den", "sah", ",", "zu", "prah\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "PPER", "APPRART", "NN", "VVFIN", "$,", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "Exempelsch\u00e4tze, Sittenrichter,", "tokens": ["Ex\u00b7em\u00b7pel\u00b7sch\u00e4t\u00b7ze", ",", "Sit\u00b7ten\u00b7rich\u00b7ter", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.23": {"text": "Die alten und die neuen Dichter", "tokens": ["Die", "al\u00b7ten", "und", "die", "neu\u00b7en", "Dich\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "KON", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.24": {"text": "Mit witz'gen Fingern nachzuschlagen,", "tokens": ["Mit", "witz'\u00b7gen", "Fin\u00b7gern", "nach\u00b7zu\u00b7schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.25": {"text": "Und was die sagen, und nicht sagen,", "tokens": ["Und", "was", "die", "sa\u00b7gen", ",", "und", "nicht", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "VVINF", "$,", "KON", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.26": {"text": "In einer Note abzuschreiben.", "tokens": ["In", "ei\u00b7ner", "No\u00b7te", "ab\u00b7zu\u00b7schrei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.27": {"text": "Bringt, sag' ich noch einmal, man mich gedruckt an Tag;", "tokens": ["Bringt", ",", "sag'", "ich", "noch", "ein\u00b7mal", ",", "man", "mich", "ge\u00b7druckt", "an", "Tag", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "PPER", "ADV", "ADV", "$,", "PIS", "PRF", "VVPP", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Denn in der Handschrift la\u00df ich's bleiben,", "tokens": ["Denn", "in", "der", "Hand\u00b7schrift", "la\u00df", "ich's", "blei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.29": {"text": "Weil ich mich nicht bel\u00fcgen mag.", "tokens": ["Weil", "ich", "mich", "nicht", "be\u00b7l\u00fc\u00b7gen", "mag."], "token_info": ["word", "word", "word", "word", "word", "abbreviation"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Ich fahr' in der Erz\u00e4hlung fort \u2013", "tokens": ["Ich", "fahr'", "in", "der", "Er\u00b7z\u00e4h\u00b7lung", "fort", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Doch m\u00f6cht' ich in der Tat gestehn,", "tokens": ["Doch", "m\u00f6cht'", "ich", "in", "der", "Tat", "ge\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich h\u00e4tte manchmal m\u00f6gen sehn,", "tokens": ["Ich", "h\u00e4t\u00b7te", "manch\u00b7mal", "m\u00f6\u00b7gen", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was die und die, die an den Wallfahrtsort", "tokens": ["Was", "die", "und", "die", ",", "die", "an", "den", "Wall\u00b7fahr\u00b7tsort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "ART", "KON", "ART", "$,", "PRELS", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Mit heiligen Gedanken kam,", "tokens": ["Mit", "hei\u00b7li\u00b7gen", "Ge\u00b7dan\u00b7ken", "kam", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "F\u00fcr fremde Mienen an sich nahm,", "tokens": ["F\u00fcr", "frem\u00b7de", "Mie\u00b7nen", "an", "sich", "nahm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "APPR", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wenn der verwegne Eremit,", "tokens": ["Wenn", "der", "ver\u00b7weg\u00b7ne", "E\u00b7re\u00b7mit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Fein listig. Schritt vor Schritt,", "tokens": ["Fein", "lis\u00b7tig", ".", "Schritt", "vor", "Schritt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "$.", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.9": {"text": "Vom Geist aufs Fleisch zu reden kam.", "tokens": ["Vom", "Geist", "aufs", "Fleisch", "zu", "re\u00b7den", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPRART", "NN", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Ich zweifle nicht, da\u00df die verletzte Scham", "tokens": ["Ich", "zweif\u00b7le", "nicht", ",", "da\u00df", "die", "ver\u00b7letz\u00b7te", "Scham"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.11": {"text": "Den Zorn nicht ins Gesicht getrieben,", "tokens": ["Den", "Zorn", "nicht", "ins", "Ge\u00b7sicht", "ge\u00b7trie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKNEG", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "Da\u00df Mund und Hand nicht in Bewegung kam,", "tokens": ["Da\u00df", "Mund", "und", "Hand", "nicht", "in", "Be\u00b7we\u00b7gung", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "PTKNEG", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Weil beide die Bewegung lieben;", "tokens": ["Weil", "bei\u00b7de", "die", "Be\u00b7we\u00b7gung", "lie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Allein, da\u00df die Vers\u00f6hnung ausgeblieben,", "tokens": ["Al\u00b7lein", ",", "da\u00df", "die", "Ver\u00b7s\u00f6h\u00b7nung", "aus\u00b7ge\u00b7blie\u00b7ben", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Glaub' ich, und wer die Weiber kennt,", "tokens": ["Glaub'", "ich", ",", "und", "wer", "die", "Wei\u00b7ber", "kennt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "KON", "PWS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Nicht eher, als kein Stroh mehr brennt.", "tokens": ["Nicht", "e\u00b7her", ",", "als", "kein", "Stroh", "mehr", "brennt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "$,", "KOUS", "PIAT", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Denn wird doch wohl ein L\u00f6we zahm.", "tokens": ["Denn", "wird", "doch", "wohl", "ein", "L\u00f6\u00b7we", "zahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "ADV", "ART", "NE", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Und eine Frau ist ohnedem ein Lamm.", "tokens": ["Und", "ei\u00b7ne", "Frau", "ist", "oh\u00b7ne\u00b7dem", "ein", "Lamm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PAV", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.19": {"text": "\u00bbein Lamm? du magst die Weiber kennen.\u00ab", "tokens": ["\u00bb", "ein", "Lamm", "?", "du", "magst", "die", "Wei\u00b7ber", "ken\u00b7nen", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "$.", "PPER", "VMFIN", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Je nun, man kann sie doch in so weit L\u00e4mmer nennen,", "tokens": ["Je", "nun", ",", "man", "kann", "sie", "doch", "in", "so", "weit", "L\u00e4m\u00b7mer", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "PIS", "VMFIN", "PPER", "ADV", "APPR", "ADV", "ADJD", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Als sie von selbst ins Feuer rennen.", "tokens": ["Als", "sie", "von", "selbst", "ins", "Feu\u00b7er", "ren\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADV", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "\u00bbf\u00e4hrst du in der Erz\u00e4hlung fort?", "tokens": ["\u00bb", "f\u00e4hrst", "du", "in", "der", "Er\u00b7z\u00e4h\u00b7lung", "fort", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Und bleibst mit deinem Kritisieren", "tokens": ["Und", "bleibst", "mit", "dei\u00b7nem", "Kri\u00b7ti\u00b7sie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch ewig an demselben Ort?\u00ab", "tokens": ["Doch", "e\u00b7wig", "an", "dem\u00b7sel\u00b7ben", "Ort", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADJD", "APPR", "PDAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So kann das N\u00fctzliche den Dichter auch verf\u00fchren.", "tokens": ["So", "kann", "das", "N\u00fctz\u00b7li\u00b7che", "den", "Dich\u00b7ter", "auch", "ver\u00b7f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "NN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nun gut, ich fahre fort,", "tokens": ["Nun", "gut", ",", "ich", "fah\u00b7re", "fort", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Und sag', um wirklich fort zu fahren,", "tokens": ["Und", "sag'", ",", "um", "wirk\u00b7lich", "fort", "zu", "fah\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUI", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Da\u00df nach f\u00fcnf Vierteljahren", "tokens": ["Da\u00df", "nach", "f\u00fcnf", "Vier\u00b7tel\u00b7jah\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "APPR", "CARD", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.8": {"text": "Die Schelmereien ruchbar waren.", "tokens": ["Die", "Schel\u00b7me\u00b7rei\u00b7en", "ruch\u00b7bar", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "\u00bberst nach f\u00fcnf Vierteljahren? Nu;", "tokens": ["\u00bb", "erst", "nach", "f\u00fcnf", "Vier\u00b7tel\u00b7jah\u00b7ren", "?", "Nu", ";"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "ADV", "APPR", "CARD", "NN", "$.", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Der Eremit hat wacker ausgehalten.", "tokens": ["Der", "E\u00b7re\u00b7mit", "hat", "wa\u00b7cker", "aus\u00b7ge\u00b7hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "So viel trau ich mir doch nicht zu;", "tokens": ["So", "viel", "trau", "ich", "mir", "doch", "nicht", "zu", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PPER", "ADV", "PTKNEG", "PTKVZ", "$."], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.12": {"text": "Ich m\u00f6chte nicht sein Amt ein Vierteljahr verwalten.", "tokens": ["Ich", "m\u00f6ch\u00b7te", "nicht", "sein", "Amt", "ein", "Vier\u00b7tel\u00b7jahr", "ver\u00b7wal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "PPOSAT", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Allein, wie ward es ewig kund?", "tokens": ["Al\u00b7lein", ",", "wie", "ward", "es", "e\u00b7wig", "kund", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "VAFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Hat es ein schlauer Mann erfahren?", "tokens": ["Hat", "es", "ein", "schlau\u00b7er", "Mann", "er\u00b7fah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Verriet es einer Frau waschhafter Mund?", "tokens": ["Ver\u00b7riet", "es", "ei\u00b7ner", "Frau", "waschhaf\u00b7ter", "Mund", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.16": {"text": "Wie? oder da\u00df den Hochverrat", "tokens": ["Wie", "?", "o\u00b7der", "da\u00df", "den", "Hoch\u00b7ver\u00b7rat"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "$.", "KON", "KOUS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Ein alt neugierig Weib, aus Neid, begangen hat?\u00ab", "tokens": ["Ein", "alt", "neu\u00b7gie\u00b7rig", "Weib", ",", "aus", "Neid", ",", "be\u00b7gan\u00b7gen", "hat", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ART", "ADJD", "ADJD", "NN", "$,", "APPR", "NN", "$,", "VVPP", "VAFIN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "O nein; hier mu\u00df man besser raten,", "tokens": ["O", "nein", ";", "hier", "mu\u00df", "man", "bes\u00b7ser", "ra\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKANT", "$.", "ADV", "VMFIN", "PIS", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Zwei muntre M\u00e4dchen hatten Schuld,", "tokens": ["Zwei", "mun\u00b7tre", "M\u00e4d\u00b7chen", "hat\u00b7ten", "Schuld", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Die voller frommen Ungeduld", "tokens": ["Die", "vol\u00b7ler", "from\u00b7men", "Un\u00b7ge\u00b7duld"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "Das taten, was die M\u00fctter taten;", "tokens": ["Das", "ta\u00b7ten", ",", "was", "die", "M\u00fct\u00b7ter", "ta\u00b7ten", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "PRELS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "Und dennoch wollten sich die M\u00fctter nicht bequemen,", "tokens": ["Und", "den\u00b7noch", "woll\u00b7ten", "sich", "die", "M\u00fct\u00b7ter", "nicht", "be\u00b7que\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VMFIN", "PRF", "ART", "NN", "PTKNEG", "ADJA", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Die guten Kinder mit zu nehmen.", "tokens": ["Die", "gu\u00b7ten", "Kin\u00b7der", "mit", "zu", "neh\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.24": {"text": "\u00bbsie merkten also wohl den Braten?\u00ab \u2013", "tokens": ["\u00bb", "sie", "merk\u00b7ten", "al\u00b7so", "wohl", "den", "Bra\u00b7ten", "?", "\u00ab", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ADV", "ADV", "ART", "NN", "$.", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.25": {"text": "Und haben ihn gar dem Papa verraten.", "tokens": ["Und", "ha\u00b7ben", "ihn", "gar", "dem", "Pa\u00b7pa", "ver\u00b7ra\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.26": {"text": "\u00bbdie T\u00f6chter sagtens dem Papa?", "tokens": ["\u00bb", "die", "T\u00f6ch\u00b7ter", "sag\u00b7tens", "dem", "Pa\u00b7pa", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.27": {"text": "Wo blieb die Liebe zur Mama?\u00ab", "tokens": ["Wo", "blieb", "die", "Lie\u00b7be", "zur", "Ma\u00b7ma", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVFIN", "ART", "NN", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.28": {"text": "O! die kann nichts darunter leiden;", "tokens": ["O", "!", "die", "kann", "nichts", "da\u00b7run\u00b7ter", "lei\u00b7den", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "PDS", "VMFIN", "PIS", "PAV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.29": {"text": "Denn wenn ein M\u00e4dchen auch die Mutter liebt,", "tokens": ["Denn", "wenn", "ein", "M\u00e4d\u00b7chen", "auch", "die", "Mut\u00b7ter", "liebt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.30": {"text": "Da\u00df es der Mutter in der Not", "tokens": ["Da\u00df", "es", "der", "Mut\u00b7ter", "in", "der", "Not"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.31": {"text": "Den letzten Bissen Brot", "tokens": ["Den", "letz\u00b7ten", "Bis\u00b7sen", "Brot"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.32": {"text": "Aus seinem Munde gibt;", "tokens": ["Aus", "sei\u00b7nem", "Mun\u00b7de", "gibt", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.33": {"text": "So kann das M\u00e4dchen doch die Mutter hier beneiden,", "tokens": ["So", "kann", "das", "M\u00e4d\u00b7chen", "doch", "die", "Mut\u00b7ter", "hier", "be\u00b7nei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "NN", "ADV", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Hier, wo so Lieb' als Klugheit spricht:", "tokens": ["Hier", ",", "wo", "so", "Lieb'", "als", "Klug\u00b7heit", "spricht", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "ADV", "NN", "KOUS", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.35": {"text": "Ihr Sch\u00f6nen, trotz der Kinderpflicht,", "tokens": ["Ihr", "Sch\u00f6\u00b7nen", ",", "trotz", "der", "Kin\u00b7der\u00b7pflicht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.36": {"text": "Verge\u00dft euch selber nicht!", "tokens": ["Ver\u00b7ge\u00dft", "euch", "sel\u00b7ber", "nicht", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PTKNEG", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.37": {"text": "Kurz, durch die M\u00e4dchen kams ans Licht,", "tokens": ["Kurz", ",", "durch", "die", "M\u00e4d\u00b7chen", "kams", "ans", "Licht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "APPR", "ART", "NN", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.38": {"text": "Da\u00df er, der Eremit, beinah die ganze Stadt", "tokens": ["Da\u00df", "er", ",", "der", "E\u00b7re\u00b7mit", ",", "bei\u00b7nah", "die", "gan\u00b7ze", "Stadt"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "ART", "NN", "$,", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Zu Schw\u00e4gern oder Kindern hat.", "tokens": ["Zu", "Schw\u00e4\u00b7gern", "o\u00b7der", "Kin\u00b7dern", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "O! der verfluchte Schelm! Wer h\u00e4tte das gedacht!", "tokens": ["O", "!", "der", "ver\u00b7fluch\u00b7te", "Schelm", "!", "Wer", "h\u00e4t\u00b7te", "das", "ge\u00b7dacht", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ART", "ADJA", "NN", "$.", "PWS", "VAFIN", "PDS", "VVPP", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Die ganze Stadt ward aufgebracht,", "tokens": ["Die", "gan\u00b7ze", "Stadt", "ward", "auf\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und jeder Ehmann schwur, da\u00df in der ersten Nacht,", "tokens": ["Und", "je\u00b7der", "Eh\u00b7mann", "schwur", ",", "da\u00df", "in", "der", "ers\u00b7ten", "Nacht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "$,", "KOUS", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Er und sein Mitgeno\u00df der Hain,", "tokens": ["Er", "und", "sein", "Mit\u00b7ge\u00b7no\u00df", "der", "Hain", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "KON", "PPOSAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Des Feuers Beute m\u00fcsse sein.", "tokens": ["Des", "Feu\u00b7ers", "Beu\u00b7te", "m\u00fcs\u00b7se", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Schon rotteten sich ganze Scharen,", "tokens": ["Schon", "rot\u00b7te\u00b7ten", "sich", "gan\u00b7ze", "Scha\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Die zu der Rache fertig waren.", "tokens": ["Die", "zu", "der", "Ra\u00b7che", "fer\u00b7tig", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Doch ein hochweiser Magistrat", "tokens": ["Doch", "ein", "hoch\u00b7wei\u00b7ser", "Ma\u00b7gist\u00b7rat"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Besetzt das Tor, und sperrt die Stadt,", "tokens": ["Be\u00b7setzt", "das", "Tor", ",", "und", "sperrt", "die", "Stadt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "KON", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Der Eigenrache vorzukommen,", "tokens": ["Der", "Ei\u00b7gen\u00b7ra\u00b7che", "vor\u00b7zu\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Und schicket alsobald", "tokens": ["Und", "schi\u00b7cket", "al\u00b7so\u00b7bald"], "token_info": ["word", "word", "word"], "pos": ["KON", "VVFIN", "NE"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.12": {"text": "Die Schergen in den Wald,", "tokens": ["Die", "Scher\u00b7gen", "in", "den", "Wald", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "Die ihn vom Kreuze weg, und in Verhaft genommen.", "tokens": ["Die", "ihn", "vom", "Kreu\u00b7ze", "weg", ",", "und", "in", "Ver\u00b7haft", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPRART", "NN", "PTKVZ", "$,", "KON", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Man redte schon von Galgen und von Rad,", "tokens": ["Man", "red\u00b7te", "schon", "von", "Gal\u00b7gen", "und", "von", "Rad", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.15": {"text": "So sehr schien sein Verbrechen h\u00e4\u00dflich;", "tokens": ["So", "sehr", "schien", "sein", "Ver\u00b7bre\u00b7chen", "h\u00e4\u00df\u00b7lich", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Und keine Strafe war so gr\u00e4\u00dflich,", "tokens": ["Und", "kei\u00b7ne", "Stra\u00b7fe", "war", "so", "gr\u00e4\u00df\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Die, wie man sagt, er nicht verdienet hat.", "tokens": ["Die", ",", "wie", "man", "sagt", ",", "er", "nicht", "ver\u00b7die\u00b7net", "hat", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PWAV", "PIS", "VVFIN", "$,", "PPER", "PTKNEG", "VVFIN", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.18": {"text": "Und nur ein Hagestolz, ein schlauer Advokat,", "tokens": ["Und", "nur", "ein", "Ha\u00b7ge\u00b7stolz", ",", "ein", "schlau\u00b7er", "Ad\u00b7vo\u00b7kat", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Sprach: \u00bbo! dem k\u00f6mmt man nicht ans Leben,", "tokens": ["Sprach", ":", "\u00bb", "o", "!", "dem", "k\u00f6mmt", "man", "nicht", "ans", "Le\u00b7ben", ","], "token_info": ["word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "FM", "$.", "ART", "VVFIN", "PIS", "PTKNEG", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.20": {"text": "Der es Unz\u00e4hlichen zu geben,", "tokens": ["Der", "es", "Un\u00b7z\u00e4h\u00b7li\u00b7chen", "zu", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.21": {"text": "So r\u00fchmlich sich beflissen hat.\u00ab", "tokens": ["So", "r\u00fchm\u00b7lich", "sich", "be\u00b7flis\u00b7sen", "hat", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJD", "PRF", "VVPP", "VAFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Der Eremite, der die Nacht", "tokens": ["Der", "E\u00b7re\u00b7mi\u00b7te", ",", "der", "die", "Nacht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im Kerker ungewi\u00df und sorgend durchgewacht,", "tokens": ["Im", "Ker\u00b7ker", "un\u00b7ge\u00b7wi\u00df", "und", "sor\u00b7gend", "durch\u00b7ge\u00b7wacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "KON", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ward morgen ins Verh\u00f6r gebracht.", "tokens": ["Ward", "mor\u00b7gen", "ins", "Ver\u00b7h\u00f6r", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Richter war ein schalkscher Mann,", "tokens": ["Der", "Rich\u00b7ter", "war", "ein", "schalk\u00b7scher", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der jeden mit Vergn\u00fcgen schraubte,", "tokens": ["Der", "je\u00b7den", "mit", "Ver\u00b7gn\u00fc\u00b7gen", "schraub\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und doch \u2013 (wie man sich irren kann!)", "tokens": ["Und", "doch", "\u2013", "(", "wie", "man", "sich", "ir\u00b7ren", "kann", "!", ")"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ADV", "$(", "$(", "PWAV", "PIS", "PRF", "VVINF", "VMFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Von seiner Frau das Beste glaubte.", "tokens": ["Von", "sei\u00b7ner", "Frau", "das", "Bes\u00b7te", "glaub\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "\u00bbsie ist ein Ausbund aller Frommen,", "tokens": ["\u00bb", "sie", "ist", "ein", "Aus\u00b7bund", "al\u00b7ler", "From\u00b7men", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Und nur einmal in Wald gekommen,", "tokens": ["Und", "nur", "ein\u00b7mal", "in", "Wald", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Den Pater Eremit zu sehn.", "tokens": ["Den", "Pa\u00b7ter", "E\u00b7re\u00b7mit", "zu", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Einmal! Was kann da viel geschehn?\u00ab", "tokens": ["Ein\u00b7mal", "!", "Was", "kann", "da", "viel", "ge\u00b7schehn", "?", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "$.", "PWS", "VMFIN", "ADV", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "So denkt der g\u00fctige Herr Richter.", "tokens": ["So", "denkt", "der", "g\u00fc\u00b7ti\u00b7ge", "Herr", "Rich\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Denk' immer so, zu deiner Ruh,", "tokens": ["Denk'", "im\u00b7mer", "so", ",", "zu", "dei\u00b7ner", "Ruh", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "ADV", "$,", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Lacht gleich die Wahrheit und der Dichter,", "tokens": ["Lacht", "gleich", "die", "Wahr\u00b7heit", "und", "der", "Dich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Und deine fromme Frau dazu.", "tokens": ["Und", "dei\u00b7ne", "from\u00b7me", "Frau", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Nun tritt der Eremit vor ihn.", "tokens": ["Nun", "tritt", "der", "E\u00b7re\u00b7mit", "vor", "ihn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbmein Freund, wollt Ihr von selbst die nennen,", "tokens": ["\u00bb", "mein", "Freund", ",", "wollt", "Ihr", "von", "selbst", "die", "nen\u00b7nen", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "VMFIN", "PPER", "APPR", "ADV", "ART", "VVINF", "$,"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Die \u2013 die Ihr kennt, und die Euch kennen:", "tokens": ["Die", "\u2013", "die", "Ihr", "kennt", ",", "und", "die", "Euch", "ken\u00b7nen", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$(", "PRELS", "PPER", "VVFIN", "$,", "KON", "ART", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So k\u00f6nnt Ihr der Tortur entfliehn.", "tokens": ["So", "k\u00f6nnt", "Ihr", "der", "Tor\u00b7tur", "ent\u00b7fliehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "VVINF", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Doch\u00ab \u2013 \u00bb", "tokens": ["Doch", "\u00ab", "\u2013", "\u00bb"], "token_info": ["word", "punct", "punct", "punct"], "pos": ["KON", "$(", "$(", "$("], "meter": "+", "measure": "single.up"}, "line.6": {"text": "Der Eremit entdecket sie?", "tokens": ["Der", "E\u00b7re\u00b7mit", "ent\u00b7de\u00b7cket", "sie", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ein Eremite kann nicht schweigen?", "tokens": ["Ein", "E\u00b7re\u00b7mi\u00b7te", "kann", "nicht", "schwei\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Sonst ist das Plaudern nur den Stutzern eigen.", "tokens": ["Sonst", "ist", "das", "Plau\u00b7dern", "nur", "den", "Stut\u00b7zern", "ei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Der Richter schrieb. \u00bb", "tokens": ["Der", "Rich\u00b7ter", "schrieb", ".", "\u00bb"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "\u00bbder Henker mag sie alle fassen,", "tokens": ["\u00bb", "der", "Hen\u00b7ker", "mag", "sie", "al\u00b7le", "fas\u00b7sen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "VMFIN", "PPER", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "Gemach! und eine nach der andern fein!", "tokens": ["Ge\u00b7mach", "!", "und", "ei\u00b7ne", "nach", "der", "an\u00b7dern", "fein", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "KON", "ART", "APPR", "ART", "ADJA", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Denn eine nur vorbei zu lassen\u00ab \u2013", "tokens": ["Denn", "ei\u00b7ne", "nur", "vor\u00b7bei", "zu", "las\u00b7sen", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "ADV", "ADV", "PTKZU", "VVINF", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Wird wohl kein gro\u00dfer Schade sein,", "tokens": ["Wird", "wohl", "kein", "gro\u00b7\u00dfer", "Scha\u00b7de", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIAT", "ADJA", "NN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Fiel jeder Ratsherr ihm ins Wort.", "tokens": ["Fiel", "je\u00b7der", "Rats\u00b7herr", "ihm", "ins", "Wort", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.15": {"text": "\u00bbh\u00f6rt, schrieen sie, erz\u00e4hlt nur fort!\u00ab", "tokens": ["\u00bb", "h\u00f6rt", ",", "schri\u00b7een", "sie", ",", "er\u00b7z\u00e4hlt", "nur", "fort", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "$,", "VVFIN", "PPER", "$,", "VVFIN", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Weil jeder Ratsherr in Gefahr,", "tokens": ["Weil", "je\u00b7der", "Rats\u00b7herr", "in", "Ge\u00b7fahr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.17": {"text": "Sein eigen Weib zu h\u00f6ren war.", "tokens": ["Sein", "ei\u00b7gen", "Weib", "zu", "h\u00f6\u00b7ren", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "PTKZU", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "\u00bbihr Herren, schrie der Richter, nein!", "tokens": ["\u00bb", "ihr", "Her\u00b7ren", ",", "schrie", "der", "Rich\u00b7ter", ",", "nein", "!"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "VVFIN", "ART", "NN", "$,", "PTKANT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.19": {"text": "Die Wahrheit mu\u00df am Tage sein;", "tokens": ["Die", "Wahr\u00b7heit", "mu\u00df", "am", "Ta\u00b7ge", "sein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "APPRART", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Was k\u00f6nnen wir sonst f\u00fcr ein Urteil fassen?\u00ab", "tokens": ["Was", "k\u00f6n\u00b7nen", "wir", "sonst", "f\u00fcr", "ein", "Ur\u00b7teil", "fas\u00b7sen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PPER", "ADV", "APPR", "ART", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.21": {"text": "Ihn, schrieen alle, gehn zu lassen.", "tokens": ["Ihn", ",", "schri\u00b7een", "al\u00b7le", ",", "gehn", "zu", "las\u00b7sen", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "VVFIN", "PIS", "$,", "VVINF", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "\u00bbnein, die Gerechtigkeit\u00ab \u2013 und kurz der Delinquent", "tokens": ["\u00bb", "nein", ",", "die", "Ge\u00b7rech\u00b7tig\u00b7keit", "\u00ab", "\u2013", "und", "kurz", "der", "De\u00b7lin\u00b7quent"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["$(", "PTKANT", "$,", "ART", "NN", "$(", "$(", "KON", "ADJD", "ART", "NN"], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.23": {"text": "Hat jede noch einmal genennt,", "tokens": ["Hat", "je\u00b7de", "noch", "ein\u00b7mal", "ge\u00b7nennt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.24": {"text": "Und jeder hing der Richter dann", "tokens": ["Und", "je\u00b7der", "hing", "der", "Rich\u00b7ter", "dann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIS", "VVFIN", "ART", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.25": {"text": "Ein loses Wort f\u00fcr ihren Hahnrei an.", "tokens": ["Ein", "lo\u00b7ses", "Wort", "f\u00fcr", "ih\u00b7ren", "Hahn\u00b7rei", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.26": {"text": "Das Hundert war schon mehr als voll;", "tokens": ["Das", "Hun\u00b7dert", "war", "schon", "mehr", "als", "voll", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "PIAT", "KOKOM", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.27": {"text": "Der Eremit, der mehr gestehen soll,", "tokens": ["Der", "E\u00b7re\u00b7mit", ",", "der", "mehr", "ge\u00b7ste\u00b7hen", "soll", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.28": {"text": "Stockt, weigert sich, scheut sich zu sprechen \u2013", "tokens": ["Stockt", ",", "wei\u00b7gert", "sich", ",", "scheut", "sich", "zu", "spre\u00b7chen", "\u2013"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "PRF", "$,", "VVFIN", "PRF", "PTKZU", "VVINF", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.29": {"text": "\u00bbnu, nu, nur fort! was zwingt Euch wohl,", "tokens": ["\u00bb", "nu", ",", "nu", ",", "nur", "fort", "!", "was", "zwingt", "Euch", "wohl", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$,", "ADV", "$,", "ADV", "PTKVZ", "$.", "PWS", "VVFIN", "PPER", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.30": {"text": "So unvermutet abzubrechen?\u00ab", "tokens": ["So", "un\u00b7ver\u00b7mu\u00b7tet", "ab\u00b7zu\u00b7bre\u00b7chen", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADJD", "VVIZU", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.31": {"text": "\u00bb", "tokens": ["\u00bb"], "token_info": ["punct"], "pos": ["$("]}, "line.32": {"text": "Ein Held wie Ihr! Gestehet nur, gesteht!", "tokens": ["Ein", "Held", "wie", "Ihr", "!", "Ge\u00b7ste\u00b7het", "nur", ",", "ge\u00b7steht", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "KOKOM", "PPER", "$.", "NN", "ADV", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.33": {"text": "Die letzten waren, wie Ihr seht:", "tokens": ["Die", "letz\u00b7ten", "wa\u00b7ren", ",", "wie", "Ihr", "seht", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.34": {"text": "Klara, Pulcheria, Susanne,", "tokens": ["Kla\u00b7ra", ",", "Pulc\u00b7he\u00b7ria", ",", "Su\u00b7san\u00b7ne", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.35": {"text": "Charlotte, Mariane, Hanne.", "tokens": ["Char\u00b7lot\u00b7te", ",", "Ma\u00b7ri\u00b7a\u00b7ne", ",", "Han\u00b7ne", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NE", "$,", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.36": {"text": "Denkt nach! ich la\u00df Euch Zeit dazu!\u00ab", "tokens": ["Denkt", "nach", "!", "ich", "la\u00df", "Euch", "Zeit", "da\u00b7zu", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PTKVZ", "$.", "PPER", "VVFIN", "PPER", "NN", "PAV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.37": {"text": "\u00bb", "tokens": ["\u00bb"], "token_info": ["punct"], "pos": ["$("]}, "line.38": {"text": "Macht, eh wir sch\u00e4rfer in Euch dringen!\u00ab", "tokens": ["Macht", ",", "eh", "wir", "sch\u00e4r\u00b7fer", "in", "Euch", "drin\u00b7gen", "!", "\u00ab"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$,", "KOUS", "PPER", "ADJD", "APPR", "PPER", "VVINF", "$.", "$("], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.39": {"text": "\u00bb", "tokens": ["\u00bb"], "token_info": ["punct"], "pos": ["$("]}, "line.40": {"text": "\u00bbha! ha! ich seh, man soll Euch zwingen\u00ab \u2013 \u2013", "tokens": ["\u00bb", "ha", "!", "ha", "!", "ich", "seh", ",", "man", "soll", "Euch", "zwin\u00b7gen", "\u00ab", "\u2013", "\u2013"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "ITJ", "$.", "ITJ", "$.", "PPER", "VVFIN", "$,", "PIS", "VMFIN", "PPER", "VVINF", "$(", "$(", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.41": {"text": "\u00bb", "tokens": ["\u00bb"], "token_info": ["punct"], "pos": ["$("]}}, "stanza.16": {"line.1": {"text": "Da\u00df man von der Erz\u00e4hlung nicht", "tokens": ["Da\u00df", "man", "von", "der", "Er\u00b7z\u00e4h\u00b7lung", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "ART", "NN", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als einem Weiberm\u00e4rchen spricht,", "tokens": ["Als", "ei\u00b7nem", "Wei\u00b7ber\u00b7m\u00e4r\u00b7chen", "spricht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So mach' ich sie zum Lehrgedicht,", "tokens": ["So", "mach'", "ich", "sie", "zum", "Lehr\u00b7ge\u00b7dicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Durch beigef\u00fcgten Unterricht:", "tokens": ["Durch", "bei\u00b7ge\u00b7f\u00fcg\u00b7ten", "Un\u00b7ter\u00b7richt", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wer seines N\u00e4chsten Schande sucht,", "tokens": ["Wer", "sei\u00b7nes", "N\u00e4chs\u00b7ten", "Schan\u00b7de", "sucht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wird selber seine Schande finden!", "tokens": ["Wird", "sel\u00b7ber", "sei\u00b7ne", "Schan\u00b7de", "fin\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Nicht wahr, so liest man mich mit Frucht?", "tokens": ["Nicht", "wahr", ",", "so", "liest", "man", "mich", "mit", "Frucht", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$,", "ADV", "VVFIN", "PIS", "PRF", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und ich erz\u00e4hle sonder S\u00fcnden?", "tokens": ["Und", "ich", "er\u00b7z\u00e4h\u00b7le", "son\u00b7der", "S\u00fcn\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}