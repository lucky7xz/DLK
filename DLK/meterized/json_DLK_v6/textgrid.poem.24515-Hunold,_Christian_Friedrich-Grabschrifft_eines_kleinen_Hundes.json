{"textgrid.poem.24515": {"metadata": {"author": {"name": "Hunold, Christian Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "Grabschrifft eines kleinen Hundes", "genre": "verse", "period": "N.A.", "pub_year": 1701, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Drum hie\u00df ich auch ", "tokens": ["Drum", "hie\u00df", "ich", "auch"], "token_info": ["word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PPER", "ADV"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Das Gl\u00fcck verschaffte mir so manche s\u00fc\u00dfe Stunde/", "tokens": ["Das", "Gl\u00fcck", "ver\u00b7schaff\u00b7te", "mir", "so", "man\u00b7che", "s\u00fc\u00b7\u00dfe", "Stun\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Zwar Haasen fieng ich nicht/ doch fra\u00df ich solche gerne/", "tokens": ["Zwar", "Haa\u00b7sen", "fi\u00b7eng", "ich", "nicht", "/", "doch", "fra\u00df", "ich", "sol\u00b7che", "ger\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VVFIN", "PPER", "PTKNEG", "$(", "ADV", "VVFIN", "PPER", "PIAT", "ADV", "$("], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Denn schlieff ich s\u00fc\u00df und sanfft bey meiner Fr\u00e4ulein ein.", "tokens": ["Denn", "schlieff", "ich", "s\u00fc\u00df", "und", "sanfft", "bey", "mei\u00b7ner", "Fr\u00e4u\u00b7lein", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "KON", "VVFIN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und gleichwohl konte Dir", "tokens": ["Und", "gleich\u00b7wohl", "kon\u00b7te", "Dir"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "VMFIN", "PPER"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "So wachsam auf der Hut/ als ich im Bellen seyn.", "tokens": ["So", "wach\u00b7sam", "auf", "der", "Hut", "/", "als", "ich", "im", "Bel\u00b7len", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "$(", "KOUS", "PPER", "APPRART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Drey Jungen trug ich letzt/ doch bin ich nicht genesen.", "tokens": ["Drey", "Jun\u00b7gen", "trug", "ich", "letzt", "/", "doch", "bin", "ich", "nicht", "ge\u00b7ne\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VVFIN", "PPER", "ADV", "$(", "ADV", "VAFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Mein Leser/ lache nicht/ betrachte wer ich war.", "tokens": ["Mein", "Le\u00b7ser", "/", "la\u00b7che", "nicht", "/", "be\u00b7trach\u00b7te", "wer", "ich", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "VVFIN", "PTKNEG", "$(", "VVFIN", "PWS", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Wenn du so artig bist/ als wie ", "tokens": ["Wenn", "du", "so", "ar\u00b7tig", "bist", "/", "als", "wie"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$(", "KOUS", "KOKOM"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "So nimm zur Lagerstatt dir seine Todten-Bahr.", "tokens": ["So", "nimm", "zur", "La\u00b7ger\u00b7statt", "dir", "sei\u00b7ne", "Tod\u00b7ten\u00b7Bahr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "APPRART", "NN", "PPER", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}