{"dta.poem.1707": {"metadata": {"author": {"name": "Abschatz, Hans Assmann von", "birth": "N.A.", "death": "N.A."}, "title": "Umwechslung aller Dinge.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1704", "urn": "urn:nbn:de:kobv:b4-200905199889", "language": ["de:0.99"], "booktitle": "Abschatz, Hans Assmann von: Poetische Ubersetzungen und Gedichte. Leipzig, 1704."}, "poem": {"stanza.1": {"line.1": {"text": "Ich will dir nichts als Gla\u00df/ mein Kind/ f\u00fcr di\u00dfmahk", "tokens": ["Ich", "will", "dir", "nichts", "als", "Gla\u00df", "/", "mein", "Kind", "/", "f\u00fcr", "di\u00df\u00b7mahk"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "PIS", "KOKOM", "NN", "$(", "PPOSAT", "NN", "$(", "APPR", "NE"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "schencken.", "tokens": ["schen\u00b7cken", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Was sind wir selbst als Gla\u00df/ wenn wir uns recht beden-", "tokens": ["Was", "sind", "wir", "selbst", "als", "Gla\u00df", "/", "wenn", "wir", "uns", "recht", "be\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "PPER", "ADV", "KOUS", "NN", "$(", "KOUS", "PPER", "PRF", "ADJD", "TRUNC"], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.4": {"text": "cken!", "tokens": ["cken", "!"], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "-", "measure": "single.down"}, "line.5": {"text": "Von Asche kommt di\u00df her; wir sind von Staub und Erden;", "tokens": ["Von", "A\u00b7sche", "kommt", "di\u00df", "her", ";", "wir", "sind", "von", "Staub", "und", "Er\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PDS", "PTKVZ", "$.", "PPER", "VAFIN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Di\u00df wird in Glutt gezeugt; wie hei\u00df mu\u00df uns offt werden!", "tokens": ["Di\u00df", "wird", "in", "Glutt", "ge\u00b7zeugt", ";", "wie", "hei\u00df", "mu\u00df", "uns", "offt", "wer\u00b7den", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "NN", "VVPP", "$.", "PWAV", "ADJD", "VMFIN", "PPER", "ADV", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Di\u00df/ wann es ausgekl\u00e4rt/ wie pranget seine Zier:", "tokens": ["Di\u00df", "/", "wann", "es", "aus\u00b7ge\u00b7kl\u00e4rt", "/", "wie", "pran\u00b7get", "sei\u00b7ne", "Zier", ":"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$(", "PWAV", "PPER", "VVPP", "$(", "PWAV", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Des Sch\u00f6pffers liebstes Werck und sch\u00f6nstes Bild sind wir.", "tokens": ["Des", "Sch\u00f6pf\u00b7fers", "liebs\u00b7tes", "Werck", "und", "sch\u00f6ns\u00b7tes", "Bild", "sind", "wir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "KON", "ADJA", "NN", "VAFIN", "PPER", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Doch leichte bricht das Gla\u00df/ und w\u00e4r es von Crystallen;", "tokens": ["Doch", "leich\u00b7te", "bricht", "das", "Gla\u00df", "/", "und", "w\u00e4r", "es", "von", "Crys\u00b7tal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "VVFIN", "ART", "NN", "$(", "KON", "VAFIN", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wie leichtlich kan der Mensch in Noth und Tod verfallen!", "tokens": ["Wie", "leicht\u00b7lich", "kan", "der", "Mensch", "in", "Noth", "und", "Tod", "ver\u00b7fal\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VMFIN", "ART", "NN", "APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Da\u00df wir den lieben Tag in Freuden wieder sehen/", "tokens": ["Da\u00df", "wir", "den", "lie\u00b7ben", "Tag", "in", "Freu\u00b7den", "wie\u00b7der", "se\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPR", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Ist durch des H\u00f6chsten Schutz und Gunst allein geschehen.", "tokens": ["Ist", "durch", "des", "H\u00f6chs\u00b7ten", "Schutz", "und", "Gunst", "al\u00b7lein", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "ADJA", "NN", "KON", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wir dancken ihm daf\u00fcr! Ich w\u00fcnsche di\u00df dabey/", "tokens": ["Wir", "dan\u00b7cken", "ihm", "da\u00b7f\u00fcr", "!", "Ich", "w\u00fcn\u00b7sche", "di\u00df", "da\u00b7bey", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PAV", "$.", "PPER", "VVFIN", "PDS", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Da\u00df er noch offt erschein/ und dir erfreulich sey!", "tokens": ["Da\u00df", "er", "noch", "offt", "er\u00b7schein", "/", "und", "dir", "er\u00b7freu\u00b7lich", "sey", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ADJD", "$(", "KON", "PPER", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}