{"textgrid.poem.53866": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Berliner B\u00e4lle", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.57", "en:0.42"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbmit dir \u2013 mit dir \u2013 m\u00f6cht ich mal sonntags angeln gehn \u2013", "tokens": ["\u00bb", "mit", "dir", "\u2013", "mit", "dir", "\u2013", "m\u00f6cht", "ich", "mal", "sonn\u00b7tags", "an\u00b7geln", "gehn", "\u2013"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "PPER", "$(", "APPR", "PPER", "$(", "VMFIN", "PPER", "ADV", "ADV", "VVINF", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Yes, Sir, that's my baby!", "tokens": ["Yes", ",", "Sir", ",", "that's", "my", "ba\u00b7by", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["FM", "$,", "NN", "$,", "FM", "FM", "FM", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Mit dir \u2013 mit dir \u2013 da denk ich mir das wundersch\u00f6n! \u2013", "tokens": ["Mit", "dir", "\u2013", "mit", "dir", "\u2013", "da", "denk", "ich", "mir", "das", "wun\u00b7der\u00b7sch\u00f6n", "!", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "$(", "APPR", "PPER", "$(", "ADV", "VVFIN", "PPER", "PPER", "ART", "ADJD", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "I wonder, where my baby is to night \u2013\u00ab", "tokens": ["I", "won\u00b7der", ",", "whe\u00b7re", "my", "ba\u00b7by", "is", "to", "night", "\u2013", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["CARD", "NN", "$,", "FM.en", "FM.en", "FM.en", "FM.en", "FM.en", "FM.en", "$(", "$("], "meter": "+--+-+--+-+", "measure": "iambic.penta.invert"}}, "stanza.2": {"line.1": {"text": "Junge Rechtsanw\u00e4lte biegen sich im Boston \u2013", "tokens": ["Jun\u00b7ge", "Rechts\u00b7an\u00b7w\u00e4l\u00b7te", "bie\u00b7gen", "sich", "im", "Bos\u00b7ton", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "PRF", "APPRART", "NN", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": " dies M\u00e4dchen ist nicht von hier; die ist aus dem Osten!", "tokens": ["dies", "M\u00e4d\u00b7chen", "ist", "nicht", "von", "hier", ";", "die", "ist", "aus", "dem", "Os\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VAFIN", "PTKNEG", "APPR", "ADV", "$.", "PDS", "VAFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": " kleine Modezeichner schlenkern viel zu viel mit die Beine \u2013", "tokens": ["klei\u00b7ne", "Mo\u00b7de\u00b7zeich\u00b7ner", "schlen\u00b7kern", "viel", "zu", "viel", "mit", "die", "Bei\u00b7ne", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "ADV", "PTKA", "PIS", "APPR", "ART", "NN", "$("], "meter": "+-+-+-+-+-+--+-", "measure": "trochaic.septa.relaxed"}, "line.4": {"text": " ein dubioser Kerl tanzt im Rund seinen Charleston alleine.", "tokens": ["ein", "du\u00b7bi\u00b7o\u00b7ser", "Kerl", "tanzt", "im", "Rund", "sei\u00b7nen", "Char\u00b7les\u00b7ton", "al\u00b7lei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPRART", "NN", "PPOSAT", "NN", "ADV", "$."], "meter": "-+-+-+--+--+-+-+-", "measure": "iambic.septa.relaxed"}}, "stanza.3": {"line.1": {"text": "\u00bbwo", "tokens": ["\u00bb", "wo"], "token_info": ["punct", "word"], "pos": ["$(", "PWAV"], "meter": "+", "measure": "single.up"}, "line.2": {"text": "sind deine Haare \u2013", "tokens": ["sind", "dei\u00b7ne", "Haa\u00b7re", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "What did I kiss that girl,", "tokens": ["What", "did", "I", "kiss", "that", "girl", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "FM.la", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "du mu\u00dft nach Berlin,", "tokens": ["du", "mu\u00dft", "nach", "Ber\u00b7lin", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "NE", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Barcelona \u2013 Parlez-vous fran\u00e7ais?\u00ab", "tokens": ["Bar\u00b7ce\u00b7lo\u00b7na", "\u2013", "Pa\u00b7rle\u00b7zvous", "fran\u00e7ais", "?", "\u00ab"], "token_info": ["word", "punct", "word", "word", "punct", "punct"], "pos": ["NE", "$(", "FM", "FM", "$.", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.4": {"line.1": {"text": "In allen Ateliers n\u00e4seln die Grammophone;", "tokens": ["In", "al\u00b7len", "A\u00b7te\u00b7liers", "n\u00e4\u00b7seln", "die", "Gram\u00b7mo\u00b7pho\u00b7ne", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": " weinrot stehn die Lampions in der grauen Luft \u2013 die Frau ist gar nicht so ohne \u2013", "tokens": ["wein\u00b7rot", "stehn", "die", "Lam\u00b7pi\u00b7ons", "in", "der", "grau\u00b7en", "Luft", "\u2013", "die", "Frau", "ist", "gar", "nicht", "so", "oh\u00b7ne", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$(", "ART", "NN", "VAFIN", "ADV", "PTKNEG", "ADV", "APPR", "$("], "meter": "-+--+-+--+-+-+--+-+-", "measure": "iambic.octa.plus.relaxed"}, "line.3": {"text": " kein Licht machen! Treten Sie nicht auf die Paare!", "tokens": ["kein", "Licht", "ma\u00b7chen", "!", "Tre\u00b7ten", "Sie", "nicht", "auf", "die", "Paa\u00b7re", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVINF", "$.", "NN", "PPER", "PTKNEG", "APPR", "ART", "NN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": " wo sind deine Haare \u2013?", "tokens": ["wo", "sind", "dei\u00b7ne", "Haa\u00b7re", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VAFIN", "PPOSAT", "NN", "$(", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": " august . . .", "tokens": ["au\u00b7gust", ".", ".", "."], "token_info": ["word", "punct", "punct", "punct"], "pos": ["VVFIN", "$.", "$.", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.5": {"line.1": {"text": "berliner Knabe, der du dich kaum noch bem\u00fchst!", "tokens": ["ber\u00b7li\u00b7ner", "Kna\u00b7be", ",", "der", "du", "dich", "kaum", "noch", "be\u00b7m\u00fchst", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "PRELS", "PPER", "PRF", "ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das Wo ist meistens schwieriger als das Ob \u2013", "tokens": ["Das", "Wo", "ist", "meis\u00b7tens", "schwie\u00b7ri\u00b7ger", "als", "das", "Ob", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PWAV", "VAFIN", "ADV", "ADJD", "KOKOM", "ART", "NN", "$("], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Aphrodite mit dem berliner Kopp!", "tokens": ["A\u00b7phro\u00b7di\u00b7te", "mit", "dem", "ber\u00b7li\u00b7ner", "Kopp", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "Aphrodite, schaumgeborne, la\u00df mal sehn,", "tokens": ["A\u00b7phro\u00b7di\u00b7te", ",", "schaum\u00b7ge\u00b7bor\u00b7ne", ",", "la\u00df", "mal", "sehn", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "$,", "VVIMP", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.2": {"text": " wie sie alle, alle mit dir angeln gehn!", "tokens": ["wie", "sie", "al\u00b7le", ",", "al\u00b7le", "mit", "dir", "an\u00b7geln", "gehn", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PIS", "$,", "PIS", "APPR", "PPER", "VVINF", "VVINF", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.8": {"line.1": {"text": "\u00bbhallo? Wie is Ihn denn gestern bekomm? Gut? ja? Ausgeschlafen?", "tokens": ["\u00bb", "hal\u00b7lo", "?", "Wie", "is", "Ihn", "denn", "ge\u00b7stern", "be\u00b7komm", "?", "Gut", "?", "ja", "?", "Aus\u00b7ge\u00b7schla\u00b7fen", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["$(", "ITJ", "$.", "PWAV", "FM", "PPER", "ADV", "ADV", "VVFIN", "$.", "NN", "$.", "ADV", "$.", "NN", "$."], "meter": "+--+-+-+--+-+-+-", "measure": "iambic.septa.invert"}}, "stanza.9": {"line.1": {"text": "Hach! Daran kann ich mich gahnich erinnern. Nein. Der hat doch", "tokens": ["Hach", "!", "Da\u00b7ran", "kann", "ich", "mich", "gah\u00b7nich", "e\u00b7rin\u00b7nern", ".", "Nein", ".", "Der", "hat", "doch"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NE", "$.", "PAV", "VMFIN", "PPER", "PRF", "ADJD", "VVINF", "$.", "PTKANT", "$.", "PDS", "VAFIN", "ADV"], "meter": "+-+--++-++--+-+", "measure": "trochaic.octa.plus.relaxed"}}, "stanza.10": {"line.1": {"text": "Sonja das Chinesenkost\u00fcm geliehn . . .!\u00ab", "tokens": ["Son\u00b7ja", "das", "Chi\u00b7ne\u00b7sen\u00b7kos\u00b7t\u00fcm", "ge\u00b7liehn", ".", ".", ".", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "punct", "punct", "punct"], "pos": ["ADV", "ART", "NN", "VVINF", "$.", "$.", "$.", "$.", "$("], "meter": "+--+-+-+-+", "measure": "iambic.penta.invert"}}, "stanza.11": {"line.1": {"text": "Als w\u00e4r nie nichts gewesen", "tokens": ["Als", "w\u00e4r", "nie", "nichts", "ge\u00b7we\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "VAFIN", "ADV", "PIS", "VAPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "telefonieren drei\u00dfigtausend Paare in Berlin.", "tokens": ["te\u00b7le\u00b7fo\u00b7nie\u00b7ren", "drei\u00b7\u00dfig\u00b7tau\u00b7send", "Paa\u00b7re", "in", "Ber\u00b7lin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "CARD", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+-+--++", "measure": "iambic.septa.relaxed"}}}}}