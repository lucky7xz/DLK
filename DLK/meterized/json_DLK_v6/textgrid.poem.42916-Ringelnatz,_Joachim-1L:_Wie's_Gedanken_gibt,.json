{"textgrid.poem.42916": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wie's Gedanken gibt,", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie's Gedanken gibt,", "tokens": ["Wie's", "Ge\u00b7dan\u00b7ken", "gibt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "NN", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Die durch Stein und Welten gehn,", "tokens": ["Die", "durch", "Stein", "und", "Wel\u00b7ten", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NN", "KON", "NN", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Kann's geschehn,", "tokens": ["Kann's", "ge\u00b7schehn", ","], "token_info": ["word", "word", "punct"], "pos": ["NE", "VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.4": {"text": "Da\u00df die Fliege den Ichthyosaurus liebt.", "tokens": ["Da\u00df", "die", "Flie\u00b7ge", "den", "Ich\u00b7thy\u00b7o\u00b7sau\u00b7rus", "liebt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NE", "VVFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.2": {"line.1": {"text": "Still ist's im Museumssaal.", "tokens": ["Still", "ist's", "im", "Mu\u00b7se\u00b7ums\u00b7saal", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "APPRART", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.3": {"line.1": {"text": "\u00bblieber Freund, ich liege", "tokens": ["\u00bb", "lie\u00b7ber", "Freund", ",", "ich", "lie\u00b7ge"], "token_info": ["punct", "word", "word", "punct", "word", "word"], "pos": ["$(", "ADV", "NN", "$,", "PPER", "VVFIN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Fest in Bernstein\u00ab, sagt die Fliege,", "tokens": ["Fest", "in", "Bern\u00b7stein", "\u00ab", ",", "sagt", "die", "Flie\u00b7ge", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$(", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbbernstein ist ein Mineral.", "tokens": ["\u00bb", "bern\u00b7stein", "ist", "ein", "Mi\u00b7ne\u00b7ral", "."], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJD", "VAFIN", "ART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Und ich liebe dich, du Riesenexemplar,", "tokens": ["Und", "ich", "lie\u00b7be", "dich", ",", "du", "Rie\u00b7sen\u00b7ex\u00b7emp\u00b7lar", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "$,", "PPER", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.5": {"text": "Und ich m\u00f6chte deinetwegen", "tokens": ["Und", "ich", "m\u00f6ch\u00b7te", "dei\u00b7net\u00b7we\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "PPOSAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Nur noch einmal Eier legen.\u00ab", "tokens": ["Nur", "noch", "ein\u00b7mal", "Ei\u00b7er", "le\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADV", "ADV", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "\u00bbbernstein?", "tokens": ["\u00bb", "bern\u00b7stein", "?"], "token_info": ["punct", "word", "punct"], "pos": ["$(", "PTKVZ", "$."], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Kann gern sein\u00ab,", "tokens": ["Kann", "gern", "sein", "\u00ab", ","], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ADV", "VAINF", "$(", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Sagt das Ichthyosau,", "tokens": ["Sagt", "das", "Ich\u00b7thy\u00b7o\u00b7sau", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "\u00bbaber ich bin auch eine Frau,", "tokens": ["\u00bb", "a\u00b7ber", "ich", "bin", "auch", "ei\u00b7ne", "Frau", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "PPER", "VAFIN", "ADV", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Eine sehr entschlossene sogar.", "tokens": ["Ei\u00b7ne", "sehr", "ent\u00b7schlos\u00b7se\u00b7ne", "so\u00b7gar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "Weil ich noch in dem Momente,", "tokens": ["Weil", "ich", "noch", "in", "dem", "Mo\u00b7men\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.7": {"text": "Als gewisse Elemente", "tokens": ["Als", "ge\u00b7wis\u00b7se", "E\u00b7le\u00b7men\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "ADJA", "NN"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.8": {"text": "Mich erstickten, noch ein Kind halb gebar.\u00ab", "tokens": ["Mich", "er\u00b7stick\u00b7ten", ",", "noch", "ein", "Kind", "halb", "ge\u00b7bar", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "$,", "ADV", "ART", "NN", "ADJD", "ADJD", "$.", "$("], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.5": {"line.1": {"text": "\u00bbeier oder lebendig \u2013 \u2013\u00ab,", "tokens": ["\u00bb", "ei\u00b7er", "o\u00b7der", "le\u00b7ben\u00b7dig", "\u2013", "\u2013", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["$(", "ADJD", "KON", "ADJD", "$(", "$(", "$(", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "Sagt die Fliege, \u00bbwir wohnen", "tokens": ["Sagt", "die", "Flie\u00b7ge", ",", "\u00bb", "wir", "woh\u00b7nen"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "$(", "PPER", "VVINF"], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Beide auf der Welt seit Millionen", "tokens": ["Bei\u00b7de", "auf", "der", "Welt", "seit", "Mil\u00b7lion\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "APPR", "ART", "NN", "APPR", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Jahren. \u2013 Wissen Sie die Zahl noch auswendig?\u00ab", "tokens": ["Jah\u00b7ren", ".", "\u2013", "Wis\u00b7sen", "Sie", "die", "Zahl", "noch", "aus\u00b7wen\u00b7dig", "?", "\u00ab"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$.", "$(", "VVFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "\u00bbnicht so ganz genau\u00ab,", "tokens": ["\u00bb", "nicht", "so", "ganz", "ge\u00b7nau", "\u00ab", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PTKNEG", "ADV", "ADV", "ADJD", "$(", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Sagt Frau Ichthyosau,", "tokens": ["Sagt", "Frau", "Ich\u00b7thy\u00b7o\u00b7sau", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "NE", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbaber wollen wir doch nicht sentimental", "tokens": ["\u00bb", "a\u00b7ber", "wol\u00b7len", "wir", "doch", "nicht", "sen\u00b7ti\u00b7men\u00b7tal"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "VMFIN", "PPER", "ADV", "PTKNEG", "ADJD"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Fl\u00f6ten oder winseln.", "tokens": ["Fl\u00f6\u00b7ten", "o\u00b7der", "win\u00b7seln", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "Nein, versuchen wir jetzt wieder einmal,", "tokens": ["Nein", ",", "ver\u00b7su\u00b7chen", "wir", "jetzt", "wie\u00b7der", "ein\u00b7mal", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "VVFIN", "PPER", "ADV", "ADV", "ADV", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.6": {"text": "Ganz verliebt einander anzublinzeln.\u00ab", "tokens": ["Ganz", "ver\u00b7liebt", "ein\u00b7an\u00b7der", "an\u00b7zu\u00b7blin\u00b7zeln", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PRF", "VVIZU", "$.", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "Da betrat den Museumssaal", "tokens": ["Da", "be\u00b7trat", "den", "Mu\u00b7se\u00b7ums\u00b7saal"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN"], "meter": "--+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Der pensionsberechtigte Museumsw\u00e4rter.", "tokens": ["Der", "pen\u00b7si\u00b7ons\u00b7be\u00b7rech\u00b7tig\u00b7te", "Mu\u00b7se\u00b7ums\u00b7w\u00e4r\u00b7ter", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und da blinzelten die beiden nicht.", "tokens": ["Und", "da", "blin\u00b7zel\u00b7ten", "die", "bei\u00b7den", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "PIAT", "PTKNEG", "$."], "meter": "--+---+-+", "measure": "anapaest.init"}, "line.4": {"text": "Denn solch W\u00e4rter", "tokens": ["Denn", "solch", "W\u00e4r\u00b7ter"], "token_info": ["word", "word", "word"], "pos": ["KON", "PIAT", "NN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Tut eben seine Pflicht", "tokens": ["Tut", "e\u00b7ben", "sei\u00b7ne", "Pflicht"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Und sch\u00fcrft nicht tiefer.", "tokens": ["Und", "sch\u00fcrft", "nicht", "tie\u00b7fer", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Denn Beamtenpflicht ist h\u00e4rter", "tokens": ["Denn", "Be\u00b7am\u00b7ten\u00b7pflicht", "ist", "h\u00e4r\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "NN", "VAFIN", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.8": {"text": "Als Bernstein und Schiefer.", "tokens": ["Als", "Bern\u00b7stein", "und", "Schie\u00b7fer", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}}}}