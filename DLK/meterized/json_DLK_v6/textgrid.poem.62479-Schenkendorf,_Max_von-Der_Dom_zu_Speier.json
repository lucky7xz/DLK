{"textgrid.poem.62479": {"metadata": {"author": {"name": "Schenkendorf, Max von", "birth": "N.A.", "death": "N.A."}, "title": "Der Dom zu Speier", "genre": "verse", "period": "N.A.", "pub_year": 1800, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich kenn' ein altes Gotteshaus", "tokens": ["Ich", "kenn'", "ein", "al\u00b7tes", "Got\u00b7tes\u00b7haus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An einem sch\u00f6nen Flu\u00df.", "tokens": ["An", "ei\u00b7nem", "sch\u00f6\u00b7nen", "Flu\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da l\u00f6schen alle Lampen aus,", "tokens": ["Da", "l\u00f6\u00b7schen", "al\u00b7le", "Lam\u00b7pen", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da h\u00f6rt die Jungfrau keinen Gru\u00df,", "tokens": ["Da", "h\u00f6rt", "die", "Jung\u00b7frau", "kei\u00b7nen", "Gru\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der Schiffer, der vor\u00fcberzieht", "tokens": ["Der", "Schif\u00b7fer", ",", "der", "vor\u00b7\u00fc\u00b7berz\u00b7ieht"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und seufzend nach den Tr\u00fcmmern sieht,", "tokens": ["Und", "seuf\u00b7zend", "nach", "den", "Tr\u00fcm\u00b7mern", "sieht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Erz\u00e4hlt von ferner Tage Feier \u2013", "tokens": ["Er\u00b7z\u00e4hlt", "von", "fer\u00b7ner", "Ta\u00b7ge", "Fei\u00b7er", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Das ist der hohe Dom zu Speier.", "tokens": ["Das", "ist", "der", "ho\u00b7he", "Dom", "zu", "Spei\u00b7er", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ich kenn' ein altes Kaisergrab,", "tokens": ["Ich", "kenn'", "ein", "al\u00b7tes", "Kai\u00b7ser\u00b7grab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein tiefes festes Haus,", "tokens": ["Ein", "tie\u00b7fes", "fes\u00b7tes", "Haus", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da stieg ein Heldenchor hinab,", "tokens": ["Da", "stieg", "ein", "Hel\u00b7den\u00b7chor", "hin\u00b7ab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zu ruh'n von langer Arbeit aus.", "tokens": ["Zu", "ruh'n", "von", "lan\u00b7ger", "Ar\u00b7beit", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die Kaisergr\u00e4ber sind entweiht,", "tokens": ["Die", "Kai\u00b7ser\u00b7gr\u00e4\u00b7ber", "sind", "ent\u00b7weiht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Kaisergr\u00e4ber sind entweiht,", "tokens": ["Die", "Kai\u00b7ser\u00b7gr\u00e4\u00b7ber", "sind", "ent\u00b7weiht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Erbrochen wurden diese Gr\u00fcfte,", "tokens": ["Er\u00b7bro\u00b7chen", "wur\u00b7den", "die\u00b7se", "Gr\u00fcf\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die Asche flog in alle L\u00fcfte.", "tokens": ["Die", "A\u00b7sche", "flog", "in", "al\u00b7le", "L\u00fcf\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Der lang einst unbegraben lag,", "tokens": ["Der", "lang", "einst", "un\u00b7be\u00b7gra\u00b7ben", "lag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat wieder keine Gruft,", "tokens": ["Hat", "wie\u00b7der", "kei\u00b7ne", "Gruft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Der Heinrich, welcher manchen Tag", "tokens": ["Der", "Hein\u00b7rich", ",", "wel\u00b7cher", "man\u00b7chen", "Tag"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NE", "$,", "PRELS", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Pilgrim stand in Winterluft;", "tokens": ["Ein", "Pil\u00b7grim", "stand", "in", "Win\u00b7ter\u00b7luft", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Philipp und Albrecht sind vom Schwert", "tokens": ["Phi\u00b7lipp", "und", "Al\u00b7brecht", "sind", "vom", "Schwert"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "KON", "NN", "VAFIN", "APPRART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.6": {"text": "So schmerzlich nicht, als hier versehrt.", "tokens": ["So", "schmerz\u00b7lich", "nicht", ",", "als", "hier", "ver\u00b7sehrt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKNEG", "$,", "KOUS", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "O Rudolph, der das Reich errettet,", "tokens": ["O", "Ru\u00b7dolph", ",", "der", "das", "Reich", "er\u00b7ret\u00b7tet", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "PRELS", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Wie schimpflich wurde dir gebettet.", "tokens": ["Wie", "schimpf\u00b7lich", "wur\u00b7de", "dir", "ge\u00b7bet\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Die lagen hier und manches Herz,", "tokens": ["Die", "la\u00b7gen", "hier", "und", "man\u00b7ches", "Herz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das lang geseufzt nach Ruh';", "tokens": ["Das", "lang", "ge\u00b7seufzt", "nach", "Ruh'", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "VVPP", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "O Leichenspott, o Leichenschmerz,", "tokens": ["O", "Lei\u00b7chen\u00b7spott", ",", "o", "Lei\u00b7chen\u00b7schmerz", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "FM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wer r\u00e4chet dich? Wann endest du?", "tokens": ["Wer", "r\u00e4\u00b7chet", "dich", "?", "Wann", "en\u00b7dest", "du", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$.", "PWAV", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wer war es, der die Gr\u00e4ber brach,", "tokens": ["Wer", "war", "es", ",", "der", "die", "Gr\u00e4\u00b7ber", "brach", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und hier die Gottesl\u00e4st'rung sprach?", "tokens": ["Und", "hier", "die", "Got\u00b7tes\u00b7l\u00e4st'\u00b7rung", "sprach", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Laut werd' es aller Welt verk\u00fcndigt:", "tokens": ["Laut", "werd'", "es", "al\u00b7ler", "Welt", "ver\u00b7k\u00fcn\u00b7digt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die Welschen haben so ges\u00fcndigt!", "tokens": ["Die", "Wel\u00b7schen", "ha\u00b7ben", "so", "ge\u00b7s\u00fcn\u00b7digt", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "O Deutschland, reiches Vaterland,", "tokens": ["O", "Deutschland", ",", "rei\u00b7ches", "Va\u00b7ter\u00b7land", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ein Grab f\u00fcr deine Herrn!", "tokens": ["Ein", "Grab", "f\u00fcr", "dei\u00b7ne", "Herrn", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Nur Stein und Erde, wenig Sand!", "tokens": ["Nur", "Stein", "und", "Er\u00b7de", ",", "we\u00b7nig", "Sand", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "NN", "$,", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In deutscher Erde ruh'n sie gern.", "tokens": ["In", "deut\u00b7scher", "Er\u00b7de", "ruh'n", "sie", "gern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dann grabe du dem Leichenstein", "tokens": ["Dann", "gra\u00b7be", "du", "dem", "Lei\u00b7chen\u00b7stein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Heldenwort, ein deutsches, ein:", "tokens": ["Ein", "Hel\u00b7den\u00b7wort", ",", "ein", "deut\u00b7sches", ",", "ein", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJA", "$,", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Die Schmach der Gr\u00e4ber ist gerochen,", "tokens": ["Die", "Schmach", "der", "Gr\u00e4\u00b7ber", "ist", "ge\u00b7ro\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und Babels Mauern sind gebrochen.", "tokens": ["Und", "Ba\u00b7bels", "Mau\u00b7ern", "sind", "ge\u00b7bro\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "O Bischofsthum, o Gotteshaus,", "tokens": ["O", "Bi\u00b7schofst\u00b7hum", ",", "o", "Got\u00b7tes\u00b7haus", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "FM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu zeugen am Gericht,", "tokens": ["Zu", "zeu\u00b7gen", "am", "Ge\u00b7richt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Steht immerfort in Schutt und Graus \u2013", "tokens": ["Steht", "im\u00b7mer\u00b7fort", "in", "Schutt", "und", "Graus", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wir bau'n euch f\u00fcrder nicht.", "tokens": ["Wir", "bau'n", "euch", "f\u00fcr\u00b7der", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKNEG", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Doch unsern Kaisern wird ein Mal", "tokens": ["Doch", "un\u00b7sern", "Kai\u00b7sern", "wird", "ein", "Mal"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Erheben sich im Sonnenstrahl:", "tokens": ["Er\u00b7he\u00b7ben", "sich", "im", "Son\u00b7nen\u00b7strahl", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Man soll das ganze Reich der Freien", "tokens": ["Man", "soll", "das", "gan\u00b7ze", "Reich", "der", "Frei\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PIS", "VMFIN", "ART", "ADJA", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Zum Denkmal deutscher Helden weihen.", "tokens": ["Zum", "Denk\u00b7mal", "deut\u00b7scher", "Hel\u00b7den", "wei\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}