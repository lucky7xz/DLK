{"textgrid.poem.48206": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "R\u00fcckblick", "genre": "verse", "period": "N.A.", "pub_year": 1886, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es geht zu End', und ich blicke zur\u00fcck.", "tokens": ["Es", "geht", "zu", "End'", ",", "und", "ich", "bli\u00b7cke", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,", "KON", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wie war mein Leben? wie war mein Gl\u00fcck?", "tokens": ["Wie", "war", "mein", "Le\u00b7ben", "?", "wie", "war", "mein", "Gl\u00fcck", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPOSAT", "NN", "$.", "PWAV", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.2": {"line.1": {"text": "Ich sa\u00df und machte meine Schuh;", "tokens": ["Ich", "sa\u00df", "und", "mach\u00b7te", "mei\u00b7ne", "Schuh", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Unter Lob und Tadel sah man mir zu.", "tokens": ["Un\u00b7ter", "Lob", "und", "Ta\u00b7del", "sah", "man", "mir", "zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVFIN", "PIS", "PPER", "PTKVZ", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.3": {"line.1": {"text": "\u00bbdu dichtest, das ist das Wichtigste ...\u00ab", "tokens": ["\u00bb", "du", "dich\u00b7test", ",", "das", "ist", "das", "Wich\u00b7tigs\u00b7te", "...", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "$,", "PDS", "VAFIN", "ART", "NN", "$(", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "\u00bbdu dichtest, das ist das Nichtigste.\u00ab", "tokens": ["\u00bb", "du", "dich\u00b7test", ",", "das", "ist", "das", "Nich\u00b7tigs\u00b7te", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "$,", "PDS", "VAFIN", "ART", "NN", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "\u00bbwenn Dichtung uns nicht zum Himmel tr\u00fcge ...\u00ab", "tokens": ["\u00bb", "wenn", "Dich\u00b7tung", "uns", "nicht", "zum", "Him\u00b7mel", "tr\u00fc\u00b7ge", "...", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KOUS", "NN", "PPER", "PTKNEG", "APPRART", "NN", "VVFIN", "$(", "$("], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbphantastereien, Unsinn, L\u00fcge!\u00ab", "tokens": ["\u00bb", "phan\u00b7tas\u00b7te\u00b7rei\u00b7en", ",", "Un\u00b7sinn", ",", "L\u00fc\u00b7ge", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "$,", "NN", "$,", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "\u00bbg\u00f6ttlicher Funke, Prometheusfeuer ...\u00ab", "tokens": ["\u00bb", "g\u00f6tt\u00b7li\u00b7cher", "Fun\u00b7ke", ",", "Pro\u00b7me\u00b7theus\u00b7feu\u00b7er", "...", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "NN", "$(", "$("], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "\u00bbzirpende Grille, leere Scheuer!\u00ab", "tokens": ["\u00bb", "zir\u00b7pen\u00b7de", "Gril\u00b7le", ",", "lee\u00b7re", "Scheu\u00b7er", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "ADJA", "NN", "$.", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.6": {"line.1": {"text": "Von hundert geliebt, von tausend mi\u00dfacht't,", "tokens": ["Von", "hun\u00b7dert", "ge\u00b7liebt", ",", "von", "tau\u00b7send", "mi\u00df\u00b7acht't", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "VVPP", "$,", "APPR", "CARD", "VVFIN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "So hab' ich meine Tage verbracht.", "tokens": ["So", "hab'", "ich", "mei\u00b7ne", "Ta\u00b7ge", "ver\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}}}}