{"dta.poem.1609": {"metadata": {"author": {"name": "Abschatz, Hans Assmann von", "birth": "N.A.", "death": "N.A."}, "title": "Ehren-Gedichte.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1704", "urn": "urn:nbn:de:kobv:b4-200905199889", "language": ["de:0.99"], "booktitle": "Abschatz, Hans Assmann von: Poetische Ubersetzungen und Gedichte. Leipzig, 1704."}, "poem": {"stanza.1": {"line.1": {"text": "Was ist der kurtze Ruff/ der mit ins Grab versinckt/ ", "tokens": ["Was", "ist", "der", "kurt\u00b7ze", "Ruff", "/", "der", "mit", "ins", "Grab", "ver\u00b7sinckt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "$(", "ART", "APPR", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Dafern er aus der Grufft nicht ewig widerschallet?", "tokens": ["Da\u00b7fern", "er", "aus", "der", "Grufft", "nicht", "e\u00b7wig", "wi\u00b7der\u00b7schal\u00b7let", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ART", "NN", "PTKNEG", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein schneller Blitz/ der zwar von Ost bi\u00df Westen blinckt/", "tokens": ["Ein", "schnel\u00b7ler", "Blitz", "/", "der", "zwar", "von", "Ost", "bi\u00df", "Wes\u00b7ten", "blinckt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "ADV", "APPR", "NN", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Doch bald vergessen ist/ wenn drauff kein Donner knallet/", "tokens": ["Doch", "bald", "ver\u00b7ges\u00b7sen", "ist", "/", "wenn", "drauff", "kein", "Don\u00b7ner", "knal\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVPP", "VAFIN", "$(", "KOUS", "PAV", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ein Rauch/ der bald verfliegt/ ein Wind/ der bald verstreichet/", "tokens": ["Ein", "Rauch", "/", "der", "bald", "ver\u00b7fliegt", "/", "ein", "Wind", "/", "der", "bald", "ver\u00b7strei\u00b7chet", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "ART", "ADV", "VVPP", "$(", "ART", "NN", "$(", "ART", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ein Irrlicht/ dessen Schein f\u00fcr neuer Sonn erbleichet.", "tokens": ["Ein", "Irr\u00b7licht", "/", "des\u00b7sen", "Schein", "f\u00fcr", "neu\u00b7er", "Sonn", "er\u00b7blei\u00b7chet", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELAT", "NN", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Wie bald verkocht in uns die Hand voll k\u00fchnes Blutt?", "tokens": ["Wie", "bald", "ver\u00b7kocht", "in", "uns", "die", "Hand", "voll", "k\u00fch\u00b7nes", "Blutt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVFIN", "APPR", "PPER", "ART", "NN", "ADJD", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wie eilends pflegt das Tacht des Lebens auszubrennen/", "tokens": ["Wie", "ei\u00b7lends", "pflegt", "das", "Tacht", "des", "Le\u00b7bens", "aus\u00b7zu\u00b7bren\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Noch Hand noch Sch\u00e4del weist den edlen Geist und Mutt;", "tokens": ["Noch", "Hand", "noch", "Sch\u00e4\u00b7del", "weist", "den", "ed\u00b7len", "Geist", "und", "Mutt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "ADV", "NE", "VVFIN", "ART", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wer will den Zunder in der Todten-Asch erkennen?", "tokens": ["Wer", "will", "den", "Zun\u00b7der", "in", "der", "Tod\u00b7ten\u00b7Asch", "er\u00b7ken\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der/ welcher unser Lob erhalten soll auff Erden/", "tokens": ["Der", "/", "wel\u00b7cher", "un\u00b7ser", "Lob", "er\u00b7hal\u00b7ten", "soll", "auff", "Er\u00b7den", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$(", "PWAT", "PPOSAT", "NN", "VVINF", "VMFIN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Mu\u00df de\u00df in kurtzer Zeit ein stummer Zeuge werden.", "tokens": ["Mu\u00df", "de\u00df", "in", "kurt\u00b7zer", "Zeit", "ein", "stum\u00b7mer", "Zeu\u00b7ge", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "APPR", "ADJA", "NN", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Was hilffts den\u0303/ da\u00df ein Mensch nach grossem Nahmen strebt/", "tokens": ["Was", "hilffts", "de\u00f1", "/", "da\u00df", "ein", "Mensch", "nach", "gros\u00b7sem", "Nah\u00b7men", "strebt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "NE", "$(", "KOUS", "ART", "NN", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wenn sein Ged\u00e4chtnis nicht kan zu der Nachwelt dringen?", "tokens": ["Wenn", "sein", "Ge\u00b7d\u00e4cht\u00b7nis", "nicht", "kan", "zu", "der", "Nach\u00b7welt", "drin\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "PTKNEG", "VMFIN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "F\u00fcr Agamemnons Zeit hat mancher Held gelebt/", "tokens": ["F\u00fcr", "A\u00b7ga\u00b7mem\u00b7nons", "Zeit", "hat", "man\u00b7cher", "Held", "ge\u00b7lebt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "VAFIN", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Den seiner Tugend Prei\u00df zun Sternen k\u00f6nnen bringen/", "tokens": ["Den", "sei\u00b7ner", "Tu\u00b7gend", "Prei\u00df", "zun", "Ster\u00b7nen", "k\u00f6n\u00b7nen", "brin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "NN", "APPRART", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Weil aber kein Homer zu ihm sich hat gefunden/", "tokens": ["Weil", "a\u00b7ber", "kein", "Ho\u00b7mer", "zu", "ihm", "sich", "hat", "ge\u00b7fun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "NE", "APPR", "PPER", "PRF", "VAFIN", "VVPP", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Ist seiner Thaten Glantz in dunckler Nacht verschwunden.", "tokens": ["Ist", "sei\u00b7ner", "Tha\u00b7ten", "Glantz", "in", "dunck\u00b7ler", "Nacht", "ver\u00b7schwun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "NN", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Braucht allen Aloe und Balsam alter Welt/", "tokens": ["Braucht", "al\u00b7len", "A\u00b7loe", "und", "Bal\u00b7sam", "al\u00b7ter", "Welt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "KON", "NN", "ADJA", "NN", "$("], "meter": "-+----+-+-+", "measure": "dactylic.init"}, "line.2": {"text": "Bemahlt/ nach Sothis Art/ die theuren Leichen-Kittel/", "tokens": ["Be\u00b7mahlt", "/", "nach", "Sot\u00b7his", "Art", "/", "die", "theu\u00b7ren", "Lei\u00b7chen\u00b7Kit\u00b7tel", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "APPR", "NE", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Schnizt feste Cedern aus mit fremden Leim verquellt/", "tokens": ["Sch\u00b7nizt", "fes\u00b7te", "Ce\u00b7dern", "aus", "mit", "frem\u00b7den", "Leim", "ver\u00b7quellt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJA", "NN", "APPR", "APPR", "ADJA", "NN", "VVPP", "$("], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.4": {"text": "Bezeichnet Tuch und Sarg mit Bildern gro\u00dfer Tittel;", "tokens": ["Be\u00b7zeich\u00b7net", "Tuch", "und", "Sarg", "mit", "Bil\u00b7dern", "gro\u00b7\u00dfer", "Tit\u00b7tel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "KON", "NN", "APPR", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wird nicht ein Oedipus die schwartze Brust entdecken/", "tokens": ["Wird", "nicht", "ein", "O\u00b7e\u00b7di\u00b7pus", "die", "schwart\u00b7ze", "Brust", "ent\u00b7de\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "ART", "NN", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.6": {"text": "Bleibt im Verwesen doch eur Stand und Wesen stecken.", "tokens": ["Bleibt", "im", "Ver\u00b7we\u00b7sen", "doch", "eur", "Stand", "und", "We\u00b7sen", "ste\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "ADV", "PPOSAT", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Baut hohe Gr\u00e4ber auff/ bedeckt mit einer Last", "tokens": ["Baut", "ho\u00b7he", "Gr\u00e4\u00b7ber", "auff", "/", "be\u00b7deckt", "mit", "ei\u00b7ner", "Last"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJA", "NN", "APPR", "$(", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von Jaspis und Porphyr die dorrenden Gebeine/", "tokens": ["Von", "Jas\u00b7pis", "und", "Por\u00b7phyr", "die", "dor\u00b7ren\u00b7den", "Ge\u00b7bei\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NN", "ART", "ADJA", "NN", "$("], "meter": "-+--+--+---+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Schreibt Nahmen/ Thun und Amt in Taffend und Damast/", "tokens": ["Schreibt", "Nah\u00b7men", "/", "Thun", "und", "Amt", "in", "Taf\u00b7fend", "und", "Da\u00b7mast", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$(", "NN", "KON", "NN", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "In Holtz/ in Gold und Ertz/ in festen Stahl und Steine;", "tokens": ["In", "Holtz", "/", "in", "Gold", "und", "Ertz", "/", "in", "fes\u00b7ten", "Stahl", "und", "Stei\u00b7ne", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "APPR", "NN", "KON", "NN", "$(", "APPR", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Zeit/ Moder/ F\u00e4ule/ Rost wei\u00df alles zu entstalten:", "tokens": ["Zeit", "/", "Mo\u00b7der", "/", "F\u00e4u\u00b7le", "/", "Rost", "wei\u00df", "al\u00b7les", "zu", "ent\u00b7stal\u00b7ten", ":"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "NN", "$(", "NN", "VVFIN", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Des Nachruhms Ewigkeit ist anders zu erhalten.", "tokens": ["Des", "Nach\u00b7ruhms", "E\u00b7wig\u00b7keit", "ist", "an\u00b7ders", "zu", "er\u00b7hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Sucht in des C\u00f6rpers Glutt f\u00fcr todten Nahmen Licht/", "tokens": ["Sucht", "in", "des", "C\u00f6r\u00b7pers", "Glutt", "f\u00fcr", "tod\u00b7ten", "Nah\u00b7men", "Licht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "NN", "APPR", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es wird sein Glantz so bald als diese Flamme schwinden.", "tokens": ["Es", "wird", "sein", "Glantz", "so", "bald", "als", "die\u00b7se", "Flam\u00b7me", "schwin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "ADV", "ADV", "KOUS", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ein unverzehrlich Oel/ wenn sein Gef\u00e4sse bricht/", "tokens": ["Ein", "un\u00b7ver\u00b7zehr\u00b7lich", "O\u00b7el", "/", "wenn", "sein", "Ge\u00b7f\u00e4s\u00b7se", "bricht", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "$(", "KOUS", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Mu\u00df durch die Lufft ber\u00fchrt samt eurem Ruhm erblinden.", "tokens": ["Mu\u00df", "durch", "die", "Lufft", "be\u00b7r\u00fchrt", "samt", "eu\u00b7rem", "Ruhm", "er\u00b7blin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ART", "NN", "VVFIN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der Mahler pflegt sein Licht mit Schatten zu erh\u00f6hen:", "tokens": ["Der", "Mah\u00b7ler", "pflegt", "sein", "Licht", "mit", "Schat\u00b7ten", "zu", "er\u00b7h\u00f6\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "In schwartzen Schrifften bleibt die Tugend helle stehen.", "tokens": ["In", "schwart\u00b7zen", "Schriff\u00b7ten", "bleibt", "die", "Tu\u00b7gend", "hel\u00b7le", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "ART", "NN", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Weil in Pela\u00dfger-Land die K\u00fcnste hielten Hau\u00df/", "tokens": ["Weil", "in", "Pe\u00b7la\u00df\u00b7ger\u00b7Land", "die", "K\u00fcns\u00b7te", "hiel\u00b7ten", "Hau\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "NN", "ART", "NN", "VVFIN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Sind seine Lorbeer-Zweig auch unversehrt beklieben;", "tokens": ["Sind", "sei\u00b7ne", "Lor\u00b7beer\u00b7Zweig", "auch", "un\u00b7ver\u00b7sehrt", "be\u00b7klie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Rom breit\u2019te seinen Ruhm durch Schwerdt und Feder aus:", "tokens": ["Rom", "breit'\u00b7te", "sei\u00b7nen", "Ruhm", "durch", "Schwerdt", "und", "Fe\u00b7der", "aus", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Was C\u00e4sar hat gethan/ das hat er auch geschrieben.", "tokens": ["Was", "C\u00e4\u00b7sar", "hat", "ge\u00b7than", "/", "das", "hat", "er", "auch", "ge\u00b7schrie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VAFIN", "VVPP", "$(", "PDS", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der Deutschen Dichterey/ der Barden Helden-Lieder", "tokens": ["Der", "Deut\u00b7schen", "Dich\u00b7te\u00b7rey", "/", "der", "Bar\u00b7den", "Hel\u00b7den\u00b7Lie\u00b7der"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Belebten Mannus Geist/ Tuiscons Asche wieder.", "tokens": ["Be\u00b7leb\u00b7ten", "Man\u00b7nus", "Geist", "/", "Tu\u00b7is\u00b7cons", "A\u00b7sche", "wie\u00b7der", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "NN", "$(", "NE", "NE", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Wem w\u00e4r Epaminond/ ohn kluge Schrifft/ bekandt?", "tokens": ["Wem", "w\u00e4r", "E\u00b7pa\u00b7mi\u00b7nond", "/", "ohn", "klu\u00b7ge", "Schrifft", "/", "be\u00b7kandt", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "VAFIN", "NE", "$(", "APPR", "ADJA", "NN", "$(", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wer wolte nach Athens und Spartens F\u00fcrsten fragen?", "tokens": ["Wer", "wol\u00b7te", "nach", "A\u00b7thens", "und", "Spar\u00b7tens", "F\u00fcrs\u00b7ten", "fra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "APPR", "NE", "KON", "NN", "NN", "VVINF", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Wo bliebe Lysimach/ der Leuen \u00fcberwand?", "tokens": ["Wo", "blie\u00b7be", "Ly\u00b7si\u00b7mach", "/", "der", "Leu\u00b7en", "\u00fc\u00b7ber\u00b7wand", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "NE", "$(", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "W\u00fcrd auch die Welt was mehr vom Gro\u00dfen Grichen sagen?", "tokens": ["W\u00fcrd", "auch", "die", "Welt", "was", "mehr", "vom", "Gro\u00b7\u00dfen", "Gri\u00b7chen", "sa\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "PWS", "ADV", "APPRART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Es h\u00e4tt ihr Nahme l\u00e4ngst/ wie sie/ vermodern m\u00fcssen/", "tokens": ["Es", "h\u00e4tt", "ihr", "Nah\u00b7me", "l\u00e4ngst", "/", "wie", "sie", "/", "ver\u00b7mo\u00b7dern", "m\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "ADV", "$(", "PWAV", "PPER", "$(", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wenn sie kein weises Buch der Sterbligkeit entrissen.", "tokens": ["Wenn", "sie", "kein", "wei\u00b7ses", "Buch", "der", "Ster\u00b7blig\u00b7keit", "ent\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "ADJA", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Izt w\u00e4r Horatius auff beyden Augen blind/", "tokens": ["Izt", "w\u00e4r", "Ho\u00b7ra\u00b7ti\u00b7us", "auff", "bey\u00b7den", "Au\u00b7gen", "blind", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "APPR", "PIAT", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Flamme k\u00fchner Hand/ die sich so frey vergriffen", "tokens": ["Die", "Flam\u00b7me", "k\u00fch\u00b7ner", "Hand", "/", "die", "sich", "so", "frey", "ver\u00b7grif\u00b7fen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN", "$(", "PRELS", "PRF", "ADV", "ADJD", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und freyer noch gestrafft/ verrauchet in den Wind/", "tokens": ["Und", "frey\u00b7er", "noch", "ge\u00b7strafft", "/", "ver\u00b7rau\u00b7chet", "in", "den", "Wind", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADV", "VVPP", "$(", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Duil umsonst/ so offt er Essen gieng/ bepfiffen/", "tokens": ["Du\u00b7il", "um\u00b7sonst", "/", "so", "offt", "er", "Es\u00b7sen", "gieng", "/", "be\u00b7pfif\u00b7fen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NE", "ADV", "$(", "ADV", "ADV", "PPER", "NN", "VVFIN", "$(", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Roms Schutz-Stab/ Scipio/ verfaulet und zerbrochen/", "tokens": ["Roms", "Schutz\u00b7Stab", "/", "Sci\u00b7pio", "/", "ver\u00b7fau\u00b7let", "und", "zer\u00b7bro\u00b7chen", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$(", "NE", "$(", "VVFIN", "KON", "VVPP", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.6": {"text": "Wenn nicht ein Livius f\u00fcr sie das Wort gesprochen.", "tokens": ["Wenn", "nicht", "ein", "Li\u00b7vius", "f\u00fcr", "sie", "das", "Wort", "ge\u00b7spro\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PTKNEG", "ART", "NE", "APPR", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.10": {"line.1": {"text": "Doch weil der Eitelkeit ein enges Ziel gesteckt/", "tokens": ["Doch", "weil", "der", "Ei\u00b7tel\u00b7keit", "ein", "en\u00b7ges", "Ziel", "ge\u00b7steckt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Weil B\u00fccher auch vergehn/ und Ehren-S\u00e4ulen wancken/", "tokens": ["Weil", "B\u00fc\u00b7cher", "auch", "ver\u00b7gehn", "/", "und", "Eh\u00b7ren\u00b7S\u00e4u\u00b7len", "wan\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "VVINF", "$(", "KON", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Siegs-Zeichen fallen um/ und Grau\u00df den Marmel deckt/", "tokens": ["Siegs\u00b7Zei\u00b7chen", "fal\u00b7len", "um", "/", "und", "Grau\u00df", "den", "Mar\u00b7mel", "deckt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "$(", "KON", "NN", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Weil Schrifften sich verliern aus Augen und Gedancken/", "tokens": ["Weil", "Schriff\u00b7ten", "sich", "ver\u00b7li\u00b7ern", "aus", "Au\u00b7gen", "und", "Ge\u00b7dan\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "PRF", "VVFIN", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Mu\u00df sie ein kluger Geist zu Zeiten wieder regen/", "tokens": ["Mu\u00df", "sie", "ein", "klu\u00b7ger", "Geist", "zu", "Zei\u00b7ten", "wie\u00b7der", "re\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "ADJA", "NN", "APPR", "NN", "ADV", "ADJA", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und auff die alte M\u00fcntz ein neues Bildnis pr\u00e4gen.", "tokens": ["Und", "auff", "die", "al\u00b7te", "M\u00fcntz", "ein", "neu\u00b7es", "Bild\u00b7nis", "pr\u00e4\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Eh Guttenberg die Kunst zu schreiben ohne Kiel/", "tokens": ["Eh", "Gut\u00b7ten\u00b7berg", "die", "Kunst", "zu", "schrei\u00b7ben", "oh\u00b7ne", "Kiel", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ART", "NN", "PTKZU", "VVINF", "APPR", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Zu reden f\u00fcr das Aug und W\u00f6rter abzumahlen", "tokens": ["Zu", "re\u00b7den", "f\u00fcr", "das", "Aug", "und", "W\u00f6r\u00b7ter", "ab\u00b7zu\u00b7mah\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "APPR", "ART", "NN", "KON", "NN", "VVIZU"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "In Deutschland auffgebracht/ als nur ein Rohr vom Ril/", "tokens": ["In", "Deutschland", "auff\u00b7ge\u00b7bracht", "/", "als", "nur", "ein", "Rohr", "vom", "Ril", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVPP", "$(", "KOKOM", "ADV", "ART", "NN", "APPRART", "NN", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Als Leinwand oder Wachs/ als Bl\u00e4tter oder Schalen/", "tokens": ["Als", "Lein\u00b7wand", "o\u00b7der", "Wachs", "/", "als", "Bl\u00e4t\u00b7ter", "o\u00b7der", "Scha\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "$(", "KOUS", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Als eines Thieres Haut allein gedient zu Schrifften/", "tokens": ["Als", "ei\u00b7nes", "Thie\u00b7res", "Haut", "al\u00b7lein", "ge\u00b7dient", "zu", "Schriff\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "NN", "ADV", "VVPP", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wer konte da der Welt ein lang Ged\u00e4chtnis stifften?", "tokens": ["Wer", "kon\u00b7te", "da", "der", "Welt", "ein", "lang", "Ge\u00b7d\u00e4cht\u00b7nis", "stiff\u00b7ten", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ADV", "ART", "NN", "ART", "ADJD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Wie sind Polybius und Dio mangelhafft?", "tokens": ["Wie", "sind", "Po\u00b7ly\u00b7bius", "und", "Dio", "man\u00b7gel\u00b7hafft", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "NE", "KON", "NE", "VVFIN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Was hat uns nicht die Zeit vom Tacitus genommen/", "tokens": ["Was", "hat", "uns", "nicht", "die", "Zeit", "vom", "Ta\u00b7ci\u00b7tus", "ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "PTKNEG", "ART", "NN", "APPRART", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Vom Curtius geraubt/ vom Crispus weggerafft/", "tokens": ["Vom", "Cur\u00b7tius", "ge\u00b7raubt", "/", "vom", "Cris\u00b7pus", "weg\u00b7ge\u00b7rafft", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NE", "VVPP", "$(", "APPRART", "NN", "VVFIN", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Was ist vom Ammian in unsre H\u00e4nde kommen?", "tokens": ["Was", "ist", "vom", "Am\u00b7mi\u00b7an", "in", "uns\u00b7re", "H\u00e4n\u00b7de", "kom\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "APPRART", "NE", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Viel andre haben zwar von andern viel geschrieben/", "tokens": ["Viel", "and\u00b7re", "ha\u00b7ben", "zwar", "von", "an\u00b7dern", "viel", "ge\u00b7schrie\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VAFIN", "ADV", "APPR", "PIS", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ihr Nahmen aber selbst ist uns kaum \u00fcbrig blieben.", "tokens": ["Ihr", "Nah\u00b7men", "a\u00b7ber", "selbst", "ist", "uns", "kaum", "\u00fcb\u00b7rig", "blie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "So hat der leichte Wind vorl\u00e4ngst davon gef\u00fchrt", "tokens": ["So", "hat", "der", "leich\u00b7te", "Wind", "vor\u00b7l\u00e4ngst", "da\u00b7von", "ge\u00b7f\u00fchrt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN", "VVFIN", "PAV", "VVPP"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Was Libis auffgesezt/ die Barden abgesungen.", "tokens": ["Was", "Li\u00b7bis", "auff\u00b7ge\u00b7sezt", "/", "die", "Bar\u00b7den", "ab\u00b7ge\u00b7sun\u00b7gen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "VVPP", "$(", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wo wird der zehnde Theil von diesem mehr gesp\u00fcrt/", "tokens": ["Wo", "wird", "der", "zehn\u00b7de", "Theil", "von", "die\u00b7sem", "mehr", "ge\u00b7sp\u00fcrt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "ADJA", "NN", "APPR", "PDAT", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Was noch zu Celtens Zeit geschwebt auff tausend Zungen?", "tokens": ["Was", "noch", "zu", "Cel\u00b7tens", "Zeit", "ge\u00b7schwebt", "auff", "tau\u00b7send", "Zun\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "APPR", "NN", "NN", "VVPP", "APPR", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und mu\u00df/ was \u00fcbrig ist/ nicht vollends untergehen/", "tokens": ["Und", "mu\u00df", "/", "was", "\u00fcb\u00b7rig", "ist", "/", "nicht", "vol\u00b7lends", "un\u00b7ter\u00b7ge\u00b7hen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "$(", "PWS", "ADJD", "VAFIN", "$(", "PTKNEG", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Weil kaum der Deutsche mehr den Deutschen kan verstehen?", "tokens": ["Weil", "kaum", "der", "Deut\u00b7sche", "mehr", "den", "Deut\u00b7schen", "kan", "ver\u00b7ste\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ADV", "ART", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Manch Ritter edlen Blutts besang was er gethan/", "tokens": ["Manch", "Rit\u00b7ter", "ed\u00b7len", "Blutts", "be\u00b7sang", "was", "er", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADJA", "NN", "VVFIN", "PWS", "PPER", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ob gleich sein Helden-Reim nicht klang in zarten Ohren/", "tokens": ["Ob", "gleich", "sein", "Hel\u00b7den\u00b7Reim", "nicht", "klang", "in", "zar\u00b7ten", "Oh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPOSAT", "NN", "PTKNEG", "VVFIN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Man trifft von alter Zeit mehr als ein Merckmahl an/", "tokens": ["Man", "trifft", "von", "al\u00b7ter", "Zeit", "mehr", "als", "ein", "Merck\u00b7mahl", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "ADJA", "NN", "PIAT", "KOKOM", "ART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Da\u00df unser Schlesien zur Dichterey gebohren/", "tokens": ["Da\u00df", "un\u00b7ser", "Schle\u00b7si\u00b7en", "zur", "Dich\u00b7te\u00b7rey", "ge\u00b7boh\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wann selber dessen F\u00fcrst/ ein Heinrich/ uns sein Lieben", "tokens": ["Wann", "sel\u00b7ber", "des\u00b7sen", "F\u00fcrst", "/", "ein", "Hein\u00b7rich", "/", "uns", "sein", "Lie\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "ADV", "ART", "NN", "$(", "ART", "NE", "$(", "PPER", "PPOSAT", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "(und anders mehr vielleicht) in Liedern hat beschrieben.", "tokens": ["(", "und", "an\u00b7ders", "mehr", "viel\u00b7leicht", ")", "in", "Lie\u00b7dern", "hat", "be\u00b7schrie\u00b7ben", "."], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "ADV", "ADV", "ADV", "$(", "APPR", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Die St\u00fccke sind zwar schlecht die auff uns kommen seyn/", "tokens": ["Die", "St\u00fc\u00b7cke", "sind", "zwar", "schlecht", "die", "auff", "uns", "kom\u00b7men", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "ART", "APPR", "PPER", "VVFIN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und kan man wenig Licht in solchem Schatten finden/", "tokens": ["Und", "kan", "man", "we\u00b7nig", "Licht", "in", "sol\u00b7chem", "Schat\u00b7ten", "fin\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "PIAT", "NN", "APPR", "PIAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die Funcken geben blo\u00df aus bleichen Kohlen Schein/", "tokens": ["Die", "Fun\u00b7cken", "ge\u00b7ben", "blo\u00df", "aus", "blei\u00b7chen", "Koh\u00b7len", "Schein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "APPR", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Doch sind sie unsern Sinn noch f\u00e4hig zu entz\u00fcnden:", "tokens": ["Doch", "sind", "sie", "un\u00b7sern", "Sinn", "noch", "f\u00e4\u00b7hig", "zu", "ent\u00b7z\u00fcn\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPOSAT", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und da\u00df die Kinder auch/ was Ahnen th\u00e4ten/ lernen/", "tokens": ["Und", "da\u00df", "die", "Kin\u00b7der", "auch", "/", "was", "Ah\u00b7nen", "th\u00e4\u00b7ten", "/", "ler\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "ADV", "$(", "PWS", "NN", "VVFIN", "$(", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "So mu\u00df ein neuer Glantz ihr dunckles Grab besternen.", "tokens": ["So", "mu\u00df", "ein", "neu\u00b7er", "Glantz", "ihr", "dunck\u00b7les", "Grab", "bes\u00b7ter\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "ADJA", "NN", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Ein Fremder schreibt von uns mit ungewisser Hand/", "tokens": ["Ein", "Frem\u00b7der", "schreibt", "von", "uns", "mit", "un\u00b7ge\u00b7wis\u00b7ser", "Hand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "PPER", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Sieht mit geborgtem Aug\u2019 und redt mit andrem Munde/", "tokens": ["Sieht", "mit", "ge\u00b7borg\u00b7tem", "Aug'", "und", "redt", "mit", "an\u00b7drem", "Mun\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ADJA", "NN", "KON", "VVFIN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ihm ist des Landes Art und Gegend unbekandt/", "tokens": ["Ihm", "ist", "des", "Lan\u00b7des", "Art", "und", "Ge\u00b7gend", "un\u00b7be\u00b7kandt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "NN", "KON", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Gemeiner Wahn und Ruff dient ihm zum falschen Grunde.", "tokens": ["Ge\u00b7mei\u00b7ner", "Wahn", "und", "Ruff", "dient", "ihm", "zum", "fal\u00b7schen", "Grun\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "KON", "NN", "VVFIN", "PPER", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Offt nimmt er Ort f\u00fcr Mann/ und/ was er recht soll nennen/", "tokens": ["Offt", "nimmt", "er", "Ort", "f\u00fcr", "Mann", "/", "und", "/", "was", "er", "recht", "soll", "nen\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "APPR", "NN", "$(", "KON", "$(", "PWS", "PPER", "ADV", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wird doch der Landsmann kaum in seiner Sprache kennen.", "tokens": ["Wird", "doch", "der", "Lands\u00b7mann", "kaum", "in", "sei\u00b7ner", "Spra\u00b7che", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}