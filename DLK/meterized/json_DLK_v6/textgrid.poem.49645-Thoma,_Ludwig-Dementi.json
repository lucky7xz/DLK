{"textgrid.poem.49645": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Dementi", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Man schrieb und sprach, wir Deutschen mischen", "tokens": ["Man", "schrieb", "und", "sprach", ",", "wir", "Deut\u00b7schen", "mi\u00b7schen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PIS", "VVFIN", "KON", "VVFIN", "$,", "PPER", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Uns bald in Ru\u00dflands Kunterbunt.", "tokens": ["Uns", "bald", "in", "Ru\u00df\u00b7lands", "Kun\u00b7ter\u00b7bunt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "APPR", "NE", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein solches M\u00e4rchen aufzutischen,", "tokens": ["Ein", "sol\u00b7ches", "M\u00e4r\u00b7chen", "auf\u00b7zu\u00b7ti\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Erscheint doch geistig ungesund!", "tokens": ["Er\u00b7scheint", "doch", "geis\u00b7tig", "un\u00b7ge\u00b7sund", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Der sch\u00f6pft' aus der Gemeinheit Tiefe,", "tokens": ["Der", "sch\u00f6pft'", "aus", "der", "Ge\u00b7mein\u00b7heit", "Tie\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der das aus seinen Fingern sog", "tokens": ["Der", "das", "aus", "sei\u00b7nen", "Fin\u00b7gern", "sog"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ART", "APPR", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und ohne alle positive", "tokens": ["Und", "oh\u00b7ne", "al\u00b7le", "po\u00b7si\u00b7ti\u00b7ve"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "PIAT", "ADJA"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Verdachtsmomente einfach log.", "tokens": ["Ver\u00b7dachts\u00b7mo\u00b7men\u00b7te", "ein\u00b7fach", "log", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Nur nicht auf hohem Rosse reiten!", "tokens": ["Nur", "nicht", "auf", "ho\u00b7hem", "Ros\u00b7se", "rei\u00b7ten", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und nur im Urteil nicht zu streng!", "tokens": ["Und", "nur", "im", "Ur\u00b7teil", "nicht", "zu", "streng", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPRART", "NN", "PTKNEG", "PTKZU", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn das Gebiet der M\u00f6glichkeiten,", "tokens": ["Denn", "das", "Ge\u00b7biet", "der", "M\u00f6g\u00b7lich\u00b7kei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es ist bei uns nicht allzu eng.", "tokens": ["Es", "ist", "bei", "uns", "nicht", "all\u00b7zu", "eng", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "PTKNEG", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "H\u00fcllt sich Berlin in tiefes Schweigen", "tokens": ["H\u00fcllt", "sich", "Ber\u00b7lin", "in", "tie\u00b7fes", "Schwei\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "NE", "APPR", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Und bietet seinen Senf nicht feil,", "tokens": ["Und", "bie\u00b7tet", "sei\u00b7nen", "Senf", "nicht", "feil", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "PTKNEG", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So wollen wir uns dankbar zeigen.", "tokens": ["So", "wol\u00b7len", "wir", "uns", "dank\u00b7bar", "zei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PRF", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch ", "tokens": ["Doch"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}}}}}