{"textgrid.poem.41393": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "Ben Haly", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Gelehrter Kenner der Gesetze,", "tokens": ["Ge\u00b7lehr\u00b7ter", "Ken\u00b7ner", "der", "Ge\u00b7set\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bei dem im Herzen Recht, im Munde Wahrheit gilt;", "tokens": ["Bei", "dem", "im", "Her\u00b7zen", "Recht", ",", "im", "Mun\u00b7de", "Wahr\u00b7heit", "gilt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "APPRART", "NN", "NN", "$,", "APPRART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Der nie mit m\u00fc\u00dfigem Geschw\u00e4tze", "tokens": ["Der", "nie", "mit", "m\u00fc\u00b7\u00dfi\u00b7gem", "Ge\u00b7schw\u00e4t\u00b7ze"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Hammoniens Gericht erf\u00fcllt!", "tokens": ["Ham\u00b7mo\u00b7ni\u00b7ens", "Ge\u00b7richt", "er\u00b7f\u00fcllt", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "NN", "VVPP", "$."], "meter": "---+-+-+", "measure": "unknown.measure.tri"}}, "stanza.2": {"line.1": {"text": "Nicht nur die Einsicht tr\u00fcber Sachen;", "tokens": ["Nicht", "nur", "die", "Ein\u00b7sicht", "tr\u00fc\u00b7ber", "Sa\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auch ein durch Ernst gem\u00e4\u00dfigt Lachen,", "tokens": ["Auch", "ein", "durch", "Ernst", "ge\u00b7m\u00e4\u00b7\u00dfigt", "La\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "APPR", "NE", "VVPP", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Auch Witz und Dichtkunst steht dir an.", "tokens": ["Auch", "Witz", "und", "Dicht\u00b7kunst", "steht", "dir", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "KON", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Erlaube mir, so gut ich kann,", "tokens": ["Er\u00b7lau\u00b7be", "mir", ",", "so", "gut", "ich", "kann", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "ADV", "ADJD", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den rechtserfahrnen Muselmann,", "tokens": ["Den", "rech\u00b7tser\u00b7fahr\u00b7nen", "Mu\u00b7sel\u00b7mann", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ben Haly dir bekannt zu machen.", "tokens": ["Ben", "Ha\u00b7ly", "dir", "be\u00b7kannt", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "PPER", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ein T\u00fcrk', der von Byzanz auf ferne Reisen eilet,", "tokens": ["Ein", "T\u00fcrk'", ",", "der", "von", "By\u00b7zanz", "auf", "fer\u00b7ne", "Rei\u00b7sen", "ei\u00b7let", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "APPR", "NE", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Besucht zum Abschied seinen Freund,", "tokens": ["Be\u00b7sucht", "zum", "Ab\u00b7schied", "sei\u00b7nen", "Freund", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den er getreu zu sein vermeint,", "tokens": ["Den", "er", "ge\u00b7treu", "zu", "sein", "ver\u00b7meint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADJD", "PTKZU", "VAINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit dem er oft sein Leid, und stets die Freude theilet.", "tokens": ["Mit", "dem", "er", "oft", "sein", "Leid", ",", "und", "stets", "die", "Freu\u00b7de", "thei\u00b7let", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADV", "PPOSAT", "NN", "$,", "KON", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Er spricht: Mich hat mit dir die beste Wahl vereint.", "tokens": ["Er", "spricht", ":", "Mich", "hat", "mit", "dir", "die", "bes\u00b7te", "Wahl", "ver\u00b7eint", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VAFIN", "APPR", "PPER", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Du wei\u00dft, wieviel ich schon durch Flei\u00df und Gl\u00fcck erworben;", "tokens": ["Du", "wei\u00dft", ",", "wie\u00b7viel", "ich", "schon", "durch", "Flei\u00df", "und", "Gl\u00fcck", "er\u00b7wor\u00b7ben", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWAV", "PPER", "ADV", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nur etwas ist dir unbekannt:", "tokens": ["Nur", "et\u00b7was", "ist", "dir", "un\u00b7be\u00b7kannt", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mein Schwager Amurat, der in Algier gestorben,", "tokens": ["Mein", "Schwa\u00b7ger", "A\u00b7mu\u00b7rat", ",", "der", "in", "Al\u00b7gier", "ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NE", "$,", "PRELS", "APPR", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Hat mir den feinsten Diamant", "tokens": ["Hat", "mir", "den", "feins\u00b7ten", "Di\u00b7a\u00b7mant"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Durch ein Verm\u00e4chtni\u00df zugewandt.", "tokens": ["Durch", "ein", "Ver\u00b7m\u00e4cht\u00b7ni\u00df", "zu\u00b7ge\u00b7wandt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Hier ist er! Ich bemerk's, auch dich erfreut mein Gl\u00fcck.", "tokens": ["Hier", "ist", "er", "!", "Ich", "be\u00b7merk's", ",", "auch", "dich", "er\u00b7freut", "mein", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$.", "PPER", "VVFIN", "$,", "ADV", "PPER", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Dir dank ich f\u00fcr das Freundschaftszeichen.", "tokens": ["Dir", "dank", "ich", "f\u00fcr", "das", "Freund\u00b7schafts\u00b7zei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Verwahr' ihn! dir allein darf ich ihn \u00fcberreichen:", "tokens": ["Ver\u00b7wahr'", "ihn", "!", "dir", "al\u00b7lein", "darf", "ich", "ihn", "\u00fc\u00b7berr\u00b7ei\u00b7chen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "$.", "PPER", "ADV", "VMFIN", "PPER", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Nimm ihn f\u00fcr mich in Acht; ich komme bald zur\u00fcck.", "tokens": ["Nimm", "ihn", "f\u00fcr", "mich", "in", "Acht", ";", "ich", "kom\u00b7me", "bald", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPR", "PPER", "APPR", "CARD", "$.", "PPER", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Es sei! versetzt Orchan, mein Selim kann gebieten;", "tokens": ["Es", "sei", "!", "ver\u00b7setzt", "Or\u00b7chan", ",", "mein", "Se\u00b7lim", "kann", "ge\u00b7bie\u00b7ten", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$.", "VVFIN", "NE", "$,", "PPOSAT", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Orchan wird jeden Augenblick", "tokens": ["Or\u00b7chan", "wird", "je\u00b7den", "Au\u00b7gen\u00b7blick"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VAFIN", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dies Kleinod wie sein Auge h\u00fcten;", "tokens": ["Dies", "Klei\u00b7nod", "wie", "sein", "Au\u00b7ge", "h\u00fc\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "KOKOM", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Er, dein Getreuer bis ins Grab.", "tokens": ["Er", ",", "dein", "Ge\u00b7treu\u00b7er", "bis", "ins", "Grab", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PPOSAT", "NN", "APPR", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Drauf folgt ein Abschiedsku\u00df; der Reisende geht ab.", "tokens": ["Drauf", "folgt", "ein", "Ab\u00b7schieds\u00b7ku\u00df", ";", "der", "Rei\u00b7sen\u00b7de", "geht", "ab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "$.", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Allein, wo soll man Seelen finden,", "tokens": ["Al\u00b7lein", ",", "wo", "soll", "man", "See\u00b7len", "fin\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PWAV", "VMFIN", "PIS", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die nicht auf Eigennutz die Heuchlerdienste gr\u00fcnden?", "tokens": ["Die", "nicht", "auf", "Ei\u00b7gen\u00b7nutz", "die", "Heuc\u00b7hler\u00b7diens\u00b7te", "gr\u00fcn\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "APPR", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wo ist nicht Treu' und Glaube schwach?", "tokens": ["Wo", "ist", "nicht", "Treu'", "und", "Glau\u00b7be", "schwach", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PTKNEG", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Lust, wann wir die Zeit ersehen,", "tokens": ["Die", "Lust", ",", "wann", "wir", "die", "Zeit", "er\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PWAV", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Den N\u00e4chsten schlau zu hintergehen,", "tokens": ["Den", "N\u00e4chs\u00b7ten", "schlau", "zu", "hin\u00b7ter\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Schleicht B\u00f6sen aller Orten nach:", "tokens": ["Schleicht", "B\u00f6\u00b7sen", "al\u00b7ler", "Or\u00b7ten", "nach", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Den Christen in ihr Betgemach;", "tokens": ["Den", "Chris\u00b7ten", "in", "ihr", "Bet\u00b7ge\u00b7mach", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und Muselm\u00e4nnern in Moscheen.", "tokens": ["Und", "Mu\u00b7sel\u00b7m\u00e4n\u00b7nern", "in", "Mo\u00b7scheen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Der frohe Selim k\u00f6mmt in Pera wieder an,", "tokens": ["Der", "fro\u00b7he", "Se\u00b7lim", "k\u00f6mmt", "in", "Pe\u00b7ra", "wie\u00b7der", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPR", "NE", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und rennt, sein Kleinod abzuholen,", "tokens": ["Und", "rennt", ",", "sein", "Klei\u00b7nod", "ab\u00b7zu\u00b7ho\u00b7len", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PPOSAT", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das er, zu treuer Hut, dem falschen Freund empfohlen.", "tokens": ["Das", "er", ",", "zu", "treu\u00b7er", "Hut", ",", "dem", "fal\u00b7schen", "Freund", "emp\u00b7foh\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPER", "$,", "APPR", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der aber lacht, und spricht: Ist Selim nicht ein Mann,", "tokens": ["Der", "a\u00b7ber", "lacht", ",", "und", "spricht", ":", "Ist", "Se\u00b7lim", "nicht", "ein", "Mann", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVFIN", "$,", "KON", "VVFIN", "$.", "VAFIN", "NE", "PTKNEG", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der unvergleichlich scherzen kann? ...", "tokens": ["Der", "un\u00b7ver\u00b7gleich\u00b7lich", "scher\u00b7zen", "kann", "?", "..."], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJD", "VVINF", "VMFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Was? Scherzen? Gab ich nicht? ... Ja, weil ich's r\u00fchmen soll:", "tokens": ["Was", "?", "Scher\u00b7zen", "?", "Gab", "ich", "nicht", "?", "...", "Ja", ",", "weil", "ich's", "r\u00fch\u00b7men", "soll", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "$.", "NN", "$.", "VVFIN", "PPER", "PTKNEG", "$.", "$(", "PTKANT", "$,", "KOUS", "PIS", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Du gabst mir einen Ku\u00df, der war recht freundschaftsvoll ...", "tokens": ["Du", "gabst", "mir", "ei\u00b7nen", "Ku\u00df", ",", "der", "war", "recht", "freund\u00b7schafts\u00b7voll", "..."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "$,", "PRELS", "VAFIN", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Wo ist mein Diamant? ... Dein Diamant! dir tr\u00e4umt ...", "tokens": ["Wo", "ist", "mein", "Di\u00b7a\u00b7mant", "?", "...", "Dein", "Di\u00b7a\u00b7mant", "!", "dir", "tr\u00e4umt", "..."], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPOSAT", "NN", "$.", "$(", "PPOSAT", "NN", "$.", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Hier sind nicht viele Reden n\u00f6thig.", "tokens": ["Hier", "sind", "nicht", "vie\u00b7le", "Re\u00b7den", "n\u00f6\u00b7thig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.10": {"text": "Fort! mit zum Cadi! nicht ges\u00e4umt! ...", "tokens": ["Fort", "!", "mit", "zum", "Ca\u00b7di", "!", "nicht", "ge\u00b7s\u00e4umt", "!", "..."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "$.", "APPR", "APPRART", "NE", "$.", "PTKNEG", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Ja, ja, mein Herr, ich bin's erb\u00f6tig.", "tokens": ["Ja", ",", "ja", ",", "mein", "Herr", ",", "ich", "bin's", "er\u00b7b\u00f6\u00b7tig", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PTKANT", "$,", "PPOSAT", "NN", "$,", "PPER", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Sie eilen zum Ben Haly hin,", "tokens": ["Sie", "ei\u00b7len", "zum", "Ben", "Ha\u00b7ly", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "NE", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das war des Cadi Nam'; und in des Sultans Reichen", "tokens": ["Das", "war", "des", "Ca\u00b7di", "Nam'", ";", "und", "in", "des", "Sul\u00b7tans", "Rei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "NE", "NN", "$.", "KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "War ihm an Billigkeit kein Haly zu vergleichen,", "tokens": ["War", "ihm", "an", "Bil\u00b7lig\u00b7keit", "kein", "Ha\u00b7ly", "zu", "ver\u00b7glei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Dafern ich recht berichtet bin.", "tokens": ["Da\u00b7fern", "ich", "recht", "be\u00b7rich\u00b7tet", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der arme Selim sucht dem Richter seine Klagen", "tokens": ["Der", "ar\u00b7me", "Se\u00b7lim", "sucht", "dem", "Rich\u00b7ter", "sei\u00b7ne", "Kla\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NE", "VVFIN", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Mit vielen Worten vorzutragen.", "tokens": ["Mit", "vie\u00b7len", "Wor\u00b7ten", "vor\u00b7zu\u00b7tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Er denkt, ein langer Satz scheint manchem Richter sch\u00f6n.", "tokens": ["Er", "denkt", ",", "ein", "lan\u00b7ger", "Satz", "scheint", "man\u00b7chem", "Rich\u00b7ter", "sch\u00f6n", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "ART", "ADJA", "NN", "VVFIN", "PIAT", "NN", "ADJD", "$."], "meter": "-+-+-++--+-+", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Orchan l\u00e4rmt zehnmal mehr. Dem Kl\u00e4ger fehlen Zeugen.", "tokens": ["Or\u00b7chan", "l\u00e4rmt", "zehn\u00b7mal", "mehr", ".", "Dem", "Kl\u00e4\u00b7ger", "feh\u00b7len", "Zeu\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "ADV", "$.", "ART", "NN", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Er gibt zum \u00f6ftern zu verstehn,", "tokens": ["Er", "gibt", "zum", "\u00f6f\u00b7tern", "zu", "ver\u00b7stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "VVINF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Bei einem Baume sei's geschehn.", "tokens": ["Bei", "ei\u00b7nem", "Bau\u00b7me", "sei's", "ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Das hilft ihm wenig; B\u00e4ume schweigen.", "tokens": ["Das", "hilft", "ihm", "we\u00b7nig", ";", "B\u00e4u\u00b7me", "schwei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "$.", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Beim Allah! schw\u00f6rt Orchan: der Kl\u00e4ger schwatzt im Traum;", "tokens": ["Beim", "Al\u00b7lah", "!", "schw\u00f6rt", "Or\u00b7chan", ":", "der", "Kl\u00e4\u00b7ger", "schwatzt", "im", "Traum", ";"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$.", "VVFIN", "NE", "$.", "ART", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-----+-+-+", "measure": "dactylic.init"}, "line.2": {"text": "Ich kenne beide nicht, kein Kleinod, keinen Baum.", "tokens": ["Ich", "ken\u00b7ne", "bei\u00b7de", "nicht", ",", "kein", "Klei\u00b7nod", ",", "kei\u00b7nen", "Baum", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PTKNEG", "$,", "PIAT", "NN", "$,", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "H\u00f6rt! spricht der Cadi drauf, noch ist hier kein Beweis.", "tokens": ["H\u00f6rt", "!", "spricht", "der", "Ca\u00b7di", "drauf", ",", "noch", "ist", "hier", "kein", "Be\u00b7weis", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "$.", "VVFIN", "ART", "NE", "PTKVZ", "$,", "ADV", "VAFIN", "ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Kennt Selim noch den Baum? ... Wie sollt' ich den nicht kennen! ...", "tokens": ["Kennt", "Se\u00b7lim", "noch", "den", "Baum", "?", "...", "Wie", "sollt'", "ich", "den", "nicht", "ken\u00b7nen", "!", "..."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "NE", "ADV", "ART", "NN", "$.", "$(", "PWAV", "VMFIN", "PPER", "ART", "PTKNEG", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Verziehe nicht, dahin zu rennen,", "tokens": ["Ver\u00b7zie\u00b7he", "nicht", ",", "da\u00b7hin", "zu", "ren\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "PAV", "PTKZU", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und hole mir sofort ein Reis.", "tokens": ["Und", "ho\u00b7le", "mir", "so\u00b7fort", "ein", "Reis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Er geht. Ben Haly setzt sich nieder;", "tokens": ["Er", "geht", ".", "Ben", "Ha\u00b7ly", "setzt", "sich", "nie\u00b7der", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "NE", "NE", "VVFIN", "PRF", "PTKVZ", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und endlich fragt er mit Verdru\u00df:", "tokens": ["Und", "end\u00b7lich", "fragt", "er", "mit", "Ver\u00b7dru\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie k\u00f6mmt's, da\u00df man hier warten mu\u00df?", "tokens": ["Wie", "k\u00f6mmt's", ",", "da\u00df", "man", "hier", "war\u00b7ten", "mu\u00df", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "$,", "KOUS", "PIS", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "K\u00f6mmt denn dein Gegner noch nicht wieder?", "tokens": ["K\u00f6mmt", "denn", "dein", "Geg\u00b7ner", "noch", "nicht", "wie\u00b7der", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PPOSAT", "NN", "ADV", "PTKNEG", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Von Rechten hat er nichts gelernt.", "tokens": ["Von", "Rech\u00b7ten", "hat", "er", "nichts", "ge\u00b7lernt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Was will er, da\u00df sein Baum beweise?", "tokens": ["Was", "will", "er", ",", "da\u00df", "sein", "Baum", "be\u00b7wei\u00b7se", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "$,", "KOUS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Ist dieser Baum so weit entfernt?", "tokens": ["Ist", "die\u00b7ser", "Baum", "so", "weit", "ent\u00b7fernt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Braucht's, ihn zu finden, einer Reise?", "tokens": ["Braucht's", ",", "ihn", "zu", "fin\u00b7den", ",", "ei\u00b7ner", "Rei\u00b7se", "?"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "PTKZU", "VVINF", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Nein, einer Reise braucht es nicht;", "tokens": ["Nein", ",", "ei\u00b7ner", "Rei\u00b7se", "braucht", "es", "nicht", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "ART", "NN", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Baum ist nahe g'nug ... Entdeckter B\u00f6sewicht!", "tokens": ["Der", "Baum", "ist", "na\u00b7he", "g'\u00b7nug", "...", "Ent\u00b7deck\u00b7ter", "B\u00f6\u00b7se\u00b7wicht", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "ADV", "$(", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "(ruft Haly z\u00fcrnend aus) vor einer halben Stunde", "tokens": ["(", "ruft", "Ha\u00b7ly", "z\u00fcr\u00b7nend", "aus", ")", "vor", "ei\u00b7ner", "hal\u00b7ben", "Stun\u00b7de"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "VVFIN", "NE", "VVPP", "APPR", "$(", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "War weder Baum noch Diamant,", "tokens": ["War", "we\u00b7der", "Baum", "noch", "Di\u00b7a\u00b7mant", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KON", "NE", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So wie du schwurest, dir bekannt;", "tokens": ["So", "wie", "du", "schwu\u00b7rest", ",", "dir", "be\u00b7kannt", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "PPER", "VVFIN", "$,", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und nun verdammst du dich mit deinem eignen Munde.", "tokens": ["Und", "nun", "ver\u00b7dammst", "du", "dich", "mit", "dei\u00b7nem", "eig\u00b7nen", "Mun\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PRF", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Wohlan! da\u00df jetzt, vor aller Welt,", "tokens": ["Wo\u00b7hlan", "!", "da\u00df", "jetzt", ",", "vor", "al\u00b7ler", "Welt", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "KOUS", "ADV", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Ein jeder das, was ihm geb\u00fchrt, empfange!", "tokens": ["Ein", "je\u00b7der", "das", ",", "was", "ihm", "ge\u00b7b\u00fchrt", ",", "emp\u00b7fan\u00b7ge", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PIAT", "PDS", "$,", "PWS", "PPER", "VVPP", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Dem Selim werde flugs sein Kleinod zugestellt!", "tokens": ["Dem", "Se\u00b7lim", "wer\u00b7de", "flugs", "sein", "Klei\u00b7nod", "zu\u00b7ge\u00b7stellt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VAFIN", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Orchan bereite sich zum Strange!", "tokens": ["Or\u00b7chan", "be\u00b7rei\u00b7te", "sich", "zum", "Stran\u00b7ge", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PRF", "APPRART", "NN", "$."], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}}, "stanza.13": {"line.1": {"text": "Der T\u00fcrk' besa\u00df die Klugheit nicht,", "tokens": ["Der", "T\u00fcrk'", "be\u00b7sa\u00df", "die", "Klug\u00b7heit", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die vielen Christen H\u00e4user bauet,", "tokens": ["Die", "vie\u00b7len", "Chris\u00b7ten", "H\u00e4u\u00b7ser", "bau\u00b7et", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da mit so blinder Zuversicht", "tokens": ["Da", "mit", "so", "blin\u00b7der", "Zu\u00b7ver\u00b7sicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kein Bruder hier dem andern trauet.", "tokens": ["Kein", "Bru\u00b7der", "hier", "dem", "an\u00b7dern", "trau\u00b7et", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADV", "ART", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Der Irrthum alter deutscher Treu'", "tokens": ["Der", "Irr\u00b7thum", "al\u00b7ter", "deut\u00b7scher", "Treu'"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ist mit der alten Zeit vorbei.", "tokens": ["Ist", "mit", "der", "al\u00b7ten", "Zeit", "vor\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wir sind der h\u00f6hern Kunst Exempel;", "tokens": ["Wir", "sind", "der", "h\u00f6\u00b7hern", "Kunst", "Ex\u00b7em\u00b7pel", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Die Einfalt nahm den Handschlag an.", "tokens": ["Die", "Ein\u00b7falt", "nahm", "den", "Hand\u00b7schlag", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Was fordert jetzt ein kluger Mann?", "tokens": ["Was", "for\u00b7dert", "jetzt", "ein", "klu\u00b7ger", "Mann", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Verschreibung, Zeugen, Pfand und Stempel.", "tokens": ["Ver\u00b7schrei\u00b7bung", ",", "Zeu\u00b7gen", ",", "Pfand", "und", "Stem\u00b7pel", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}