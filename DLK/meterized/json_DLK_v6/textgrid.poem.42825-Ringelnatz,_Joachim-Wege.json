{"textgrid.poem.42825": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Wege", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.85", "da:0.14"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Schwindel barmte laut und bog", "tokens": ["Der", "Schwin\u00b7del", "barm\u00b7te", "laut", "und", "bog"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADJD", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sich tief, dann dicht, und log und log.", "tokens": ["Sich", "tief", ",", "dann", "dicht", ",", "und", "log", "und", "log", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADJD", "$,", "ADV", "ADJD", "$,", "KON", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ein Ehrlicher schlich hinterher", "tokens": ["Ein", "Ehr\u00b7li\u00b7cher", "schlich", "hin\u00b7ter\u00b7her"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "PTKVZ"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.2": {"text": "Und hielt sich still und tat sich schwer.", "tokens": ["Und", "hielt", "sich", "still", "und", "tat", "sich", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADJD", "KON", "VVFIN", "PRF", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Der Schwindel klebte sich wie Leim,", "tokens": ["Der", "Schwin\u00b7del", "kleb\u00b7te", "sich", "wie", "Leim", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gab gro\u00df, nahm klein und sprach von \u00bbHeim\u00ab,", "tokens": ["Gab", "gro\u00df", ",", "nahm", "klein", "und", "sprach", "von", "\u00bb", "Heim", "\u00ab", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["VVFIN", "ADJD", "$,", "VVFIN", "ADJD", "KON", "VVFIN", "APPR", "$(", "NN", "$(", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Erwarb sich Kenntnis und Vertraun", "tokens": ["Er\u00b7warb", "sich", "Kennt\u00b7nis", "und", "Ver\u00b7traun"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "NE", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und steckte sich dann hinter Fraun,", "tokens": ["Und", "steck\u00b7te", "sich", "dann", "hin\u00b7ter", "Fraun", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "ADV", "APPR", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Ward unterst\u00fctzt, ward fest und steif,", "tokens": ["Ward", "un\u00b7ter\u00b7st\u00fctzt", ",", "ward", "fest", "und", "steif", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "VAFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gab klein, nahm gro\u00df und f\u00fchlte \u00bbreif\u00ab.", "tokens": ["Gab", "klein", ",", "nahm", "gro\u00df", "und", "f\u00fchl\u00b7te", "\u00bb", "reif", "\u00ab", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["VVFIN", "ADJD", "$,", "VVFIN", "ADJD", "KON", "VVFIN", "$(", "ADJD", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Der Schwindel trotzte unverbl\u00fcmt.", "tokens": ["Der", "Schwin\u00b7del", "trotz\u00b7te", "un\u00b7ver\u00b7bl\u00fcmt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er ward bekannt. Er ward ber\u00fchmt.", "tokens": ["Er", "ward", "be\u00b7kannt", ".", "Er", "ward", "be\u00b7r\u00fchmt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$.", "PPER", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Er zog nach unten hin Vergleich.", "tokens": ["Er", "zog", "nach", "un\u00b7ten", "hin", "Ver\u00b7gleich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er r\u00fcckte ab. Er wurde reich.", "tokens": ["Er", "r\u00fcck\u00b7te", "ab", ".", "Er", "wur\u00b7de", "reich", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$.", "PPER", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Der Schwindel f\u00fchlte sich und scho\u00df.", "tokens": ["Der", "Schwin\u00b7del", "f\u00fchl\u00b7te", "sich", "und", "scho\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn einer widersprach, dem go\u00df", "tokens": ["Wenn", "ei\u00b7ner", "wi\u00b7der\u00b7sprach", ",", "dem", "go\u00df"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PIS", "VVFIN", "$,", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Geblufft, bezahlt, Majorit\u00e4t", "tokens": ["Ge\u00b7blufft", ",", "be\u00b7zahlt", ",", "Ma\u00b7jo\u00b7ri\u00b7t\u00e4t"], "token_info": ["word", "punct", "word", "punct", "word"], "pos": ["NN", "$,", "VVPP", "$,", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ins Auge Popularit\u00e4t.", "tokens": ["Ins", "Au\u00b7ge", "Po\u00b7pu\u00b7la\u00b7ri\u00b7t\u00e4t", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Der Schwindel war gesch\u00fctzt, gemacht,", "tokens": ["Der", "Schwin\u00b7del", "war", "ge\u00b7sch\u00fctzt", ",", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$,", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur ruhelos bei Tag wie Nacht.", "tokens": ["Nur", "ru\u00b7he\u00b7los", "bei", "Tag", "wie", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "NN", "KOKOM", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Denn er gedachte ohne Ruh", "tokens": ["Denn", "er", "ge\u00b7dach\u00b7te", "oh\u00b7ne", "Ruh"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Des Ehrlichen; doch gab's nicht zu,", "tokens": ["Des", "Ehr\u00b7li\u00b7chen", ";", "doch", "gab's", "nicht", "zu", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "ADV", "VVFIN", "PTKNEG", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Vernahm und brachte dessen Schritt", "tokens": ["Ver\u00b7nahm", "und", "brach\u00b7te", "des\u00b7sen", "Schritt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "VVFIN", "PDS", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Hohn, dann Wut in Mi\u00dfkredit.", "tokens": ["Mit", "Hohn", ",", "dann", "Wut", "in", "Mi\u00df\u00b7kre\u00b7dit", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ADV", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Der Schwindel, l\u00e4ngst gemacht, war satt,", "tokens": ["Der", "Schwin\u00b7del", ",", "l\u00e4ngst", "ge\u00b7macht", ",", "war", "satt", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ADV", "VVPP", "$,", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Stand \u00fcberall in jedem Blatt.", "tokens": ["Stand", "\u00fc\u00b7be\u00b7rall", "in", "je\u00b7dem", "Blatt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Der Ehrliche kam fromm und schwer,", "tokens": ["Der", "Ehr\u00b7li\u00b7che", "kam", "fromm", "und", "schwer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ganz m\u00fcde, sp\u00e4t, des Wegs daher,", "tokens": ["Ganz", "m\u00fc\u00b7de", ",", "sp\u00e4t", ",", "des", "Wegs", "da\u00b7her", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADJD", "$,", "ART", "NN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Ging still vorbei und fromm und schwer.", "tokens": ["Ging", "still", "vor\u00b7bei", "und", "fromm", "und", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PTKVZ", "KON", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und er erreichte sehr viel mehr.", "tokens": ["Und", "er", "er\u00b7reich\u00b7te", "sehr", "viel", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}