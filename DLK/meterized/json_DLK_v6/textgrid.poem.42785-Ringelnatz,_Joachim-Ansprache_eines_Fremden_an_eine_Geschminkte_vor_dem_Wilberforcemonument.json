{"textgrid.poem.42785": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "Ansprache eines Fremden an eine Geschminkte vor dem Wilberforcemonument", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Guten Abend, sch\u00f6ne Unbekannte! Es ist nachts halb zehn.", "tokens": ["Gu\u00b7ten", "A\u00b7bend", ",", "sch\u00f6\u00b7ne", "Un\u00b7be\u00b7kann\u00b7te", "!", "Es", "ist", "nachts", "halb", "zehn", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "$.", "PPER", "VAFIN", "ADV", "ADJD", "CARD", "$."], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.2": {"text": "W\u00fcrden Sie liebensw\u00fcrdigerweise mit mir schlafen gehn?", "tokens": ["W\u00fcr\u00b7den", "Sie", "lie\u00b7bens\u00b7w\u00fcr\u00b7di\u00b7ger\u00b7wei\u00b7se", "mit", "mir", "schla\u00b7fen", "gehn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "VVFIN", "APPR", "PPER", "VVINF", "VVINF", "$."], "meter": "+--+-+--+-+-+-+", "measure": "iambic.septa.invert"}, "line.3": {"text": "Wer ich bin? \u2013 Sie meinen, wie ich hei\u00dfe?", "tokens": ["Wer", "ich", "bin", "?", "\u2013", "Sie", "mei\u00b7nen", ",", "wie", "ich", "hei\u00b7\u00dfe", "?"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VAFIN", "$.", "$(", "PPER", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.2": {"line.1": {"text": "Liebes Kind, ich werde Sie bel\u00fcgen,", "tokens": ["Lie\u00b7bes", "Kind", ",", "ich", "wer\u00b7de", "Sie", "be\u00b7l\u00fc\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "PPER", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Denn ich schenke dir drei Pfund.", "tokens": ["Denn", "ich", "schen\u00b7ke", "dir", "drei", "Pfund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "PPER", "CARD", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Denn ich k\u00fcsse niemals auf den Mund.", "tokens": ["Denn", "ich", "k\u00fcs\u00b7se", "nie\u00b7mals", "auf", "den", "Mund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Von uns beiden bin ich der Gescheitre.", "tokens": ["Von", "uns", "bei\u00b7den", "bin", "ich", "der", "Ge\u00b7schei\u00b7tre", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PIAT", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.5": {"text": "Doch du darfst mich um drei weitre", "tokens": ["Doch", "du", "darfst", "mich", "um", "drei", "weit\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "PRF", "APPR", "CARD", "ADJA"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.6": {"text": "Pfund betr\u00fcgen.", "tokens": ["Pfund", "be\u00b7tr\u00fc\u00b7gen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "Glaube mir, liebes Kind:", "tokens": ["Glau\u00b7be", "mir", ",", "lie\u00b7bes", "Kind", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "$,", "ADJA", "NN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.2": {"text": "Wenn man einmal in Sansibar", "tokens": ["Wenn", "man", "ein\u00b7mal", "in", "San\u00b7si\u00b7bar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "ADV", "APPR", "NN"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Und in Tirol und im Gef\u00e4ngnis und in Kalkutta war,", "tokens": ["Und", "in", "Ti\u00b7rol", "und", "im", "Ge\u00b7f\u00e4ng\u00b7nis", "und", "in", "Kal\u00b7kut\u00b7ta", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NE", "KON", "APPRART", "NN", "KON", "APPR", "NE", "VAFIN", "$,"], "meter": "--+--+-+-+-+--+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Dann merkt man erst, da\u00df man nicht wei\u00df, wie sonderbar", "tokens": ["Dann", "merkt", "man", "erst", ",", "da\u00df", "man", "nicht", "wei\u00df", ",", "wie", "son\u00b7der\u00b7bar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "$,", "KOUS", "PIS", "PTKNEG", "VVFIN", "$,", "PWAV", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die Menschen sind.", "tokens": ["Die", "Men\u00b7schen", "sind", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "Deine Ehre, zum Beispiel, ist nicht dasselbe", "tokens": ["Dei\u00b7ne", "Eh\u00b7re", ",", "zum", "Bei\u00b7spiel", ",", "ist", "nicht", "das\u00b7sel\u00b7be"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "APPRART", "NN", "$,", "VAFIN", "PTKNEG", "PDAT"], "meter": "+-+--+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Wie bei Peter dem Gro\u00dfen L'honneur. \u2013", "tokens": ["Wie", "bei", "Pe\u00b7ter", "dem", "Gro\u00b7\u00dfen", "L'\u00b7hon\u00b7neur", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "APPR", "NE", "ART", "ADJA", "NN", "$.", "$("], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "\u00dcbrigens war ich \u2013 (Schenk mir das gelbe", "tokens": ["\u00dcb\u00b7ri\u00b7gens", "war", "ich", "\u2013", "(", "Schenk", "mir", "das", "gel\u00b7be"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "$(", "$(", "NN", "PPER", "ART", "ADJA"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Band!) \u2013 in Altona an der Elbe", "tokens": ["Band", "!", ")", "\u2013", "in", "Al\u00b7to\u00b7na", "an", "der", "El\u00b7be"], "token_info": ["word", "punct", "punct", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "$(", "$(", "APPR", "NE", "APPR", "ART", "NE"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Schaufensterdekorateur. \u2013", "tokens": ["Schau\u00b7fens\u00b7ter\u00b7de\u00b7ko\u00b7ra\u00b7teur", ".", "\u2013"], "token_info": ["word", "punct", "punct"], "pos": ["NN", "$.", "$("], "meter": "+---+-+", "measure": "dactylic.init"}}, "stanza.5": {"line.1": {"text": "Hast du das Tuten geh\u00f6rt?", "tokens": ["Hast", "du", "das", "Tu\u00b7ten", "ge\u00b7h\u00f6rt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Das ist Wilson Line.", "tokens": ["Das", "ist", "Wil\u00b7son", "Li\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NE", "NE", "$."], "meter": "--+-+-", "measure": "anapaest.init"}}, "stanza.6": {"line.1": {"text": "Wie? Ich sei angetrunken? O nein, nein! Nein!", "tokens": ["Wie", "?", "Ich", "sei", "an\u00b7ge\u00b7trun\u00b7ken", "?", "O", "nein", ",", "nein", "!", "Nein", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PWAV", "$.", "PPER", "VAFIN", "VVPP", "$.", "NE", "PTKANT", "$,", "PTKANT", "$.", "PTKANT", "$."], "meter": "--+--+-+--+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Ich bin v\u00f6llig besoffen und hundsgef\u00e4hrlich geistesgest\u00f6rt.", "tokens": ["Ich", "bin", "v\u00f6l\u00b7lig", "be\u00b7sof\u00b7fen", "und", "hunds\u00b7ge\u00b7f\u00e4hr\u00b7lich", "geis\u00b7tes\u00b7ge\u00b7st\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "VVPP", "KON", "ADJD", "VVPP", "$."], "meter": "--+--+--+-+-+--+", "measure": "anapaest.tri.plus"}, "line.3": {"text": "Aber sechs Pfund sind immer ein Risiko wert.", "tokens": ["A\u00b7ber", "sechs", "Pfund", "sind", "im\u00b7mer", "ein", "Ri\u00b7si\u00b7ko", "wert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "NN", "VAFIN", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "+--+-+--+--+", "measure": "iambic.penta.invert"}}, "stanza.7": {"line.1": {"text": "Wie du mi\u00dftrauisch neben mir gehst!", "tokens": ["Wie", "du", "mi\u00df\u00b7trau\u00b7isch", "ne\u00b7ben", "mir", "gehst", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Wart nur, ich erz\u00e4hle dir schnurrige Sachen.", "tokens": ["Wart", "nur", ",", "ich", "er\u00b7z\u00e4h\u00b7le", "dir", "schnur\u00b7ri\u00b7ge", "Sa\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "$,", "PPER", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Ich wei\u00df: Du wirst lachen.", "tokens": ["Ich", "wei\u00df", ":", "Du", "wirst", "la\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VAFIN", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Ich wei\u00df: da\u00df sie dich auch traurig machen.", "tokens": ["Ich", "wei\u00df", ":", "da\u00df", "sie", "dich", "auch", "trau\u00b7rig", "ma\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "KOUS", "PPER", "PRF", "ADV", "ADJD", "VVINF", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Obwohl du sie gar nicht verstehst.", "tokens": ["Ob\u00b7wohl", "du", "sie", "gar", "nicht", "ver\u00b7stehst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Und auch ich \u2013", "tokens": ["Und", "auch", "ich", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPER", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.2": {"text": "Du wirst mir vertrauen, \u2013 sp\u00e4ter, in Hose und Hemd.", "tokens": ["Du", "wirst", "mir", "ver\u00b7trau\u00b7en", ",", "\u2013", "sp\u00e4\u00b7ter", ",", "in", "Ho\u00b7se", "und", "Hemd", "."], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVINF", "$,", "$(", "ADJD", "$,", "APPR", "NE", "KON", "NN", "$."], "meter": "-+--+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "M\u00e4dchen wie du haben mir immer vertraut.", "tokens": ["M\u00e4d\u00b7chen", "wie", "du", "ha\u00b7ben", "mir", "im\u00b7mer", "ver\u00b7traut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "PPER", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.9": {"line.1": {"text": "Ich bin etwas schief ins Leben gebaut.", "tokens": ["Ich", "bin", "et\u00b7was", "schief", "ins", "Le\u00b7ben", "ge\u00b7baut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "APPRART", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wo mir alles r\u00e4tselvoll ist und fremd,", "tokens": ["Wo", "mir", "al\u00b7les", "r\u00e4t\u00b7sel\u00b7voll", "ist", "und", "fremd", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PIS", "ADJD", "VAFIN", "KON", "ADJD", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Da wohnt meine Mutter. \u2013 Quatsch! Ich bitte dich: Sei recht laut!", "tokens": ["Da", "wohnt", "mei\u00b7ne", "Mut\u00b7ter", ".", "\u2013", "Quatsch", "!", "Ich", "bit\u00b7te", "dich", ":", "Sei", "recht", "laut", "!"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "$.", "$(", "NN", "$.", "PPER", "VVFIN", "PPER", "$.", "VAIMP", "ADJD", "ADJD", "$."], "meter": "-+--+-+-+--+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.10": {"line.1": {"text": "Ich bin eine alte Kommode.", "tokens": ["Ich", "bin", "ei\u00b7ne", "al\u00b7te", "Kom\u00b7mo\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Oft mit Tinte oder Rotwein begossen;", "tokens": ["Oft", "mit", "Tin\u00b7te", "o\u00b7der", "Rot\u00b7wein", "be\u00b7gos\u00b7sen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Manchmal mit Fu\u00dftritten geschlossen.", "tokens": ["Manch\u00b7mal", "mit", "Fu\u00df\u00b7trit\u00b7ten", "ge\u00b7schlos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VVPP", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}, "line.4": {"text": "Der wird kichern, der nach meinem Tode", "tokens": ["Der", "wird", "ki\u00b7chern", ",", "der", "nach", "mei\u00b7nem", "To\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "VVINF", "$,", "PRELS", "APPR", "PPOSAT", "NN"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Mein Geheimfach entdeckt. \u2013", "tokens": ["Mein", "Ge\u00b7heim\u00b7fach", "ent\u00b7deckt", ".", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "VVPP", "$.", "$("], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Ach Kind, wenn du ahntest, wie Kunitzburger Eierkuchen schmeckt!", "tokens": ["Ach", "Kind", ",", "wenn", "du", "ahn\u00b7test", ",", "wie", "Ku\u00b7nitz\u00b7bur\u00b7ger", "Ei\u00b7er\u00b7ku\u00b7chen", "schmeckt", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "NN", "$,", "KOUS", "PPER", "VVFIN", "$,", "PWAV", "NN", "NN", "VVFIN", "$."], "meter": "-+--+--+-+-+-+-+", "measure": "amphibrach.tri.plus"}}, "stanza.11": {"line.1": {"text": "Das ist nun kein richtiger Scherz.", "tokens": ["Das", "ist", "nun", "kein", "rich\u00b7ti\u00b7ger", "Scherz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Ich bin auch nicht richtig froh.", "tokens": ["Ich", "bin", "auch", "nicht", "rich\u00b7tig", "froh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "ADJD", "ADJD", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ich habe auch kein richtiges Herz.", "tokens": ["Ich", "ha\u00b7be", "auch", "kein", "rich\u00b7ti\u00b7ges", "Herz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Ich bin nur ein kleiner, unanst\u00e4ndiger Schalk.", "tokens": ["Ich", "bin", "nur", "ein", "klei\u00b7ner", ",", "un\u00b7an\u00b7st\u00e4n\u00b7di\u00b7ger", "Schalk", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "-+--+-+-+--+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Mein richtiges Herz. Das ist anderw\u00e4rts, irgendwo", "tokens": ["Mein", "rich\u00b7ti\u00b7ges", "Herz", ".", "Das", "ist", "an\u00b7der\u00b7w\u00e4rts", ",", "ir\u00b7gend\u00b7wo"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$.", "PDS", "VAFIN", "ADV", "$,", "ADV"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Im Muschelkalk.", "tokens": ["Im", "Mu\u00b7schel\u00b7kalk", "."], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}