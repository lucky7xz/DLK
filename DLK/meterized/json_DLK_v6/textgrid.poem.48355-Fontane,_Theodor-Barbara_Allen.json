{"textgrid.poem.48355": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "Barbara Allen", "genre": "verse", "period": "N.A.", "pub_year": 1855, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es war im Herbst, im bunten Herbst,", "tokens": ["Es", "war", "im", "Herbst", ",", "im", "bun\u00b7ten", "Herbst", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "NN", "$,", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn die rotgelben Bl\u00e4tter fallen,", "tokens": ["Wenn", "die", "rot\u00b7gel\u00b7ben", "Bl\u00e4t\u00b7ter", "fal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Da wurde John Graham vor Liebe krank,", "tokens": ["Da", "wur\u00b7de", "John", "Gra\u00b7ham", "vor", "Lie\u00b7be", "krank", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "NE", "APPR", "NN", "ADJD", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Vor Liebe zu Barbara Allen.", "tokens": ["Vor", "Lie\u00b7be", "zu", "Bar\u00b7ba\u00b7ra", "Al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "NE", "NE", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.2": {"line.1": {"text": "Seine L\u00e4ufer liefen hinab in die Stadt", "tokens": ["Sei\u00b7ne", "L\u00e4u\u00b7fer", "lie\u00b7fen", "hin\u00b7ab", "in", "die", "Stadt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "APPR", "ART", "NN"], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Und suchten, bis sie gefunden:", "tokens": ["Und", "such\u00b7ten", ",", "bis", "sie", "ge\u00b7fun\u00b7den", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PPER", "VVPP", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "\u00bbach, unser Herr ist krank nach dir,", "tokens": ["\u00bb", "ach", ",", "un\u00b7ser", "Herr", "ist", "krank", "nach", "dir", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "$,", "PPOSAT", "NN", "VAFIN", "ADJD", "APPR", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Komm, Lady, und mach' ihn gesunden.\u00ab", "tokens": ["Komm", ",", "La\u00b7dy", ",", "und", "mach'", "ihn", "ge\u00b7sun\u00b7den", ".", "\u00ab"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "$,", "NE", "$,", "KON", "VVFIN", "PPER", "VVPP", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.3": {"line.1": {"text": "Die Lady schritt zum Schlo\u00df hinan,", "tokens": ["Die", "La\u00b7dy", "schritt", "zum", "Schlo\u00df", "hi\u00b7nan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schritt \u00fcber die marmornen Stufen,", "tokens": ["Schritt", "\u00fc\u00b7ber", "die", "mar\u00b7mor\u00b7nen", "Stu\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Sie trat ans Bett, sie sah ihn an:", "tokens": ["Sie", "trat", "ans", "Bett", ",", "sie", "sah", "ihn", "an", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "$,", "PPER", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbjohn Graham, du lie\u00dfest mich rufen.\u00ab", "tokens": ["\u00bb", "john", "Gra\u00b7ham", ",", "du", "lie\u00b7\u00dfest", "mich", "ru\u00b7fen", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "FM.la", "FM.la", "$,", "PPER", "VVFIN", "PPER", "VVINF", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.4": {"line.1": {"text": "\u00bbich lie\u00df dich rufen, ich bin im Herbst,", "tokens": ["\u00bb", "ich", "lie\u00df", "dich", "ru\u00b7fen", ",", "ich", "bin", "im", "Herbst", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "APPRART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und die rotgelben Bl\u00e4tter fallen \u2013", "tokens": ["Und", "die", "rot\u00b7gel\u00b7ben", "Bl\u00e4t\u00b7ter", "fal\u00b7len", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Hast du kein letztes Wort f\u00fcr mich?", "tokens": ["Hast", "du", "kein", "letz\u00b7tes", "Wort", "f\u00fcr", "mich", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "ADJA", "NN", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich sterbe, Barbara Allen.\u00ab", "tokens": ["Ich", "ster\u00b7be", ",", "Bar\u00b7ba\u00b7ra", "Al\u00b7len", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "$,", "NE", "NE", "$.", "$("], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "\u00bbjohn Graham, ich hab' ein letztes Wort,", "tokens": ["\u00bb", "john", "Gra\u00b7ham", ",", "ich", "hab'", "ein", "letz\u00b7tes", "Wort", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "FM.la", "$,", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Du warst mein all und eines;", "tokens": ["Du", "warst", "mein", "all", "und", "ei\u00b7nes", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "PIAT", "KON", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Du teiltest Pf\u00e4nder und B\u00e4nder aus,", "tokens": ["Du", "teil\u00b7test", "Pf\u00e4n\u00b7der", "und", "B\u00e4n\u00b7der", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mir aber g\u00f6nntest du keines.", "tokens": ["Mir", "a\u00b7ber", "g\u00f6nn\u00b7test", "du", "kei\u00b7nes", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "PPER", "PIS", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "John Graham, und ob du mich lieben magst,", "tokens": ["John", "Gra\u00b7ham", ",", "und", "ob", "du", "mich", "lie\u00b7ben", "magst", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "KON", "KOUS", "PPER", "PRF", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ich wei\u00df, ich hatte dich lieber,", "tokens": ["Ich", "wei\u00df", ",", "ich", "hat\u00b7te", "dich", "lie\u00b7ber", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "PPER", "ADV", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ich sah nach dir, du lachtest mich an", "tokens": ["Ich", "sah", "nach", "dir", ",", "du", "lach\u00b7test", "mich", "an"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PPER", "$,", "PPER", "VVFIN", "PPER", "PTKVZ"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Und gingest lachend vor\u00fcber.", "tokens": ["Und", "gin\u00b7gest", "la\u00b7chend", "vor\u00b7\u00fc\u00b7ber", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "PTKVZ", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.7": {"line.1": {"text": "Wir haben gewechselt, ich und du,", "tokens": ["Wir", "ha\u00b7ben", "ge\u00b7wech\u00b7selt", ",", "ich", "und", "du", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "$,", "PPER", "KON", "PPER", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die Sprossen der Liebesleiter,", "tokens": ["Die", "Spros\u00b7sen", "der", "Lie\u00b7be\u00b7slei\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Du bist nun unten, du hast es gewollt,", "tokens": ["Du", "bist", "nun", "un\u00b7ten", ",", "du", "hast", "es", "ge\u00b7wollt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "$,", "PPER", "VAFIN", "PPER", "VMPP", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ich aber bin oben und heiter.\u00ab", "tokens": ["Ich", "a\u00b7ber", "bin", "o\u00b7ben", "und", "hei\u00b7ter", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "ADV", "VAFIN", "ADV", "KON", "ADJD", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.8": {"line.1": {"text": "Sie ging zur\u00fcck. Eine Meil' oder zwei,", "tokens": ["Sie", "ging", "zu\u00b7r\u00fcck", ".", "Ei\u00b7ne", "Meil'", "o\u00b7der", "zwei", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKVZ", "$.", "ART", "NN", "KON", "CARD", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da h\u00f6rte sie Glocken schallen;", "tokens": ["Da", "h\u00f6r\u00b7te", "sie", "Glo\u00b7cken", "schal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "NN", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie sprach: \u00bbDie Glocken klingen f\u00fcr ihn,", "tokens": ["Sie", "sprach", ":", "\u00bb", "Die", "Glo\u00b7cken", "klin\u00b7gen", "f\u00fcr", "ihn", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "ART", "NN", "VVINF", "APPR", "PPER", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "F\u00fcr ihn und f\u00fcr \u2013 Barbara Allen.", "tokens": ["F\u00fcr", "ihn", "und", "f\u00fcr", "\u2013", "Bar\u00b7ba\u00b7ra", "Al\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPER", "KON", "APPR", "$(", "NE", "NE", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.9": {"line.1": {"text": "Liebe Mutter, mach ein Bett f\u00fcr mich,", "tokens": ["Lie\u00b7be", "Mut\u00b7ter", ",", "mach", "ein", "Bett", "f\u00fcr", "mich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VVFIN", "ART", "NN", "APPR", "PPER", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Unter Weiden und Eschen geborgen;", "tokens": ["Un\u00b7ter", "Wei\u00b7den", "und", "E\u00b7schen", "ge\u00b7bor\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVPP", "$."], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.3": {"text": "John Graham ist heute gestorben um mich,", "tokens": ["John", "Gra\u00b7ham", "ist", "heu\u00b7te", "ge\u00b7stor\u00b7ben", "um", "mich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VAFIN", "ADV", "VVPP", "APPR", "PPER", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Und ich sterbe um ihn morgen.\u00ab", "tokens": ["Und", "ich", "ster\u00b7be", "um", "ihn", "mor\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "PPER", "ADV", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}