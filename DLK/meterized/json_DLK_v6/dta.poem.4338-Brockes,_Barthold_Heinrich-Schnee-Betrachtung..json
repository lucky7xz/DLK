{"dta.poem.4338": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Schnee-Betrachtung.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Es flog der Schnee so dick und dichte,", "tokens": ["Es", "flog", "der", "Schnee", "so", "dick", "und", "dich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADV", "ADJD", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df er dem sch\u00e4rfesten Gesichte", "tokens": ["Da\u00df", "er", "dem", "sch\u00e4r\u00b7fes\u00b7ten", "Ge\u00b7sich\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Kaum zwanzig Schritt zu sehn erlaubt.", "tokens": ["Kaum", "zwan\u00b7zig", "Schritt", "zu", "sehn", "er\u00b7laubt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "PTKZU", "VVINF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es schien, durch einen weissen Nebel, was sonst zu sehn", "tokens": ["Es", "schien", ",", "durch", "ei\u00b7nen", "weis\u00b7sen", "Ne\u00b7bel", ",", "was", "sonst", "zu", "sehn"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "APPR", "ART", "ADJA", "NN", "$,", "PRELS", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "war, uns geraubt.", "tokens": ["war", ",", "uns", "ge\u00b7raubt", "."], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "$,", "PPER", "VVPP", "$."], "meter": "+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "Es waren H\u00e4user, B\u00e4ume, Th\u00fcrme und Scheunen, die", "tokens": ["Es", "wa\u00b7ren", "H\u00e4u\u00b7ser", ",", "B\u00e4u\u00b7me", ",", "Th\u00fcr\u00b7me", "und", "Scheu\u00b7nen", ",", "die"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,", "PRELS"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "erhaben stehn,", "tokens": ["er\u00b7ha\u00b7ben", "stehn", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "Kaum durch den weiss- und regen Duft, ja \u00f6fters gar", "tokens": ["Kaum", "durch", "den", "weiss", "und", "re\u00b7gen", "Duft", ",", "ja", "\u00f6f\u00b7ters", "gar"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "APPR", "ART", "TRUNC", "KON", "ADJA", "NN", "$,", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "nicht einst zu sehn.", "tokens": ["nicht", "einst", "zu", "sehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.10": {"text": "Doch, wenn man sie zuweilen sah, konnt\u2019 ihre dunkle", "tokens": ["Doch", ",", "wenn", "man", "sie", "zu\u00b7wei\u00b7len", "sah", ",", "konnt'", "ih\u00b7re", "dunk\u00b7le"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "KOUS", "PIS", "PPER", "ADV", "VVFIN", "$,", "VMFIN", "PPOSAT", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Schw\u00e4rz und H\u00f6h'", "tokens": ["Schw\u00e4rz", "und", "H\u00f6h'"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.12": {"text": "Uns von dem Nebel-gleichen Schnee,", "tokens": ["Uns", "von", "dem", "Ne\u00b7bel\u00b7glei\u00b7chen", "Schnee", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.13": {"text": "Trotz seiner regen Schnelligkeit,", "tokens": ["Trotz", "sei\u00b7ner", "re\u00b7gen", "Schnel\u00b7lig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Wenn er vor sie vorbey flog, eben", "tokens": ["Wenn", "er", "vor", "sie", "vor\u00b7bey", "flog", ",", "e\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["KOUS", "PPER", "APPR", "PPER", "ADV", "VVFIN", "$,", "ADV"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.15": {"text": "Auf ihres Grundes Dunkelheit", "tokens": ["Auf", "ih\u00b7res", "Grun\u00b7des", "Dun\u00b7kel\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Die eigentlichste Bildung geben.", "tokens": ["Die", "ei\u00b7gent\u00b7lichs\u00b7te", "Bil\u00b7dung", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.17": {"text": "Man kann denselben, ohn\u2019 Vergn\u00fcgen,", "tokens": ["Man", "kann", "den\u00b7sel\u00b7ben", ",", "ohn'", "Ver\u00b7gn\u00fc\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PDS", "$,", "KOUI", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.18": {"text": "In reger Emsigkeit nicht fliegen", "tokens": ["In", "re\u00b7ger", "Em\u00b7sig\u00b7keit", "nicht", "flie\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "PTKNEG", "VVINF"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.19": {"text": "Und, ohne Lust, nicht sinken sehn.", "tokens": ["Und", ",", "oh\u00b7ne", "Lust", ",", "nicht", "sin\u00b7ken", "sehn", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUI", "NN", "$,", "PTKNEG", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Man siehet lauter lichte Theile,", "tokens": ["Man", "sie\u00b7het", "lau\u00b7ter", "lich\u00b7te", "Thei\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.21": {"text": "In fl\u00fccht\u2019ger Schnelligkeit und Eile,", "tokens": ["In", "fl\u00fccht'\u00b7ger", "Schnel\u00b7lig\u00b7keit", "und", "Ei\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.22": {"text": "Verwirret durch einander gehn.", "tokens": ["Ver\u00b7wir\u00b7ret", "durch", "ein\u00b7an\u00b7der", "gehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PRF", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.23": {"text": "Von Osten sieht man einen Schnee-Strich, von Westen", "tokens": ["Von", "Os\u00b7ten", "sieht", "man", "ei\u00b7nen", "Schnee\u00b7Strich", ",", "von", "Wes\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PIS", "ART", "NN", "$,", "APPR", "NN"], "meter": "-+-+-+-++-+-", "measure": "unknown.measure.hexa"}, "line.24": {"text": "einen gegen ihn,", "tokens": ["ei\u00b7nen", "ge\u00b7gen", "ihn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPER", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.25": {"text": "In einem ja so strengen Zug, aus tausend weissen Theilchen,", "tokens": ["In", "ei\u00b7nem", "ja", "so", "stren\u00b7gen", "Zug", ",", "aus", "tau\u00b7send", "weis\u00b7sen", "Theil\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADV", "ADV", "ADJA", "NN", "$,", "APPR", "CARD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.26": {"text": "ziehn.", "tokens": ["ziehn", "."], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "+", "measure": "single.up"}, "line.27": {"text": "Man hei\u00dft es Schnee-Jagd, und mit Recht, weil alles,", "tokens": ["Man", "hei\u00dft", "es", "Schnee\u00b7Jagd", ",", "und", "mit", "Recht", ",", "weil", "al\u00b7les", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "NN", "$,", "KON", "APPR", "NN", "$,", "KOUS", "PIS", "$,"], "meter": "-+-+++-+-+-", "measure": "unknown.measure.hexa"}, "line.28": {"text": "was man sieht, sich j\u00e4get,", "tokens": ["was", "man", "sieht", ",", "sich", "j\u00e4\u00b7get", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVFIN", "$,", "PRF", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.29": {"text": "Sich gleichsam sto\u00dft, durchdringt, verfolgt, sich gleichsam", "tokens": ["Sich", "gleich\u00b7sam", "sto\u00dft", ",", "durch\u00b7dringt", ",", "ver\u00b7folgt", ",", "sich", "gleich\u00b7sam"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["PRF", "ADJD", "VVFIN", "$,", "VVFIN", "$,", "VVPP", "$,", "PRF", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.30": {"text": "drenget, pre\u00dft und schl\u00e4get.", "tokens": ["dren\u00b7get", ",", "pre\u00dft", "und", "schl\u00e4\u00b7get", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "KON", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Wenn man darauf die Augen lenket,", "tokens": ["Wenn", "man", "da\u00b7rauf", "die", "Au\u00b7gen", "len\u00b7ket", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PAV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und auf das Spiel der Flocken denket,", "tokens": ["Und", "auf", "das", "Spiel", "der", "Flo\u00b7cken", "den\u00b7ket", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Bald aber etwan abwerts sieht; so ist inzwischen Land", "tokens": ["Bald", "a\u00b7ber", "et\u00b7wan", "ab\u00b7werts", "sieht", ";", "so", "ist", "in\u00b7zwi\u00b7schen", "Land"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ADV", "ADV", "VVFIN", "$.", "ADV", "VAFIN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.4": {"text": "und Feld,", "tokens": ["und", "Feld", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.5": {"text": "F\u00fcr unsern Blick, als wie verschwunden,", "tokens": ["F\u00fcr", "un\u00b7sern", "Blick", ",", "als", "wie", "ver\u00b7schwun\u00b7den", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "KOUS", "KOKOM", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und von der Fl\u00e4che unsrer Welt", "tokens": ["Und", "von", "der", "Fl\u00e4\u00b7che", "uns\u00b7rer", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Wird kaum annoch die Spur gefunden.", "tokens": ["Wird", "kaum", "an\u00b7noch", "die", "Spur", "ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ART", "NN", "VVPP", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.8": {"text": "Die dick- gefallne Flocken haben", "tokens": ["Die", "dick", "ge\u00b7fall\u00b7ne", "Flo\u00b7cken", "ha\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "TRUNC", "ADJA", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Die Vorw\u00fcrf\u2019 alle fast begraben,", "tokens": ["Die", "Vor\u00b7w\u00fcr\u00b7f'", "al\u00b7le", "fast", "be\u00b7gra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PIS", "ADV", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Die Tief- und H\u00f6hen gleich gemacht,", "tokens": ["Die", "Tie\u00b7f", "und", "H\u00f6\u00b7hen", "gleich", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "TRUNC", "KON", "NN", "ADV", "VVPP", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "Und was wir sonst in Tiefen und auf H\u00f6hen", "tokens": ["Und", "was", "wir", "sonst", "in", "Tie\u00b7fen", "und", "auf", "H\u00f6\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "PPER", "ADV", "APPR", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "F\u00fcr mancherley Figur gesehen,", "tokens": ["F\u00fcr", "man\u00b7cher\u00b7ley", "Fi\u00b7gur", "ge\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Verh\u00fcllet eine weisse Nacht.", "tokens": ["Ver\u00b7h\u00fcl\u00b7let", "ei\u00b7ne", "weis\u00b7se", "Nacht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Doch dient\u2019 die neue Augen-Weide,", "tokens": ["Doch", "dient'", "die", "neu\u00b7e", "Au\u00b7gen\u00b7Wei\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In der Ver\u00e4ndrung, uns zur Freude.", "tokens": ["In", "der", "Ver\u00b7\u00e4n\u00b7drung", ",", "uns", "zur", "Freu\u00b7de", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "An einem andern Ort hingegen, wohin der Schnee so stark", "tokens": ["An", "ei\u00b7nem", "an\u00b7dern", "Ort", "hin\u00b7ge\u00b7gen", ",", "wo\u00b7hin", "der", "Schnee", "so", "stark"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "ADV", "$,", "PWAV", "ART", "NN", "ADV", "ADJD"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "nicht fiel,", "tokens": ["nicht", "fiel", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.5": {"text": "Sieht man f\u00fcr unsern Blick ein ganz verschiednes Ziel.", "tokens": ["Sieht", "man", "f\u00fcr", "un\u00b7sern", "Blick", "ein", "ganz", "ver\u00b7schied\u00b7nes", "Ziel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "PPOSAT", "NN", "ART", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Dort lassen sich von Tiefen und von H\u00f6hen,", "tokens": ["Dort", "las\u00b7sen", "sich", "von", "Tie\u00b7fen", "und", "von", "H\u00f6\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Wo sie zumtheil beschneit, so vielerley Figuren", "tokens": ["Wo", "sie", "zumt\u00b7heil", "be\u00b7schneit", ",", "so", "vie\u00b7ler\u00b7ley", "Fi\u00b7gu\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "PPER", "KOUS", "NN", "$,", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Von mannigfaltgen Creaturen,", "tokens": ["Von", "man\u00b7nig\u00b7falt\u00b7gen", "Crea\u00b7tu\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.9": {"text": "Die sonst verwirrt, versteckt, und nicht zu sehen, sehn.", "tokens": ["Die", "sonst", "ver\u00b7wirrt", ",", "ver\u00b7steckt", ",", "und", "nicht", "zu", "se\u00b7hen", ",", "sehn", "."], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "$,", "VVPP", "$,", "KON", "PTKNEG", "PTKZU", "VVINF", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Es sticht sich itzo Schwarz und Wei\u00df,", "tokens": ["Es", "sticht", "sich", "it\u00b7zo", "Schwarz", "und", "Wei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "ADV", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Besieht es unser Blick mit Flei\u00df,", "tokens": ["Be\u00b7sieht", "es", "un\u00b7ser", "Blick", "mit", "Flei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So deutlich von einander ab,", "tokens": ["So", "deut\u00b7lich", "von", "ein\u00b7an\u00b7der", "ab", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df es noch mehr, als sonst, mir zu bewundern gab.", "tokens": ["Da\u00df", "es", "noch", "mehr", ",", "als", "sonst", ",", "mir", "zu", "be\u00b7wun\u00b7dern", "gab", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "$,", "KOUS", "ADV", "$,", "PPER", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Auf allen halb beschneit-halb schwarzen Zweigen", "tokens": ["Auf", "al\u00b7len", "halb", "be\u00b7schneit\u00b7halb", "schwar\u00b7zen", "Zwei\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADJD", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Scheint gleichsam ein Gemisch von Tag und Nacht,", "tokens": ["Scheint", "gleich\u00b7sam", "ein", "Ge\u00b7misch", "von", "Tag", "und", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Von Schatten und von Licht,", "tokens": ["Von", "Schat\u00b7ten", "und", "von", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Uns eine D\u00e4mmerung zu zeigen.", "tokens": ["Uns", "ei\u00b7ne", "D\u00e4m\u00b7me\u00b7rung", "zu", "zei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Kein Zweiglein ist so d\u00fcnn und zart,", "tokens": ["Kein", "Zwei\u00b7glein", "ist", "so", "d\u00fcnn", "und", "zart", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAFIN", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auf welchem nicht,", "tokens": ["Auf", "wel\u00b7chem", "nicht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PTKNEG", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Auf ungez\u00e4hlte Art,", "tokens": ["Auf", "un\u00b7ge\u00b7z\u00e4hl\u00b7te", "Art", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "So weisses Mo\u00df, als tausend Kr\u00e4uterlein,", "tokens": ["So", "weis\u00b7ses", "Mo\u00df", ",", "als", "tau\u00b7send", "Kr\u00e4u\u00b7ter\u00b7lein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "$,", "KOUS", "CARD", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Sieht man sie in der N\u00e4h\u2019, zu sehen seyn.", "tokens": ["Sieht", "man", "sie", "in", "der", "N\u00e4h'", ",", "zu", "se\u00b7hen", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "APPR", "ART", "NN", "$,", "PTKZU", "VVINF", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Es scheinet auch der kleinste Ast,", "tokens": ["Es", "schei\u00b7net", "auch", "der", "kleins\u00b7te", "Ast", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Zumahl im hellen Glanz der Sonnen,", "tokens": ["Zu\u00b7mahl", "im", "hel\u00b7len", "Glanz", "der", "Son\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Als w\u00e4r er \u00fcberall mit Silber-Drat umsponnen,", "tokens": ["Als", "w\u00e4r", "er", "\u00fc\u00b7be\u00b7rall", "mit", "Sil\u00b7ber\u00b7Drat", "um\u00b7spon\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "ADV", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "In weissem Schmelz-Werk eingefa\u00dft.", "tokens": ["In", "weis\u00b7sem", "Schmelz\u00b7Werk", "ein\u00b7ge\u00b7fa\u00dft", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Und kurz, auch in der Winters-Zeit,", "tokens": ["Und", "kurz", ",", "auch", "in", "der", "Win\u00b7ter\u00b7sZeit", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "$,", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Wo Gott euch nur Bequemlichkeit,", "tokens": ["Wo", "Gott", "euch", "nur", "Be\u00b7quem\u00b7lich\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "PPER", "ADV", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Da\u00df ihr nicht leiden d\u00fcrfet, g\u00f6nnet,", "tokens": ["Da\u00df", "ihr", "nicht", "lei\u00b7den", "d\u00fcr\u00b7fet", ",", "g\u00f6n\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVINF", "VMFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Sind \u00fcberall, sind fern und nah", "tokens": ["Sind", "\u00fc\u00b7be\u00b7rall", ",", "sind", "fern", "und", "nah"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "$,", "VAFIN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Viel tausend Gegenw\u00fcrfe da,", "tokens": ["Viel", "tau\u00b7send", "Ge\u00b7gen\u00b7w\u00fcr\u00b7fe", "da", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Woran ihr euch vergn\u00fcgen k\u00f6nnet.", "tokens": ["Wo\u00b7ran", "ihr", "euch", "ver\u00b7gn\u00fc\u00b7gen", "k\u00f6n\u00b7net", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPER", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Da nun, bey diesem Schmuck, die Welt", "tokens": ["Da", "nun", ",", "bey", "die\u00b7sem", "Schmuck", ",", "die", "Welt"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "$,", "APPR", "PDAT", "NN", "$,", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sich durch den Frost zugleich erneuert und erh\u00e4lt;", "tokens": ["Sich", "durch", "den", "Frost", "zu\u00b7gleich", "er\u00b7neu\u00b7ert", "und", "er\u00b7h\u00e4lt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "ADV", "VVPP", "KON", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So la\u00dft uns denn, bey dieser Aendrung eben,", "tokens": ["So", "la\u00dft", "uns", "denn", ",", "bey", "die\u00b7ser", "A\u00b7en\u00b7drung", "e\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "ADV", "$,", "APPR", "PDAT", "NN", "ADV", "$,"], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Dem, der die Welt so wunderbar regieret,", "tokens": ["Dem", ",", "der", "die", "Welt", "so", "wun\u00b7der\u00b7bar", "re\u00b7gie\u00b7ret", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$,", "PRELS", "ART", "NN", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Den Dank, das Lob, das Jhm daf\u00fcr geb\u00fchret,", "tokens": ["Den", "Dank", ",", "das", "Lob", ",", "das", "Jhm", "da\u00b7f\u00fcr", "ge\u00b7b\u00fch\u00b7ret", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "PRELS", "PPER", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Mit Lust, Bewunderung und Ehrfurcht geben,", "tokens": ["Mit", "Lust", ",", "Be\u00b7wun\u00b7de\u00b7rung", "und", "Ehr\u00b7furcht", "ge\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Uns seiner weisen F\u00fchrung freuen,", "tokens": ["Uns", "sei\u00b7ner", "wei\u00b7sen", "F\u00fch\u00b7rung", "freu\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPOSAT", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und in gegr\u00fcndeter vergn\u00fcgter Hoffnung stehn,", "tokens": ["Und", "in", "ge\u00b7gr\u00fcn\u00b7de\u00b7ter", "ver\u00b7gn\u00fcg\u00b7ter", "Hoff\u00b7nung", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Mit Lust zu rechter Zeit zu sehn,", "tokens": ["Mit", "Lust", "zu", "rech\u00b7ter", "Zeit", "zu", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Wie Erde, Luft und Fluht,", "tokens": ["Wie", "Er\u00b7de", ",", "Luft", "und", "Fluht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Durch der bald n\u00e4hern Sonnen Gluht,", "tokens": ["Durch", "der", "bald", "n\u00e4\u00b7hern", "Son\u00b7nen", "Gluht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "ADV", "VVFIN", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Mit tausend Lieblichkeit und Anmuht sich erneuen.", "tokens": ["Mit", "tau\u00b7send", "Lieb\u00b7lich\u00b7keit", "und", "An\u00b7muht", "sich", "er\u00b7neu\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "KON", "NN", "PRF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}