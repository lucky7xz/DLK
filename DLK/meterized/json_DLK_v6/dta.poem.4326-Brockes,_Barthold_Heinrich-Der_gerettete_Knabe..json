{"dta.poem.4326": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Der gerettete Knabe.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.53", "no:0.46"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Gott Lob! da\u00df er errettet ist! Dir, HErr! sey Dank,", "tokens": ["Gott", "Lob", "!", "da\u00df", "er", "er\u00b7ret\u00b7tet", "ist", "!", "Dir", ",", "Herr", "!", "sey", "Dank", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "NN", "$.", "KOUS", "PPER", "VVPP", "VAFIN", "$.", "PPER", "$,", "NN", "$.", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "da\u00df er erhalten!", "tokens": ["da\u00df", "er", "er\u00b7hal\u00b7ten", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Da, ausser dem Zusammenlauf verschiedner Umst\u00e4nd\u2019,", "tokens": ["Da", ",", "aus\u00b7ser", "dem", "Zu\u00b7sam\u00b7men\u00b7lauf", "ver\u00b7schied\u00b7ner", "Um\u00b7st\u00e4nd'", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+--++-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "er erkalten,", "tokens": ["er", "er\u00b7kal\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Und, ohne H\u00fclf\u2019, ersaufen m\u00fcssen. Die Umst\u00e4nd\u2019 alle", "tokens": ["Und", ",", "oh\u00b7ne", "H\u00fcl\u00b7f'", ",", "er\u00b7sau\u00b7fen", "m\u00fcs\u00b7sen", ".", "Die", "Um\u00b7st\u00e4nd'", "al\u00b7le"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "KOUI", "NN", "$,", "VVINF", "VMINF", "$.", "ART", "NN", "PIAT"], "meter": "-+-+--+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "scheinen klein,", "tokens": ["schei\u00b7nen", "klein", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "ADJD", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.7": {"text": "Doch sieht man den Zusammenhang, mit etwas Ueber-", "tokens": ["Doch", "sieht", "man", "den", "Zu\u00b7sam\u00b7men\u00b7hang", ",", "mit", "et\u00b7was", "Ue\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PIS", "ART", "NN", "$,", "APPR", "PIAT", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "legen, ein;", "tokens": ["le\u00b7gen", ",", "ein", ";"], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVINF", "$,", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.9": {"text": "So findet sich, wenn nur ein Glied aus dieser Kette wo", "tokens": ["So", "fin\u00b7det", "sich", ",", "wenn", "nur", "ein", "Glied", "aus", "die\u00b7ser", "Ket\u00b7te", "wo"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "$,", "KOUS", "ADV", "ART", "NN", "APPR", "PDAT", "NN", "PWAV"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.10": {"text": "gefehlet,", "tokens": ["ge\u00b7feh\u00b7let", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.11": {"text": "Die H\u00fclfe w\u00e4r umsonst gewesen, und l\u00e4ge ", "tokens": ["Die", "H\u00fcl\u00b7fe", "w\u00e4r", "um\u00b7sonst", "ge\u00b7we\u00b7sen", ",", "und", "l\u00e4\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADV", "VAPP", "$,", "KON", "VVFIN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "entseelet.", "tokens": ["ent\u00b7see\u00b7let", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.13": {"text": "So ist es ja ganz \u00fcberzeuglich, da\u00df hier kein blindes Un-", "tokens": ["So", "ist", "es", "ja", "ganz", "\u00fc\u00b7berz\u00b7eug\u00b7lich", ",", "da\u00df", "hier", "kein", "blin\u00b7des", "Un"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$,", "KOUS", "ADV", "PIAT", "ADJA", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.14": {"text": "gefehr", "tokens": ["ge\u00b7fehr"], "token_info": ["word"], "pos": ["ADV"], "meter": "-+", "measure": "iambic.single"}, "line.15": {"text": "So manchen Umstand, welcher n\u00f6htig, zu rechter Stunde", "tokens": ["So", "man\u00b7chen", "Um\u00b7stand", ",", "wel\u00b7cher", "n\u00f6h\u00b7tig", ",", "zu", "rech\u00b7ter", "Stun\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "PIAT", "NN", "$,", "PRELS", "ADJD", "$,", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.16": {"text": "hergef\u00fchret,", "tokens": ["her\u00b7ge\u00b7f\u00fch\u00b7ret", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.17": {"text": "Nein, da\u00df die Macht, voll Lieb und Weisheit, wie \u00fcberall,", "tokens": ["Nein", ",", "da\u00df", "die", "Macht", ",", "voll", "Lieb", "und", "Weis\u00b7heit", ",", "wie", "\u00fc\u00b7be\u00b7rall", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKANT", "$,", "KOUS", "ART", "NN", "$,", "ADJD", "NN", "KON", "NN", "$,", "PWAV", "ADV", "$,"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "auch hier regieret.", "tokens": ["auch", "hier", "re\u00b7gie\u00b7ret", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.19": {"text": "Damit nun die\u00df, wie, leider! vieles, nicht auch geschwinde", "tokens": ["Da\u00b7mit", "nun", "die\u00df", ",", "wie", ",", "lei\u00b7der", "!", "vie\u00b7les", ",", "nicht", "auch", "ge\u00b7schwin\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["PAV", "ADV", "PDS", "$,", "PWAV", "$,", "ADV", "$.", "PIS", "$,", "PTKNEG", "ADV", "ADJA"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.20": {"text": "sey vergessen;", "tokens": ["sey", "ver\u00b7ges\u00b7sen", ";"], "token_info": ["word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.21": {"text": "So will ich diesen Fall erzehlen, und GOttes Huld dabey", "tokens": ["So", "will", "ich", "die\u00b7sen", "Fall", "er\u00b7zeh\u00b7len", ",", "und", "Got\u00b7tes", "Huld", "da\u00b7bey"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PDAT", "NN", "VVINF", "$,", "KON", "NN", "NN", "PAV"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.22": {"text": "ermessen.", "tokens": ["er\u00b7mes\u00b7sen", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.2": {"line.1": {"text": "Ich stand, um meines Schlosses Graben besch\u00e4ftigt", "tokens": ["Ich", "stand", ",", "um", "mei\u00b7nes", "Schlos\u00b7ses", "Gra\u00b7ben", "be\u00b7sch\u00e4f\u00b7tigt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUI", "PPOSAT", "NN", "NN", "VVPP"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "einen Weg zu f\u00fchren,", "tokens": ["ei\u00b7nen", "Weg", "zu", "f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Und, um ihn Regel-recht zu haben, mit Linien ihn abzu-", "tokens": ["Und", ",", "um", "ihn", "Re\u00b7gel\u00b7recht", "zu", "ha\u00b7ben", ",", "mit", "Li\u00b7ni\u00b7en", "ihn", "ab\u00b7zu"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "$,", "KOUI", "PPER", "NN", "PTKZU", "VAINF", "$,", "APPR", "NN", "PPER", "TRUNC"], "meter": "-+-+-+-+-+-++-+-", "measure": "unknown.measure.octa.plus"}, "line.4": {"text": "schn\u00fcren.", "tokens": ["schn\u00fc\u00b7ren", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.5": {"text": "Der G\u00e4rtner hatt\u2019, an jener Seite, sie fest zu machen", "tokens": ["Der", "G\u00e4rt\u00b7ner", "hatt'", ",", "an", "je\u00b7ner", "Sei\u00b7te", ",", "sie", "fest", "zu", "ma\u00b7chen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "$,", "APPR", "PDAT", "NN", "$,", "PPER", "ADJD", "PTKZU", "VVINF"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "angefangen,", "tokens": ["an\u00b7ge\u00b7fan\u00b7gen", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "Als ich, mit lauter Stimm\u2019, ihm zurief: es w\u00e4re besser,", "tokens": ["Als", "ich", ",", "mit", "lau\u00b7ter", "Stimm'", ",", "ihm", "zu\u00b7rief", ":", "es", "w\u00e4\u00b7re", "bes\u00b7ser", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "PIAT", "NN", "$,", "PPER", "VVFIN", "$.", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "wo ich st\u00fcnde,", "tokens": ["wo", "ich", "st\u00fcn\u00b7de", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Da\u00df er die lange Linie von dieser Seite feste b\u00fcnde.", "tokens": ["Da\u00df", "er", "die", "lan\u00b7ge", "Li\u00b7nie", "von", "die\u00b7ser", "Sei\u00b7te", "fes\u00b7te", "b\u00fcn\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPR", "PDAT", "NN", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+-+-+-", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Dadurch nun kam er ganz von weiten nach dieser Stelle", "tokens": ["Da\u00b7durch", "nun", "kam", "er", "ganz", "von", "wei\u00b7ten", "nach", "die\u00b7ser", "Stel\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "hingegangen,", "tokens": ["hin\u00b7ge\u00b7gan\u00b7gen", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.6": {"text": "Indem erhub sich ein Geschrey, zur Rechten, das ich nicht", "tokens": ["In\u00b7dem", "er\u00b7hub", "sich", "ein", "Ge\u00b7schrey", ",", "zur", "Rech\u00b7ten", ",", "das", "ich", "nicht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NN", "$,", "APPRART", "NN", "$,", "PRELS", "PPER", "PTKNEG"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.7": {"text": "verstand.", "tokens": ["ver\u00b7stand", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.8": {"text": "Ich sah (und seh sie noch vor Augen) Soldaten durch den", "tokens": ["Ich", "sah", "(", "und", "seh", "sie", "noch", "vor", "Au\u00b7gen", ")", "Sol\u00b7da\u00b7ten", "durch", "den"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$(", "KON", "VVFIN", "PPER", "ADV", "APPR", "NN", "$(", "NN", "APPR", "ART"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "Garten springen,", "tokens": ["Gar\u00b7ten", "sprin\u00b7gen", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.10": {"text": "Und, mit den Lanzen in den H\u00e4nden, durch Strauch und", "tokens": ["Und", ",", "mit", "den", "Lan\u00b7zen", "in", "den", "H\u00e4n\u00b7den", ",", "durch", "Strauch", "und"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "APPR", "ART", "NN", "APPR", "ART", "NN", "$,", "APPR", "NN", "KON"], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.11": {"text": "Busch in Eile dringen,", "tokens": ["Busch", "in", "Ei\u00b7le", "drin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.12": {"text": "Sie eilten einem Boote zu, das sich ganz nahe bey mir fand,", "tokens": ["Sie", "eil\u00b7ten", "ei\u00b7nem", "Boo\u00b7te", "zu", ",", "das", "sich", "ganz", "na\u00b7he", "bey", "mir", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKVZ", "$,", "PRELS", "PRF", "ADV", "ADJD", "APPR", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}, "line.13": {"text": "Sie sagten nicht ein einzigs Wort (vermuhtlich mich", "tokens": ["Sie", "sag\u00b7ten", "nicht", "ein", "ein\u00b7zigs", "Wort", "(", "ver\u00b7muht\u00b7lich", "mich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "ART", "ADJA", "NN", "$(", "VVIMP", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "nicht zu erschrecken)", "tokens": ["nicht", "zu", "er\u00b7schre\u00b7cken", ")"], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "PTKZU", "VVINF", "$("], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.15": {"text": "Und ohn\u2019 von ihres Laufens Ursach mir das geringste zu", "tokens": ["Und", "ohn'", "von", "ih\u00b7res", "Lau\u00b7fens", "Ur\u00b7sach", "mir", "das", "ge\u00b7rings\u00b7te", "zu"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "APPR", "PPOSAT", "NN", "NN", "PPER", "ART", "ADJA", "PTKZU"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.16": {"text": "entdecken.", "tokens": ["ent\u00b7de\u00b7cken", "."], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.4": {"line.1": {"text": "Der G\u00e4rtner, der den bangen Zufall so gleich, und eh", "tokens": ["Der", "G\u00e4rt\u00b7ner", ",", "der", "den", "ban\u00b7gen", "Zu\u00b7fall", "so", "gleich", ",", "und", "eh"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "ART", "ADJA", "NN", "ADV", "ADV", "$,", "KON", "KOUS"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "als ich, geh\u00f6rt,", "tokens": ["als", "ich", ",", "ge\u00b7h\u00f6rt", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Da\u00df nemlich eins von meinen Kindern im Graben und", "tokens": ["Da\u00df", "nem\u00b7lich", "eins", "von", "mei\u00b7nen", "Kin\u00b7dern", "im", "Gra\u00b7ben", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PIS", "APPR", "PPOSAT", "NN", "APPRART", "NN", "KON"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "im Wasser lage,", "tokens": ["im", "Was\u00b7ser", "la\u00b7ge", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Sprang alsobald mit in das Boot, das er nur zu regieren", "tokens": ["Sprang", "al\u00b7so\u00b7bald", "mit", "in", "das", "Boot", ",", "das", "er", "nur", "zu", "re\u00b7gie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "APPR", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.6": {"text": "wu\u00dfte,", "tokens": ["wu\u00df\u00b7te", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Denn der Soldat verstand es nicht. Ein einzig Ruder", "tokens": ["Denn", "der", "Sol\u00b7dat", "ver\u00b7stand", "es", "nicht", ".", "Ein", "ein\u00b7zig", "Ru\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "PTKNEG", "$.", "ART", "ADJD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "war nur da,", "tokens": ["war", "nur", "da", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.9": {"text": "Womit er denn, f\u00fcr grosser Eil, dem andern Ufer immer", "tokens": ["Wo\u00b7mit", "er", "denn", ",", "f\u00fcr", "gros\u00b7ser", "Eil", ",", "dem", "an\u00b7dern", "U\u00b7fer", "im\u00b7mer"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "$,", "APPR", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.10": {"text": "nah,", "tokens": ["nah", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-", "measure": "single.down"}, "line.11": {"text": "Und, weil kein Steur gebrauchet ward, in der Verwirrung", "tokens": ["Und", ",", "weil", "kein", "Steur", "ge\u00b7brau\u00b7chet", "ward", ",", "in", "der", "Ver\u00b7wir\u00b7rung"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "$,", "KOUS", "PIAT", "NN", "VVPP", "VAFIN", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "seitwerts kehrte,", "tokens": ["seit\u00b7werts", "kehr\u00b7te", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.13": {"text": "Und an das ander\u2019 Ufer erst, nachher so gar ins Schilf", "tokens": ["Und", "an", "das", "an\u00b7der'", "U\u00b7fer", "erst", ",", "nach\u00b7her", "so", "gar", "ins", "Schilf"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN", "ADV", "$,", "ADV", "ADV", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.14": {"text": "vertrieb,", "tokens": ["ver\u00b7trieb", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.15": {"text": "So ich, wie leichtlich zu erachten, ohn\u2019 Unmuht, Angst", "tokens": ["So", "ich", ",", "wie", "leicht\u00b7lich", "zu", "e\u00b7rach\u00b7ten", ",", "ohn'", "Un\u00b7muht", ",", "Angst"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["ADV", "PPER", "$,", "PWAV", "ADJD", "PTKZU", "VVINF", "$,", "KOUI", "NN", "$,", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.16": {"text": "und Zorn nicht sah.", "tokens": ["und", "Zorn", "nicht", "sah", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PTKNEG", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Ich eilt\u2019 am Strand, und sah das Kind recht mitten", "tokens": ["Ich", "eilt'", "am", "Strand", ",", "und", "sah", "das", "Kind", "recht", "mit\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "$,", "KON", "VVFIN", "ART", "NN", "ADV", "ADV"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "in dem Graben liegen,", "tokens": ["in", "dem", "Gra\u00b7ben", "lie\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Wo er mit seinen kleinen H\u00e4nden noch an dem Schiffgen", "tokens": ["Wo", "er", "mit", "sei\u00b7nen", "klei\u00b7nen", "H\u00e4n\u00b7den", "noch", "an", "dem", "Schiff\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "APPR", "PPOSAT", "ADJA", "NN", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "h\u00e4ngen blieb.", "tokens": ["h\u00e4n\u00b7gen", "blieb", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VVFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Wir schrien ihm alle heftig zu: Er w\u00fcrde schleunig H\u00fclfe", "tokens": ["Wir", "schri\u00b7en", "ihm", "al\u00b7le", "hef\u00b7tig", "zu", ":", "Er", "w\u00fcr\u00b7de", "schleu\u00b7nig", "H\u00fcl\u00b7fe"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PIAT", "ADJD", "PTKVZ", "$.", "PPER", "VAFIN", "ADJD", "NN"], "meter": "-+--+-+-+-+-+-+-", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "kriegen,", "tokens": ["krie\u00b7gen", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Er sollte sich nur feste halten. Inzwischen nahte sich", "tokens": ["Er", "soll\u00b7te", "sich", "nur", "fes\u00b7te", "hal\u00b7ten", ".", "I\u00b7nzwi\u00b7schen", "nah\u00b7te", "sich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PRF", "ADV", "ADJA", "VVINF", "$.", "ADV", "VVFIN", "PRF"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.8": {"text": "das Boot,", "tokens": ["das", "Boot", ","], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.9": {"text": "Nachdem es lange gnug gew\u00e4hret, entri\u00df ihn der Gefahr", "tokens": ["Nach\u00b7dem", "es", "lan\u00b7ge", "gnug", "ge\u00b7w\u00e4h\u00b7ret", ",", "ent\u00b7ri\u00df", "ihn", "der", "Ge\u00b7fahr"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VVPP", "$,", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.10": {"text": "und Noht,", "tokens": ["und", "Noht", ","], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.11": {"text": "Worinn, ohn\u2019 da\u00df man, da der Grabe so breit und tief", "tokens": ["Wo\u00b7rinn", ",", "ohn'", "da\u00df", "man", ",", "da", "der", "Gra\u00b7be", "so", "breit", "und", "tief"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "$,", "KOUI", "KOUS", "PIS", "$,", "KOUS", "ART", "NN", "ADV", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "war, helfen kunt,", "tokens": ["war", ",", "hel\u00b7fen", "kunt", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "$,", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.13": {"text": "Indem es in der Mitte lag, und jeder ferne von ihm", "tokens": ["In\u00b7dem", "es", "in", "der", "Mit\u00b7te", "lag", ",", "und", "je\u00b7der", "fer\u00b7ne", "von", "ihm"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "ART", "NN", "VVFIN", "$,", "KON", "PIAT", "ADJA", "APPR", "PPER"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.14": {"text": "stund.", "tokens": ["stund", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-", "measure": "single.down"}, "line.15": {"text": "Das aller\u00e4ngstlichste nun war, da\u00df man ihn sehen liegen", "tokens": ["Das", "al\u00b7le\u00b7r\u00e4ngst\u00b7lichs\u00b7te", "nun", "war", ",", "da\u00df", "man", "ihn", "se\u00b7hen", "lie\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADV", "VAFIN", "$,", "KOUS", "PIS", "PPER", "VVINF", "VVFIN"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.16": {"text": "mu\u00dfte,", "tokens": ["mu\u00df\u00b7te", ","], "token_info": ["word", "punct"], "pos": ["VMFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.17": {"text": "Ohn\u2019 da\u00df man ihm zu H\u00fclfe kommen, noch Mittel, ihm", "tokens": ["Ohn'", "da\u00df", "man", "ihm", "zu", "H\u00fcl\u00b7fe", "kom\u00b7men", ",", "noch", "Mit\u00b7tel", ",", "ihm"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["APPR", "KOUS", "PIS", "PPER", "APPR", "NN", "VVINF", "$,", "ADV", "NN", "$,", "PPER"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "zu rahten, wu\u00dfte.", "tokens": ["zu", "rah\u00b7ten", ",", "wu\u00df\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.6": {"line.1": {"text": "Nachdem er nun, durch GOttes Gnade, gerettet;", "tokens": ["Nach\u00b7dem", "er", "nun", ",", "durch", "Got\u00b7tes", "Gna\u00b7de", ",", "ge\u00b7ret\u00b7tet", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "$,", "APPR", "NN", "NN", "$,", "VVPP", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "denkt mein Geist dabey,", "tokens": ["denkt", "mein", "Geist", "da\u00b7bey", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "PAV", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Wie der Zusammenhang der Dinge Bewunderns wehrt", "tokens": ["Wie", "der", "Zu\u00b7sam\u00b7men\u00b7hang", "der", "Din\u00b7ge", "Be\u00b7wun\u00b7derns", "wehrt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "ART", "NN", "NN", "VVFIN"], "meter": "+-+--+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "gewesen sey,", "tokens": ["ge\u00b7we\u00b7sen", "sey", ","], "token_info": ["word", "word", "punct"], "pos": ["VAPP", "VAFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Da\u00df, wenn von so verschiedenen ein einz\u2019ger Umstand", "tokens": ["Da\u00df", ",", "wenn", "von", "so", "ver\u00b7schie\u00b7de\u00b7nen", "ein", "einz'\u00b7ger", "Um\u00b7stand"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "$,", "KOUS", "APPR", "ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-----+---+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "nur gefehlet,", "tokens": ["nur", "ge\u00b7feh\u00b7let", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "Nun, menschlichem Begriffe nach, des Kindes C\u00f6rperchen", "tokens": ["Nun", ",", "menschli\u00b7chem", "Be\u00b7grif\u00b7fe", "nach", ",", "des", "Kin\u00b7des", "C\u00f6r\u00b7per\u00b7chen"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "ADJA", "NN", "PTKVZ", "$,", "ART", "NN", "NE"], "meter": "-+--+-+-+-+--", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "entseelet,", "tokens": ["ent\u00b7see\u00b7let", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.9": {"text": "Jm Sarge vor mir liegen w\u00fcrde. Zum ersten hat von", "tokens": ["Jm", "Sar\u00b7ge", "vor", "mir", "lie\u00b7gen", "w\u00fcr\u00b7de", ".", "Zum", "ers\u00b7ten", "hat", "von"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "PPER", "VVINF", "VAFIN", "$.", "APPRART", "ADJA", "VAFIN", "APPR"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "ungefehr", "tokens": ["un\u00b7ge\u00b7fehr"], "token_info": ["word"], "pos": ["ADJD"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Der Informator an dem Ort, wo Niemand sonst gewesen", "tokens": ["Der", "In\u00b7for\u00b7ma\u00b7tor", "an", "dem", "Ort", ",", "wo", "Nie\u00b7mand", "sonst", "ge\u00b7we\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$,", "PWAV", "PIS", "ADV", "VAPP"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.12": {"text": "w\u00e4r,", "tokens": ["w\u00e4r", ","], "token_info": ["word", "punct"], "pos": ["VAFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.13": {"text": "Noch l\u00e4nger, als er selbst gewollt, sich aufgehalten, und", "tokens": ["Noch", "l\u00e4n\u00b7ger", ",", "als", "er", "selbst", "ge\u00b7wollt", ",", "sich", "auf\u00b7ge\u00b7hal\u00b7ten", ",", "und"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["ADV", "ADJD", "$,", "KOUS", "PPER", "ADV", "VMPP", "$,", "PRF", "VVPP", "$,", "KON"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.14": {"text": "gelesen.", "tokens": ["ge\u00b7le\u00b7sen", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.7": {"line.1": {"text": "Vors andere, da\u00df, durch die Neigung zu einem Hunde,", "tokens": ["Vors", "an\u00b7de\u00b7re", ",", "da\u00df", ",", "durch", "die", "Nei\u00b7gung", "zu", "ei\u00b7nem", "Hun\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$,", "KOUS", "$,", "APPR", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "er beroogen,", "tokens": ["er", "be\u00b7ro\u00b7o\u00b7gen", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVINF", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "In Meynung, da er klatschen h\u00f6rt, er l\u00e4g im Wasser,", "tokens": ["In", "Mey\u00b7nung", ",", "da", "er", "klat\u00b7schen", "h\u00f6rt", ",", "er", "l\u00e4g", "im", "Was\u00b7ser", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "KOUS", "PPER", "VVINF", "VVFIN", "$,", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "hingezogen,", "tokens": ["hin\u00b7ge\u00b7zo\u00b7gen", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Um ihm zu helfen, da er denn am selben Ort von unge-", "tokens": ["Um", "ihm", "zu", "hel\u00b7fen", ",", "da", "er", "denn", "am", "sel\u00b7ben", "Ort", "von", "un\u00b7ge"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUI", "PPER", "PTKZU", "VVINF", "$,", "KOUS", "PPER", "ADV", "APPRART", "ADJA", "NN", "APPR", "TRUNC"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.6": {"text": "fehr", "tokens": ["fehr"], "token_info": ["word"], "pos": ["ADJD"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Das kleine Boot, worinn er erst das Kind gesehen hatte,", "tokens": ["Das", "klei\u00b7ne", "Boot", ",", "wo\u00b7rinn", "er", "erst", "das", "Kind", "ge\u00b7se\u00b7hen", "hat\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PWAV", "PPER", "ADV", "ART", "NN", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.8": {"text": "leer,", "tokens": ["leer", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-", "measure": "single.down"}, "line.9": {"text": "Und ihn im Wasser zappeln sieht. Worauf er denn", "tokens": ["Und", "ihn", "im", "Was\u00b7ser", "zap\u00b7peln", "sieht", ".", "Wo\u00b7rauf", "er", "denn"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPER", "APPRART", "NN", "VVINF", "VVFIN", "$.", "PAV", "PPER", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "geschwinde lief,", "tokens": ["ge\u00b7schwin\u00b7de", "lief", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.11": {"text": "Und der noch ziemlich weit davon entfernten Wache schleu-", "tokens": ["Und", "der", "noch", "ziem\u00b7lich", "weit", "da\u00b7von", "ent\u00b7fern\u00b7ten", "Wa\u00b7che", "schleu"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADV", "ADV", "ADJD", "PAV", "ADJA", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.12": {"text": "nig rief,", "tokens": ["nig", "rief", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.13": {"text": "Die denn zwar schnell gelaufen kam; doch aber nicht zu", "tokens": ["Die", "denn", "zwar", "schnell", "ge\u00b7lau\u00b7fen", "kam", ";", "doch", "a\u00b7ber", "nicht", "zu"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADV", "ADJD", "VVPP", "VVFIN", "$.", "ADV", "ADV", "PTKNEG", "PTKZU"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.14": {"text": "helfen wu\u00dfte;", "tokens": ["hel\u00b7fen", "wu\u00df\u00b7te", ";"], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VVFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "Bis einer auf das andre Boot sich noch besinnt. Das", "tokens": ["Bis", "ei\u00b7ner", "auf", "das", "and\u00b7re", "Boot", "sich", "noch", "be\u00b7sinnt", ".", "Das"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["APPR", "ART", "APPR", "ART", "ADJA", "NN", "PRF", "ADV", "ADJD", "$.", "ART"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "aber lag", "tokens": ["a\u00b7ber", "lag"], "token_info": ["word", "word"], "pos": ["ADV", "VVFIN"], "meter": "+-+", "measure": "trochaic.di"}, "line.17": {"text": "Noch ziemlich weit; doch aber n\u00e4her, als wie es sonst", "tokens": ["Noch", "ziem\u00b7lich", "weit", ";", "doch", "a\u00b7ber", "n\u00e4\u00b7her", ",", "als", "wie", "es", "sonst"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ADJD", "$.", "ADV", "ADV", "ADJD", "$,", "KOUS", "PWAV", "PPER", "ADV"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "zu liegen pflag,", "tokens": ["zu", "lie\u00b7gen", "pflag", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.19": {"text": "Und noch zum Gl\u00fcck an diesem Ufer. Woher man es", "tokens": ["Und", "noch", "zum", "Gl\u00fcck", "an", "die\u00b7sem", "U\u00b7fer", ".", "Wo\u00b7her", "man", "es"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "APPRART", "NN", "APPR", "PDAT", "NN", "$.", "PWAV", "PIS", "PPER"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.20": {"text": "denn holen mu\u00dfte.", "tokens": ["denn", "ho\u00b7len", "mu\u00df\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.21": {"text": "Die\u00df lag nun nahe, wo ich stund, und, wie ich es bereits", "tokens": ["Die\u00df", "lag", "nun", "na\u00b7he", ",", "wo", "ich", "stund", ",", "und", ",", "wie", "ich", "es", "be\u00b7reits"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "ADJD", "$,", "PWAV", "PPER", "VVFIN", "$,", "KON", "$,", "PWAV", "PPER", "PPER", "ADV"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.22": {"text": "erzehlt,", "tokens": ["er\u00b7zehlt", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.23": {"text": "Wo nicht der G\u00e4rtner eben kommen, und er an diesem", "tokens": ["Wo", "nicht", "der", "G\u00e4rt\u00b7ner", "e\u00b7ben", "kom\u00b7men", ",", "und", "er", "an", "die\u00b7sem"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "PTKNEG", "ART", "NN", "ADV", "VVINF", "$,", "KON", "PPER", "APPR", "PDAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.24": {"text": "Ort gefehlt;", "tokens": ["Ort", "ge\u00b7fehlt", ";"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.25": {"text": "W\u00e4r jemand in das Boot gefallen, der es zwar von dem", "tokens": ["W\u00e4r", "je\u00b7mand", "in", "das", "Boot", "ge\u00b7fal\u00b7len", ",", "der", "es", "zwar", "von", "dem"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PIS", "APPR", "ART", "NN", "VVPP", "$,", "PRELS", "PPER", "ADV", "APPR", "ART"], "meter": "-+-+-+-+-+-+--", "measure": "unknown.measure.hexa"}, "line.26": {"text": "Ufer trennen,", "tokens": ["U\u00b7fer", "tren\u00b7nen", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.27": {"text": "Doch, da er selbes nicht regieren, dem Knaben auch nicht", "tokens": ["Doch", ",", "da", "er", "sel\u00b7bes", "nicht", "re\u00b7gie\u00b7ren", ",", "dem", "Kna\u00b7ben", "auch", "nicht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "$,", "KOUS", "PPER", "ADV", "PTKNEG", "VVINF", "$,", "ART", "NN", "ADV", "PTKNEG"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.28": {"text": "helfen k\u00f6nnen,", "tokens": ["hel\u00b7fen", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.29": {"text": "Und uns des Werkzeugs noch beraubt, ohn\u2019 welches wir", "tokens": ["Und", "uns", "des", "Werk\u00b7zeugs", "noch", "be\u00b7raubt", ",", "ohn'", "wel\u00b7ches", "wir"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPER", "ART", "NN", "ADV", "VVPP", "$,", "APPR", "PRELS", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "nicht zu ihm nah'n,", "tokens": ["nicht", "zu", "ihm", "nah'n", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "PPER", "VVFIN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.31": {"text": "Noch ihm zur Rettung kommen k\u00f6nnen. Noch mehr,", "tokens": ["Noch", "ihm", "zur", "Ret\u00b7tung", "kom\u00b7men", "k\u00f6n\u00b7nen", ".", "Noch", "mehr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "PPER", "APPRART", "NN", "VVINF", "VMINF", "$.", "ADV", "ADV", "$,"], "meter": "-+-+-+-+---", "measure": "unknown.measure.tetra"}, "line.32": {"text": "da\u00df nur ein wenig Wind,", "tokens": ["da\u00df", "nur", "ein", "we\u00b7nig", "Wind", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "PIAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Und doch so viel war, da\u00df noch eben das Boot, aus welchem", "tokens": ["Und", "doch", "so", "viel", "war", ",", "da\u00df", "noch", "e\u00b7ben", "das", "Boot", ",", "aus", "wel\u00b7chem"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "ADV", "ADV", "ADV", "VAFIN", "$,", "KOUS", "ADV", "ADV", "ART", "NN", "$,", "APPR", "PWAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "erst das Kind", "tokens": ["erst", "das", "Kind"], "token_info": ["word", "word", "word"], "pos": ["ADV", "ART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Gefallen, nach ihm hingetrieben, und, wie es einmahl", "tokens": ["Ge\u00b7fal\u00b7len", ",", "nach", "ihm", "hin\u00b7ge\u00b7trie\u00b7ben", ",", "und", ",", "wie", "es", "ein\u00b7mahl"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "APPR", "PPER", "VVPP", "$,", "KON", "$,", "PWAV", "PPER", "ADV"], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "ihm entglitten,", "tokens": ["ihm", "ent\u00b7glit\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Nicht weg, noch einmahl nach ihm trieb. Noch mehr,", "tokens": ["Nicht", "weg", ",", "noch", "ein\u00b7mahl", "nach", "ihm", "trieb", ".", "Noch", "mehr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "PTKVZ", "$,", "ADV", "ADV", "APPR", "PPER", "VVFIN", "$.", "ADV", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "da\u00df er den Tag vorher", "tokens": ["da\u00df", "er", "den", "Tag", "vor\u00b7her"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Selbst eben eine kleine Schnur zum Schwerdt am Boot", "tokens": ["Selbst", "e\u00b7ben", "ei\u00b7ne", "klei\u00b7ne", "Schnur", "zum", "Schwerdt", "am", "Boot"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN", "APPRART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "zurecht geschnitten,", "tokens": ["zu\u00b7recht", "ge\u00b7schnit\u00b7ten", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKVZ", "VVPP", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "Auf welcher er den Fu\u00df gesetzt, und da\u00df, ob es gleich", "tokens": ["Auf", "wel\u00b7cher", "er", "den", "Fu\u00df", "ge\u00b7setzt", ",", "und", "da\u00df", ",", "ob", "es", "gleich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "ART", "NN", "VVPP", "$,", "KON", "KOUS", "$,", "KOUS", "PPER", "ADV"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.10": {"text": "schwach, er schwehr,", "tokens": ["schwach", ",", "er", "schwehr", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$,", "PPER", "ADV", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Es dennoch nicht entzwey gebrochen. Die\u00df ist nun eine", "tokens": ["Es", "den\u00b7noch", "nicht", "ent\u00b7zwey", "ge\u00b7bro\u00b7chen", ".", "Die\u00df", "ist", "nun", "ei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "PTKNEG", "PTKVZ", "VVPP", "$.", "PDS", "VAFIN", "ADV", "ART"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "grosse Reih", "tokens": ["gros\u00b7se", "Reih"], "token_info": ["word", "word"], "pos": ["ADJA", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.13": {"text": "Von manchem Umstand, die wir wissen, wovon nicht", "tokens": ["Von", "man\u00b7chem", "Um\u00b7stand", ",", "die", "wir", "wis\u00b7sen", ",", "wo\u00b7von", "nicht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PIAT", "NN", "$,", "PRELS", "PPER", "VVINF", "$,", "PWAV", "PTKNEG"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "einer fehlen m\u00fcssen;", "tokens": ["ei\u00b7ner", "feh\u00b7len", "m\u00fcs\u00b7sen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "VVINF", "VMINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.15": {"text": "Sonst w\u00e4re meinem kleinen ", "tokens": ["Sonst", "w\u00e4\u00b7re", "mei\u00b7nem", "klei\u00b7nen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPOSAT", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.16": {"text": "abgerissen.", "tokens": ["ab\u00b7ge\u00b7ris\u00b7sen", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.17": {"text": "Nun ists vermuhtlich, da\u00df der Umst\u00e4nd\u2019 weit eine gr\u00f6\u00dfre", "tokens": ["Nun", "ists", "ver\u00b7muht\u00b7lich", ",", "da\u00df", "der", "Um\u00b7st\u00e4nd'", "weit", "ei\u00b7ne", "gr\u00f6\u00df\u00b7re"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "ADJD", "ART", "ADJA"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "Zahl noch sey,", "tokens": ["Zahl", "noch", "sey", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "VAFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.19": {"text": "Wovon uns der Zusammenhang noch nicht bekannt.", "tokens": ["Wo\u00b7von", "uns", "der", "Zu\u00b7sam\u00b7men\u00b7hang", "noch", "nicht", "be\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "ADV", "PTKNEG", "ADJD", "$."], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.20": {"text": "Wof\u00fcr wir eben", "tokens": ["Wo\u00b7f\u00fcr", "wir", "e\u00b7ben"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "PPER", "ADV"], "meter": "-+-+-", "measure": "iambic.di"}, "line.21": {"text": "So wohl, als die, so wir erkennen, dem Sch\u00f6pfer Lob", "tokens": ["So", "wohl", ",", "als", "die", ",", "so", "wir", "er\u00b7ken\u00b7nen", ",", "dem", "Sch\u00f6p\u00b7fer", "Lob"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "$,", "KOUS", "ART", "$,", "ADV", "PPER", "VVINF", "$,", "ART", "NN", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.22": {"text": "und Dank zu geben", "tokens": ["und", "Dank", "zu", "ge\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "NN", "PTKZU", "VVINF"], "meter": "-+-+-", "measure": "iambic.di"}, "line.23": {"text": "Verbunden und gehalten seyn. Regierer aller Dinge!", "tokens": ["Ver\u00b7bun\u00b7den", "und", "ge\u00b7hal\u00b7ten", "seyn", ".", "Re\u00b7gie\u00b7rer", "al\u00b7ler", "Din\u00b7ge", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "VAINF", "$.", "NN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.24": {"text": "Dir,", "tokens": ["Dir", ","], "token_info": ["word", "punct"], "pos": ["PPER", "$,"], "meter": "+", "measure": "single.up"}, "line.25": {"text": "Dem ewig Preis und Dank geb\u00fchret, sey ewig Preis und", "tokens": ["Dem", "e\u00b7wig", "Preis", "und", "Dank", "ge\u00b7b\u00fch\u00b7ret", ",", "sey", "e\u00b7wig", "Preis", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "NN", "KON", "NN", "VVFIN", "$,", "VAFIN", "ADJD", "NN", "KON"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.26": {"text": "Dank daf\u00fcr!", "tokens": ["Dank", "da\u00b7f\u00fcr", "!"], "token_info": ["word", "word", "punct"], "pos": ["APPR", "PAV", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.9": {"line.1": {"text": "Nun m\u00f6gte jemand meiner Leser vielleicht gedenken:", "tokens": ["Nun", "m\u00f6g\u00b7te", "je\u00b7mand", "mei\u00b7ner", "Le\u00b7ser", "viel\u00b7leicht", "ge\u00b7den\u00b7ken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "PPOSAT", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Dieser Fall", "tokens": ["Die\u00b7ser", "Fall"], "token_info": ["word", "word"], "pos": ["PDAT", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Betrifft nur dich. Was sollen wir daraus f\u00fcr Trost und", "tokens": ["Be\u00b7tr\u00b7ifft", "nur", "dich", ".", "Was", "sol\u00b7len", "wir", "da\u00b7raus", "f\u00fcr", "Trost", "und"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "PPER", "$.", "PWS", "VMFIN", "PPER", "PAV", "APPR", "NN", "KON"], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Lehre nehmen?", "tokens": ["Leh\u00b7re", "neh\u00b7men", "?"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.10": {"line.1": {"text": "\u201cso wie es hier mit meinem Kinde, geht es wahrhaftig", "tokens": ["\u201c", "so", "wie", "es", "hier", "mit", "mei\u00b7nem", "Kin\u00b7de", ",", "geht", "es", "wahr\u00b7haf\u00b7tig"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "ADV", "KOKOM", "PPER", "ADV", "APPR", "PPOSAT", "NN", "$,", "VVFIN", "PPER", "ADJD"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "\u00fcberall.", "tokens": ["\u00fc\u00b7be\u00b7rall", "."], "token_info": ["word", "punct"], "pos": ["ADV", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "\u201ewenn wir nur GOttes weiser F\u00fchrung mehr nachzu-", "tokens": ["\u201e", "wenn", "wir", "nur", "Got\u00b7tes", "wei\u00b7ser", "F\u00fch\u00b7rung", "mehr", "nach\u00b7zu"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "KOUS", "PPER", "ADV", "NN", "ADJA", "NN", "ADV", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "denken uns bequehmen;", "tokens": ["den\u00b7ken", "uns", "be\u00b7queh\u00b7men", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "\u201eso wird all\u2019 Augenblick ein jeder f\u00fcr abgekehrte Plag\u2019", "tokens": ["\u201e", "so", "wird", "all'", "Au\u00b7gen\u00b7blick", "ein", "je\u00b7der", "f\u00fcr", "ab\u00b7ge\u00b7kehr\u00b7te", "Plag'"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "VAFIN", "PIAT", "NN", "ART", "PIS", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "und Pein", "tokens": ["und", "Pein"], "token_info": ["word", "word"], "pos": ["KON", "NN"], "meter": "-+", "measure": "iambic.single"}, "line.7": {"text": "\u201edem, Der sie gn\u00e4dig abgekehrt, Lob, Preis und Ehre", "tokens": ["\u201e", "dem", ",", "Der", "sie", "gn\u00e4\u00b7dig", "ab\u00b7ge\u00b7kehrt", ",", "Lob", ",", "Preis", "und", "Eh\u00b7re"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["$(", "ART", "$,", "ART", "PPER", "ADJD", "VVPP", "$,", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "schuldig seyn.", "tokens": ["schul\u00b7dig", "seyn", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VAINF", "$."], "meter": "+-+", "measure": "trochaic.di"}}}}}