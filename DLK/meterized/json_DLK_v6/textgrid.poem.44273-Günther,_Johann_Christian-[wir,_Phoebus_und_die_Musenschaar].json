{"textgrid.poem.44273": {"metadata": {"author": {"name": "G\u00fcnther, Johann Christian", "birth": "N.A.", "death": "N.A."}, "title": "[wir, Phoebus und die Musenschaar]", "genre": "verse", "period": "N.A.", "pub_year": 1709, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wir, Phoebus und die Musenschaar,", "tokens": ["Wir", ",", "Phoe\u00b7bus", "und", "die", "Mu\u00b7sen\u00b7schaar", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "NE", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bekennen hiermit ofenbahr,", "tokens": ["Be\u00b7ken\u00b7nen", "hier\u00b7mit", "o\u00b7fen\u00b7bahr", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PAV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wo noth, vor all- und jeden St\u00e4nden,", "tokens": ["Wo", "noth", ",", "vor", "all", "und", "je\u00b7den", "St\u00e4n\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$,", "APPR", "TRUNC", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df unser junger Zettriz frey", "tokens": ["Da\u00df", "un\u00b7ser", "jun\u00b7ger", "Zett\u00b7riz", "frey"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und weiter nicht gehalten sey,", "tokens": ["Und", "wei\u00b7ter", "nicht", "ge\u00b7hal\u00b7ten", "sey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Mit Paucken l\u00e4ngre Zeit als Lehrling zu verschwenden.", "tokens": ["Mit", "Pau\u00b7cken", "l\u00e4ng\u00b7re", "Zeit", "als", "Lehr\u00b7ling", "zu", "ver\u00b7schwen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "KOUS", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Wir zeugen mehr aus Recht als Gunst,", "tokens": ["Wir", "zeu\u00b7gen", "mehr", "aus", "Recht", "als", "Gunst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df er in dieser Heldenkunst", "tokens": ["Da\u00df", "er", "in", "die\u00b7ser", "Hel\u00b7den\u00b7kunst"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den gr\u00f6sten Vortheil weggetragen", "tokens": ["Den", "gr\u00f6s\u00b7ten", "Vor\u00b7theil", "weg\u00b7ge\u00b7tra\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und w\u00fcrdig w\u00e4re, dem Eugen,", "tokens": ["Und", "w\u00fcr\u00b7dig", "w\u00e4\u00b7re", ",", "dem", "Eu\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "$,", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Vor welchem Hahn und Hund nicht stehn,", "tokens": ["Vor", "wel\u00b7chem", "Hahn", "und", "Hund", "nicht", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "KON", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Den st\u00e4rcksten Siegesmarch bey Belgrad vorzuschlagen.", "tokens": ["Den", "st\u00e4rcks\u00b7ten", "Sie\u00b7ges\u00b7march", "bey", "Bel\u00b7grad", "vor\u00b7zu\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Calliopens Trompetenschall", "tokens": ["Cal\u00b7li\u00b7o\u00b7pens", "Trom\u00b7pe\u00b7ten\u00b7schall"], "token_info": ["word", "word"], "pos": ["NE", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Liebt seiner Schl\u00e4gel Wechselfall", "tokens": ["Liebt", "sei\u00b7ner", "Schl\u00e4\u00b7gel", "Wech\u00b7sel\u00b7fall"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und spielt mit Lust zu ihrem Springen,", "tokens": ["Und", "spielt", "mit", "Lust", "zu", "ih\u00b7rem", "Sprin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Nymphen um den Boberstrand", "tokens": ["Die", "Nym\u00b7phen", "um", "den", "Bo\u00b7bers\u00b7trand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Bewundern die geschwinde Hand", "tokens": ["Be\u00b7wun\u00b7dern", "die", "ge\u00b7schwin\u00b7de", "Hand"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und h\u00f6ren ihre Kraft durch Wald und Ufer dringen.", "tokens": ["Und", "h\u00f6\u00b7ren", "ih\u00b7re", "Kraft", "durch", "Wald", "und", "U\u00b7fer", "drin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Nun, weil es oft gebr\u00e4uchlich ist,", "tokens": ["Nun", ",", "weil", "es", "oft", "ge\u00b7br\u00e4uch\u00b7lich", "ist", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df, wo man einen Lehrbrief list,", "tokens": ["Da\u00df", ",", "wo", "man", "ei\u00b7nen", "Lehr\u00b7brief", "list", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "PWAV", "PIS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die ehrliche Geburth sich zeige,", "tokens": ["Die", "ehr\u00b7li\u00b7che", "Ge\u00b7burth", "sich", "zei\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So schw\u00f6ren wir hier mit Bedacht,", "tokens": ["So", "schw\u00f6\u00b7ren", "wir", "hier", "mit", "Be\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da\u00df ihn ein Haus hervorgebracht,", "tokens": ["Da\u00df", "ihn", "ein", "Haus", "her\u00b7vor\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "An dem die Tugend sieht, wie hoch ihr Adel steige.", "tokens": ["An", "dem", "die", "Tu\u00b7gend", "sieht", ",", "wie", "hoch", "ihr", "A\u00b7del", "stei\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "NN", "VVFIN", "$,", "PWAV", "ADJD", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Des Vaters Amt, Verstand und Ruhm", "tokens": ["Des", "Va\u00b7ters", "Amt", ",", "Ver\u00b7stand", "und", "Ruhm"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schm\u00fcckt seines Schildes Alterthum", "tokens": ["Schm\u00fcckt", "sei\u00b7nes", "Schil\u00b7des", "Al\u00b7ter\u00b7thum"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Mit neuem Werth und frischen Kr\u00e4nzen,", "tokens": ["Mit", "neu\u00b7em", "Werth", "und", "fri\u00b7schen", "Kr\u00e4n\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der Mutter Sch\u00f6nheit, Blut und Wiz", "tokens": ["Der", "Mut\u00b7ter", "Sch\u00f6n\u00b7heit", ",", "Blut", "und", "Wiz"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Baut ihrer Tugend da den Siz,", "tokens": ["Baut", "ih\u00b7rer", "Tu\u00b7gend", "da", "den", "Siz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wo Damen seltner Art am Ehrenhimmel gl\u00e4nzen.", "tokens": ["Wo", "Da\u00b7men", "selt\u00b7ner", "Art", "am", "Eh\u00b7ren\u00b7him\u00b7mel", "gl\u00e4n\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "ADJA", "NN", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Wie nun kein L\u00f6w ein Schaaf gebiehrt,", "tokens": ["Wie", "nun", "kein", "L\u00f6w", "ein", "Schaaf", "ge\u00b7biehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PIAT", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Kein Pilz Orangenb\u00e4ume ziert", "tokens": ["Kein", "Pilz", "O\u00b7ran\u00b7gen\u00b7b\u00e4u\u00b7me", "ziert"], "token_info": ["word", "word", "word", "word"], "pos": ["PIAT", "NN", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und Adler blos von Adlern kommen,", "tokens": ["Und", "Ad\u00b7ler", "blos", "von", "Ad\u00b7lern", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So zeigt auch dieser unser Sohn", "tokens": ["So", "zeigt", "auch", "die\u00b7ser", "un\u00b7ser", "Sohn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "PDAT", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "An Mienen, Neigung und Person,", "tokens": ["An", "Mie\u00b7nen", ",", "Nei\u00b7gung", "und", "Per\u00b7son", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+++", "measure": "unknown.measure.penta"}, "line.6": {"text": "Von wem sein muntrer Geist das Feuer angenommen.", "tokens": ["Von", "wem", "sein", "mun\u00b7trer", "Geist", "das", "Feu\u00b7er", "an\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWS", "PPOSAT", "ADJA", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "So wie die helle Morgenzeit", "tokens": ["So", "wie", "die", "hel\u00b7le", "Mor\u00b7gen\u00b7zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den sch\u00f6nsten Mittag prophezeit,", "tokens": ["Den", "sch\u00f6ns\u00b7ten", "Mit\u00b7tag", "pro\u00b7phe\u00b7zeit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So will auch schon sein Flei\u00df und Spielen,", "tokens": ["So", "will", "auch", "schon", "sein", "Flei\u00df", "und", "Spie\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ADV", "ADV", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das mehr galant als kindisch ist", "tokens": ["Das", "mehr", "ga\u00b7lant", "als", "kin\u00b7disch", "ist"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "ADV", "ADJD", "KOKOM", "ADJD", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und Maas und Wohlstand nicht vergi\u00dft,", "tokens": ["Und", "Maas", "und", "Wohl\u00b7stand", "nicht", "ver\u00b7gi\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Auf Thaten voller Ruhm des reifen Alters zielen.", "tokens": ["Auf", "Tha\u00b7ten", "vol\u00b7ler", "Ruhm", "des", "rei\u00b7fen", "Al\u00b7ters", "zie\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Daher bedencken wir uns nicht,", "tokens": ["Da\u00b7her", "be\u00b7den\u00b7cken", "wir", "uns", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PRF", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihm, wo und wenn ihm was gebricht,", "tokens": ["Ihm", ",", "wo", "und", "wenn", "ihm", "was", "ge\u00b7bricht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PWAV", "KON", "KOUS", "PPER", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Carls Schuz und Gnade wahrzusagen;", "tokens": ["Carls", "Schuz", "und", "Gna\u00b7de", "wahr\u00b7zu\u00b7sa\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wir b\u00fcrgen selbst vor seine Treu,", "tokens": ["Wir", "b\u00fcr\u00b7gen", "selbst", "vor", "sei\u00b7ne", "Treu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie werde, gehn zehn Jahr vorbey,", "tokens": ["Sie", "wer\u00b7de", ",", "gehn", "zehn", "Jahr", "vor\u00b7bey", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "VVFIN", "CARD", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sich vor das Vaterland mit Stahl und Feder wagen.", "tokens": ["Sich", "vor", "das", "Va\u00b7ter\u00b7land", "mit", "Stahl", "und", "Fe\u00b7der", "wa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Doch \u00fcberhaupt empfehlen wir", "tokens": ["Doch", "\u00fc\u00b7ber\u00b7haupt", "emp\u00b7feh\u00b7len", "wir"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihn und sein Alter, Pallas, dir:", "tokens": ["Ihn", "und", "sein", "Al\u00b7ter", ",", "Pal\u00b7las", ",", "dir", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "KON", "PPOSAT", "NN", "$,", "NE", "$,", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Entdeck ihm alle Wei\u00dfheitssch\u00e4ze", "tokens": ["Ent\u00b7deck", "ihm", "al\u00b7le", "Wei\u00df\u00b7heits\u00b7sch\u00e4\u00b7ze"], "token_info": ["word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und f\u00fchr ihn auf der Ehrenbahn", "tokens": ["Und", "f\u00fchr", "ihn", "auf", "der", "Eh\u00b7ren\u00b7bahn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nach seines Vaters Beyspiel an,", "tokens": ["Nach", "sei\u00b7nes", "Va\u00b7ters", "Bey\u00b7spiel", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Damit des Landes Heil ihm einst ein Denckmahl seze.", "tokens": ["Da\u00b7mit", "des", "Lan\u00b7des", "Heil", "ihm", "einst", "ein", "Denck\u00b7mahl", "se\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "NN", "PPER", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Nach diesem st\u00e4hl ihm Mars den Muth", "tokens": ["Nach", "die\u00b7sem", "st\u00e4hl", "ihm", "Mars", "den", "Muth"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "VVFIN", "PPER", "NN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und f\u00fchr ihn zwar nicht ohne Blut,", "tokens": ["Und", "f\u00fchr", "ihn", "zwar", "nicht", "oh\u00b7ne", "Blut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch ohne Fall durch Dampf und Blizen.", "tokens": ["Doch", "oh\u00b7ne", "Fall", "durch", "Dampf", "und", "Bli\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich seh bereits sein siegreich Pferd", "tokens": ["Ich", "seh", "be\u00b7reits", "sein", "sieg\u00b7reich", "Pferd"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Mit Beute, Staub und Lob beschwert", "tokens": ["Mit", "Beu\u00b7te", ",", "Staub", "und", "Lob", "be\u00b7schwert"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ost- oder westw\u00e4rts her auf seiner R\u00fcckkunft schwizen.", "tokens": ["Ost", "o\u00b7der", "west\u00b7w\u00e4rts", "her", "auf", "sei\u00b7ner", "R\u00fcck\u00b7kunft", "schwi\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["TRUNC", "KON", "ADV", "ADV", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}}, "stanza.11": {"line.1": {"text": "Du aber, Venus, solst zulezt", "tokens": ["Du", "a\u00b7ber", ",", "Ve\u00b7nus", ",", "solst", "zu\u00b7lezt"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["PPER", "ADV", "$,", "NN", "$,", "VMFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihm alles, was nach M\u00fch erg\u00f6zt,", "tokens": ["Ihm", "al\u00b7les", ",", "was", "nach", "M\u00fch", "er\u00b7g\u00f6zt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "$,", "PRELS", "APPR", "NE", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "In deiner Wollustmuschel reichen.", "tokens": ["In", "dei\u00b7ner", "Wol\u00b7lust\u00b7mu\u00b7schel", "rei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Du liebst so gut Musick als Wein,", "tokens": ["Du", "liebst", "so", "gut", "Mu\u00b7sick", "als", "Wein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "NN", "KOUS", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Drum fl\u00f6\u00df ihm einst den Handgrif ein,", "tokens": ["Drum", "fl\u00f6\u00df", "ihm", "einst", "den", "Hand\u00b7grif", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Im Tempel deiner Lust die Saythen wohl zu streichen.", "tokens": ["Im", "Tem\u00b7pel", "dei\u00b7ner", "Lust", "die", "Say\u00b7then", "wohl", "zu", "strei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "ART", "NN", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Ja, wirf ihm dann bey Scherz und Ruh", "tokens": ["Ja", ",", "wirf", "ihm", "dann", "bey", "Scherz", "und", "Ruh"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "VVIMP", "PPER", "ADV", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die beste Violine zu,", "tokens": ["Die", "bes\u00b7te", "Vi\u00b7o\u00b7li\u00b7ne", "zu", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da wirstu Wunder sehn und h\u00f6ren;", "tokens": ["Da", "wirs\u00b7tu", "Wun\u00b7der", "sehn", "und", "h\u00f6\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "VVINF", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da wird er als ein danckbar Gast", "tokens": ["Da", "wird", "er", "als", "ein", "dan\u00b7ck\u00b7bar", "Gast"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "KOUS", "ART", "ADJD", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Die sch\u00f6nste Nymphe, so du hast,", "tokens": ["Die", "sch\u00f6ns\u00b7te", "Nym\u00b7phe", ",", "so", "du", "hast", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ADV", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Den W\u00fcrbel k\u00fcnstlich drehn und Paucken tragen lehren.", "tokens": ["Den", "W\u00fcr\u00b7bel", "k\u00fcnst\u00b7lich", "drehn", "und", "Pau\u00b7cken", "tra\u00b7gen", "leh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVINF", "KON", "NN", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Von jedem bitten wir dabey,", "tokens": ["Von", "je\u00b7dem", "bit\u00b7ten", "wir", "da\u00b7bey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVFIN", "PPER", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von was vor Stand und W\u00fcrd er sey,", "tokens": ["Von", "was", "vor", "Stand", "und", "W\u00fcrd", "er", "sey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "APPR", "NN", "KON", "VAFIN", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er woll ihm Lieb und Ehr erweisen", "tokens": ["Er", "woll", "ihm", "Lieb", "und", "Ehr", "er\u00b7wei\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "NN", "KON", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und seine wohlerlernte Kunst", "tokens": ["Und", "sei\u00b7ne", "woh\u00b7ler\u00b7lern\u00b7te", "Kunst"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nach M\u00f6gligkeit mit Rath und Gunst", "tokens": ["Nach", "M\u00f6g\u00b7lig\u00b7keit", "mit", "Rath", "und", "Gunst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Geheim und \u00f6fentlich zu seinem Gl\u00fccke preisen.", "tokens": ["Ge\u00b7heim", "und", "\u00f6\u00b7fent\u00b7lich", "zu", "sei\u00b7nem", "Gl\u00fc\u00b7cke", "prei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJD", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Sind K\u00f6pfe von der Thorenzunft,", "tokens": ["Sind", "K\u00f6p\u00b7fe", "von", "der", "Tho\u00b7ren\u00b7zunft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die uns aus Neid und Unvernunft", "tokens": ["Die", "uns", "aus", "Neid", "und", "Un\u00b7ver\u00b7nunft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "In dieser Bitte wiederstreben,", "tokens": ["In", "die\u00b7ser", "Bit\u00b7te", "wie\u00b7der\u00b7stre\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die sollen pl\u00f6zlich am Parna\u00df", "tokens": ["Die", "sol\u00b7len", "pl\u00f6z\u00b7lich", "am", "Par\u00b7na\u00df"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "ADJD", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wie mein Ver\u00e4chter Marsyas", "tokens": ["Wie", "mein", "Ver\u00b7\u00e4ch\u00b7ter", "Mar\u00b7syas"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PPOSAT", "NN", "NE"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Das Fell vom Leibe ziehn und auf die Paucken geben.", "tokens": ["Das", "Fell", "vom", "Lei\u00b7be", "ziehn", "und", "auf", "die", "Pau\u00b7cken", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "VVINF", "KON", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Den andern Monath nach dem May", "tokens": ["Den", "an\u00b7dern", "Mo\u00b7nath", "nach", "dem", "May"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Sechs- und eilfhundert zwanzig zwey,", "tokens": ["Sechs", "und", "eilf\u00b7hun\u00b7dert", "zwan\u00b7zig", "zwey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["TRUNC", "KON", "VVFIN", "CARD", "CARD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Neumond hies gleich Margarethe,", "tokens": ["Der", "Neu\u00b7mond", "hies", "gleich", "Mar\u00b7ga\u00b7re\u00b7the", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "NE", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Der ", "tokens": ["Der"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.5": {"text": "Von der vor diesmahl Canzler war", "tokens": ["Von", "der", "vor", "dies\u00b7mahl", "Canz\u00b7ler", "war"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "PDAT", "NN", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ein Mediciner und Poete.", "tokens": ["Ein", "Me\u00b7di\u00b7ci\u00b7ner", "und", "Poe\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.16": {"line.1": {"text": "Mit eigner Hand auf meiner Stuben", "tokens": ["Mit", "eig\u00b7ner", "Hand", "auf", "mei\u00b7ner", "Stu\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Schrieb ich als Zeuge", "tokens": ["Schrieb", "ich", "als", "Zeu\u00b7ge"], "token_info": ["word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "KOUS", "NN"], "meter": "++-+-", "measure": "iambic.di"}, "line.3": {"text": "G\u00f6rge Dluben,", "tokens": ["G\u00f6r\u00b7ge", "Dlu\u00b7ben", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Bestallter Musicus der Stadt,", "tokens": ["Be\u00b7stall\u00b7ter", "Mu\u00b7si\u00b7cus", "der", "Stadt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Die Land und Hut im Nahmen hat.", "tokens": ["Die", "Land", "und", "Hut", "im", "Nah\u00b7men", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "APPRART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}