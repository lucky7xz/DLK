{"textgrid.poem.32593": {"metadata": {"author": {"name": "Weise, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Mein R\u00f6sgen, meine Lust, mein Kind, das ich erw\u00e4hle,", "genre": "verse", "period": "N.A.", "pub_year": 1675, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein R\u00f6sgen, meine Lust, mein Kind, das ich erw\u00e4hle,", "tokens": ["Mein", "R\u00f6s\u00b7gen", ",", "mei\u00b7ne", "Lust", ",", "mein", "Kind", ",", "das", "ich", "er\u00b7w\u00e4h\u00b7le", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Ach geh du falsche Seele.", "tokens": ["Ach", "geh", "du", "fal\u00b7sche", "See\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Was sagt sie, bin ich falsch, da ich so freundlich thu?", "tokens": ["Was", "sagt", "sie", ",", "bin", "ich", "falsch", ",", "da", "ich", "so", "freund\u00b7lich", "thu", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$,", "VAFIN", "PPER", "ADJD", "$,", "KOUS", "PPER", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Schreibts meiner Einfalt zu.", "tokens": ["Schreibts", "mei\u00b7ner", "Ein\u00b7falt", "zu", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Sie ist mein Hertzens-Trost, mein Reichthum, mein Gel\u00fccke.", "tokens": ["Sie", "ist", "mein", "Hert\u00b7zens\u00b7Trost", ",", "mein", "Reicht\u00b7hum", ",", "mein", "Ge\u00b7l\u00fc\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Da\u00df dich mein Leibgen dr\u00fccke.", "tokens": ["Da\u00df", "dich", "mein", "Leib\u00b7gen", "dr\u00fc\u00b7cke", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Wie gl\u00e4ntzt ihr Angesicht, kein Bl\u00fcmgen ist so nett.", "tokens": ["Wie", "gl\u00e4ntzt", "ihr", "An\u00b7ge\u00b7sicht", ",", "kein", "Bl\u00fcm\u00b7gen", "ist", "so", "nett", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPOSAT", "NN", "$,", "PIAT", "NN", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Hat er nun auch geredt?", "tokens": ["Hat", "er", "nun", "auch", "ge\u00b7redt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Die hellen Augen seh ich als zwey Sterne scheinen,", "tokens": ["Die", "hel\u00b7len", "Au\u00b7gen", "seh", "ich", "als", "zwey", "Ster\u00b7ne", "schei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "KOKOM", "CARD", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Zwey Sterne wird er meinen,", "tokens": ["Zwey", "Ster\u00b7ne", "wird", "er", "mei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Und dieser sch\u00f6ne Glantz hat mich verliebt gemacht,", "tokens": ["Und", "die\u00b7ser", "sch\u00f6\u00b7ne", "Glantz", "hat", "mich", "ver\u00b7liebt", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDAT", "ADJA", "NN", "VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Ich h\u00e4t es nicht gedacht.", "tokens": ["Ich", "h\u00e4t", "es", "nicht", "ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Ich schwere bey der Hand, die ich so sehnlich k\u00fcsse,", "tokens": ["Ich", "schwe\u00b7re", "bey", "der", "Hand", ",", "die", "ich", "so", "sehn\u00b7lich", "k\u00fcs\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "$,", "PRELS", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Ich dachte was mich bisse.", "tokens": ["Ich", "dach\u00b7te", "was", "mich", "bis\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Drum stell ich mich bey ihr in tieffster Demuth ein,", "tokens": ["Drum", "stell", "ich", "mich", "bey", "ihr", "in", "tieffs\u00b7ter", "De\u00b7muth", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PRF", "APPR", "PPOSAT", "APPR", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Kan er auch h\u00f6hnisch seyn?", "tokens": ["Kan", "er", "auch", "h\u00f6h\u00b7nisch", "seyn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "ADJD", "VAINF", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.17": {"line.1": {"text": "Sie mu\u00df die Auslegung auch nicht so b\u00f6se machen,", "tokens": ["Sie", "mu\u00df", "die", "Aus\u00b7le\u00b7gung", "auch", "nicht", "so", "b\u00f6\u00b7se", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ADV", "PTKNEG", "ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "F\u00fcrwar, ich mu\u00df nur lachen.", "tokens": ["F\u00fcr\u00b7war", ",", "ich", "mu\u00df", "nur", "la\u00b7chen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "PPER", "VMFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Sie lacht, und gibt mir doch im Lachen einen Stich,", "tokens": ["Sie", "lacht", ",", "und", "gibt", "mir", "doch", "im", "La\u00b7chen", "ei\u00b7nen", "Stich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "VVFIN", "PPER", "ADV", "APPRART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.20": {"line.1": {"text": "Ach Herr, versorge mich.", "tokens": ["Ach", "Herr", ",", "ver\u00b7sor\u00b7ge", "mich", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "NN", "$,", "VVFIN", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Und dannoch werd ich stets zu ihren Diensten stehen,", "tokens": ["Und", "dan\u00b7noch", "werd", "ich", "stets", "zu", "ih\u00b7ren", "Diens\u00b7ten", "ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Er lasse sichs vergehen.", "tokens": ["Er", "las\u00b7se", "sichs", "ver\u00b7ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.23": {"line.1": {"text": "Wie werd ich doch veracht, ich armer Schmetterling.", "tokens": ["Wie", "werd", "ich", "doch", "ver\u00b7acht", ",", "ich", "ar\u00b7mer", "Schmet\u00b7ter\u00b7ling", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "ADV", "VVFIN", "$,", "PPER", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.24": {"line.1": {"text": "Ach V\u00e4ttergen, mein Ding.", "tokens": ["Ach", "V\u00e4t\u00b7ter\u00b7gen", ",", "mein", "Ding", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ITJ", "NN", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.25": {"line.1": {"text": "Mein Kind, was flucht sie so, sie f\u00fcrchte sich der Straffe,", "tokens": ["Mein", "Kind", ",", "was", "flucht", "sie", "so", ",", "sie", "f\u00fcrch\u00b7te", "sich", "der", "Straf\u00b7fe", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWS", "VVFIN", "PPER", "ADV", "$,", "PPER", "VVFIN", "PRF", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.26": {"line.1": {"text": "Er redt gewi\u00df im Schlaffe.", "tokens": ["Er", "redt", "ge\u00b7wi\u00df", "im", "Schlaf\u00b7fe", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.27": {"line.1": {"text": "Sie wecke mich nur auff, sonst schlaff ich h\u00e4rter ein,", "tokens": ["Sie", "we\u00b7cke", "mich", "nur", "auff", ",", "sonst", "schlaff", "ich", "h\u00e4r\u00b7ter", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PTKVZ", "$,", "ADV", "VVFIN", "PPER", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.28": {"line.1": {"text": "Vor di\u00dfmahl kans nicht seyn.", "tokens": ["Vor", "di\u00df\u00b7mahl", "kans", "nicht", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "VMFIN", "PTKNEG", "VAINF", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.29": {"line.1": {"text": "Und also bleibt mein Hertz allzeit in ihr verschlossen,", "tokens": ["Und", "al\u00b7so", "bleibt", "mein", "Hertz", "all\u00b7zeit", "in", "ihr", "ver\u00b7schlos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPOSAT", "NN", "ADV", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.30": {"line.1": {"text": "Das Ding gibt keinen Possen.", "tokens": ["Das", "Ding", "gibt", "kei\u00b7nen", "Pos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.31": {"line.1": {"text": "Ihr Hertze gegen meins, das w\u00e4r ein sch\u00f6ner Tausch,", "tokens": ["Ihr", "Hert\u00b7ze", "ge\u00b7gen", "meins", ",", "das", "w\u00e4r", "ein", "sch\u00f6\u00b7ner", "Tausch", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "$,", "PDS", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.32": {"line.1": {"text": "Er hat doch einen Rausch.", "tokens": ["Er", "hat", "doch", "ei\u00b7nen", "Rausch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.33": {"line.1": {"text": "Es scheint, als w\u00e4r ich gantz von ihrer Gunst geschieden,", "tokens": ["Es", "scheint", ",", "als", "w\u00e4r", "ich", "gantz", "von", "ih\u00b7rer", "Gunst", "ge\u00b7schie\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOKOM", "VAFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.34": {"line.1": {"text": "Er lasse mich zu frieden.", "tokens": ["Er", "las\u00b7se", "mich", "zu", "frie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.35": {"line.1": {"text": "Sie rede doch mit mir, wo meine Bitte gilt.", "tokens": ["Sie", "re\u00b7de", "doch", "mit", "mir", ",", "wo", "mei\u00b7ne", "Bit\u00b7te", "gilt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPER", "$,", "PWAV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.36": {"line.1": {"text": "Ach nein, die Mutter schilt.", "tokens": ["Ach", "nein", ",", "die", "Mut\u00b7ter", "schilt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PTKANT", "$,", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.37": {"line.1": {"text": "Sie hat mich doch nicht lieb, sie sagt mirs mit Geberden.", "tokens": ["Sie", "hat", "mich", "doch", "nicht", "lieb", ",", "sie", "sagt", "mirs", "mit", "Ge\u00b7ber\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$,", "PPER", "VVFIN", "NE", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.38": {"line.1": {"text": "Er sol ein Rahtsherr werden.", "tokens": ["Er", "sol", "ein", "Rahts\u00b7herr", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.39": {"line.1": {"text": "Indessen bleib ich doch verpicht auffs liebe Brod,", "tokens": ["In\u00b7des\u00b7sen", "bleib", "ich", "doch", "ver\u00b7picht", "auffs", "lie\u00b7be", "Brod", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.40": {"line.1": {"text": "Mit ihm hats keine Noth.", "tokens": ["Mit", "ihm", "hats", "kei\u00b7ne", "Noth", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "PIAT", "NN", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}}, "stanza.41": {"line.1": {"text": "Sie lebe wohl mein Kind, ich wil sie nicht verst\u00f6ren,", "tokens": ["Sie", "le\u00b7be", "wohl", "mein", "Kind", ",", "ich", "wil", "sie", "nicht", "ver\u00b7st\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "NN", "$,", "PPER", "VMFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.42": {"line.1": {"text": "Es ist mir lieb zu h\u00f6ren,", "tokens": ["Es", "ist", "mir", "lieb", "zu", "h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.43": {"line.1": {"text": "Ich hoffe ja sie wird auch meinen Schertz verstehen,", "tokens": ["Ich", "hof\u00b7fe", "ja", "sie", "wird", "auch", "mei\u00b7nen", "Schertz", "ver\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PPER", "VAFIN", "ADV", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.44": {"line.1": {"text": "Ich dacht er wolte gehen.", "tokens": ["Ich", "dacht", "er", "wol\u00b7te", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.45": {"line.1": {"text": "Ich geh, indem ich sie zur Vnzeit angetroffen,", "tokens": ["Ich", "geh", ",", "in\u00b7dem", "ich", "sie", "zur", "Vn\u00b7zeit", "an\u00b7ge\u00b7trof\u00b7fen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "PPER", "PPER", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.46": {"line.1": {"text": "Der Thorweg steht ihm offen.", "tokens": ["Der", "Thor\u00b7weg", "steht", "ihm", "of\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.47": {"line.1": {"text": "Jedoch ", "tokens": ["Je\u00b7doch"], "token_info": ["word"], "pos": ["ADV"], "meter": "-+", "measure": "iambic.single"}}, "stanza.48": {"line.1": {"text": "Er sey nur unbem\u00fcht.", "tokens": ["Er", "sey", "nur", "un\u00b7be\u00b7m\u00fcht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}