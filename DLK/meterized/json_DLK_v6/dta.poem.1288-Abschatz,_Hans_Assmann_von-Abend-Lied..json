{"dta.poem.1288": {"metadata": {"author": {"name": "Abschatz, Hans Assmann von", "birth": "N.A.", "death": "N.A."}, "title": "Abend-Lied.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1704", "urn": "urn:nbn:de:kobv:b4-200905199889", "language": ["de:0.99"], "booktitle": "Abschatz, Hans Assmann von: Poetische Ubersetzungen und Gedichte. Leipzig, 1704."}, "poem": {"stanza.1": {"line.1": {"text": "Die Sonne birgt nunmehr ihr angenehmes Licht/", "tokens": ["Die", "Son\u00b7ne", "birgt", "nun\u00b7mehr", "ihr", "an\u00b7ge\u00b7neh\u00b7mes", "Licht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der Abend will die Welt der Arbeit \u00fcberheben/", "tokens": ["Der", "A\u00b7bend", "will", "die", "Welt", "der", "Ar\u00b7beit", "\u00fc\u00b7ber\u00b7he\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Es fordert meine Pflicht/", "tokens": ["Es", "for\u00b7dert", "mei\u00b7ne", "Pflicht", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Dem H\u00f6chsten f\u00fcr den Schutz des Tages Danck zu geben.", "tokens": ["Dem", "H\u00f6chs\u00b7ten", "f\u00fcr", "den", "Schutz", "des", "Ta\u00b7ges", "Danck", "zu", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "ART", "NN", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Was mein Beruff erheischt/ ist wohl zu Ende bracht/", "tokens": ["Was", "mein", "Be\u00b7ruff", "er\u00b7heischt", "/", "ist", "wohl", "zu", "En\u00b7de", "bracht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "VVFIN", "$(", "VAFIN", "ADV", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Leib und Verm\u00f6gen sind noch frey von allem Schaden/", "tokens": ["Leib", "und", "Ver\u00b7m\u00f6\u00b7gen", "sind", "noch", "frey", "von", "al\u00b7lem", "Scha\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VAFIN", "ADV", "ADJD", "APPR", "PIS", "NN", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Ich kan mich mit der Nacht", "tokens": ["Ich", "kan", "mich", "mit", "der", "Nacht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Ohn Ungl\u00fcck und Beschwer der Sorgen-Last entladen.", "tokens": ["Ohn", "Un\u00b7gl\u00fcck", "und", "Be\u00b7schwer", "der", "Sor\u00b7gen\u00b7Last", "ent\u00b7la\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Viel/ leyder/ klagen sich verlezt durch Feind und Glutt/", "tokens": ["Viel", "/", "ley\u00b7der", "/", "kla\u00b7gen", "sich", "ver\u00b7lezt", "durch", "Feind", "und", "Glutt", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "ADV", "$(", "VVFIN", "PRF", "VVPP", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und andre f\u00fchlen sich bekr\u00e4nckt durch alle Glieder/", "tokens": ["Und", "and\u00b7re", "f\u00fch\u00b7len", "sich", "be\u00b7kr\u00e4nckt", "durch", "al\u00b7le", "Glie\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "PRF", "VVFIN", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Durch GOttes Engel-Hutt", "tokens": ["Durch", "Got\u00b7tes", "En\u00b7gel\u00b7Hutt"], "token_info": ["word", "word", "word"], "pos": ["APPR", "NN", "NE"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Leg ich mich unversehrt zur sanfften Ruhe nieder.", "tokens": ["Leg", "ich", "mich", "un\u00b7ver\u00b7sehrt", "zur", "sanff\u00b7ten", "Ru\u00b7he", "nie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PRF", "ADJD", "APPRART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Wie werd ich dir/ O GOtt/ daf\u00fcr nun danckbar seyn?", "tokens": ["Wie", "werd", "ich", "dir", "/", "O", "Gott", "/", "da\u00b7f\u00fcr", "nun", "dan\u00b7ck\u00b7bar", "seyn", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPER", "PPER", "$(", "NE", "NN", "$(", "PAV", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.2": {"text": "Mein schn\u00f6des Hertz ist voll von leeren Eitelkeiten:", "tokens": ["Mein", "schn\u00f6\u00b7des", "Hertz", "ist", "voll", "von", "lee\u00b7ren", "Ei\u00b7tel\u00b7kei\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VAFIN", "ADJD", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Stell ichs zum Opffer ein/", "tokens": ["Stell", "ichs", "zum", "Opf\u00b7fer", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "APPRART", "NN", "ART", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "So kan ich solches doch nicht nach Geb\u00fchr bereiten.", "tokens": ["So", "kan", "ich", "sol\u00b7ches", "doch", "nicht", "nach", "Ge\u00b7b\u00fchr", "be\u00b7rei\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PIS", "ADV", "PTKNEG", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Mein Auge scheuet sich den Himmel anzusehn/", "tokens": ["Mein", "Au\u00b7ge", "scheu\u00b7et", "sich", "den", "Him\u00b7mel", "an\u00b7zu\u00b7sehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "PRF", "ART", "NN", "VVIZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der Abend-R\u00f6the Glantz besch\u00e4met meine Wangen/", "tokens": ["Der", "A\u00b7ben\u00b7dR\u00f6\u00b7the", "Glantz", "be\u00b7sch\u00e4\u00b7met", "mei\u00b7ne", "Wan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was diesen Tag geschehn/", "tokens": ["Was", "die\u00b7sen", "Tag", "ge\u00b7schehn", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "NN", "VVPP", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Hat Straffe nur verdient (nicht Segen) zu erlangen.", "tokens": ["Hat", "Straf\u00b7fe", "nur", "ver\u00b7dient", "(", "nicht", "Se\u00b7gen", ")", "zu", "er\u00b7lan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "VVPP", "$(", "PTKNEG", "NN", "$(", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Doch denck ich an die Nacht/ da JEsus mich vertrat", "tokens": ["Doch", "denck", "ich", "an", "die", "Nacht", "/", "da", "Je\u00b7sus", "mich", "ver\u00b7trat"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVIMP", "PPER", "APPR", "ART", "NN", "$(", "KOUS", "NE", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "F\u00fcr deinem Richter-Stul in tuncklem Oelbergs-Schatten/", "tokens": ["F\u00fcr", "dei\u00b7nem", "Rich\u00b7ter\u00b7Stul", "in", "tunck\u00b7lem", "O\u00b7el\u00b7bergs\u00b7Schat\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Was er da th\u00e4t und bat/", "tokens": ["Was", "er", "da", "th\u00e4t", "und", "bat", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVFIN", "KON", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "K\u00f6mmt mir und aller Welt noch heilsamlich zu statten.", "tokens": ["K\u00f6mmt", "mir", "und", "al\u00b7ler", "Welt", "noch", "heil\u00b7sam\u00b7lich", "zu", "stat\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KON", "PIAT", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Die schwere Nacht verbirgt und decket meine Schuld/", "tokens": ["Die", "schwe\u00b7re", "Nacht", "ver\u00b7birgt", "und", "de\u00b7cket", "mei\u00b7ne", "Schuld", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "KON", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mein Heyland hat sie selbst geb\u00fcsset und begraben/", "tokens": ["Mein", "Hey\u00b7land", "hat", "sie", "selbst", "ge\u00b7b\u00fcs\u00b7set", "und", "be\u00b7gra\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "ADV", "VVPP", "KON", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Erworben deine Huld/", "tokens": ["Er\u00b7wor\u00b7ben", "dei\u00b7ne", "Huld", "/"], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "L\u00e4st mich zu dir in Bu\u00df und Glauben Zutritt haben.", "tokens": ["L\u00e4st", "mich", "zu", "dir", "in", "Bu\u00df", "und", "Glau\u00b7ben", "Zu\u00b7tritt", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPER", "APPR", "NN", "KON", "NN", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Drum klag ich mich zwar selbst mit Neue bey dir an/", "tokens": ["Drum", "klag", "ich", "mich", "zwar", "selbst", "mit", "Neu\u00b7e", "bey", "dir", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PRF", "ADV", "ADV", "APPR", "ADJA", "APPR", "PPER", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Glaub aber auch durch dich Verzeihung zu erwerben/", "tokens": ["Glaub", "a\u00b7ber", "auch", "durch", "dich", "Ver\u00b7zei\u00b7hung", "zu", "er\u00b7wer\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "APPR", "PPER", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wenn meiner Hoffnung Kahn", "tokens": ["Wenn", "mei\u00b7ner", "Hoff\u00b7nung", "Kahn"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Den starcken Ancker fast/ so kan ich nicht verderben.", "tokens": ["Den", "star\u00b7cken", "An\u00b7cker", "fast", "/", "so", "kan", "ich", "nicht", "ver\u00b7der\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "$(", "ADV", "VMFIN", "PPER", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Ich dancke f\u00fcr die Gnad entwichner Tages-Zeit/", "tokens": ["Ich", "dan\u00b7cke", "f\u00fcr", "die", "Gnad", "ent\u00b7wich\u00b7ner", "Ta\u00b7ges\u00b7Zeit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und kan ich diese Nacht derselben auch gen\u00fcssen/", "tokens": ["Und", "kan", "ich", "die\u00b7se", "Nacht", "der\u00b7sel\u00b7ben", "auch", "ge\u00b7n\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PDAT", "NN", "PDAT", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Werd ich aus Schuldigkeit/", "tokens": ["Werd", "ich", "aus", "Schul\u00b7dig\u00b7keit", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Dir neuen Morgen-Danck zu bringen seyn beflissen.", "tokens": ["Dir", "neu\u00b7en", "Mor\u00b7gen\u00b7Danck", "zu", "brin\u00b7gen", "seyn", "be\u00b7flis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "PTKZU", "VVINF", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}