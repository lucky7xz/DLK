{"textgrid.poem.55417": {"metadata": {"author": {"name": "Goethe, Johann Wolfgang", "birth": "N.A.", "death": "N.A."}, "title": "Zahme Xenien", "genre": "verse", "period": "N.A.", "pub_year": 1825, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "La\u00dft zahme Xenien immer walten,", "tokens": ["La\u00dft", "zah\u00b7me", "Xe\u00b7ni\u00b7en", "im\u00b7mer", "wal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADJA", "NN", "ADV", "VVINF", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der Dichter nimmer geb\u00fcckt ist.", "tokens": ["Der", "Dich\u00b7ter", "nim\u00b7mer", "ge\u00b7b\u00fcckt", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ihr lie\u00dft verr\u00fcckten Werther schalten,", "tokens": ["Ihr", "lie\u00dft", "ver\u00b7r\u00fcck\u00b7ten", "Wert\u00b7her", "schal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So lernt nun, wie das Alter verr\u00fcckt ist.", "tokens": ["So", "lernt", "nun", ",", "wie", "das", "Al\u00b7ter", "ver\u00b7r\u00fcckt", "ist", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "$,", "PWAV", "ART", "NN", "VVPP", "VAFIN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Den Vorteil hat der Dichter:", "tokens": ["Den", "Vor\u00b7teil", "hat", "der", "Dich\u00b7ter", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Wie die Gemeinde pr\u00fcft und probt,", "tokens": ["Wie", "die", "Ge\u00b7mein\u00b7de", "pr\u00fcft", "und", "probt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "So ist sie auch sein Richter;", "tokens": ["So", "ist", "sie", "auch", "sein", "Rich\u00b7ter", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da wird er nun gescholten, gelobt,", "tokens": ["Da", "wird", "er", "nun", "ge\u00b7schol\u00b7ten", ",", "ge\u00b7lobt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$,", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Und bleibt immer ein Dichter.", "tokens": ["Und", "bleibt", "im\u00b7mer", "ein", "Dich\u00b7ter."], "token_info": ["word", "word", "word", "word", "abbreviation"], "pos": ["KON", "VVFIN", "ADV", "ART", "NN"], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Es schnurrt mein Tagebuch", "tokens": ["Es", "schnurrt", "mein", "Ta\u00b7ge\u00b7buch"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Am Bratenwender:", "tokens": ["Am", "Bra\u00b7ten\u00b7wen\u00b7der", ":"], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Nichts schreibt sich leichter voll", "tokens": ["Nichts", "schreibt", "sich", "leich\u00b7ter", "voll"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PRF", "ADJD", "ADJD"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Als ein Kalender.", "tokens": ["Als", "ein", "Ka\u00b7len\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "$."], "meter": "+-+--", "measure": "unknown.measure.di"}}, "stanza.4": {"line.1": {"text": "\u00bbruf ich, da will mir keiner horchen;", "tokens": ["\u00bb", "ruf", "ich", ",", "da", "will", "mir", "kei\u00b7ner", "hor\u00b7chen", ";"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "KOUS", "VMFIN", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hab ich das um die Leute verdient?\u00ab", "tokens": ["Hab", "ich", "das", "um", "die", "Leu\u00b7te", "ver\u00b7dient", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "ART", "APPR", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Es m\u00f6chte niemand mehr gehorchen,", "tokens": ["Es", "m\u00f6ch\u00b7te", "nie\u00b7mand", "mehr", "ge\u00b7hor\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "W\u00e4ren aber alle gern gut bedient.", "tokens": ["W\u00e4\u00b7ren", "a\u00b7ber", "al\u00b7le", "gern", "gut", "be\u00b7dient", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIS", "ADV", "ADJD", "VVPP", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}, "stanza.5": {"line.1": {"text": "\u00bbwann wird der Herr seine Freude sehn?\u00ab", "tokens": ["\u00bb", "wann", "wird", "der", "Herr", "sei\u00b7ne", "Freu\u00b7de", "sehn", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VAFIN", "ART", "NN", "PPOSAT", "NN", "VVINF", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn er befiehlt mit Sinnen Ehrlichen", "tokens": ["Wenn", "er", "be\u00b7fiehlt", "mit", "Sin\u00b7nen", "Ehr\u00b7li\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "VVFIN", "APPR", "NN", "NN"], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Leuten, die's recht verstehn,", "tokens": ["Leu\u00b7ten", ",", "die's", "recht", "ver\u00b7stehn", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "ADJD", "VVINF", "$,"], "meter": "+----+", "measure": "dactylic.init"}, "line.4": {"text": "Und l\u00e4\u00dft sie was gewinnen.", "tokens": ["Und", "l\u00e4\u00dft", "sie", "was", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PIS", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "\u00bbwer ist ein unbrauchbarer Mann?\u00ab", "tokens": ["\u00bb", "wer", "ist", "ein", "un\u00b7brauch\u00b7ba\u00b7rer", "Mann", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VAFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "---+-+-+", "measure": "unknown.measure.tri"}, "line.2": {"text": "Der nicht befehlen und auch nicht gehorchen kann.", "tokens": ["Der", "nicht", "be\u00b7feh\u00b7len", "und", "auch", "nicht", "ge\u00b7hor\u00b7chen", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "VVINF", "KON", "ADV", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "\u00bbsage, warum dich die Menschen verlassen?\u00ab", "tokens": ["\u00bb", "sa\u00b7ge", ",", "wa\u00b7rum", "dich", "die", "Men\u00b7schen", "ver\u00b7las\u00b7sen", "?", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "$,", "PWAV", "PPER", "ART", "NN", "VVINF", "$.", "$("], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Glaubet nicht, da\u00df sie mich deshalb hassen;", "tokens": ["Glau\u00b7bet", "nicht", ",", "da\u00df", "sie", "mich", "des\u00b7halb", "has\u00b7sen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "KOUS", "PPER", "PRF", "PAV", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.3": {"text": "Auch bei mir will sich die Lust verlieren,", "tokens": ["Auch", "bei", "mir", "will", "sich", "die", "Lust", "ver\u00b7lie\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPER", "VMFIN", "PRF", "ART", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mit irgend jemand zu konversieren.", "tokens": ["Mit", "ir\u00b7gend", "je\u00b7mand", "zu", "kon\u00b7ver\u00b7sie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "So hoch die Nase reicht, da mag's wohl gehn,", "tokens": ["So", "hoch", "die", "Na\u00b7se", "reicht", ",", "da", "mag's", "wohl", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "VVFIN", "$,", "KOUS", "NE", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Was aber dr\u00fcber ist, k\u00f6nnen sie nicht sehn.", "tokens": ["Was", "a\u00b7ber", "dr\u00fc\u00b7ber", "ist", ",", "k\u00f6n\u00b7nen", "sie", "nicht", "sehn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "PAV", "VAFIN", "$,", "VMFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.9": {"line.1": {"text": "Wie einer ist, so ist sein Gott,", "tokens": ["Wie", "ei\u00b7ner", "ist", ",", "so", "ist", "sein", "Gott", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VAFIN", "$,", "ADV", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Darum ward Gott so oft zu Spott.", "tokens": ["Da\u00b7rum", "ward", "Gott", "so", "oft", "zu", "Spott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "NN", "ADV", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Geh ich, so wird der Schade gr\u00f6\u00dfer!", "tokens": ["Geh", "ich", ",", "so", "wird", "der", "Scha\u00b7de", "gr\u00f6\u00b7\u00dfer", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "$,", "ADV", "VAFIN", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bleib ich, so wird es auch nicht besser.", "tokens": ["Bleib", "ich", ",", "so", "wird", "es", "auch", "nicht", "bes\u00b7ser", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "$,", "ADV", "VAFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "\u00bbsei einmal ehrlich nur:", "tokens": ["\u00bb", "sei", "ein\u00b7mal", "ehr\u00b7lich", "nur", ":"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "ADV", "ADJD", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Wo findest du in deutscher Literatur", "tokens": ["Wo", "fin\u00b7dest", "du", "in", "deut\u00b7scher", "Li\u00b7te\u00b7ra\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Die gr\u00f6\u00dfte Verf\u00e4nglichkeit?\u00ab", "tokens": ["Die", "gr\u00f6\u00df\u00b7te", "Ver\u00b7f\u00e4ng\u00b7lich\u00b7keit", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["ART", "ADJA", "NN", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Wir sind von vielen Seiten gro\u00df,", "tokens": ["Wir", "sind", "von", "vie\u00b7len", "Sei\u00b7ten", "gro\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PIAT", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Doch hie und da gibt sich blo\u00df", "tokens": ["Doch", "hie", "und", "da", "gibt", "sich", "blo\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "KON", "ADV", "VVFIN", "PRF", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Bedauerlichste Unzul\u00e4nglichkeit.", "tokens": ["Be\u00b7dau\u00b7er\u00b7lichs\u00b7te", "Un\u00b7zu\u00b7l\u00e4ng\u00b7lich\u00b7keit", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.12": {"line.1": {"text": "\u00bbverzeihe mir, du gef\u00e4llst mir nicht,", "tokens": ["\u00bb", "ver\u00b7zei\u00b7he", "mir", ",", "du", "ge\u00b7f\u00e4llst", "mir", "nicht", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und schiltst du nicht, so schneidst ein Gesicht,", "tokens": ["Und", "schiltst", "du", "nicht", ",", "so", "schneidst", "ein", "Ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "$,", "ADV", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Wo s\u00e4mtliche loben und preisen!\u00ab", "tokens": ["Wo", "s\u00e4mt\u00b7li\u00b7che", "lo\u00b7ben", "und", "prei\u00b7sen", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PIS", "VVINF", "KON", "VVFIN", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Da\u00df, wenn man das eine von vornen bedeckt,", "tokens": ["Da\u00df", ",", "wenn", "man", "das", "ei\u00b7ne", "von", "vor\u00b7nen", "be\u00b7deckt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "KOUS", "PIS", "ART", "ART", "APPR", "ADV", "VVPP", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.5": {"text": "Das andre bleibt hinten hinausgestreckt,", "tokens": ["Das", "and\u00b7re", "bleibt", "hin\u00b7ten", "hin\u00b7aus\u00b7ge\u00b7streckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "ADV", "VVPP", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Das soll ein Anstand hei\u00dfen!", "tokens": ["Das", "soll", "ein", "An\u00b7stand", "hei\u00b7\u00dfen", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "\u00bbsage, wie es dir nur gef\u00e4llt,", "tokens": ["\u00bb", "sa\u00b7ge", ",", "wie", "es", "dir", "nur", "ge\u00b7f\u00e4llt", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "PWAV", "PPER", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Solch zerst\u00fcckeltes Zeug zu treiben?\u00ab", "tokens": ["Solch", "zer\u00b7st\u00fc\u00b7ckel\u00b7tes", "Zeug", "zu", "trei\u00b7ben", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PIAT", "ADJA", "NN", "PTKZU", "VVINF", "$.", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Seht nur hin: f\u00fcr gebildete Welt", "tokens": ["Seht", "nur", "hin", ":", "f\u00fcr", "ge\u00b7bil\u00b7de\u00b7te", "Welt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "PTKVZ", "$.", "APPR", "ADJA", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Darf man nichts anders beginnen und schreiben.", "tokens": ["Darf", "man", "nichts", "an\u00b7ders", "be\u00b7gin\u00b7nen", "und", "schrei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "PIS", "ADV", "VVINF", "KON", "VVINF", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}}, "stanza.14": {"line.1": {"text": "\u00bbwarum willst du das junge Blut", "tokens": ["\u00bb", "wa\u00b7rum", "willst", "du", "das", "jun\u00b7ge", "Blut"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "PWAV", "VMFIN", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So schn\u00f6de von dir entfernen?\u00ab", "tokens": ["So", "schn\u00f6\u00b7de", "von", "dir", "ent\u00b7fer\u00b7nen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPER", "VVINF", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie machen's alle h\u00fcbsch und gut,", "tokens": ["Sie", "ma\u00b7chen's", "al\u00b7le", "h\u00fcbsch", "und", "gut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Aber sie wollen nichts lernen.", "tokens": ["A\u00b7ber", "sie", "wol\u00b7len", "nichts", "ler\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VMFIN", "PIS", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.15": {"line.1": {"text": "Die holden jungen Geister", "tokens": ["Die", "hol\u00b7den", "jun\u00b7gen", "Geis\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Sind alle von einem Schlag,", "tokens": ["Sind", "al\u00b7le", "von", "ei\u00b7nem", "Schlag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Sie nennen mich ihren Meister", "tokens": ["Sie", "nen\u00b7nen", "mich", "ih\u00b7ren", "Meis\u00b7ter"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PPOSAT", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und gehn der Nase nach.", "tokens": ["Und", "gehn", "der", "Na\u00b7se", "nach", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Mit seltsamen Geb\u00e4rden", "tokens": ["Mit", "selt\u00b7sa\u00b7men", "Ge\u00b7b\u00e4r\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "Gibt man sich viele Pein,", "tokens": ["Gibt", "man", "sich", "vie\u00b7le", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PRF", "PIAT", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "Kein Mensch will etwas werden,", "tokens": ["Kein", "Mensch", "will", "et\u00b7was", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PIS", "VAINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ein jeder will schon was sein.", "tokens": ["Ein", "je\u00b7der", "will", "schon", "was", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VMFIN", "ADV", "PIS", "VAINF", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.17": {"line.1": {"text": "\u00bbwillst dich nicht gern vom Alten entfernen?", "tokens": ["\u00bb", "willst", "dich", "nicht", "gern", "vom", "Al\u00b7ten", "ent\u00b7fer\u00b7nen", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPER", "PTKNEG", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Hat denn das Neue so gar kein Gewicht?\u00ab", "tokens": ["Hat", "denn", "das", "Neu\u00b7e", "so", "gar", "kein", "Ge\u00b7wicht", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "ADV", "ADV", "PIAT", "NN", "$.", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Umlernen m\u00fc\u00dfte man immer, umlernen!", "tokens": ["Um\u00b7ler\u00b7nen", "m\u00fc\u00df\u00b7te", "man", "im\u00b7mer", ",", "um\u00b7ler\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVINF", "VMFIN", "PIS", "ADV", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und wenn man umlernt, da lebt man nicht.", "tokens": ["Und", "wenn", "man", "um\u00b7lernt", ",", "da", "lebt", "man", "nicht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIS", "VVPP", "$,", "ADV", "VVFIN", "PIS", "PTKNEG", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.18": {"line.1": {"text": "\u00bbsag uns Jungen doch auch was zuliebe.\u00ab", "tokens": ["\u00bb", "sag", "uns", "Jun\u00b7gen", "doch", "auch", "was", "zu\u00b7lie\u00b7be", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "PPER", "NN", "ADV", "ADV", "PWS", "VVFIN", "$.", "$("], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Nun, da\u00df ich euch Jungen gar herzlichen liebe!", "tokens": ["Nun", ",", "da\u00df", "ich", "euch", "Jun\u00b7gen", "gar", "herz\u00b7li\u00b7chen", "lie\u00b7be", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PPER", "PPER", "NN", "ADV", "VVINF", "VVFIN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.3": {"text": "Denn als ich war als Junge gesetzt,", "tokens": ["Denn", "als", "ich", "war", "als", "Jun\u00b7ge", "ge\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VAFIN", "KOKOM", "NN", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Hatt ich mich auch viel lieber als jetzt.", "tokens": ["Hatt", "ich", "mich", "auch", "viel", "lie\u00b7ber", "als", "jetzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PRF", "ADV", "ADV", "ADV", "KOKOM", "ADV", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}, "stanza.19": {"line.1": {"text": "Ich neide nichts, ich la\u00df es gehn", "tokens": ["Ich", "nei\u00b7de", "nichts", ",", "ich", "la\u00df", "es", "gehn"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIS", "$,", "PPER", "VVFIN", "PPER", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und kann mich immer manchem gleich erhalten;", "tokens": ["Und", "kann", "mich", "im\u00b7mer", "man\u00b7chem", "gleich", "er\u00b7hal\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADV", "PIAT", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Zahnreihen aber, junge, neidlos anzusehn,", "tokens": ["Zahn\u00b7rei\u00b7hen", "a\u00b7ber", ",", "jun\u00b7ge", ",", "neid\u00b7los", "an\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADV", "$,", "ADJA", "$,", "ADJD", "VVIZU", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Das ist die gr\u00f6\u00dfte Pr\u00fcfung mein, des Alten.", "tokens": ["Das", "ist", "die", "gr\u00f6\u00df\u00b7te", "Pr\u00fc\u00b7fung", "mein", ",", "des", "Al\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "PPOSAT", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.20": {"line.1": {"text": "K\u00fcnstler! dich selbst zu adeln,", "tokens": ["K\u00fcnst\u00b7ler", "!", "dich", "selbst", "zu", "a\u00b7deln", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "ADV", "PTKZU", "VVINF", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Mu\u00dft du bescheiden prahlen;", "tokens": ["Mu\u00dft", "du", "be\u00b7schei\u00b7den", "prah\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADJD", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "La\u00df dich heute loben, morgen tadeln", "tokens": ["La\u00df", "dich", "heu\u00b7te", "lo\u00b7ben", ",", "mor\u00b7gen", "ta\u00b7deln"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VVIMP", "PPER", "ADV", "VVINF", "$,", "ADV", "VVINF"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Und immer bezahlen.", "tokens": ["Und", "im\u00b7mer", "be\u00b7zah\u00b7len", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.21": {"line.1": {"text": "Als Knabe nahm ich mir's zur Lehre,", "tokens": ["Als", "Kna\u00b7be", "nahm", "ich", "mir's", "zur", "Leh\u00b7re", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "PPER", "NE", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Welt sei ein allerliebster Spa\u00df,", "tokens": ["Welt", "sei", "ein", "al\u00b7ler\u00b7liebs\u00b7ter", "Spa\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Als wenn es Vater und Mutter w\u00e4re;", "tokens": ["Als", "wenn", "es", "Va\u00b7ter", "und", "Mut\u00b7ter", "w\u00e4\u00b7re", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "NN", "KON", "NN", "VAFIN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Dann \u2013 etwas anders fand ich das.", "tokens": ["Dann", "\u2013", "et\u00b7was", "an\u00b7ders", "fand", "ich", "das", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PIS", "ADV", "VVFIN", "PPER", "PDS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Die klugen Leute gefallen mir nicht", "tokens": ["Die", "klu\u00b7gen", "Leu\u00b7te", "ge\u00b7fal\u00b7len", "mir", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVPP", "PPER", "PTKNEG"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "(ich tadle mich selbst auch wohl zuweilen):", "tokens": ["(", "ich", "tad\u00b7le", "mich", "selbst", "auch", "wohl", "zu\u00b7wei\u00b7len", ")", ":"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "ADV", "ADV", "ADV", "ADV", "$(", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Sie hei\u00dfen das Vorsicht,", "tokens": ["Sie", "hei\u00b7\u00dfen", "das", "Vor\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Wenn sie sich \u00fcbereilen.", "tokens": ["Wenn", "sie", "sich", "\u00fc\u00b7be\u00b7rei\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.23": {"line.1": {"text": "\u00bbanders lesen Knaben den Terenz,", "tokens": ["\u00bb", "an\u00b7ders", "le\u00b7sen", "Kna\u00b7ben", "den", "Te\u00b7renz", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVINF", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Anders Grotius.\u00ab", "tokens": ["An\u00b7ders", "Gro\u00b7ti\u00b7us", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct"], "pos": ["ADV", "NE", "$.", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Mich Knaben \u00e4rgerte die Sentenz,", "tokens": ["Mich", "Kna\u00b7ben", "\u00e4r\u00b7ger\u00b7te", "die", "Sen\u00b7tenz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die ich nun gelten lassen mu\u00df.", "tokens": ["Die", "ich", "nun", "gel\u00b7ten", "las\u00b7sen", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VVINF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "\u00bbso widerstrebe! Das wird dich adeln;", "tokens": ["\u00bb", "so", "wi\u00b7der\u00b7stre\u00b7be", "!", "Das", "wird", "dich", "a\u00b7deln", ";"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "$.", "PDS", "VAFIN", "PPER", "VVINF", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Willst vor der Feierstunde schon ruhn?\u00ab", "tokens": ["Willst", "vor", "der", "Fei\u00b7er\u00b7stun\u00b7de", "schon", "ruhn", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "APPR", "ART", "NN", "ADV", "VVINF", "$.", "$("], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Ich bin zu alt, um etwas zu tadeln,", "tokens": ["Ich", "bin", "zu", "alt", ",", "um", "et\u00b7was", "zu", "ta\u00b7deln", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKA", "ADJD", "$,", "KOUI", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Doch immer jung genug, etwas zu tun.", "tokens": ["Doch", "im\u00b7mer", "jung", "ge\u00b7nug", ",", "et\u00b7was", "zu", "tun", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "ADV", "$,", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.25": {"line.1": {"text": "\u00bbdu bist ein wunderlicher Mann,", "tokens": ["\u00bb", "du", "bist", "ein", "wun\u00b7der\u00b7li\u00b7cher", "Mann", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Warum verstummst du vor diesem Gesicht?\u00ab", "tokens": ["Wa\u00b7rum", "ver\u00b7stummst", "du", "vor", "die\u00b7sem", "Ge\u00b7sicht", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "APPR", "PDAT", "NN", "$.", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Was ich nicht loben kann,", "tokens": ["Was", "ich", "nicht", "lo\u00b7ben", "kann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Davon sprech ich nicht.", "tokens": ["Da\u00b7von", "sprech", "ich", "nicht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PTKNEG", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.26": {"line.1": {"text": "\u00bbbei mancherlei Gesch\u00e4ftigkeit", "tokens": ["\u00bb", "bei", "man\u00b7cher\u00b7lei", "Ge\u00b7sch\u00e4f\u00b7tig\u00b7keit"], "token_info": ["punct", "word", "word", "word"], "pos": ["$(", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hast dich ungeschickt benommen.\u00ab", "tokens": ["Hast", "dich", "un\u00b7ge\u00b7schickt", "be\u00b7nom\u00b7men", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVPP", "$.", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Ohne jene Verr\u00fccktheit", "tokens": ["Oh\u00b7ne", "je\u00b7ne", "Ver\u00b7r\u00fcckt\u00b7heit"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PDAT", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "W\u00e4r ich nicht so weit gekommen.", "tokens": ["W\u00e4r", "ich", "nicht", "so", "weit", "ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADV", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.27": {"line.1": {"text": "\u00bbla\u00df doch, was du halb vollbracht,", "tokens": ["\u00bb", "la\u00df", "doch", ",", "was", "du", "halb", "voll\u00b7bracht", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVIMP", "ADV", "$,", "PWS", "PPER", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Mich und andre kennen!\u00ab", "tokens": ["Mich", "und", "and\u00b7re", "ken\u00b7nen", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "KON", "PIS", "VVINF", "$.", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Weil es uns nur irremacht,", "tokens": ["Weil", "es", "uns", "nur", "ir\u00b7re\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wollen wir's verbrennen.", "tokens": ["Wol\u00b7len", "wir's", "ver\u00b7bren\u00b7nen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.28": {"line.1": {"text": "\u00bbwillst du uns denn nicht auch was g\u00f6nnen:", "tokens": ["\u00bb", "willst", "du", "uns", "denn", "nicht", "auch", "was", "g\u00f6n\u00b7nen", ":"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPER", "PPER", "ADV", "PTKNEG", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kannst ja, was mancher andre kann.\u00ab", "tokens": ["Kannst", "ja", ",", "was", "man\u00b7cher", "and\u00b7re", "kann", ".", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "ADV", "$,", "PRELS", "PIAT", "PIS", "VMFIN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn sie mich heute verbrauchen k\u00f6nnen,", "tokens": ["Wenn", "sie", "mich", "heu\u00b7te", "ver\u00b7brau\u00b7chen", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADV", "VVINF", "VMINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Dann bin ich ihnen ein rechter Mann.", "tokens": ["Dann", "bin", "ich", "ih\u00b7nen", "ein", "rech\u00b7ter", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.29": {"line.1": {"text": "Das alles ist nicht mein Bereich \u2013", "tokens": ["Das", "al\u00b7les", "ist", "nicht", "mein", "Be\u00b7reich", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VAFIN", "PTKNEG", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was soll ich mir viel Sorge machen?", "tokens": ["Was", "soll", "ich", "mir", "viel", "Sor\u00b7ge", "ma\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PPER", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+++-+-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Die Fische schwimmen glatt im Teich", "tokens": ["Die", "Fi\u00b7sche", "schwim\u00b7men", "glatt", "im", "Teich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADJD", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und k\u00fcmmern sich nicht um den Nachen.", "tokens": ["Und", "k\u00fcm\u00b7mern", "sich", "nicht", "um", "den", "Na\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PTKNEG", "APPR", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.30": {"line.1": {"text": "Mit der Welt mu\u00df niemand leben,", "tokens": ["Mit", "der", "Welt", "mu\u00df", "nie\u00b7mand", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VMFIN", "PIS", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Als wer sie brauchen will;", "tokens": ["Als", "wer", "sie", "brau\u00b7chen", "will", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWS", "PPER", "VVINF", "VMFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ist er brauchbar und still,", "tokens": ["Ist", "er", "brauch\u00b7bar", "und", "still", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Sollt er sich lieber dem Teufel ergeben", "tokens": ["Sollt", "er", "sich", "lie\u00b7ber", "dem", "Teu\u00b7fel", "er\u00b7ge\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "PRF", "ADV", "ART", "NN", "VVINF"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.5": {"text": "Als zu tun, was sie will.", "tokens": ["Als", "zu", "tun", ",", "was", "sie", "will", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PTKZU", "VVINF", "$,", "PRELS", "PPER", "VMFIN", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}}, "stanza.31": {"line.1": {"text": "\u00bbwas lehr ich dich vor allen Dingen?\u00ab", "tokens": ["\u00bb", "was", "lehr", "ich", "dich", "vor", "al\u00b7len", "Din\u00b7gen", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VVFIN", "PPER", "PRF", "APPR", "PIAT", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "M\u00f6chte \u00fcber meinen eignen Schatten springen!", "tokens": ["M\u00f6ch\u00b7te", "\u00fc\u00b7ber", "mei\u00b7nen", "eig\u00b7nen", "Schat\u00b7ten", "sprin\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.32": {"line.1": {"text": "Sie m\u00f6chten gerne frei sein,", "tokens": ["Sie", "m\u00f6ch\u00b7ten", "ger\u00b7ne", "frei", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Lange kann das einerlei sein;", "tokens": ["Lan\u00b7ge", "kann", "das", "ei\u00b7ner\u00b7lei", "sein", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PDS", "PIS", "VAINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo es aber drunter und dr\u00fcber geht,", "tokens": ["Wo", "es", "a\u00b7ber", "drun\u00b7ter", "und", "dr\u00fc\u00b7ber", "geht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PAV", "KON", "PAV", "VVFIN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Ein Heiliger wird angefleht,", "tokens": ["Ein", "Hei\u00b7li\u00b7ger", "wird", "an\u00b7ge\u00b7fleht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und wollen die alten uns nicht befreien,", "tokens": ["Und", "wol\u00b7len", "die", "al\u00b7ten", "uns", "nicht", "be\u00b7frei\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ART", "ADJA", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "So macht man sich behend einen neuen;", "tokens": ["So", "macht", "man", "sich", "be\u00b7hend", "ei\u00b7nen", "neu\u00b7en", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PRF", "ADJD", "ART", "ADJA", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Im Schiffbruch jammert jedermann,", "tokens": ["Im", "Schiff\u00b7bruch", "jam\u00b7mert", "je\u00b7der\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Da\u00df keiner mehr als der andre kann.", "tokens": ["Da\u00df", "kei\u00b7ner", "mehr", "als", "der", "and\u00b7re", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PIS", "KOKOM", "ART", "PIS", "VMFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.33": {"line.1": {"text": "Grenzlose Lebenspein,", "tokens": ["Grenz\u00b7lo\u00b7se", "Le\u00b7ben\u00b7spein", ","], "token_info": ["word", "word", "punct"], "pos": ["NE", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.2": {"text": "Fast, fast erdr\u00fcckt sie mich!", "tokens": ["Fast", ",", "fast", "er\u00b7dr\u00fcckt", "sie", "mich", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADV", "VVFIN", "PPER", "PRF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Das wollen alle Herren sein,", "tokens": ["Das", "wol\u00b7len", "al\u00b7le", "Her\u00b7ren", "sein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PIAT", "NN", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und keiner ist Herr von sich.", "tokens": ["Und", "kei\u00b7ner", "ist", "Herr", "von", "sich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "NN", "APPR", "PRF", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.34": {"line.1": {"text": "Und wenn man auch den Tyrannen ersticht,", "tokens": ["Und", "wenn", "man", "auch", "den", "Ty\u00b7ran\u00b7nen", "er\u00b7sticht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIS", "ADV", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ist immer noch viel zu verlieren.", "tokens": ["Ist", "im\u00b7mer", "noch", "viel", "zu", "ver\u00b7lie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Sie g\u00f6nnten C\u00e4sarn das Reich nicht", "tokens": ["Sie", "g\u00f6nn\u00b7ten", "C\u00e4\u00b7sarn", "das", "Reich", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "ART", "NN", "PTKNEG"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Und wu\u00dften's nicht zu regieren.", "tokens": ["Und", "wu\u00df\u00b7ten's", "nicht", "zu", "re\u00b7gie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "PTKZU", "VVINF", "$."], "meter": "-+-++-+-", "measure": "unknown.measure.tetra"}}, "stanza.35": {"line.1": {"text": "Warum mir aber in neuster Welt", "tokens": ["Wa\u00b7rum", "mir", "a\u00b7ber", "in", "neus\u00b7ter", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Anarchie gar so wohl gef\u00e4llt?", "tokens": ["An\u00b7ar\u00b7chie", "gar", "so", "wohl", "ge\u00b7f\u00e4llt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein jeder lebt nach seinem Sinn,", "tokens": ["Ein", "je\u00b7der", "lebt", "nach", "sei\u00b7nem", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das ist nun also auch mein Gewinn.", "tokens": ["Das", "ist", "nun", "al\u00b7so", "auch", "mein", "Ge\u00b7winn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "ADV", "PPOSAT", "NN", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.5": {"text": "Ich la\u00df einem jeden sein Bestreben,", "tokens": ["Ich", "la\u00df", "ei\u00b7nem", "je\u00b7den", "sein", "Be\u00b7stre\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "PIAT", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.6": {"text": "Um auch nach meinem Sinne zu leben.", "tokens": ["Um", "auch", "nach", "mei\u00b7nem", "Sin\u00b7ne", "zu", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.36": {"line.1": {"text": "Da kann man frank und fr\u00f6hlich leben,", "tokens": ["Da", "kann", "man", "frank", "und", "fr\u00f6h\u00b7lich", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "VVFIN", "KON", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Niemanden wird recht gegeben,", "tokens": ["Nie\u00b7man\u00b7den", "wird", "recht", "ge\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADJD", "VVPP", "$,"], "meter": "++--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Daf\u00fcr gibt man wieder niemand recht,", "tokens": ["Da\u00b7f\u00fcr", "gibt", "man", "wie\u00b7der", "nie\u00b7mand", "recht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIS", "ADV", "PIS", "ADJD", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Macht's eben gut, macht's eben schlecht;", "tokens": ["Macht's", "e\u00b7ben", "gut", ",", "macht's", "e\u00b7ben", "schlecht", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ADJD", "$,", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Im ganzen aber, wie man sieht,", "tokens": ["Im", "gan\u00b7zen", "a\u00b7ber", ",", "wie", "man", "sieht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "ADV", "$,", "PWAV", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Im Weltlauf immer doch etwas geschieht.", "tokens": ["Im", "Welt\u00b7lauf", "im\u00b7mer", "doch", "et\u00b7was", "ge\u00b7schieht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADV", "PIS", "VVFIN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Was Kluges, Dummes auch je geschah,", "tokens": ["Was", "Klu\u00b7ges", ",", "Dum\u00b7mes", "auch", "je", "ge\u00b7schah", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "$,", "NN", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Das nennt man Welthistoria;", "tokens": ["Das", "nennt", "man", "Welt\u00b7his\u00b7to\u00b7ria", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.9": {"text": "Und die Herrn Bredows k\u00fcnft'ger Zeiten", "tokens": ["Und", "die", "Herrn", "Bre\u00b7dows", "k\u00fcnft'\u00b7ger", "Zei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "NE", "ADJA", "NN"], "meter": "--++-+-+-", "measure": "anapaest.init"}, "line.10": {"text": "Werden daraus Tabellen bereiten,", "tokens": ["Wer\u00b7den", "da\u00b7raus", "Ta\u00b7bel\u00b7len", "be\u00b7rei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PAV", "NN", "VVINF", "$,"], "meter": "+-+-++--+-", "measure": "trochaic.penta.relaxed"}, "line.11": {"text": "Darin studiert die Jugend mit Flei\u00df,", "tokens": ["Da\u00b7rin", "stu\u00b7diert", "die", "Ju\u00b7gend", "mit", "Flei\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "APPR", "NN", "$,"], "meter": "-----+--+", "measure": "iambic.di.chol"}, "line.12": {"text": "Was sie nie zu begreifen wei\u00df.", "tokens": ["Was", "sie", "nie", "zu", "be\u00b7grei\u00b7fen", "wei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}}, "stanza.37": {"line.1": {"text": "Wie es in der Welt so geht \u2013", "tokens": ["Wie", "es", "in", "der", "Welt", "so", "geht", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "ART", "NN", "ADV", "VVFIN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wei\u00df man, was geschah?", "tokens": ["Wei\u00df", "man", ",", "was", "ge\u00b7schah", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "$,", "PWS", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Und was auf dem Papiere steht,", "tokens": ["Und", "was", "auf", "dem", "Pa\u00b7pie\u00b7re", "steht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das steht eben da.", "tokens": ["Das", "steht", "e\u00b7ben", "da", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ADV", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.38": {"line.1": {"text": "Das Weltregiment \u2013 \u00fcber Nacht", "tokens": ["Das", "Welt\u00b7re\u00b7gi\u00b7ment", "\u2013", "\u00fc\u00b7ber", "Nacht"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "$(", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Seine Formen hab ich durchgedacht:", "tokens": ["Sei\u00b7ne", "For\u00b7men", "hab", "ich", "durch\u00b7ge\u00b7dacht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Den hehren Despoten lieb ich im Krieg,", "tokens": ["Den", "heh\u00b7ren", "Des\u00b7po\u00b7ten", "lieb", "ich", "im", "Krieg", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Verst\u00e4ndigen Monarchen gleich hinter dem Sieg;", "tokens": ["Ver\u00b7st\u00e4n\u00b7di\u00b7gen", "Mon\u00b7ar\u00b7chen", "gleich", "hin\u00b7ter", "dem", "Sieg", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+---+--+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Dann w\u00fcnscht ich jedoch, da\u00df alle die Trauten", "tokens": ["Dann", "w\u00fcnscht", "ich", "je\u00b7doch", ",", "da\u00df", "al\u00b7le", "die", "Trau\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "KOUS", "PIS", "ART", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Sich nicht gleich neben und mit ihm erbauten.", "tokens": ["Sich", "nicht", "gleich", "ne\u00b7ben", "und", "mit", "ihm", "er\u00b7bau\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PTKNEG", "ADV", "APPR", "KON", "APPR", "PPER", "VVINF", "$."], "meter": "--+---+--+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Und wie ich das hoffe, so kommt mir die Menge,", "tokens": ["Und", "wie", "ich", "das", "hof\u00b7fe", ",", "so", "kommt", "mir", "die", "Men\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "PDS", "VVFIN", "$,", "ADV", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.8": {"text": "Nimmt h\u00fcben und dr\u00fcben mich derb ins Gedr\u00e4nge;", "tokens": ["Nimmt", "h\u00fc\u00b7ben", "und", "dr\u00fc\u00b7ben", "mich", "derb", "ins", "Ge\u00b7dr\u00e4n\u00b7ge", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "KON", "ADV", "PPER", "ADJD", "APPRART", "NN", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.9": {"text": "Von da verlier ich alle Spur. \u2013", "tokens": ["Von", "da", "ver\u00b7lier", "ich", "al\u00b7le", "Spur", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ADV", "VVFIN", "PPER", "PIAT", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Was will mir Gott f\u00fcr Lehre daraus g\u00f6nnen?", "tokens": ["Was", "will", "mir", "Gott", "f\u00fcr", "Leh\u00b7re", "da\u00b7raus", "g\u00f6n\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "NN", "APPR", "NN", "PAV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.11": {"text": "Da\u00df wir uns eben alle nur", "tokens": ["Da\u00df", "wir", "uns", "e\u00b7ben", "al\u00b7le", "nur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPER", "ADV", "PIS", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Auf kurze Zeit regieren k\u00f6nnen.", "tokens": ["Auf", "kur\u00b7ze", "Zeit", "re\u00b7gie\u00b7ren", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Ich tadl' euch nicht,", "tokens": ["Ich", "tadl'", "euch", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.2": {"text": "Ich lob euch nicht,", "tokens": ["Ich", "lob", "euch", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Aber ich spa\u00dfe;", "tokens": ["A\u00b7ber", "ich", "spa\u00b7\u00dfe", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Dem klugen Wicht", "tokens": ["Dem", "klu\u00b7gen", "Wicht"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "F\u00e4hrt's ins Gesicht", "tokens": ["F\u00e4hrt's", "ins", "Ge\u00b7sicht"], "token_info": ["word", "word", "word"], "pos": ["NE", "APPRART", "NN"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.6": {"text": "Und in die Nase.", "tokens": ["Und", "in", "die", "Na\u00b7se", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.40": {"line.1": {"text": "Und wenn er ganz gewaltig niest,", "tokens": ["Und", "wenn", "er", "ganz", "ge\u00b7wal\u00b7tig", "niest", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wer wei\u00df, was dann daher entsprie\u00dft", "tokens": ["Wer", "wei\u00df", ",", "was", "dann", "da\u00b7her", "ent\u00b7sprie\u00dft"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "$,", "PRELS", "ADV", "PAV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und was er alles mache:", "tokens": ["Und", "was", "er", "al\u00b7les", "ma\u00b7che", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "PIS", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Besinnung aber hinterdrein,", "tokens": ["Be\u00b7sin\u00b7nung", "a\u00b7ber", "hin\u00b7ter\u00b7drein", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Verstand, Vernunft, wo m\u00f6glich rein,", "tokens": ["Ver\u00b7stand", ",", "Ver\u00b7nunft", ",", "wo", "m\u00f6g\u00b7lich", "rein", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "PWAV", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Das ist die rechte Sache.", "tokens": ["Das", "ist", "die", "rech\u00b7te", "Sa\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.41": {"line.1": {"text": "Soll man euch immer und immer beplappern?", "tokens": ["Soll", "man", "euch", "im\u00b7mer", "und", "im\u00b7mer", "be\u00b7plap\u00b7pern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "PPER", "ADV", "KON", "ADV", "VVINF", "$."], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "Gewinnt ihr nie einen freien Blick?", "tokens": ["Ge\u00b7winnt", "ihr", "nie", "ei\u00b7nen", "frei\u00b7en", "Blick", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Sie frieren, da\u00df ihnen die Z\u00e4hne klappern,", "tokens": ["Sie", "frie\u00b7ren", ",", "da\u00df", "ih\u00b7nen", "die", "Z\u00e4h\u00b7ne", "klap\u00b7pern", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "$,", "KOUS", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Das hei\u00dfen sie nachher Kritik.", "tokens": ["Das", "hei\u00b7\u00dfen", "sie", "nach\u00b7her", "Kri\u00b7tik", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "\u00bbdu sagst gar wunderliche Dinge!\u00ab", "tokens": ["\u00bb", "du", "sagst", "gar", "wun\u00b7der\u00b7li\u00b7che", "Din\u00b7ge", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ADV", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Beschaut sie nur, sie sind geringe;", "tokens": ["Be\u00b7schaut", "sie", "nur", ",", "sie", "sind", "ge\u00b7rin\u00b7ge", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "PPER", "VAFIN", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wird Vers und Reim denn angeklagt,", "tokens": ["Wird", "Vers", "und", "Reim", "denn", "an\u00b7ge\u00b7klagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "KON", "NN", "KON", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn Leben und Prosa das Tollste sagt?", "tokens": ["Wenn", "Le\u00b7ben", "und", "Pro\u00b7sa", "das", "Tolls\u00b7te", "sagt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.43": {"line.1": {"text": "\u00bbdu gehst so freien Angesichts,", "tokens": ["\u00bb", "du", "gehst", "so", "frei\u00b7en", "An\u00b7ge\u00b7sichts", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit muntern, offnen Augen!\u00ab", "tokens": ["Mit", "mun\u00b7tern", ",", "off\u00b7nen", "Au\u00b7gen", "!", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["APPR", "VVINF", "$,", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ihr tauget eben alle nichts,", "tokens": ["Ihr", "tau\u00b7get", "e\u00b7ben", "al\u00b7le", "nichts", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIAT", "PIS", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Warum sollt ich was taugen?", "tokens": ["Wa\u00b7rum", "sollt", "ich", "was", "tau\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "PIS", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.44": {"line.1": {"text": "\u00bbwarum bist du so hochm\u00fctig?", "tokens": ["\u00bb", "wa\u00b7rum", "bist", "du", "so", "hoch\u00b7m\u00fc\u00b7tig", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "+-+-+++-", "measure": "unknown.measure.penta"}, "line.2": {"text": "Hast sonst nicht so die Leute gescholten!\u00ab", "tokens": ["Hast", "sonst", "nicht", "so", "die", "Leu\u00b7te", "ge\u00b7schol\u00b7ten", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ADV", "PTKNEG", "ADV", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "W\u00e4re sehr gerne dem\u00fctig,", "tokens": ["W\u00e4\u00b7re", "sehr", "ger\u00b7ne", "de\u00b7m\u00fc\u00b7tig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.4": {"text": "Wenn sie mich nur so lassen wollten.", "tokens": ["Wenn", "sie", "mich", "nur", "so", "las\u00b7sen", "woll\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADV", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Wenn ich dumm bin, lassen sie mich gelten;", "tokens": ["Wenn", "ich", "dumm", "bin", ",", "las\u00b7sen", "sie", "mich", "gel\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$,", "VVFIN", "PPER", "PRF", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Wenn ich recht hab, wollen sie mich schelten.", "tokens": ["Wenn", "ich", "recht", "hab", ",", "wol\u00b7len", "sie", "mich", "schel\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$,", "VMFIN", "PPER", "PPER", "VVFIN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.46": {"line.1": {"text": "\u00dcberzeugung soll mir niemand rauben,", "tokens": ["\u00dc\u00b7berz\u00b7eu\u00b7gung", "soll", "mir", "nie\u00b7mand", "rau\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "PIS", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Wer's besser wei\u00df, der mag es glauben.", "tokens": ["Wer's", "bes\u00b7ser", "wei\u00df", ",", "der", "mag", "es", "glau\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVFIN", "$,", "PRELS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Dem ist es schlecht in seiner Haut,", "tokens": ["Dem", "ist", "es", "schlecht", "in", "sei\u00b7ner", "Haut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADJD", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der in seinen eignen Busen schaut.", "tokens": ["Der", "in", "sei\u00b7nen", "eig\u00b7nen", "Bu\u00b7sen", "schaut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPOSAT", "ADJA", "NN", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.48": {"line.1": {"text": "\u00bbwohin wir bei unsern Gebresten", "tokens": ["\u00bb", "wo\u00b7hin", "wir", "bei", "un\u00b7sern", "Ge\u00b7bres\u00b7ten"], "token_info": ["punct", "word", "word", "word", "word", "word"], "pos": ["$(", "PWAV", "PPER", "APPR", "PPOSAT", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Uns im Augenblick richten sollen?\u00ab", "tokens": ["Uns", "im", "Au\u00b7gen\u00b7blick", "rich\u00b7ten", "sol\u00b7len", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "APPRART", "NN", "VVINF", "VMFIN", "$.", "$("], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Denke nur immer an die Besten,", "tokens": ["Den\u00b7ke", "nur", "im\u00b7mer", "an", "die", "Bes\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sie m\u00f6gen stecken, wo sie wollen.", "tokens": ["Sie", "m\u00f6\u00b7gen", "ste\u00b7cken", ",", "wo", "sie", "wol\u00b7len", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "$,", "PWAV", "PPER", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Den Reichtum mu\u00df der Neid beteuern:", "tokens": ["Den", "Reich\u00b7tum", "mu\u00df", "der", "Neid", "be\u00b7teu\u00b7ern", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Denn er kreucht nie in leere Scheuern.", "tokens": ["Denn", "er", "kreucht", "nie", "in", "lee\u00b7re", "Scheu\u00b7ern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Soll der Neider zerplatzen,", "tokens": ["Soll", "der", "Nei\u00b7der", "zer\u00b7plat\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "VVINF", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "Begib dich deiner Fratzen.", "tokens": ["Be\u00b7gib", "dich", "dei\u00b7ner", "Frat\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.51": {"line.1": {"text": "Soll es reichlich zu dir flie\u00dfen,", "tokens": ["Soll", "es", "reich\u00b7lich", "zu", "dir", "flie\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADJD", "APPR", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Reichlich andre la\u00df genie\u00dfen.", "tokens": ["Reich\u00b7lich", "and\u00b7re", "la\u00df", "ge\u00b7nie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.52": {"line.1": {"text": "\u00bbist dein Geschenk wohl angekommen?\u00ab", "tokens": ["\u00bb", "ist", "dein", "Ge\u00b7schenk", "wohl", "an\u00b7ge\u00b7kom\u00b7men", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VAFIN", "PPOSAT", "NN", "ADV", "VVPP", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie haben es eben nicht \u00fcbelgenommen.", "tokens": ["Sie", "ha\u00b7ben", "es", "e\u00b7ben", "nicht", "\u00fc\u00b7bel\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}}, "stanza.53": {"line.1": {"text": "Der Teufel! sie ist nicht gering,", "tokens": ["Der", "Teu\u00b7fel", "!", "sie", "ist", "nicht", "ge\u00b7ring", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PPER", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Wie ich von weitem sp\u00fcre;", "tokens": ["Wie", "ich", "von", "wei\u00b7tem", "sp\u00fc\u00b7re", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "PIS", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Nun schelten sie das arme Ding,", "tokens": ["Nun", "schel\u00b7ten", "sie", "das", "ar\u00b7me", "Ding", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df sie euch so verf\u00fchre.", "tokens": ["Da\u00df", "sie", "euch", "so", "ver\u00b7f\u00fch\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Erinnert euch, verfluchtes Pack,", "tokens": ["E\u00b7rin\u00b7nert", "euch", ",", "ver\u00b7fluch\u00b7tes", "Pack", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Des paradiesischen Falles!", "tokens": ["Des", "pa\u00b7ra\u00b7die\u00b7si\u00b7schen", "Fal\u00b7les", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Hat euch die Sch\u00f6ne nur im Sack,", "tokens": ["Hat", "euch", "die", "Sch\u00f6\u00b7ne", "nur", "im", "Sack", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "So gilt sie euch f\u00fcr alles.", "tokens": ["So", "gilt", "sie", "euch", "f\u00fcr", "al\u00b7les", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "APPR", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.54": {"line.1": {"text": "Wenn dir's bei uns nun nicht gef\u00e4llt,", "tokens": ["Wenn", "dir's", "bei", "uns", "nun", "nicht", "ge\u00b7f\u00e4llt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "PPER", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So geh in deine \u00f6stliche Welt.", "tokens": ["So", "geh", "in", "dei\u00b7ne", "\u00f6st\u00b7li\u00b7che", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.55": {"line.1": {"text": "Ich w\u00fcnsche mir eine h\u00fcbsche Frau,", "tokens": ["Ich", "w\u00fcn\u00b7sche", "mir", "ei\u00b7ne", "h\u00fcb\u00b7sche", "Frau", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die nicht alles n\u00e4hme gar zu genau,", "tokens": ["Die", "nicht", "al\u00b7les", "n\u00e4h\u00b7me", "gar", "zu", "ge\u00b7nau", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "PIS", "VVFIN", "ADV", "PTKA", "ADJD", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.3": {"text": "Doch aber zugleich am besten verst\u00e4nde,", "tokens": ["Doch", "a\u00b7ber", "zu\u00b7gleich", "am", "bes\u00b7ten", "ver\u00b7st\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "PTKA", "ADJD", "ADJA", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wie ich mich selbst am besten bef\u00e4nde.", "tokens": ["Wie", "ich", "mich", "selbst", "am", "bes\u00b7ten", "be\u00b7f\u00e4n\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PRF", "ADV", "PTKA", "ADJD", "VVFIN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.56": {"line.1": {"text": "W\u00e4re Gott und ", "tokens": ["W\u00e4\u00b7re", "Gott", "und"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "NN", "KON"], "meter": "+-+-", "measure": "trochaic.di"}, "line.2": {"text": "So w\u00e4re mein Lied nicht kleine.", "tokens": ["So", "w\u00e4\u00b7re", "mein", "Lied", "nicht", "klei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "PTKNEG", "ADJA", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.57": {"line.1": {"text": "Gott hab ich und die Kleine", "tokens": ["Gott", "hab", "ich", "und", "die", "Klei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "KON", "ART", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Im Lied erhalten reine.", "tokens": ["Im", "Lied", "er\u00b7hal\u00b7ten", "rei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.58": {"line.1": {"text": "So la\u00dft mir das Ged\u00e4chtnis", "tokens": ["So", "la\u00dft", "mir", "das", "Ge\u00b7d\u00e4cht\u00b7nis"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVIMP", "PPER", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Als fr\u00f6hliches Verm\u00e4chtnis.", "tokens": ["Als", "fr\u00f6h\u00b7li\u00b7ches", "Ver\u00b7m\u00e4cht\u00b7nis", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.59": {"line.1": {"text": "\u00bbsie betrog dich geraume Zeit,", "tokens": ["\u00bb", "sie", "be\u00b7trog", "dich", "ge\u00b7rau\u00b7me", "Zeit", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "ADJA", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Nun siehst du wohl, sie war ein Schein.\u00ab", "tokens": ["Nun", "siehst", "du", "wohl", ",", "sie", "war", "ein", "Schein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "PPER", "VAFIN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was wei\u00dft du denn von Wirklichkeit;", "tokens": ["Was", "wei\u00dft", "du", "denn", "von", "Wirk\u00b7lich\u00b7keit", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "War sie drum weniger mein?", "tokens": ["War", "sie", "drum", "we\u00b7ni\u00b7ger", "mein", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PAV", "ADV", "PPOSAT", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.60": {"line.1": {"text": "\u00bbbetrogen bist du zum Erbarmen,", "tokens": ["\u00bb", "be\u00b7tro\u00b7gen", "bist", "du", "zum", "Er\u00b7bar\u00b7men", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVPP", "VAFIN", "PPER", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nun l\u00e4\u00dft sie dich allein!\u00ab", "tokens": ["Nun", "l\u00e4\u00dft", "sie", "dich", "al\u00b7lein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und war es nur ein Schein:", "tokens": ["Und", "war", "es", "nur", "ein", "Schein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Sie lag in meinen Armen,", "tokens": ["Sie", "lag", "in", "mei\u00b7nen", "Ar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "War sie drum weniger mein?", "tokens": ["War", "sie", "drum", "we\u00b7ni\u00b7ger", "mein", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PAV", "ADV", "PPOSAT", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.61": {"line.1": {"text": "Gern h\u00f6ren wir allerlei gute Lehr,", "tokens": ["Gern", "h\u00f6\u00b7ren", "wir", "al\u00b7ler\u00b7lei", "gu\u00b7te", "Lehr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "ADJA", "NN", "$,"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Doch Schm\u00e4hen und Schimpfen noch viel mehr.", "tokens": ["Doch", "Schm\u00e4\u00b7hen", "und", "Schimp\u00b7fen", "noch", "viel", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "ADV", "ADV", "ADV", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.62": {"line.1": {"text": "Glaube dich nicht allzu gut gebettet;", "tokens": ["Glau\u00b7be", "dich", "nicht", "all\u00b7zu", "gut", "ge\u00b7bet\u00b7tet", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKNEG", "PTKA", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Ein gewarnter Mann ist halb gerettet.", "tokens": ["Ein", "ge\u00b7warn\u00b7ter", "Mann", "ist", "halb", "ge\u00b7ret\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADJD", "VVPP", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.63": {"line.1": {"text": "Wein macht munter geistreichen Mann,", "tokens": ["Wein", "macht", "mun\u00b7ter", "geist\u00b7rei\u00b7chen", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADJD", "ADJA", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Weihrauch ohne Feuer man nicht riechen kann.", "tokens": ["Weih\u00b7rauch", "oh\u00b7ne", "Feu\u00b7er", "man", "nicht", "rie\u00b7chen", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "PIS", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.64": {"line.1": {"text": "Willst du Weihrauchs Geruch erregen,", "tokens": ["Willst", "du", "Weih\u00b7rauchs", "Ge\u00b7ruch", "er\u00b7re\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "NN", "NN", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Feurige Kohlen mu\u00dft unterlegen.", "tokens": ["Feu\u00b7ri\u00b7ge", "Koh\u00b7len", "mu\u00dft", "un\u00b7ter\u00b7le\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VMFIN", "VVINF", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.65": {"line.1": {"text": "Wem ich ein besser Schicksal g\u00f6nnte?", "tokens": ["Wem", "ich", "ein", "bes\u00b7ser", "Schick\u00b7sal", "g\u00f6nn\u00b7te", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es sind die erk\u00fcnstelten Talente:", "tokens": ["Es", "sind", "die", "er\u00b7k\u00fcns\u00b7tel\u00b7ten", "Ta\u00b7len\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+--", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "An diesem, an jenem, am Besten gebricht's,", "tokens": ["An", "die\u00b7sem", ",", "an", "je\u00b7nem", ",", "am", "Bes\u00b7ten", "ge\u00b7bricht's", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "$,", "APPR", "PDAT", "$,", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Sie m\u00fchen und zw\u00e4ngen und kommen zu nichts.", "tokens": ["Sie", "m\u00fc\u00b7hen", "und", "zw\u00e4n\u00b7gen", "und", "kom\u00b7men", "zu", "nichts", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "KON", "VVINF", "KON", "VVFIN", "APPR", "PIS", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}, "stanza.66": {"line.1": {"text": "\u00bbsage deutlicher, wie und wenn;", "tokens": ["\u00bb", "sa\u00b7ge", "deut\u00b7li\u00b7cher", ",", "wie", "und", "wenn", ";"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "ADJD", "$,", "PWAV", "KON", "KOUS", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Du bist uns nicht immer klar.\u00ab", "tokens": ["Du", "bist", "uns", "nicht", "im\u00b7mer", "klar", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PTKNEG", "ADV", "ADJD", "$.", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Gute Leute, wi\u00dft ihr denn,", "tokens": ["Gu\u00b7te", "Leu\u00b7te", ",", "wi\u00dft", "ihr", "denn", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ob ich mir's selber war?", "tokens": ["Ob", "ich", "mir's", "sel\u00b7ber", "war", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NE", "ADV", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.67": {"line.1": {"text": "\u00bbwir qu\u00e4len uns immerfort", "tokens": ["\u00bb", "wir", "qu\u00e4\u00b7len", "uns", "im\u00b7mer\u00b7fort"], "token_info": ["punct", "word", "word", "word", "word"], "pos": ["$(", "PPER", "VVFIN", "PPER", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "In des Irrtums Banden.\u00ab", "tokens": ["In", "des", "Irr\u00b7tums", "Ban\u00b7den", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$.", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Wie manches verst\u00e4ndliche Wort", "tokens": ["Wie", "man\u00b7ches", "ver\u00b7st\u00e4nd\u00b7li\u00b7che", "Wort"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PIS", "ADJA", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Habt ihr mi\u00dfverstanden.", "tokens": ["Habt", "ihr", "mi\u00df\u00b7ver\u00b7stan\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.68": {"line.1": {"text": "Einem unverst\u00e4ndigen Wort", "tokens": ["Ei\u00b7nem", "un\u00b7ver\u00b7st\u00e4n\u00b7di\u00b7gen", "Wort"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Habt ihr Sinn geliehen;", "tokens": ["Habt", "ihr", "Sinn", "ge\u00b7lie\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Und so geht's immer fort;", "tokens": ["Und", "so", "geht's", "im\u00b7mer", "fort", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Verzeiht, euch wird verziehen.", "tokens": ["Ver\u00b7zeiht", ",", "euch", "wird", "ver\u00b7zie\u00b7hen", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.69": {"line.1": {"text": "Nehmt nur mein Leben hin, in Bausch", "tokens": ["Nehmt", "nur", "mein", "Le\u00b7ben", "hin", ",", "in", "Bausch"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "ADV", "PPOSAT", "NN", "PTKVZ", "$,", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und Bogen, wie ich's f\u00fchre;", "tokens": ["Und", "Bo\u00b7gen", ",", "wie", "ich's", "f\u00fch\u00b7re", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "PWAV", "PIS", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Andre verschlafen ihren Rausch,", "tokens": ["And\u00b7re", "ver\u00b7schla\u00b7fen", "ih\u00b7ren", "Rausch", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Meiner steht auf dem Papiere.", "tokens": ["Mei\u00b7ner", "steht", "auf", "dem", "Pa\u00b7pie\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.70": {"line.1": {"text": "Besser betteln als borgen!", "tokens": ["Bes\u00b7ser", "bet\u00b7teln", "als", "bor\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVINF", "KOKOM", "VVINF", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "Warum sollen zwei denn sorgen?", "tokens": ["Wa\u00b7rum", "sol\u00b7len", "zwei", "denn", "sor\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "CARD", "KON", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenn einer sorgt und redlich denkt,", "tokens": ["Wenn", "ei\u00b7ner", "sorgt", "und", "red\u00b7lich", "denkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "KON", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kommt andrer wohl und heiter und schenkt.", "tokens": ["Kommt", "an\u00b7drer", "wohl", "und", "hei\u00b7ter", "und", "schenkt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "KON", "ADJD", "KON", "VVFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Das sind die besten Intressen,", "tokens": ["Das", "sind", "die", "bes\u00b7ten", "I\u00b7ntres\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Die Schuldner und Gl\u00e4ubiger vergessen.", "tokens": ["Die", "Schuld\u00b7ner", "und", "Gl\u00e4u\u00b7bi\u00b7ger", "ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.71": {"line.1": {"text": "\u00bbich bin ein armer Mann,", "tokens": ["\u00bb", "ich", "bin", "ein", "ar\u00b7mer", "Mann", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Sch\u00e4tze mich aber nicht gering:", "tokens": ["Sch\u00e4t\u00b7ze", "mich", "a\u00b7ber", "nicht", "ge\u00b7ring", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PTKNEG", "ADJD", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Die Armut ist ein ehrlich Ding,", "tokens": ["Die", "Ar\u00b7mut", "ist", "ein", "ehr\u00b7lich", "Ding", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wer mit umgehn kann.\u00ab", "tokens": ["Wer", "mit", "um\u00b7gehn", "kann", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "APPR", "VVINF", "VMFIN", "$.", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.72": {"line.1": {"text": "Erlauchte Bettler hab ich gekannt,", "tokens": ["Er\u00b7lauch\u00b7te", "Bett\u00b7ler", "hab", "ich", "ge\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "K\u00fcnstler und Philosophen genannt;", "tokens": ["K\u00fcnst\u00b7ler", "und", "Phi\u00b7lo\u00b7so\u00b7phen", "ge\u00b7nannt", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVPP", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Doch w\u00fc\u00dft ich niemand, ungeprahlt,", "tokens": ["Doch", "w\u00fc\u00dft", "ich", "nie\u00b7mand", ",", "un\u00b7ge\u00b7prahlt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PIS", "$,", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der seine Zeche besser bezahlt.", "tokens": ["Der", "sei\u00b7ne", "Ze\u00b7che", "bes\u00b7ser", "be\u00b7zahlt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.73": {"line.1": {"text": "\u00bbwas hat dich nur von uns entfernt?\u00ab", "tokens": ["\u00bb", "was", "hat", "dich", "nur", "von", "uns", "ent\u00b7fernt", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VAFIN", "PPER", "ADV", "APPR", "PPER", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab immer den Plutarch gelesen.", "tokens": ["Hab", "im\u00b7mer", "den", "Plu\u00b7tarch", "ge\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "\u00bbwas hast du denn dabei gelernt?\u00ab", "tokens": ["\u00bb", "was", "hast", "du", "denn", "da\u00b7bei", "ge\u00b7lernt", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "VAFIN", "PPER", "ADV", "PAV", "VVPP", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sind eben alles Menschen gewesen.", "tokens": ["Sind", "e\u00b7ben", "al\u00b7les", "Men\u00b7schen", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIAT", "NN", "VAPP", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.74": {"line.1": {"text": "Cato wollte wohl andre strafen;", "tokens": ["Ca\u00b7to", "woll\u00b7te", "wohl", "and\u00b7re", "stra\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "ADV", "PIS", "VVFIN", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Selbander mocht er gerne schlafen.", "tokens": ["Sel\u00b7ban\u00b7der", "mocht", "er", "ger\u00b7ne", "schla\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.75": {"line.1": {"text": "Deshalb er sich zur Unzeit", "tokens": ["Des\u00b7halb", "er", "sich", "zur", "Un\u00b7zeit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "PPER", "PRF", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Mit Schwiegertochter und Sohn entzweit,", "tokens": ["Mit", "Schwie\u00b7ger\u00b7toch\u00b7ter", "und", "Sohn", "ent\u00b7zweit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "PTKVZ", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Auch eine junge Frau genommen,", "tokens": ["Auch", "ei\u00b7ne", "jun\u00b7ge", "Frau", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Welches ihm gar nicht wohl bekommen;", "tokens": ["Wel\u00b7ches", "ihm", "gar", "nicht", "wohl", "be\u00b7kom\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "PTKNEG", "ADV", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Wie Kaiser Friedrich der Letzte", "tokens": ["Wie", "Kai\u00b7ser", "Fried\u00b7rich", "der", "Letz\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "NE", "ART", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "V\u00e4terlich auseinandersetzte.", "tokens": ["V\u00e4\u00b7ter\u00b7lich", "aus\u00b7ein\u00b7an\u00b7der\u00b7setz\u00b7te", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.76": {"line.1": {"text": "\u00bbwas willst du, redend zur Menge,", "tokens": ["\u00bb", "was", "willst", "du", ",", "re\u00b7dend", "zur", "Men\u00b7ge", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VMFIN", "PPER", "$,", "ADJD", "APPRART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Dich selbst f\u00fcrtrefflich preisen?\u00ab", "tokens": ["Dich", "selbst", "f\u00fcr\u00b7treff\u00b7lich", "prei\u00b7sen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "ADV", "ADJD", "VVFIN", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Cato selbst war ruhmredig, der Strenge,", "tokens": ["Ca\u00b7to", "selbst", "war", "ruhm\u00b7re\u00b7dig", ",", "der", "Stren\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "ADV", "VAFIN", "ADJD", "$,", "ART", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Plutarch will's ihm gar ernst verweisen.", "tokens": ["Plu\u00b7tarch", "will's", "ihm", "gar", "ernst", "ver\u00b7wei\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VMFIN", "PPER", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.77": {"line.1": {"text": "Man k\u00f6nnt erzogene Kinder geb\u00e4ren,", "tokens": ["Man", "k\u00f6nnt", "er\u00b7zo\u00b7ge\u00b7ne", "Kin\u00b7der", "ge\u00b7b\u00e4\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn die Eltern erzogen w\u00e4ren.", "tokens": ["Wenn", "die", "El\u00b7tern", "er\u00b7zo\u00b7gen", "w\u00e4\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVPP", "VAFIN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}}, "stanza.78": {"line.1": {"text": "Was ich in meinem Haus ertrag,", "tokens": ["Was", "ich", "in", "mei\u00b7nem", "Haus", "er\u00b7trag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das sieht ein Fremder am ersten Tag;", "tokens": ["Das", "sieht", "ein", "Frem\u00b7der", "am", "ers\u00b7ten", "Tag", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Doch \u00e4ndert er sich's nicht zuliebe,", "tokens": ["Doch", "\u00e4n\u00b7dert", "er", "sich's", "nicht", "zu\u00b7lie\u00b7be", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PRF", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wenn er hundert Jahre bliebe.", "tokens": ["Und", "wenn", "er", "hun\u00b7dert", "Jah\u00b7re", "blie\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.79": {"line.1": {"text": "Wie auch die Welt sich stellen mag,", "tokens": ["Wie", "auch", "die", "Welt", "sich", "stel\u00b7len", "mag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ART", "NN", "PRF", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Tag immer bel\u00fcgt den Tag.", "tokens": ["Der", "Tag", "im\u00b7mer", "be\u00b7l\u00fcgt", "den", "Tag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.80": {"line.1": {"text": "Dagegen man auch nicht gerne h\u00f6rt,", "tokens": ["Da\u00b7ge\u00b7gen", "man", "auch", "nicht", "ger\u00b7ne", "h\u00f6rt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIS", "ADV", "PTKNEG", "ADV", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn der Tag den Tag zerst\u00f6rt.", "tokens": ["Wenn", "der", "Tag", "den", "Tag", "zer\u00b7st\u00f6rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.81": {"line.1": {"text": "Ich bin euch s\u00e4mtlichen zur Last,", "tokens": ["Ich", "bin", "euch", "s\u00e4mt\u00b7li\u00b7chen", "zur", "Last", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Einigen auch sogar verha\u00dft;", "tokens": ["Ei\u00b7ni\u00b7gen", "auch", "so\u00b7gar", "ver\u00b7ha\u00dft", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das hat aber gar nichts zu sagen:", "tokens": ["Das", "hat", "a\u00b7ber", "gar", "nichts", "zu", "sa\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.4": {"text": "Denn mir behagt's in alten Tagen,", "tokens": ["Denn", "mir", "be\u00b7hagt's", "in", "al\u00b7ten", "Ta\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So wie es mir in jungen behagte,", "tokens": ["So", "wie", "es", "mir", "in", "jun\u00b7gen", "be\u00b7hag\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "PPER", "PRF", "APPR", "ADJA", "VVFIN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Da\u00df ich nach alt und jung nicht fragte.", "tokens": ["Da\u00df", "ich", "nach", "alt", "und", "jung", "nicht", "frag\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADJD", "KON", "ADJD", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.82": {"line.1": {"text": "Mit sich selbst zu Rate gehn,", "tokens": ["Mit", "sich", "selbst", "zu", "Ra\u00b7te", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRF", "ADV", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Immer wird's am besten stehn:", "tokens": ["Im\u00b7mer", "wird's", "am", "bes\u00b7ten", "stehn", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKA", "ADJD", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Gern im Freien, gern zu Haus,", "tokens": ["Gern", "im", "Frei\u00b7en", ",", "gern", "zu", "Haus", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$,", "ADV", "APPR", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Lausche da und dort hinaus,", "tokens": ["Lau\u00b7sche", "da", "und", "dort", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "KON", "ADV", "APZR", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Und kontrolliere dich f\u00fcr und f\u00fcr,", "tokens": ["Und", "kont\u00b7rol\u00b7lie\u00b7re", "dich", "f\u00fcr", "und", "f\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PRF", "APPR", "KON", "APPR", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Da horchen alt und jung nach dir.", "tokens": ["Da", "hor\u00b7chen", "alt", "und", "jung", "nach", "dir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJD", "KON", "ADJD", "APPR", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.83": {"line.1": {"text": "Die Xenien, sie wandeln zahm,", "tokens": ["Die", "Xe\u00b7ni\u00b7en", ",", "sie", "wan\u00b7deln", "zahm", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Dichter h\u00e4lt sich nicht f\u00fcr lahm;", "tokens": ["Der", "Dich\u00b7ter", "h\u00e4lt", "sich", "nicht", "f\u00fcr", "lahm", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "PTKNEG", "APPR", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Belieben euch aber gesch\u00e4rftere Sachen,", "tokens": ["Be\u00b7lie\u00b7ben", "euch", "a\u00b7ber", "ge\u00b7sch\u00e4rf\u00b7te\u00b7re", "Sa\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADJA", "NN", "$,"], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.4": {"text": "So wartet, bis die wilden erwachen.", "tokens": ["So", "war\u00b7tet", ",", "bis", "die", "wil\u00b7den", "er\u00b7wa\u00b7chen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.84": {"line.1": {"text": "Sibyllinisch mit meinem Gesicht", "tokens": ["Si\u00b7byl\u00b7li\u00b7nisch", "mit", "mei\u00b7nem", "Ge\u00b7sicht"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Soll ich im Alter prahlen!", "tokens": ["Soll", "ich", "im", "Al\u00b7ter", "prah\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Je mehr es ihm an F\u00fclle gebricht,", "tokens": ["Je", "mehr", "es", "ihm", "an", "F\u00fcl\u00b7le", "ge\u00b7bricht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "PPER", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Desto \u00f6fter wollen sie's malen!", "tokens": ["Des\u00b7to", "\u00f6f\u00b7ter", "wol\u00b7len", "sie's", "ma\u00b7len", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VMFIN", "PIS", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}}, "stanza.85": {"line.1": {"text": "\u00bbist's in der N\u00e4h? Kam's aus der Ferne?", "tokens": ["\u00bb", "ist's", "in", "der", "N\u00e4h", "?", "Kam's", "aus", "der", "Fer\u00b7ne", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "APPR", "ART", "NN", "$.", "NE", "APPR", "ART", "NN", "$."], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}, "line.2": {"text": "Was beugt dich Heute so schwer?\u00ab", "tokens": ["Was", "beugt", "dich", "Heu\u00b7te", "so", "schwer", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "ADV", "ADJD", "$.", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Ich spa\u00dfte wohl am Abend gerne,", "tokens": ["Ich", "spa\u00df\u00b7te", "wohl", "am", "A\u00b7bend", "ger\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn nur der Tag nicht so ernsthaft w\u00e4r.", "tokens": ["Wenn", "nur", "der", "Tag", "nicht", "so", "ernst\u00b7haft", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "PTKNEG", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.86": {"line.1": {"text": "Spricht man mit jedermann,", "tokens": ["Spricht", "man", "mit", "je\u00b7der\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "PIS", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.2": {"text": "Da h\u00f6rt man keinen;", "tokens": ["Da", "h\u00f6rt", "man", "kei\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PIAT", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "Stets wird ein andrer Mann", "tokens": ["Stets", "wird", "ein", "an\u00b7drer", "Mann"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Auch anders meinen;", "tokens": ["Auch", "an\u00b7ders", "mei\u00b7nen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Was w\u00e4re Rat sodann,", "tokens": ["Was", "w\u00e4\u00b7re", "Rat", "so\u00b7dann", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "NN", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Sie zu verstehen?", "tokens": ["Sie", "zu", "ver\u00b7ste\u00b7hen", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.7": {"text": "Kennst du nicht Mann f\u00fcr Mann,", "tokens": ["Kennst", "du", "nicht", "Mann", "f\u00fcr", "Mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "NN", "APPR", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.8": {"text": "Es wird nicht gehen.", "tokens": ["Es", "wird", "nicht", "ge\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.87": {"line.1": {"text": "Gott hat die Gradheit selbst ans Herz genommen,", "tokens": ["Gott", "hat", "die", "Grad\u00b7heit", "selbst", "ans", "Herz", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "ADV", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Auf gradem Weg ist niemand umgekommen.", "tokens": ["Auf", "gra\u00b7dem", "Weg", "ist", "nie\u00b7mand", "um\u00b7ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.88": {"line.1": {"text": "Wirst du die frommen Wahrheitswege gehen,", "tokens": ["Wirst", "du", "die", "from\u00b7men", "Wahr\u00b7heits\u00b7we\u00b7ge", "ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dich selbst und andere triegst du nie.", "tokens": ["Dich", "selbst", "und", "an\u00b7de\u00b7re", "triegst", "du", "nie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "KON", "PIS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die Fr\u00f6mmelei l\u00e4\u00dft Falsches auch bestehen,", "tokens": ["Die", "Fr\u00f6m\u00b7me\u00b7lei", "l\u00e4\u00dft", "Fal\u00b7sches", "auch", "be\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NN", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Derwegen ha\u00df ich sie.", "tokens": ["Der\u00b7we\u00b7gen", "ha\u00df", "ich", "sie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.89": {"line.1": {"text": "Du sehnst dich, weit hinaus zu wandern,", "tokens": ["Du", "sehnst", "dich", ",", "weit", "hin\u00b7aus", "zu", "wan\u00b7dern", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "ADJD", "APZR", "PTKZU", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Bereitest dich zu raschem Flug;", "tokens": ["Be\u00b7rei\u00b7test", "dich", "zu", "ra\u00b7schem", "Flug", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dir selbst sei treu und treu den andern,", "tokens": ["Dir", "selbst", "sei", "treu", "und", "treu", "den", "an\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "ADJD", "KON", "ADJD", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Dann ist die Enge weit genug.", "tokens": ["Dann", "ist", "die", "En\u00b7ge", "weit", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADJD", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.90": {"line.1": {"text": "Halte dich nur im stillen rein,", "tokens": ["Hal\u00b7te", "dich", "nur", "im", "stil\u00b7len", "rein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "APPRART", "ADJA", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Und la\u00df es um dich wettern;", "tokens": ["Und", "la\u00df", "es", "um", "dich", "wet\u00b7tern", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "PPER", "APPR", "PRF", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Je mehr du f\u00fchlst, ein Mensch zu sein,", "tokens": ["Je", "mehr", "du", "f\u00fchlst", ",", "ein", "Mensch", "zu", "sein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "VVFIN", "$,", "ART", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Desto \u00e4hnlicher bist du den G\u00f6ttern.", "tokens": ["Des\u00b7to", "\u00e4hn\u00b7li\u00b7cher", "bist", "du", "den", "G\u00f6t\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}}, "stanza.91": {"line.1": {"text": "Was h\u00e4tte man vom Zeitungstraum,", "tokens": ["Was", "h\u00e4t\u00b7te", "man", "vom", "Zei\u00b7tungs\u00b7traum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PIS", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der leidigen Ephemere,", "tokens": ["Der", "lei\u00b7di\u00b7gen", "E\u00b7phe\u00b7me\u00b7re", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Wenn es uns nicht im stillen Raum", "tokens": ["Wenn", "es", "uns", "nicht", "im", "stil\u00b7len", "Raum"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "PTKNEG", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Noch ganz behaglich w\u00e4re!", "tokens": ["Noch", "ganz", "be\u00b7hag\u00b7lich", "w\u00e4\u00b7re", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.92": {"line.1": {"text": "Das Schlimmste, was uns widerf\u00e4hrt,", "tokens": ["Das", "Schlimms\u00b7te", ",", "was", "uns", "wi\u00b7der\u00b7f\u00e4hrt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das werden wir vom Tag gelehrt.", "tokens": ["Das", "wer\u00b7den", "wir", "vom", "Tag", "ge\u00b7lehrt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wer in dem Gestern Heute sah,", "tokens": ["Wer", "in", "dem", "Ge\u00b7stern", "Heu\u00b7te", "sah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "-+---+-+", "measure": "dactylic.init"}, "line.4": {"text": "Dem geht das Heute nicht allzu nah,", "tokens": ["Dem", "geht", "das", "Heu\u00b7te", "nicht", "all\u00b7zu", "nah", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "PTKNEG", "PTKA", "ADJD", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und wer im Heute sieht das Morgen,", "tokens": ["Und", "wer", "im", "Heu\u00b7te", "sieht", "das", "Mor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPRART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Der wird sich r\u00fchren, wird nicht sorgen.", "tokens": ["Der", "wird", "sich", "r\u00fch\u00b7ren", ",", "wird", "nicht", "sor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PRF", "VVINF", "$,", "VAFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.93": {"line.1": {"text": "Liegt dir Gestern klar und offen,", "tokens": ["Liegt", "dir", "Ge\u00b7stern", "klar", "und", "of\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wirkst du heute kr\u00e4ftig frei;", "tokens": ["Wirkst", "du", "heu\u00b7te", "kr\u00e4f\u00b7tig", "frei", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Kannst auch auf ein Morgen hoffen,", "tokens": ["Kannst", "auch", "auf", "ein", "Mor\u00b7gen", "hof\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Das nicht minder gl\u00fccklich sei.", "tokens": ["Das", "nicht", "min\u00b7der", "gl\u00fcck\u00b7lich", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PTKNEG", "ADV", "ADJD", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}