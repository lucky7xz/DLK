{"textgrid.poem.32631": {"metadata": {"author": {"name": "Pet\u00f6fi, S\u00e1ndor", "birth": "N.A.", "death": "N.A."}, "title": "1L: \u00bbdu hast gegessen, Freund, dein Zahn ist blutig,", "genre": "verse", "period": "N.A.", "pub_year": 1836, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbdu hast gegessen, Freund, dein Zahn ist blutig,", "tokens": ["\u00bb", "du", "hast", "ge\u00b7ges\u00b7sen", ",", "Freund", ",", "dein", "Zahn", "ist", "blu\u00b7tig", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "VVPP", "$,", "NN", "$,", "PPOSAT", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und uns macht Hungersqual schier todesmutig!", "tokens": ["Und", "uns", "macht", "Hun\u00b7gers\u00b7qual", "schier", "to\u00b7des\u00b7mu\u00b7tig", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "NN", "ADJD", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "'s ist bitter kalt, das Feld ist \u00f6d und traurig,", "tokens": ["'s", "ist", "bit\u00b7ter", "kalt", ",", "das", "Feld", "ist", "\u00f6d", "und", "trau\u00b7rig", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "ADJD", "$,", "ART", "NN", "VAFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "Und dr\u00fcber braust die Windsbraut wild und schaurig.", "tokens": ["Und", "dr\u00fc\u00b7ber", "braust", "die", "Winds\u00b7braut", "wild", "und", "schau\u00b7rig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "ART", "NN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Weit ist kein Mensch zu sehn und auch kein Tier,", "tokens": ["Weit", "ist", "kein", "Mensch", "zu", "sehn", "und", "auch", "kein", "Tier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PIAT", "NN", "PTKZU", "VVINF", "KON", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Drum sprich, wo ward der Schmaus beschieden dir?\u00ab", "tokens": ["Drum", "sprich", ",", "wo", "ward", "der", "Schmaus", "be\u00b7schie\u00b7den", "dir", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PAV", "VVFIN", "$,", "PWAV", "VAFIN", "ART", "NN", "VVFIN", "PPER", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Die gier'gen W\u00f6lfe gr\u00fc\u00dfen solcherweise", "tokens": ["Die", "gier'\u00b7gen", "W\u00f6l\u00b7fe", "gr\u00fc\u00b7\u00dfen", "sol\u00b7cher\u00b7wei\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVINF", "VMFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Den Freund, der eben heimkehrt von der Reise.", "tokens": ["Den", "Freund", ",", "der", "e\u00b7ben", "heim\u00b7kehrt", "von", "der", "Rei\u00b7se", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Der Satte aber z\u00f6gert l\u00e4nger nicht,", "tokens": ["Der", "Sat\u00b7te", "a\u00b7ber", "z\u00f6\u00b7gert", "l\u00e4n\u00b7ger", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVFIN", "ADJD", "PTKNEG", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und also er zu den Genossen spricht:", "tokens": ["Und", "al\u00b7so", "er", "zu", "den", "Ge\u00b7nos\u00b7sen", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPER", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "\u00bbdort auf der Pu\u00dfta steht ein H\u00e4uschen klein,", "tokens": ["\u00bb", "dort", "auf", "der", "Pu\u00df\u00b7ta", "steht", "ein", "H\u00e4usc\u00b7hen", "klein", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "APPR", "ART", "NN", "VVFIN", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Drin wohnt ein Sch\u00e4fer und sein Weibchen fein.", "tokens": ["Drin", "wohnt", "ein", "Sch\u00e4\u00b7fer", "und", "sein", "Weib\u00b7chen", "fein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "KON", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Und hinterm Hause liegt ein voller Stall,", "tokens": ["Und", "hin\u00b7term", "Hau\u00b7se", "liegt", "ein", "vol\u00b7ler", "Stall", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Draus h\u00f6rt' ich Schafe bl\u00f6ken, \u2013 just mein Fall.", "tokens": ["Draus", "h\u00f6rt'", "ich", "Scha\u00b7fe", "bl\u00f6\u00b7ken", ",", "\u2013", "just", "mein", "Fall", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "NN", "VVINF", "$,", "$(", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Zu diesem Haus nun schlichen still und sacht", "tokens": ["Zu", "die\u00b7sem", "Haus", "nun", "schli\u00b7chen", "still", "und", "sacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "ADV", "VVFIN", "ADJD", "KON", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ein junger Herr und ich in finstrer Nacht.", "tokens": ["Ein", "jun\u00b7ger", "Herr", "und", "ich", "in", "finst\u00b7rer", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "KON", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Ich lechzte nach den fetten Schafen, und", "tokens": ["Ich", "lechz\u00b7te", "nach", "den", "fet\u00b7ten", "Scha\u00b7fen", ",", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA", "NN", "$,", "KON"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ihm w\u00e4ssert' nach der Sch\u00e4ferin der Mund.", "tokens": ["Ihm", "w\u00e4s\u00b7sert'", "nach", "der", "Sch\u00e4\u00b7fe\u00b7rin", "der", "Mund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "Lang sp\u00e4ht' er da herum, die Lieb' im Sinn,", "tokens": ["Lang", "sp\u00e4ht'", "er", "da", "he\u00b7rum", ",", "die", "Lieb'", "im", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "PTKVZ", "$,", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ein Schaf erwischt' ich nicht, \u2013 da fra\u00df ich ihn!\u00ab", "tokens": ["Ein", "Schaf", "er\u00b7wi\u00b7scht'", "ich", "nicht", ",", "\u2013", "da", "fra\u00df", "ich", "ihn", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PTKNEG", "$,", "$(", "ADV", "VVFIN", "PPER", "PPER", "$.", "$("], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}}}}