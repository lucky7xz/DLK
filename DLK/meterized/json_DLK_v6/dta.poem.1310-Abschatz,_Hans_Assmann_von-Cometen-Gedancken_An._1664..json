{"dta.poem.1310": {"metadata": {"author": {"name": "Abschatz, Hans Assmann von", "birth": "N.A.", "death": "N.A."}, "title": "Cometen-Gedancken/ An. 1664.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1704", "urn": "urn:nbn:de:kobv:b4-200905199889", "language": ["de:0.99"], "booktitle": "Abschatz, Hans Assmann von: Poetische Ubersetzungen und Gedichte. Leipzig, 1704."}, "poem": {"stanza.1": {"line.1": {"text": "O Flamme/ von dem Zorn des H\u00f6chsten angesteckt/", "tokens": ["O", "Flam\u00b7me", "/", "von", "dem", "Zorn", "des", "H\u00f6chs\u00b7ten", "an\u00b7ge\u00b7steckt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$(", "APPR", "ART", "NN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Zu welcher unsre Schuld hat Stroh und Holtz gegeben/", "tokens": ["Zu", "wel\u00b7cher", "uns\u00b7re", "Schuld", "hat", "Stroh", "und", "Holtz", "ge\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "PPOSAT", "NN", "VAFIN", "NN", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Du must vor aller Welt am hohen Himmel schweben/", "tokens": ["Du", "must", "vor", "al\u00b7ler", "Welt", "am", "ho\u00b7hen", "Him\u00b7mel", "schwe\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "PIAT", "NN", "APPRART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Damit der schn\u00f6de Mensch/ vom S\u00fcnden-Schlaff erweckt/", "tokens": ["Da\u00b7mit", "der", "schn\u00f6\u00b7de", "Mensch", "/", "vom", "S\u00fcn\u00b7den\u00b7Schlaff", "er\u00b7weckt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "ADJA", "NN", "$(", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die Strahlen deiner Glutt in Marck und Bein empfinde/", "tokens": ["Die", "Strah\u00b7len", "dei\u00b7ner", "Glutt", "in", "Marck", "und", "Bein", "emp\u00b7fin\u00b7de", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Das Feuer heisser Bu\u00df in seiner Seel entz\u00fcnde.", "tokens": ["Das", "Feu\u00b7er", "heis\u00b7ser", "Bu\u00df", "in", "sei\u00b7ner", "Seel", "ent\u00b7z\u00fcn\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Es darff Tisifone der schwartzen Fackel nicht/", "tokens": ["Es", "darff", "Ti\u00b7si\u00b7fo\u00b7ne", "der", "schwart\u00b7zen", "Fa\u00b7ckel", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "ART", "ADJA", "NN", "PTKNEG", "$("], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Die ein verblendter Heyd als sch\u00e4dlich wird erkennen:", "tokens": ["Die", "ein", "ver\u00b7blend\u00b7ter", "Heyd", "als", "sch\u00e4d\u00b7lich", "wird", "er\u00b7ken\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJA", "NN", "KOKOM", "ADJD", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Was f\u00fcr ein Feuer soll ins k\u00fcnfftig wieder brennen/", "tokens": ["Was", "f\u00fcr", "ein", "Feu\u00b7er", "soll", "ins", "k\u00fcnff\u00b7tig", "wie\u00b7der", "bren\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "VMFIN", "APPRART", "ADJD", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Bezeuget mehr als viel dein dunckel-rothes Licht/", "tokens": ["Be\u00b7zeu\u00b7get", "mehr", "als", "viel", "dein", "dun\u00b7ckel\u00b7ro\u00b7thes", "Licht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "KOKOM", "ADV", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Das so viel Strahlen nicht von seiner Ruthe spreitet/", "tokens": ["Das", "so", "viel", "Strah\u00b7len", "nicht", "von", "sei\u00b7ner", "Ru\u00b7the", "sprei\u00b7tet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "PIAT", "NN", "PTKNEG", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "So viel uns Ach und Weh die Nemesis bereitet.", "tokens": ["So", "viel", "uns", "Ach", "und", "Weh", "die", "Ne\u00b7me\u00b7sis", "be\u00b7rei\u00b7tet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "NN", "KON", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Die Nemesis/ die sich nicht eh zu Frieden stellt/", "tokens": ["Die", "Ne\u00b7me\u00b7sis", "/", "die", "sich", "nicht", "eh", "zu", "Frie\u00b7den", "stellt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELS", "PRF", "PTKNEG", "KOUS", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bi\u00df da\u00df sich Blutt und Safft aus unsern Adern zehret/", "tokens": ["Bi\u00df", "da\u00df", "sich", "Blutt", "und", "Safft", "aus", "un\u00b7sern", "A\u00b7dern", "zeh\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "PRF", "ADJD", "KON", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Bi\u00df Krieg/ bi\u00df Brand und Pest/ Dorff/ Stadt und Land ver-", "tokens": ["Bi\u00df", "Krieg", "/", "bi\u00df", "Brand", "und", "Pest", "/", "Dorff", "/", "Stadt", "und", "Land", "ver"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$(", "APPR", "NN", "KON", "NN", "$(", "NE", "$(", "NN", "KON", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Bi\u00df Staub und Asch und Grau\u00df bedecket alle Welt/", "tokens": ["Bi\u00df", "Staub", "und", "Asch", "und", "Grau\u00df", "be\u00b7de\u00b7cket", "al\u00b7le", "Welt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "KON", "NN", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Bi\u00df dieser rundte Bau vom Feuer auffgefressen/", "tokens": ["Bi\u00df", "die\u00b7ser", "rund\u00b7te", "Bau", "vom", "Feu\u00b7er", "auff\u00b7ge\u00b7fres\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "APPRART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und von dem H\u00f6chsten selbst Gerichte wird gesessen.", "tokens": ["Und", "von", "dem", "H\u00f6chs\u00b7ten", "selbst", "Ge\u00b7rich\u00b7te", "wird", "ge\u00b7ses\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ADV", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Was Rath ist hier zu thun? Ein Epicurer sagt:", "tokens": ["Was", "Rath", "ist", "hier", "zu", "thun", "?", "Ein", "E\u00b7pi\u00b7cu\u00b7rer", "sagt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VAFIN", "ADV", "PTKZU", "VVINF", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Was scheuen wir die Glutt der ungewissen Flammen", "tokens": ["Was", "scheu\u00b7en", "wir", "die", "Glutt", "der", "un\u00b7ge\u00b7wis\u00b7sen", "Flam\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER", "ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Eh da\u00df sie \u00fcber Haubt und Hertze schl\u00e4gt zusammen/", "tokens": ["Eh", "da\u00df", "sie", "\u00fc\u00b7ber", "Haubt", "und", "Hert\u00b7ze", "schl\u00e4gt", "zu\u00b7sam\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOUS", "PPER", "APPR", "NN", "KON", "VVFIN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wofern der Mensch dadurch zur Straffe wird betagt/", "tokens": ["Wo\u00b7fern", "der", "Mensch", "da\u00b7durch", "zur", "Straf\u00b7fe", "wird", "be\u00b7tagt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PAV", "APPRART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Soll er der kurtzen Frist in Ruh und Lust gen\u00fcssen/", "tokens": ["Soll", "er", "der", "kurt\u00b7zen", "Frist", "in", "Ruh", "und", "Lust", "ge\u00b7n\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "ADJA", "NN", "APPR", "NN", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wo nicht/ was will er sich in eitler Sorge wissen?", "tokens": ["Wo", "nicht", "/", "was", "will", "er", "sich", "in", "eit\u00b7ler", "Sor\u00b7ge", "wis\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "$(", "PWS", "VMFIN", "PPER", "PRF", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Di\u00df aber heist das Oel dem Feuer setzen bey:", "tokens": ["Di\u00df", "a\u00b7ber", "heist", "das", "O\u00b7el", "dem", "Feu\u00b7er", "set\u00b7zen", "bey", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VVFIN", "ART", "NN", "ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Hier mu\u00df ein Christen-Hertz auff andre Mittel dencken/", "tokens": ["Hier", "mu\u00df", "ein", "Chris\u00b7ten\u00b7Hertz", "auff", "and\u00b7re", "Mit\u00b7tel", "den\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "NN", "APPR", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Des H\u00f6chsten strengen Zorn und Eyfer abzulencken/", "tokens": ["Des", "H\u00f6chs\u00b7ten", "stren\u00b7gen", "Zorn", "und", "Ey\u00b7fer", "ab\u00b7zu\u00b7len\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "KON", "NN", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Vor dem der feste Grund der Felsen reist entzwey/", "tokens": ["Vor", "dem", "der", "fes\u00b7te", "Grund", "der", "Fel\u00b7sen", "reist", "ent\u00b7zwey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "ADJA", "NN", "ART", "NN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der \u00fcber alle Zeit ohn alles Ende w\u00e4hret/", "tokens": ["Der", "\u00fc\u00b7ber", "al\u00b7le", "Zeit", "ohn", "al\u00b7les", "En\u00b7de", "w\u00e4h\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PIAT", "NN", "APPR", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und nicht wie dieser Stern sich mit der Zeit verzehret.", "tokens": ["Und", "nicht", "wie", "die\u00b7ser", "Stern", "sich", "mit", "der", "Zeit", "ver\u00b7zeh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "KOKOM", "PDAT", "NN", "PRF", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Die Busse mu\u00df allhier das beste Mittel seyn/", "tokens": ["Die", "Bus\u00b7se", "mu\u00df", "all\u00b7hier", "das", "bes\u00b7te", "Mit\u00b7tel", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "ART", "ADJA", "NN", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Das GOttes strengen Grimm in tieffen Abgrund sencket/", "tokens": ["Das", "Got\u00b7tes", "stren\u00b7gen", "Grimm", "in", "tief\u00b7fen", "Ab\u00b7grund", "sen\u00b7cket", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NE", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und in gesaltzner Flutt der Thr\u00e4nen-See ertr\u00e4ncket:", "tokens": ["Und", "in", "ge\u00b7saltz\u00b7ner", "Flutt", "der", "Thr\u00e4\u00b7nen\u00b7See", "er\u00b7tr\u00e4n\u00b7cket", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der Nachen f\u00fchret uns in sichern Hafen ein/", "tokens": ["Der", "Na\u00b7chen", "f\u00fch\u00b7ret", "uns", "in", "si\u00b7chern", "Ha\u00b7fen", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "ADJA", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wenn das geraume Schiff des Himmels und der Erden", "tokens": ["Wenn", "das", "ge\u00b7rau\u00b7me", "Schiff", "des", "Him\u00b7mels", "und", "der", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN", "ART", "NN", "KON", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Am grossen Tage wird des Feuers Beute werden.", "tokens": ["Am", "gros\u00b7sen", "Ta\u00b7ge", "wird", "des", "Feu\u00b7ers", "Beu\u00b7te", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VAFIN", "ART", "NN", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Hier liegt dein armes Volck/ o viel erz\u00fcrnter GOtt!", "tokens": ["Hier", "liegt", "dein", "ar\u00b7mes", "Volck", "/", "o", "viel", "er\u00b7z\u00fcrn\u00b7ter", "Gott", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "ADJA", "NN", "$(", "FM", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In wahrer Reu und Leyd f\u00fcr deinem hohen Throne/", "tokens": ["In", "wah\u00b7rer", "Reu", "und", "Leyd", "f\u00fcr", "dei\u00b7nem", "ho\u00b7hen", "Thro\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wir bitten: strenger HErr und liebster Vater/ schone/", "tokens": ["Wir", "bit\u00b7ten", ":", "stren\u00b7ger", "Herr", "und", "liebs\u00b7ter", "Va\u00b7ter", "/", "scho\u00b7ne", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "ADJA", "NN", "KON", "ADJA", "NN", "$(", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Verschon und wende weg die angedr\u00e4ute Noth:", "tokens": ["Ver\u00b7schon", "und", "wen\u00b7de", "weg", "die", "an\u00b7ge\u00b7dr\u00e4u\u00b7te", "Noth", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und soll ja unser Leib nach deinem Willen b\u00fcssen/", "tokens": ["Und", "soll", "ja", "un\u00b7ser", "Leib", "nach", "dei\u00b7nem", "Wil\u00b7len", "b\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "PPOSAT", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "So la\u00df die Seele doch sich frey und sicher wissen.", "tokens": ["So", "la\u00df", "die", "See\u00b7le", "doch", "sich", "frey", "und", "si\u00b7cher", "wis\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ART", "NN", "ADV", "PRF", "ADJD", "KON", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}