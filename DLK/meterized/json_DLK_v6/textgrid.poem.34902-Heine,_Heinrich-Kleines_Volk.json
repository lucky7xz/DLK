{"textgrid.poem.34902": {"metadata": {"author": {"name": "Heine, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Kleines Volk", "genre": "verse", "period": "N.A.", "pub_year": 1826, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "In einem Pi\u00dfpott kam er geschwommen,", "tokens": ["In", "ei\u00b7nem", "Pi\u00df\u00b7pott", "kam", "er", "ge\u00b7schwom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hochzeitlich geputzt, hinab den Rhein.", "tokens": ["Hoch\u00b7zeit\u00b7lich", "ge\u00b7putzt", ",", "hin\u00b7ab", "den", "Rhein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "VVPP", "$,", "ADV", "ART", "NE", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und als er nach Rotterdam gekommen,", "tokens": ["Und", "als", "er", "nach", "Rot\u00b7ter\u00b7dam", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "APPR", "NE", "VVPP", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da sprach er: \u00bbJuffr\u00e4uken, willst du mich frein?", "tokens": ["Da", "sprach", "er", ":", "\u00bb", "Juf\u00b7fr\u00e4u\u00b7ken", ",", "willst", "du", "mich", "frein", "?"], "token_info": ["word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "$(", "NN", "$,", "VMFIN", "PPER", "PRF", "PTKVZ", "$."], "meter": "-+--+-++-+", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Ich f\u00fchre dich, geliebte Sch\u00f6ne,", "tokens": ["Ich", "f\u00fch\u00b7re", "dich", ",", "ge\u00b7lieb\u00b7te", "Sch\u00f6\u00b7ne", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nach meinem Schlo\u00df, ins Brautgemach;", "tokens": ["Nach", "mei\u00b7nem", "Schlo\u00df", ",", "ins", "Braut\u00b7ge\u00b7mach", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die W\u00e4nde sind eitel Hobelsp\u00e4ne,", "tokens": ["Die", "W\u00e4n\u00b7de", "sind", "ei\u00b7tel", "Ho\u00b7bel\u00b7sp\u00e4\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "NN", "$,"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Aus H\u00e4ckerling besteht das Dach.", "tokens": ["Aus", "H\u00e4\u00b7cker\u00b7ling", "be\u00b7steht", "das", "Dach", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Da ist es so puppenniedlich und nette,", "tokens": ["Da", "ist", "es", "so", "pup\u00b7pen\u00b7nied\u00b7lich", "und", "net\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADJD", "KON", "ADJA", "$,"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da lebst du wie eine K\u00f6nigin!", "tokens": ["Da", "lebst", "du", "wie", "ei\u00b7ne", "K\u00f6\u00b7ni\u00b7gin", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "KOKOM", "ART", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Die Schale der Walnu\u00df ist unser Bette,", "tokens": ["Die", "Scha\u00b7le", "der", "Wal\u00b7nu\u00df", "ist", "un\u00b7ser", "Bet\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "-+--++-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Von Spinnweb sind die Laken drin.", "tokens": ["Von", "Spinn\u00b7web", "sind", "die", "La\u00b7ken", "drin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ameiseneier, gebraten in Butter,", "tokens": ["A\u00b7mei\u00b7sen\u00b7ei\u00b7er", ",", "ge\u00b7bra\u00b7ten", "in", "But\u00b7ter", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "VVFIN", "APPR", "NN", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "Essen wir t\u00e4glich, auch W\u00fcrmchengem\u00fcs',", "tokens": ["Es\u00b7sen", "wir", "t\u00e4g\u00b7lich", ",", "auch", "W\u00fcrm\u00b7chen\u00b7ge\u00b7m\u00fcs'", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "PPER", "ADJD", "$,", "ADV", "NE", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.3": {"text": "Und sp\u00e4ter erb ich von meiner Frau Mutter", "tokens": ["Und", "sp\u00e4\u00b7ter", "erb", "ich", "von", "mei\u00b7ner", "Frau", "Mut\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Drei Nonnenf\u00fcrzchen, die schmecken so s\u00fc\u00df.", "tokens": ["Drei", "Non\u00b7nen\u00b7f\u00fcrz\u00b7chen", ",", "die", "schme\u00b7cken", "so", "s\u00fc\u00df", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PRELS", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Ich habe Speck, ich habe Schwarten,", "tokens": ["Ich", "ha\u00b7be", "Speck", ",", "ich", "ha\u00b7be", "Schwar\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "$,", "PPER", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ich habe Fingerh\u00fcte voll Wein,", "tokens": ["Ich", "ha\u00b7be", "Fin\u00b7ger\u00b7h\u00fc\u00b7te", "voll", "Wein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "ADJD", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Auch w\u00e4chst eine R\u00fcbe in meinem Garten,", "tokens": ["Auch", "w\u00e4chst", "ei\u00b7ne", "R\u00fc\u00b7be", "in", "mei\u00b7nem", "Gar\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Du wirst wahrhaftig gl\u00fccklich sein!\u00ab", "tokens": ["Du", "wirst", "wahr\u00b7haf\u00b7tig", "gl\u00fcck\u00b7lich", "sein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "ADJD", "VAINF", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Das war ein Locken und ein Werben!", "tokens": ["Das", "war", "ein", "Lo\u00b7cken", "und", "ein", "Wer\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wohl seufzte die Braut: \u00bbAch Gott! ach Gott!\u00ab", "tokens": ["Wohl", "seufz\u00b7te", "die", "Braut", ":", "\u00bb", "Ach", "Gott", "!", "ach", "Gott", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "$(", "ITJ", "NN", "$.", "XY", "NN", "$.", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Sie war wehm\u00fctig, wie zum Sterben \u2013", "tokens": ["Sie", "war", "weh\u00b7m\u00fc\u00b7tig", ",", "wie", "zum", "Ster\u00b7ben", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "PWAV", "APPRART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Doch endlich stieg sie hinab in den Pott.", "tokens": ["Doch", "end\u00b7lich", "stieg", "sie", "hin\u00b7ab", "in", "den", "Pott", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "Sind Christenleute oder M\u00e4use", "tokens": ["Sind", "Chris\u00b7ten\u00b7leu\u00b7te", "o\u00b7der", "M\u00e4u\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Helden des Lieds? Ich wei\u00df es nicht mehr.", "tokens": ["Die", "Hel\u00b7den", "des", "Lieds", "?", "Ich", "wei\u00df", "es", "nicht", "mehr", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$.", "PPER", "VVFIN", "PPER", "PTKNEG", "ADV", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Im Beverland h\u00f6rt ich die schnurrige Weise,", "tokens": ["Im", "Be\u00b7ver\u00b7land", "h\u00f6rt", "ich", "die", "schnur\u00b7ri\u00b7ge", "Wei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-++--+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Es sind nun drei\u00dfig Jahre her.", "tokens": ["Es", "sind", "nun", "drei\u00b7\u00dfig", "Jah\u00b7re", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "CARD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}