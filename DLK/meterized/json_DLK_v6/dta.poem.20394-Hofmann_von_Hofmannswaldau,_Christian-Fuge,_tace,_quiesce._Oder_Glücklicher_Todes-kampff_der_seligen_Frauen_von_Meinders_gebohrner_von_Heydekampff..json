{"dta.poem.20394": {"metadata": {"author": {"name": "Hofmann von Hofmannswaldau, Christian", "birth": "N.A.", "death": "N.A."}, "title": "Fuge, tace, quiesce.  \n Oder  \n  Gl\u00fccklicher Todes-kampff der seligen  \n Frauen von Meinders/ gebohrner von  \n Heydekampff.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1695", "urn": "urn:nbn:de:kobv:b4-200905197751", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Wir arme sterblichen/ wir haben aug' und licht/\nUnd dennoch fliegen wir wie mutten ins verderben.\nWir f\u00fchlen/ wenn der tod uns das genicke bricht/\nNicht aber allemahl/ wann unsre seelen sterben.\nWir riechen zwar das grab/ doch nicht die seuchen an;\nWir schmecken nur das gifft/ nicht aber seine lehren:\nJa/ da wir den Galen als einen gott verehren/\nSo wird dem Moses offt das ohre zugethan:\nUnd also sterben wir vor an verstand und sinnen/\nEh' unsre lippen schnee/ die glieder ei\u00df gewinnen.", "tokens": ["Wir", "ar\u00b7me", "sterb\u00b7li\u00b7chen", "/", "wir", "ha\u00b7ben", "aug'", "und", "licht", "/", "Und", "den\u00b7noch", "flie\u00b7gen", "wir", "wie", "mut\u00b7ten", "ins", "ver\u00b7der\u00b7ben", ".", "Wir", "f\u00fch\u00b7len", "/", "wenn", "der", "tod", "uns", "das", "ge\u00b7ni\u00b7cke", "bricht", "/", "Nicht", "a\u00b7ber", "al\u00b7le\u00b7mahl", "/", "wann", "uns\u00b7re", "see\u00b7len", "ster\u00b7ben", ".", "Wir", "rie\u00b7chen", "zwar", "das", "grab", "/", "doch", "nicht", "die", "seu\u00b7chen", "an", ";", "Wir", "schme\u00b7cken", "nur", "das", "gifft", "/", "nicht", "a\u00b7ber", "sei\u00b7ne", "leh\u00b7ren", ":", "Ja", "/", "da", "wir", "den", "Ga\u00b7len", "als", "ei\u00b7nen", "gott", "ver\u00b7eh\u00b7ren", "/", "So", "wird", "dem", "Mo\u00b7ses", "offt", "das", "oh\u00b7re", "zu\u00b7ge\u00b7than", ":", "Und", "al\u00b7so", "ster\u00b7ben", "wir", "vor", "an", "ver\u00b7stand", "und", "sin\u00b7nen", "/", "Eh'", "uns\u00b7re", "lip\u00b7pen", "schnee", "/", "die", "glie\u00b7der", "ei\u00df", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "VVINF", "$(", "PPER", "VAFIN", "NN", "KON", "NN", "$(", "KON", "ADV", "VVFIN", "PPER", "KOKOM", "VVFIN", "APPRART", "VVFIN", "$.", "PPER", "VVFIN", "$(", "KOUS", "ART", "NN", "PPER", "ART", "NN", "VVFIN", "$(", "PTKNEG", "ADV", "ADV", "$(", "PWAV", "PPOSAT", "NN", "VVINF", "$.", "PPER", "VVFIN", "ADV", "ART", "NN", "$(", "ADV", "PTKNEG", "ART", "ADJA", "PTKVZ", "$.", "PPER", "VVFIN", "ADV", "ART", "NN", "$(", "PTKNEG", "ADV", "PPOSAT", "VVINF", "$.", "PTKANT", "$(", "KOUS", "PPER", "ART", "NN", "KOKOM", "ART", "NN", "VVFIN", "$(", "ADV", "VAFIN", "ART", "NN", "ADV", "ART", "NN", "VVPP", "$.", "KON", "ADV", "VVFIN", "PPER", "APPR", "APPR", "VVFIN", "KON", "VVINF", "$(", "NN", "PPOSAT", "NN", "VVFIN", "$(", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-+-+-+-+-+-+--+-+-+-+-+-+-+-+-+-+-+-+--+-+-+-+-+-+-+-+-+-+-+-+-+-+-+--+-+-+--+-+-+-+-+-+-+-+-+-+-+-+----+-+-+-+-+-", "measure": "iambic.octa.plus"}}, "stanza.2": {"line.1": {"text": "Daher entspringt die furcht des Dionysius/", "tokens": ["Da\u00b7her", "ent\u00b7springt", "die", "furcht", "des", "Di\u00b7o\u00b7ny\u00b7si\u00b7us", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "ART", "NN", "ART", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ach aber/ th\u00f6richte! was seyd ihr doch bem\u00fcht", "tokens": ["Ach", "a\u00b7ber", "/", "th\u00f6\u00b7rich\u00b7te", "!", "was", "seyd", "ihr", "doch", "be\u00b7m\u00fcht"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "ADV", "$(", "VVFIN", "$.", "PWS", "VAFIN", "PPER", "ADV", "VVFIN"], "meter": "-+-+---+-+-+", "measure": "unknown.measure.penta"}}, "stanza.4": {"line.1": {"text": "Man falle wie man will/ durch pulver oder bley/", "tokens": ["Man", "fal\u00b7le", "wie", "man", "will", "/", "durch", "pul\u00b7ver", "o\u00b7der", "bley", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KOKOM", "PIS", "VMFIN", "$(", "APPR", "NE", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Hier aber wancket offt die nadel der vernunfft;", "tokens": ["Hier", "a\u00b7ber", "wan\u00b7cket", "offt", "die", "na\u00b7del", "der", "ver\u00b7nunfft", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ADV", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Di\u00df hat vor Zeiten schon die kluge welt bedacht/", "tokens": ["Di\u00df", "hat", "vor", "Zei\u00b7ten", "schon", "die", "klu\u00b7ge", "welt", "be\u00b7dacht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "NN", "ADV", "ART", "ADJA", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Ihr/ die ihr geld und gut vor eure g\u00f6tter sch\u00e4tzt/", "tokens": ["Ihr", "/", "die", "ihr", "geld", "und", "gut", "vor", "eu\u00b7re", "g\u00f6t\u00b7ter", "sch\u00e4tzt", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$(", "ART", "PPOSAT", "NN", "KON", "ADJD", "APPR", "PPOSAT", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Ihr erster lebens-tag trat voller freuden ein/", "tokens": ["Ihr", "ers\u00b7ter", "le\u00b7bens\u00b7tag", "trat", "vol\u00b7ler", "freu\u00b7den", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "ADJA", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Inzwischen kam der tod einst bey gew\u00f6lckter nacht/", "tokens": ["I\u00b7nzwi\u00b7schen", "kam", "der", "tod", "einst", "bey", "ge\u00b7w\u00f6lck\u00b7ter", "nacht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.10": {"line.1": {"text": "Doch nein! ich irre mich/ ich irre/ fuhr er fort/", "tokens": ["Doch", "nein", "!", "ich", "ir\u00b7re", "mich", "/", "ich", "ir\u00b7re", "/", "fuhr", "er", "fort", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PTKANT", "$.", "PPER", "VVFIN", "PPER", "$(", "PPER", "VVFIN", "$(", "VVFIN", "PPER", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Di\u00df hatt er kaum gesagt/ so lie\u00df die schlangen-brut/", "tokens": ["Di\u00df", "hatt", "er", "kaum", "ge\u00b7sagt", "/", "so", "lie\u00df", "die", "schlan\u00b7gen\u00b7brut", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "VVPP", "$(", "ADV", "VVFIN", "ART", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.12": {"line.1": {"text": "Hier hast du/ liebster sohn/ sprach dieser h\u00f6llen-brand/", "tokens": ["Hier", "hast", "du", "/", "liebs\u00b7ter", "sohn", "/", "sprach", "die\u00b7ser", "h\u00f6l\u00b7len\u00b7brand", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$(", "ADJA", "NN", "$(", "VVFIN", "PDAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "Erschrick nicht/ blie\u00df sie drauff ihm in die ohren ein/", "tokens": ["Er\u00b7schrick", "nicht", "/", "blie\u00df", "sie", "drauff", "ihm", "in", "die", "oh\u00b7ren", "ein", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PTKNEG", "$(", "VVFIN", "PPER", "PAV", "PPER", "APPR", "ART", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Aurora ist so sch\u00f6n bey fr\u00fchem morgen nicht/", "tokens": ["Au\u00b7ro\u00b7ra", "ist", "so", "sch\u00f6n", "bey", "fr\u00fc\u00b7hem", "mor\u00b7gen", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "ADV", "ADJD", "APPR", "ADJA", "ADV", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Gleichwohl kam ihre pracht nicht denen andern bey/", "tokens": ["Gleich\u00b7wohl", "kam", "ih\u00b7re", "pracht", "nicht", "de\u00b7nen", "an\u00b7dern", "bey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "PTKNEG", "PRELS", "PIS", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.16": {"line.1": {"text": "Die andre \u00fcbertraff das gantze Morgenland/", "tokens": ["Die", "and\u00b7re", "\u00fc\u00b7bert\u00b7raff", "das", "gant\u00b7ze", "Mor\u00b7gen\u00b7land", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.17": {"line.1": {"text": "Hier siehstu (fing indem die s\u00fcnde wieder an)", "tokens": ["Hier", "sieh\u00b7stu", "(", "fing", "in\u00b7dem", "die", "s\u00fcn\u00b7de", "wie\u00b7der", "an", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "VVFIN", "KOUS", "ART", "ADJA", "ADV", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.18": {"line.1": {"text": "Di\u00df sagte sie/ und flog/ als wie ein plitz davon/", "tokens": ["Di\u00df", "sag\u00b7te", "sie", "/", "und", "flog", "/", "als", "wie", "ein", "plitz", "da\u00b7von", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "$(", "KON", "VVFIN", "$(", "KOUS", "KOKOM", "ART", "NN", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.19": {"line.1": {"text": "Der morgen zeigte kaum das lichte rosen-tuch/", "tokens": ["Der", "mor\u00b7gen", "zeig\u00b7te", "kaum", "das", "lich\u00b7te", "ro\u00b7sen\u00b7tuch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.20": {"line.1": {"text": "Di\u00df pfiff der seligen die schlange t\u00e4glich f\u00fcr.", "tokens": ["Di\u00df", "pfiff", "der", "se\u00b7li\u00b7gen", "die", "schlan\u00b7ge", "t\u00e4g\u00b7lich", "f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "ADJA", "ART", "ADJA", "ADJD", "APPR", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.21": {"line.1": {"text": "Und also blieb ihr hertz von aller regung frey/", "tokens": ["Und", "al\u00b7so", "blieb", "ihr", "hertz", "von", "al\u00b7ler", "re\u00b7gung", "frey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPOSAT", "NN", "APPR", "PIAT", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.22": {"line.1": {"text": "Hingegen wandte gleich ihr engel wieder ein:", "tokens": ["Hin\u00b7ge\u00b7gen", "wand\u00b7te", "gleich", "ihr", "en\u00b7gel", "wie\u00b7der", "ein", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "PPOSAT", "NN", "ADV", "PTKVZ", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}}, "stanza.23": {"line.1": {"text": "Was hilfft es? fuhr er fort/ da\u00df man die halbe welt", "tokens": ["Was", "hilfft", "es", "?", "fuhr", "er", "fort", "/", "da\u00df", "man", "die", "hal\u00b7be", "welt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER", "$.", "VVFIN", "PPER", "PTKVZ", "$(", "KOUS", "PIS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.24": {"line.1": {"text": "Nachdem der hoffart nun der bogen auch zerbrach/", "tokens": ["Nach\u00b7dem", "der", "hof\u00b7fart", "nun", "der", "bo\u00b7gen", "auch", "zer\u00b7brach", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "VVFIN", "ADV", "ART", "VVFIN", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.25": {"line.1": {"text": "So artig wissen uns die laster ihren gifft", "tokens": ["So", "ar\u00b7tig", "wis\u00b7sen", "uns", "die", "las\u00b7ter", "ih\u00b7ren", "gifft"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVFIN", "PPER", "ART", "ADJA", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.26": {"line.1": {"text": "Hier strich die selige den dampff der eitelkeit/", "tokens": ["Hier", "strich", "die", "se\u00b7li\u00b7ge", "den", "dampff", "der", "ei\u00b7tel\u00b7keit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.27": {"line.1": {"text": "Wie der Chameleon/ wenn er vor eyfer bebt/", "tokens": ["Wie", "der", "Cha\u00b7me\u00b7le\u00b7on", "/", "wenn", "er", "vor", "ey\u00b7fer", "bebt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NE", "$(", "KOUS", "PPER", "APPR", "NN", "VVFIN", "$("], "meter": "---+-+-+-+-+", "measure": "unknown.measure.penta"}}, "stanza.28": {"line.1": {"text": "In diesem stande nun fand der ergrimmte tod/", "tokens": ["In", "die\u00b7sem", "stan\u00b7de", "nun", "fand", "der", "er\u00b7grimm\u00b7te", "tod", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "ADV", "VVFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.29": {"line.1": {"text": "Gott (rieff der engel drauff) hat dieses auch erlaubt.", "tokens": ["Gott", "(", "rieff", "der", "en\u00b7gel", "drauff", ")", "hat", "die\u00b7ses", "auch", "er\u00b7laubt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "VVFIN", "ART", "NN", "PTKVZ", "$(", "VAFIN", "PDS", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.30": {"line.1": {"text": "Wer wei\u00df/ was f\u00fcr ein schatz in der gesundheit steckt/", "tokens": ["Wer", "wei\u00df", "/", "was", "f\u00fcr", "ein", "schatz", "in", "der", "ge\u00b7sund\u00b7heit", "steckt", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$(", "PWS", "APPR", "ART", "NN", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.31": {"line.1": {"text": "Und damit stellte sich nun die verzweifflung ein/", "tokens": ["Und", "da\u00b7mit", "stell\u00b7te", "sich", "nun", "die", "ver\u00b7zweif\u00b7flung", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PRF", "ADV", "ART", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.32": {"line.1": {"text": "Auff die verzweiffelung kam schmertz und ungedult/", "tokens": ["Auff", "die", "ver\u00b7zweif\u00b7fe\u00b7lung", "kam", "schmertz", "und", "un\u00b7ge\u00b7dult", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.33": {"line.1": {"text": "So schwatzte fleisch und blut; jedoch ihr treuer geist", "tokens": ["So", "schwatz\u00b7te", "fleisch", "und", "blut", ";", "je\u00b7doch", "ihr", "treu\u00b7er", "geist"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADJD", "KON", "NN", "$.", "ADV", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.34": {"line.1": {"text": "Ach aber! fuhr er fort/ ihr klagt/ und wisset nicht/", "tokens": ["Ach", "a\u00b7ber", "!", "fuhr", "er", "fort", "/", "ihr", "klagt", "/", "und", "wis\u00b7set", "nicht", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ITJ", "ADV", "$.", "VVFIN", "PPER", "PTKVZ", "$(", "PPER", "VVFIN", "$(", "KON", "VVFIN", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.35": {"line.1": {"text": "Durch dieses ward ihr hertz so wie ein mandel-baum", "tokens": ["Durch", "die\u00b7ses", "ward", "ihr", "hertz", "so", "wie", "ein", "man\u00b7del\u00b7baum"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDS", "VAFIN", "PPOSAT", "NN", "ADV", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.36": {"line.1": {"text": "Mein Meinders gute nacht! wir haben obgesiegt.", "tokens": ["Mein", "Mein\u00b7ders", "gu\u00b7te", "nacht", "!", "wir", "ha\u00b7ben", "ob\u00b7ge\u00b7siegt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADJA", "NN", "$.", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.37": {"line.1": {"text": "So sagte sie/ und gab der erden gute nacht:", "tokens": ["So", "sag\u00b7te", "sie", "/", "und", "gab", "der", "er\u00b7den", "gu\u00b7te", "nacht", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$(", "KON", "VVFIN", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.38": {"line.1": {"text": "Ihr blinden sterblichen/ laufft f\u00fcr dem tode nicht!", "tokens": ["Ihr", "blin\u00b7den", "sterb\u00b7li\u00b7chen", "/", "laufft", "f\u00fcr", "dem", "to\u00b7de", "nicht", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "VVINF", "$(", "VVFIN", "APPR", "ART", "NN", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.39": {"line.1": {"text": "Di\u00df alles ist geschehn/ der c\u00f6rper ist versenckt/", "tokens": ["Di\u00df", "al\u00b7les", "ist", "ge\u00b7schehn", "/", "der", "c\u00f6r\u00b7per", "ist", "ver\u00b7senckt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VAFIN", "VVPP", "$(", "ART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}