{"textgrid.poem.53826": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "N\u00e4chtliche Unterhaltung", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Landgerichtsdirektor schnarchte im Bett.", "tokens": ["Der", "Land\u00b7ge\u00b7richts\u00b7di\u00b7rek\u00b7tor", "schnarch\u00b7te", "im", "Bett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Seine Garderobe lag \u2013 ziemlich komplett \u2013", "tokens": ["Sei\u00b7ne", "Gar\u00b7de\u00b7ro\u00b7be", "lag", "\u2013", "ziem\u00b7lich", "kom\u00b7plett", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$(", "ADV", "ADJD", "$("], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "auf dem Stuhl. Die Nacht war so monoton . . .", "tokens": ["auf", "dem", "Stuhl", ".", "Die", "Nacht", "war", "so", "mo\u00b7no\u00b7ton", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ART", "NN", "$.", "ART", "NN", "VAFIN", "ADV", "NE", "$.", "$.", "$."], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Da machten die Kleider Konversation.", "tokens": ["Da", "mach\u00b7ten", "die", "Klei\u00b7der", "Kon\u00b7ver\u00b7sa\u00b7ti\u00b7on", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NE", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "\u00bbich\u00ab, sagte die Jacke, \u00bbwerde ausgezogen.", "tokens": ["\u00bb", "ich", "\u00ab", ",", "sag\u00b7te", "die", "Ja\u00b7cke", ",", "\u00bb", "wer\u00b7de", "aus\u00b7ge\u00b7zo\u00b7gen", "."], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "PPER", "$(", "$,", "VVFIN", "ART", "NN", "$,", "$(", "VAFIN", "VVPP", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ich h\u00e4nge \u2013 ungelogen \u2013", "tokens": ["Ich", "h\u00e4n\u00b7ge", "\u2013", "un\u00b7ge\u00b7lo\u00b7gen", "\u2013"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "ADJD", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "im Beratungszimmer", "tokens": ["im", "Be\u00b7ra\u00b7tungs\u00b7zim\u00b7mer"], "token_info": ["word", "word"], "pos": ["APPRART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "und habe keinen Schimmer,", "tokens": ["und", "ha\u00b7be", "kei\u00b7nen", "Schim\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "was mein Alter da treibt.\u00ab", "tokens": ["was", "mein", "Al\u00b7ter", "da", "treibt", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "PPOSAT", "NN", "ADV", "VVFIN", "$.", "$("], "meter": "+-+--+", "measure": "iambic.tri.chol"}}, "stanza.3": {"line.1": {"text": "\u00bbwir sprechen Recht!\u00ab sagte die Weste.", "tokens": ["\u00bb", "wir", "spre\u00b7chen", "Recht", "!", "\u00ab", "sag\u00b7te", "die", "Wes\u00b7te", "."], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "NN", "$.", "$(", "VVFIN", "ART", "NN", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbaber feste \u2013!", "tokens": ["\u00bb", "a\u00b7ber", "fes\u00b7te", "\u2013", "!"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "ADJA", "$(", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Wir schnauzen die Angeklagten an \u2013", "tokens": ["Wir", "schnau\u00b7zen", "die", "An\u00b7ge\u00b7klag\u00b7ten", "an", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKVZ", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "wir benehmen uns wie ein Edelmann.", "tokens": ["wir", "be\u00b7neh\u00b7men", "uns", "wie", "ein", "E\u00b7del\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "KOKOM", "ART", "NN", "$."], "meter": "+-+-++-+-+", "measure": "unknown.measure.hexa"}, "line.5": {"text": "Wir verbieten allen sofort den Mund", "tokens": ["Wir", "ver\u00b7bie\u00b7ten", "al\u00b7len", "so\u00b7fort", "den", "Mund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIS", "ADV", "ART", "NN"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "und reden uns selber die Lippen wund.", "tokens": ["und", "re\u00b7den", "uns", "sel\u00b7ber", "die", "Lip\u00b7pen", "wund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ART", "NN", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Wir verh\u00e4ngen \u00fcber Wehrlose Ordnungsstrafen", "tokens": ["Wir", "ver\u00b7h\u00e4n\u00b7gen", "\u00fc\u00b7ber", "Wehr\u00b7lo\u00b7se", "Ord\u00b7nungs\u00b7stra\u00b7fen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "NN"], "meter": "+-+-+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.8": {"text": "(nur, wenn wir Beisitzer sind, k\u00f6nnen wir schlafen).", "tokens": ["(", "nur", ",", "wenn", "wir", "Bei\u00b7sit\u00b7zer", "sind", ",", "k\u00f6n\u00b7nen", "wir", "schla\u00b7fen", ")", "."], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "$,", "KOUS", "PPER", "NN", "VAFIN", "$,", "VMFIN", "PPER", "VVINF", "$(", "$."], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.9": {"text": "Zum Schlu\u00df verknacken wir. Ohne Scherz.", "tokens": ["Zum", "Schlu\u00df", "ver\u00b7kna\u00b7cken", "wir", ".", "Oh\u00b7ne", "Scherz", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "$.", "APPR", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": "Unter mir schl\u00e4gt \u00fcbrigens kein Herz.\u00ab", "tokens": ["Un\u00b7ter", "mir", "schl\u00e4gt", "\u00fcb\u00b7ri\u00b7gens", "kein", "Herz", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "VVFIN", "ADV", "PIAT", "NN", "$.", "$("], "meter": "+-+++-+-+", "measure": "unknown.measure.hexa"}}, "stanza.4": {"line.1": {"text": "\u00bbwir\u00ab, sagten die Hosen, \u00bbwir habens schwer.", "tokens": ["\u00bb", "wir", "\u00ab", ",", "sag\u00b7ten", "die", "Ho\u00b7sen", ",", "\u00bb", "wir", "ha\u00b7bens", "schwer", "."], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "$(", "$,", "VVFIN", "ART", "NN", "$,", "$(", "PPER", "VAFIN", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Neulich kam der Landgerichtspr\u00e4sident daher", "tokens": ["Neu\u00b7lich", "kam", "der", "Land\u00b7ge\u00b7richts\u00b7pr\u00e4\u00b7si\u00b7dent", "da\u00b7her"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "PAV"], "meter": "+-+-+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "und hat revidiert. Er sa\u00df an der Barriere,", "tokens": ["und", "hat", "re\u00b7vi\u00b7diert", ".", "Er", "sa\u00df", "an", "der", "Bar\u00b7rie\u00b7re", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "VVPP", "$.", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "und es ging um unsre ganze Karriere.", "tokens": ["und", "es", "ging", "um", "uns\u00b7re", "gan\u00b7ze", "Kar\u00b7rie\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.5": {"text": "Vor uns ein Kommunist. Da haben wir wie wild", "tokens": ["Vor", "uns", "ein", "Kom\u00b7mu\u00b7nist", ".", "Da", "ha\u00b7ben", "wir", "wie", "wild"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "ART", "NN", "$.", "ADV", "VAFIN", "PPER", "KOKOM", "ADJD"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "geschmettert, geschnattert, gestampft und gebr\u00fcllt.", "tokens": ["ge\u00b7schmet\u00b7tert", ",", "ge\u00b7schnat\u00b7tert", ",", "ge\u00b7stampft", "und", "ge\u00b7br\u00fcllt", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "$,", "VVPP", "KON", "VVPP", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.7": {"text": "Aber wie es manchmal so geht hienieden:", "tokens": ["A\u00b7ber", "wie", "es", "manch\u00b7mal", "so", "geht", "hien\u00b7ie\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "ADV", "VVFIN", "ADV", "$."], "meter": "+------+-+-", "measure": "dactylic.init"}, "line.8": {"text": "der Pr\u00e4sident wars noch nicht zufrieden.", "tokens": ["der", "Pr\u00e4\u00b7si\u00b7dent", "wars", "noch", "nicht", "zu\u00b7frie\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Und da blieb uns die ganze Rechtswissenschaft weg,", "tokens": ["Und", "da", "blieb", "uns", "die", "gan\u00b7ze", "Rechts\u00b7wis\u00b7sen\u00b7schaft", "weg", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "PTKVZ", "$,"], "meter": "--+--+-+---+", "measure": "anapaest.di.plus"}, "line.10": {"text": "und da bekamen wir einen m\u00e4chtigen Schreck.", "tokens": ["und", "da", "be\u00b7ka\u00b7men", "wir", "ei\u00b7nen", "m\u00e4ch\u00b7ti\u00b7gen", "Schreck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+--+", "measure": "iambic.penta.relaxed"}, "line.11": {"text": "Und zum Schlu\u00df besahen wir uns den Schaden:", "tokens": ["Und", "zum", "Schlu\u00df", "be\u00b7sa\u00b7hen", "wir", "uns", "den", "Scha\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVFIN", "PPER", "PRF", "ART", "NN", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.12": {"text": "Wir Hosen hatten es auszubaden!\u00ab", "tokens": ["Wir", "Ho\u00b7sen", "hat\u00b7ten", "es", "aus\u00b7zu\u00b7ba\u00b7den", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "NN", "VAFIN", "PPER", "VVINF", "$.", "$("], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "So sprachen die Kleider in dunkler Nacht", "tokens": ["So", "spra\u00b7chen", "die", "Klei\u00b7der", "in", "dunk\u00b7ler", "Nacht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "und haben sich Konfidenzen gemacht.", "tokens": ["und", "ha\u00b7ben", "sich", "Kon\u00b7fi\u00b7den\u00b7zen", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PRF", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.6": {"line.1": {"text": "An der Wand aber hing ein stiller Hut,", "tokens": ["An", "der", "Wand", "a\u00b7ber", "hing", "ein", "stil\u00b7ler", "Hut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADV", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "dem waren die Kleider gar nicht gut.", "tokens": ["dem", "wa\u00b7ren", "die", "Klei\u00b7der", "gar", "nicht", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": "\u00bberz\u00e4hl was, Hut! Erz\u00e4hl uns was!\u00ab", "tokens": ["\u00bb", "er\u00b7z\u00e4hl", "was", ",", "Hut", "!", "Er\u00b7z\u00e4hl", "uns", "was", "!", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "PIS", "$,", "NN", "$.", "NN", "PPER", "PIS", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Hut aber sprach verlegen: \u00bbDas \u2013", "tokens": ["Der", "Hut", "a\u00b7ber", "sprach", "ver\u00b7le\u00b7gen", ":", "\u00bb", "Das", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVFIN", "VVINF", "$.", "$(", "ART", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "das wird nicht gehn.", "tokens": ["das", "wird", "nicht", "gehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Ich armer Tropf", "tokens": ["Ich", "ar\u00b7mer", "Tropf"], "token_info": ["word", "word", "word"], "pos": ["PPER", "ADJA", "NN"], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "ich sitze n\u00e4mlich bei dem auf dem Kopf.", "tokens": ["ich", "sit\u00b7ze", "n\u00e4m\u00b7lich", "bei", "dem", "auf", "dem", "Kopf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und so hab ich, ihr m\u00fc\u00dft mich nicht weiter qu\u00e4len,", "tokens": ["Und", "so", "hab", "ich", ",", "ihr", "m\u00fc\u00dft", "mich", "nicht", "wei\u00b7ter", "qu\u00e4\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "$,", "PPER", "VMFIN", "PPER", "PTKNEG", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.7": {"text": "nicht das geringste zu erz\u00e4hlen \u2013!\u00ab", "tokens": ["nicht", "das", "ge\u00b7rings\u00b7te", "zu", "er\u00b7z\u00e4h\u00b7len", "\u2013", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PTKNEG", "ART", "ADJA", "PTKZU", "VVINF", "$(", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}