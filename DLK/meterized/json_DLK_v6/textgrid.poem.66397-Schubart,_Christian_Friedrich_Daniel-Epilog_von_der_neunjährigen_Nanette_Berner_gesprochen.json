{"textgrid.poem.66397": {"metadata": {"author": {"name": "Schubart, Christian Friedrich Daniel", "birth": "N.A.", "death": "N.A."}, "title": "Epilog von der neunj\u00e4hrigen Nanette Berner gesprochen", "genre": "verse", "period": "N.A.", "pub_year": 1775, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Schon lange irrt mit Augen voll von Thr\u00e4nen,", "tokens": ["Schon", "lan\u00b7ge", "irrt", "mit", "Au\u00b7gen", "voll", "von", "Thr\u00e4\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "APPR", "NN", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Thalia in der Deutschen Eichenhain,", "tokens": ["Tha\u00b7lia", "in", "der", "Deut\u00b7schen", "Ei\u00b7chen\u00b7hain", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Und ihre Barden, voll Gef\u00fchl des Sch\u00f6nen,", "tokens": ["Und", "ih\u00b7re", "Bar\u00b7den", ",", "voll", "Ge\u00b7f\u00fchl", "des", "Sch\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "$,", "ADJD", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Bejammern laut im Mondenschein.", "tokens": ["Be\u00b7jam\u00b7mern", "laut", "im", "Mon\u00b7den\u00b7schein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Das Schicksal ihrer ewigen Gedichte,", "tokens": ["Das", "Schick\u00b7sal", "ih\u00b7rer", "e\u00b7wi\u00b7gen", "Ge\u00b7dich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Drin Deutschlands Feuermuse flammt,", "tokens": ["Drin", "Deutschlands", "Feu\u00b7er\u00b7mu\u00b7se", "flammt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und die der H\u00f6fling oft mit h\u00f6hnischem Gesichte", "tokens": ["Und", "die", "der", "H\u00f6f\u00b7ling", "oft", "mit", "h\u00f6h\u00b7ni\u00b7schem", "Ge\u00b7sich\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ART", "NN", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und gallisch-seichtem Witz verdammt.", "tokens": ["Und", "gal\u00b7lischseich\u00b7tem", "Witz", "ver\u00b7dammt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Selbst Lessing, der im hohen Trauerspiele", "tokens": ["Selbst", "Les\u00b7sing", ",", "der", "im", "ho\u00b7hen", "Trau\u00b7er\u00b7spie\u00b7le"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "NE", "$,", "PRELS", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und bald im Lustspiel Tugend lehrt,", "tokens": ["Und", "bald", "im", "Lust\u00b7spiel", "Tu\u00b7gend", "lehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPRART", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "War beim franz\u00f6sischen Berlockenspiele", "tokens": ["War", "beim", "fran\u00b7z\u00f6\u00b7si\u00b7schen", "Ber\u00b7lo\u00b7cken\u00b7spie\u00b7le"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPRART", "ADJA", "NN"], "meter": "-+-+-++--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Vom Witzling \u00fcberh\u00f6rt.", "tokens": ["Vom", "Witz\u00b7ling", "\u00fc\u00b7ber\u00b7h\u00f6rt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Im G\u00f6z beleidigt ihn der gro\u00dfe,", "tokens": ["Im", "G\u00f6z", "be\u00b7lei\u00b7digt", "ihn", "der", "gro\u00b7\u00dfe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der wahre deutsche Urgeschmack.", "tokens": ["Der", "wah\u00b7re", "deut\u00b7sche", "Ur\u00b7ge\u00b7schmack", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Kaum tritt er auf, so greift er nach der Dose", "tokens": ["Kaum", "tritt", "er", "auf", ",", "so", "greift", "er", "nach", "der", "Do\u00b7se"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$,", "ADV", "VVFIN", "PPER", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und nimmt gar zierlich Schnupftabak.", "tokens": ["Und", "nimmt", "gar", "zier\u00b7lich", "Schnupf\u00b7ta\u00b7bak", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADJD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Beim Wei\u00dfe schl\u00e4ft er ein und schnarcht beim sanften Hiller,", "tokens": ["Beim", "Wei\u00b7\u00dfe", "schl\u00e4ft", "er", "ein", "und", "schnarcht", "beim", "sanf\u00b7ten", "Hil\u00b7ler", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "PTKVZ", "KON", "VVFIN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und Wieland \u2013 Schweizern spricht er Hohn.", "tokens": ["Und", "Wie\u00b7land", "\u2013", "Schwei\u00b7zern", "spricht", "er", "Hohn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$(", "NN", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Franzosenwitz, ein welscher Triller", "tokens": ["Fran\u00b7zo\u00b7sen\u00b7witz", ",", "ein", "wel\u00b7scher", "Tril\u00b7ler"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NE", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Gilt mehr, als Deutschlands Donnerton.", "tokens": ["Gilt", "mehr", ",", "als", "Deutschlands", "Don\u00b7ner\u00b7ton", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$,", "KOUS", "NE", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Wir k\u00f6nnten ", "tokens": ["Wir", "k\u00f6nn\u00b7ten"], "token_info": ["word", "word"], "pos": ["PPER", "VMFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Gott sei's gedankt, uns fehlt's nicht an Genie.", "tokens": ["Gott", "sei's", "ge\u00b7dankt", ",", "uns", "fehlt's", "nicht", "an", "Ge\u00b7nie", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "VVPP", "$,", "PPER", "VVFIN", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Doch wer ermuntert unsre Gaben", "tokens": ["Doch", "wer", "er\u00b7mun\u00b7tert", "uns\u00b7re", "Ga\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wer belohnet sie?", "tokens": ["Und", "wer", "be\u00b7loh\u00b7net", "sie", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Wie mancher J\u00fcngling k\u00f6nnte gr\u00f6\u00dfer werden,", "tokens": ["Wie", "man\u00b7cher", "J\u00fcng\u00b7ling", "k\u00f6nn\u00b7te", "gr\u00f6\u00b7\u00dfer", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIAT", "NN", "VMFIN", "ADJD", "VAINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Als Eckhof, der uns oft entz\u00fcckt gen Himmel ri\u00df!", "tokens": ["Als", "Eck\u00b7hof", ",", "der", "uns", "oft", "ent\u00b7z\u00fcckt", "gen", "Him\u00b7mel", "ri\u00df", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "$,", "PRELS", "PPER", "ADV", "VVPP", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Doch Mangel dr\u00fcckt den Geist zur Erden", "tokens": ["Doch", "Man\u00b7gel", "dr\u00fcckt", "den", "Geist", "zur", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und Armuth l\u00f6scht die Flamme des Genies.", "tokens": ["Und", "Ar\u00b7muth", "l\u00f6scht", "die", "Flam\u00b7me", "des", "Ge\u00b7nies", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Wo ist der Lohn f\u00fcr unsre deutsche Dichter?", "tokens": ["Wo", "ist", "der", "Lohn", "f\u00fcr", "uns\u00b7re", "deut\u00b7sche", "Dich\u00b7ter", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ist nicht die gro\u00dfe Welt bei ihren Klagen taub?", "tokens": ["Ist", "nicht", "die", "gro\u00b7\u00dfe", "Welt", "bei", "ih\u00b7ren", "Kla\u00b7gen", "taub", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "ART", "ADJA", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Undeutsche Weichlinge sind ihre Richter,", "tokens": ["Un\u00b7deut\u00b7sche", "Weich\u00b7lin\u00b7ge", "sind", "ih\u00b7re", "Rich\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "-+-++--+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Ihr ganzer Lohn ein Kranz von Eichenlaub.", "tokens": ["Ihr", "gan\u00b7zer", "Lohn", "ein", "Kranz", "von", "Ei\u00b7chen\u00b7laub", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Jedoch erheitre jetzt, Thalia, deine Blicke;", "tokens": ["Je\u00b7doch", "er\u00b7hei\u00b7tre", "jetzt", ",", "Tha\u00b7lia", ",", "dei\u00b7ne", "Bli\u00b7cke", ";"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "$,", "NE", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Schau um dich her, es gl\u00e4nzet schon", "tokens": ["Schau", "um", "dich", "her", ",", "es", "gl\u00e4n\u00b7zet", "schon"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "APPR", "PPER", "PTKVZ", "$,", "PPER", "VVFIN", "ADV"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Der Schauspielkunst ein ungewohntes Gl\u00fccke", "tokens": ["Der", "Schau\u00b7spiel\u00b7kunst", "ein", "un\u00b7ge\u00b7wohn\u00b7tes", "Gl\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Vom deutschen Helikon.", "tokens": ["Vom", "deut\u00b7schen", "He\u00b7li\u00b7kon", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Hier steht die Zierde gro\u00dfer Schwaben!", "tokens": ["Hier", "steht", "die", "Zier\u00b7de", "gro\u00b7\u00dfer", "Schwa\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Erhabnen Beifall l\u00e4cheln Sie!", "tokens": ["Er\u00b7hab\u00b7nen", "Bei\u00b7fall", "l\u00e4\u00b7cheln", "Sie", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sie blicken hin auf jugendliche Gaben", "tokens": ["Sie", "bli\u00b7cken", "hin", "auf", "ju\u00b7gend\u00b7li\u00b7che", "Ga\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Und lohnen Keimen von Genie.", "tokens": ["Und", "loh\u00b7nen", "Kei\u00b7men", "von", "Ge\u00b7nie", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Mit welcher Huld, welch menschlichen Verschonen", "tokens": ["Mit", "wel\u00b7cher", "Huld", ",", "welch", "menschli\u00b7chen", "Ver\u00b7scho\u00b7nen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "$,", "PWAT", "ADJA", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Seh'n Sie nicht unsre Fehler ein!", "tokens": ["Seh'n", "Sie", "nicht", "uns\u00b7re", "Feh\u00b7ler", "ein", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn solche M\u00e4nner uns, als Kinder, schon belohnen,", "tokens": ["Wenn", "sol\u00b7che", "M\u00e4n\u00b7ner", "uns", ",", "als", "Kin\u00b7der", ",", "schon", "be\u00b7loh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "PPER", "$,", "KOUS", "NN", "$,", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wer w\u00fcnschte nicht bald gro\u00df zu sein?", "tokens": ["Wer", "w\u00fcnschte", "nicht", "bald", "gro\u00df", "zu", "sein", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PTKNEG", "ADV", "ADJD", "PTKZU", "VAINF", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.12": {"line.1": {"text": "Nehmt Alles hin \u2013 ihr weise Kenner,", "tokens": ["Nehmt", "Al\u00b7les", "hin", "\u2013", "ihr", "wei\u00b7se", "Ken\u00b7ner", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PTKVZ", "$(", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was Euch ein Kind \u2013 in Unschuld geben kann;", "tokens": ["Was", "Euch", "ein", "Kind", "\u2013", "in", "Un\u00b7schuld", "ge\u00b7ben", "kann", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "$(", "APPR", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Nehmt meinen Dank, erlauchte M\u00e4nner", "tokens": ["Nehmt", "mei\u00b7nen", "Dank", ",", "er\u00b7lauch\u00b7te", "M\u00e4n\u00b7ner"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und diese Thr\u00e4ne an.", "tokens": ["Und", "die\u00b7se", "Thr\u00e4\u00b7ne", "an", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PDAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}