{"dta.poem.16265": {"metadata": {"author": {"name": "R\u00fcckert, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "1.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1837", "urn": "urn:nbn:de:kobv:b4-200905195088", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Weil eben wir die Fahrt zu thun sind im Begriffe,", "tokens": ["Weil", "e\u00b7ben", "wir", "die", "Fahrt", "zu", "thun", "sind", "im", "Be\u00b7grif\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPER", "ART", "NN", "PTKZU", "VVINF", "VAFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Von der du bist gekehrt mit wohlbehaltnem Schiffe;", "tokens": ["Von", "der", "du", "bist", "ge\u00b7kehrt", "mit", "wohl\u00b7be\u00b7halt\u00b7nem", "Schif\u00b7fe", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "VAFIN", "VVPP", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "So gib Erfahrungen von dir uns zu Geleitern,", "tokens": ["So", "gib", "Er\u00b7fah\u00b7run\u00b7gen", "von", "dir", "uns", "zu", "Ge\u00b7lei\u00b7tern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "NN", "APPR", "PPER", "PRF", "APPR", "NN", "$,"], "meter": "-+-+----+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Damit wir sicher sind, an Klippen nicht zu scheitern.", "tokens": ["Da\u00b7mit", "wir", "si\u00b7cher", "sind", ",", "an", "Klip\u00b7pen", "nicht", "zu", "schei\u00b7tern", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "VAFIN", "$,", "APPR", "NN", "PTKNEG", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Denn schwierig ist die Fahrt, so sagt man, und gefahrvoll,", "tokens": ["Denn", "schwie\u00b7rig", "ist", "die", "Fahrt", ",", "so", "sagt", "man", ",", "und", "ge\u00b7fahr\u00b7voll", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "$,", "ADV", "VVFIN", "PIS", "$,", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und unternehmen soll ein Mann sie fein gewahrvoll. \u2014", "tokens": ["Und", "un\u00b7ter\u00b7neh\u00b7men", "soll", "ein", "Mann", "sie", "fein", "ge\u00b7wahr\u00b7voll", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVINF", "VMFIN", "ART", "NN", "PPER", "ADJD", "ADJD", "$.", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "So nehmet meinen Rath! wol braucht hier Rath ein Mann;", "tokens": ["So", "neh\u00b7met", "mei\u00b7nen", "Rath", "!", "wol", "braucht", "hier", "Rath", "ein", "Mann", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPOSAT", "NN", "$.", "ADV", "VVFIN", "ADV", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Doch wi\u00dft, da\u00df keinen Rath man hier gebrauchen kann.", "tokens": ["Doch", "wi\u00dft", ",", "da\u00df", "kei\u00b7nen", "Rath", "man", "hier", "ge\u00b7brau\u00b7chen", "kann", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUS", "PIAT", "NN", "PIS", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Wen nicht das Gl\u00fcck ber\u00e4th, wer sich nicht kann berathen,", "tokens": ["Wen", "nicht", "das", "Gl\u00fcck", "be\u00b7r\u00e4\u00b7th", ",", "wer", "sich", "nicht", "kann", "be\u00b7ra\u00b7then", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "ART", "NN", "VVFIN", "$,", "PWS", "PRF", "PTKNEG", "VMFIN", "VVINF", "$,"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Mit keinerlei Ger\u00e4th wird ihm die Fahrt gerathen.", "tokens": ["Mit", "kei\u00b7ner\u00b7lei", "Ge\u00b7r\u00e4\u00b7th", "wird", "ihm", "die", "Fahrt", "ge\u00b7ra\u00b7then", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VAFIN", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.6": {"line.1": {"text": "Die Wege sind so breit, wer schief kommt, kommt so schief;", "tokens": ["Die", "We\u00b7ge", "sind", "so", "breit", ",", "wer", "schief", "kommt", ",", "kommt", "so", "schief", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$,", "PWS", "ADJD", "VVFIN", "$,", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der Abgrund ist so weit, wer f\u00e4llt, der f\u00e4llt so tief.", "tokens": ["Der", "Ab\u00b7grund", "ist", "so", "weit", ",", "wer", "f\u00e4llt", ",", "der", "f\u00e4llt", "so", "tief", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$,", "PWS", "VVFIN", "$,", "PRELS", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "So viele Schiffe schon gefahren diese Strassen,", "tokens": ["So", "vie\u00b7le", "Schif\u00b7fe", "schon", "ge\u00b7fah\u00b7ren", "die\u00b7se", "Stras\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "ADV", "VVPP", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Hat keines hinter sich ein Fahrgeleis gelassen.", "tokens": ["Hat", "kei\u00b7nes", "hin\u00b7ter", "sich", "ein", "Fahr\u00b7ge\u00b7leis", "ge\u00b7las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "APPR", "PRF", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Sie zogen eine Spur solang nur als sie fuhren,", "tokens": ["Sie", "zo\u00b7gen", "ei\u00b7ne", "Spur", "so\u00b7lang", "nur", "als", "sie", "fuh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "VMFIN", "ADV", "KOUS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und wer nach ihnen fuhr, zog wieder andre Spuren;", "tokens": ["Und", "wer", "nach", "ih\u00b7nen", "fuhr", ",", "zog", "wie\u00b7der", "and\u00b7re", "Spu\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPR", "PPER", "VVFIN", "$,", "VVFIN", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Die, wann er ist vorbei, im Glatten wieder schwinden;", "tokens": ["Die", ",", "wann", "er", "ist", "vor\u00b7bei", ",", "im", "Glat\u00b7ten", "wie\u00b7der", "schwin\u00b7den", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "PWAV", "PPER", "VAFIN", "ADV", "$,", "APPRART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und jedem steht es frei, stets eignen Weg zu finden.", "tokens": ["Und", "je\u00b7dem", "steht", "es", "frei", ",", "stets", "eig\u00b7nen", "Weg", "zu", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "PPER", "ADJD", "$,", "ADV", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Versehn ist dieser Weg mit keinen Meilenzeigern,", "tokens": ["Ver\u00b7sehn", "ist", "die\u00b7ser", "Weg", "mit", "kei\u00b7nen", "Mei\u00b7len\u00b7zei\u00b7gern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAFIN", "PDAT", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Als nur mit Sternen, die die Anzeig' oft verweigern.", "tokens": ["Als", "nur", "mit", "Ster\u00b7nen", ",", "die", "die", "An\u00b7zeig'", "oft", "ver\u00b7wei\u00b7gern", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "APPR", "NN", "$,", "PRELS", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.11": {"line.1": {"text": "Zwar mit Marksteinen ist des Weges Rand besetzt,", "tokens": ["Zwar", "mit", "Mark\u00b7stei\u00b7nen", "ist", "des", "We\u00b7ges", "Rand", "be\u00b7setzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "VAFIN", "ART", "NN", "NN", "VVPP", "$,"], "meter": "--++-+-+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Doch merkt dein Rad sie nicht, bis es sich dran verletzt.", "tokens": ["Doch", "merkt", "dein", "Rad", "sie", "nicht", ",", "bis", "es", "sich", "dran", "ver\u00b7letzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "PPER", "PTKNEG", "$,", "KOUS", "PPER", "PRF", "PAV", "VVPP", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.12": {"line.1": {"text": "Ein h\u00f6lzern R\u00f6sslein rennt auf endlos gr\u00fcnen R\u00e4umen,", "tokens": ["Ein", "h\u00f6l\u00b7zern", "R\u00f6ss\u00b7lein", "rennt", "auf", "end\u00b7los", "gr\u00fc\u00b7nen", "R\u00e4u\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPR", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ihm w\u00e4chst kein H\u00e4lmchen Gras, es wird nur satt von Sch\u00e4umen.", "tokens": ["Ihm", "w\u00e4chst", "kein", "H\u00e4lm\u00b7chen", "Gras", ",", "es", "wird", "nur", "satt", "von", "Sch\u00e4u\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "ADJA", "NN", "$,", "PPER", "VAFIN", "ADV", "ADJD", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.13": {"line.1": {"text": "An Wasser fehlt es nicht zur Rechten noch zur Linken,", "tokens": ["An", "Was\u00b7ser", "fehlt", "es", "nicht", "zur", "Rech\u00b7ten", "noch", "zur", "Lin\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "PTKNEG", "APPRART", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Zum Trinken ist es nicht, es ist nur zum Ertrinken.", "tokens": ["Zum", "Trin\u00b7ken", "ist", "es", "nicht", ",", "es", "ist", "nur", "zum", "Er\u00b7trin\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "PTKNEG", "$,", "PPER", "VAFIN", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.14": {"line.1": {"text": "Du wei\u00dft nicht, ob der Weg wird steil seyn oder eben,", "tokens": ["Du", "wei\u00dft", "nicht", ",", "ob", "der", "Weg", "wird", "steil", "seyn", "o\u00b7der", "e\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "KOUS", "ART", "NN", "VAFIN", "ADJD", "VAINF", "KON", "ADV", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da nach Gefallen er sich senken kann und heben.", "tokens": ["Da", "nach", "Ge\u00b7fal\u00b7len", "er", "sich", "sen\u00b7ken", "kann", "und", "he\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "PPER", "PRF", "VVINF", "VMFIN", "KON", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Was hilfts, ausf\u00fchrlich dir das Fahrnis zu beschreiben?", "tokens": ["Was", "hilfts", ",", "aus\u00b7f\u00fchr\u00b7lich", "dir", "das", "Fahr\u00b7nis", "zu", "be\u00b7schrei\u00b7ben", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "ADJD", "PPER", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Erfahr es selbst, wenn du nicht willst zu Hause bleiben.", "tokens": ["Er\u00b7fahr", "es", "selbst", ",", "wenn", "du", "nicht", "willst", "zu", "Hau\u00b7se", "blei\u00b7ben", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "KOUS", "PPER", "PTKNEG", "VMFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}