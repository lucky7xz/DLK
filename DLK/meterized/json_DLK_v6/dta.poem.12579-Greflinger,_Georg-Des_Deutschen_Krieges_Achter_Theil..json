{"dta.poem.12579": {"metadata": {"author": {"name": "Greflinger, Georg", "birth": "N.A.", "death": "N.A."}, "title": "Des  \n Deutschen Krieges  \n Achter Theil.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1657", "urn": "urn:nbn:de:kobv:b4-200905199036", "language": ["de:0.99"], "booktitle": "Celadon von der Donau [i. e. Greflinger, Georg]: Der Deutschen Drey\u00dfig-J\u00e4hriger Krjeg. [s. l.], 1657."}, "poem": {"stanza.1": {"line.1": {"text": "Eh ich nun wiederum den Kriegs-gelehrten Schwe-\nden", "tokens": ["Eh", "ich", "nun", "wie\u00b7de\u00b7rum", "den", "Kriegs\u00b7ge\u00b7lehr\u00b7ten", "Schwe", "den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "ADV", "ART", "NN", "TRUNC", "ART"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "In jhrem Zuge folg\u2019/ erheischt die Zeit zu reden/", "tokens": ["In", "jhrem", "Zu\u00b7ge", "fol\u00b7g'", "/", "er\u00b7heischt", "die", "Zeit", "zu", "re\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "PTKVZ", "$(", "VVFIN", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Wie sich die Weymar-Macht nunmehr durch Franckreichs", "tokens": ["Wie", "sich", "die", "Wey\u00b7ma\u00b7rMacht", "nun\u00b7mehr", "durch", "Fran\u00b7ck\u00b7reichs"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PRF", "ART", "NN", "ADV", "APPR", "NE"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Stab", "tokens": ["Stab"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.5": {"text": "Von Volck und Gold gesch\u00fctzt/ am Reyne sich gehab\u2019.", "tokens": ["Von", "Volck", "und", "Gold", "ge\u00b7sch\u00fctzt", "/", "am", "Rey\u00b7ne", "sich", "ge\u00b7hab'", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVPP", "$(", "APPRART", "NN", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Es wuste Franckreichs Heer/ da\u00df unser gro\u00dfer K\u00e4yser/", "tokens": ["Es", "wus\u00b7te", "Fran\u00b7ck\u00b7reichs", "Heer", "/", "da\u00df", "un\u00b7ser", "gro\u00b7\u00dfer", "K\u00e4y\u00b7ser", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "NN", "$(", "KOUS", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "Nach dem er durch den Sieg die meisten F\u00fcrsten-H\u00e4user", "tokens": ["Nach", "dem", "er", "durch", "den", "Sieg", "die", "meis\u00b7ten", "F\u00fcrs\u00b7ten\u00b7H\u00e4u\u00b7ser"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "APPR", "ART", "NN", "ART", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Jhm wieder pflichtig hatt\u2019/ auf die/ so jhre Hand", "tokens": ["Jhm", "wie\u00b7der", "pflich\u00b7tig", "hatt'", "/", "auf", "die", "/", "so", "jhre", "Hand"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ADV", "ADJD", "VAFIN", "$(", "APPR", "ART", "$(", "ADV", "PPOSAT", "NN"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Mit hatten eingemischt/ und unser Deutsches Land", "tokens": ["Mit", "hat\u00b7ten", "ein\u00b7ge\u00b7mischt", "/", "und", "un\u00b7ser", "Deut\u00b7sches", "Land"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "VAFIN", "VVPP", "$(", "KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "befeindet/ ohne Rach sehr schwerlich w\u00fcrde bleiben/", "tokens": ["be\u00b7fein\u00b7det", "/", "oh\u00b7ne", "Rach", "sehr", "schwer\u00b7lich", "w\u00fcr\u00b7de", "blei\u00b7ben", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "APPR", "NN", "ADV", "ADJD", "VAFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "und darumb sandt\u2019 er Hilff dergleichen abzutreiben.", "tokens": ["und", "da\u00b7rumb", "sandt'", "er", "Hilff", "derg\u00b7lei\u00b7chen", "ab\u00b7zu\u00b7trei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "PPER", "NN", "PIS", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Es war zu seinem Schirm und auch zu gr\u00f6\u00dfrer Macht", "tokens": ["Es", "war", "zu", "sei\u00b7nem", "Schirm", "und", "auch", "zu", "gr\u00f6\u00df\u00b7rer", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "PPOSAT", "NN", "KON", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Der Jenen/ denen er verbindlich war geacht/", "tokens": ["Der", "Je\u00b7nen", "/", "de\u00b7nen", "er", "ver\u00b7bind\u00b7lich", "war", "ge\u00b7acht", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELS", "PPER", "ADJD", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Die durch den gro\u00dfen Schlag bey N\u00f6rdlingen empfangen/", "tokens": ["Die", "durch", "den", "gro\u00b7\u00dfen", "Schlag", "bey", "N\u00f6rd\u00b7lin\u00b7gen", "emp\u00b7fan\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "ADJA", "NN", "APPR", "NN", "VVPP", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.15": {"text": "In weit zerstreuter Art was waren umgegangen.", "tokens": ["In", "weit", "zer\u00b7streu\u00b7ter", "Art", "was", "wa\u00b7ren", "um\u00b7ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "ADJA", "NN", "PWS", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Die F\u00fchrer dieser Hilff aus Franckreich waren/ ", "tokens": ["Die", "F\u00fch\u00b7rer", "die\u00b7ser", "Hilff", "aus", "Fran\u00b7ck\u00b7reich", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PDAT", "NN", "APPR", "NE", "VAFIN", "$("], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.17": {"text": "La Force, Feld-Marschalck/ und neben jhm Brez&#233;.", "tokens": ["La", "For\u00b7ce", ",", "Feld\u00b7Mar\u00b7schalck", "/", "und", "ne\u00b7ben", "jhm", "Brez", "&#233;", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "XML_entity", "punct"], "pos": ["NE", "NE", "$,", "NN", "$(", "KON", "APPR", "PPER", "NE", "$(", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.18": {"text": "Es mochte sich jhr Volck mit Hertzog Bernhards Hauffen", "tokens": ["Es", "moch\u00b7te", "sich", "jhr", "Volck", "mit", "Hert\u00b7zog", "Bern\u00b7hards", "Hauf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "PPOSAT", "NN", "APPR", "NE", "NE", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Auf zwantzig tausend Mann und etwas mehr belauffen.", "tokens": ["Auf", "zwant\u00b7zig", "tau\u00b7send", "Mann", "und", "et\u00b7was", "mehr", "be\u00b7lauf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "CARD", "NN", "KON", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Ein Heer von solcher Zahl war nun die gr\u00f6ste Macht/", "tokens": ["Ein", "Heer", "von", "sol\u00b7cher", "Zahl", "war", "nun", "die", "gr\u00f6s\u00b7te", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PIAT", "NN", "VAFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "und wurde mehr hiedurch als sonst durch viel verbracht.", "tokens": ["und", "wur\u00b7de", "mehr", "hie\u00b7durch", "als", "sonst", "durch", "viel", "ver\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "PAV", "KOKOM", "ADV", "APPR", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Das H\u00e4upt von diesem Heer war Bernhard/ der die Br\u00fc-", "tokens": ["Das", "H\u00e4upt", "von", "die\u00b7sem", "Heer", "war", "Bern\u00b7hard", "/", "der", "die", "Br\u00fc"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "PDAT", "NN", "VAFIN", "NE", "$(", "ART", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Bey Mannheim \u00fcbergieng/ nach Darmstadt zu zu r\u00fccken/", "tokens": ["Bey", "Mann\u00b7heim", "\u00fc\u00b7berg\u00b7ieng", "/", "nach", "Darm\u00b7stadt", "zu", "zu", "r\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$(", "APPR", "NE", "PTKZU", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Wo er mit seinem Heer Ruh und Erquickung nahm/", "tokens": ["Wo", "er", "mit", "sei\u00b7nem", "Heer", "Ruh", "und", "Er\u00b7quic\u00b7kung", "nahm", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "PPOSAT", "NN", "NN", "KON", "NN", "VVFIN", "$("], "meter": "-+-+-++--+-+", "measure": "iambic.hexa.relaxed"}, "line.25": {"text": "Bald aber weiter gieng und recht durch Franckfurt kam/", "tokens": ["Bald", "a\u00b7ber", "wei\u00b7ter", "gieng", "und", "recht", "durch", "Fran\u00b7ck\u00b7furt", "kam", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "VVFIN", "KON", "ADJD", "APPR", "NE", "VVFIN", "$("], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.26": {"text": "Auf Man\u00dffelds seine Macht in Wetterauschen Pl\u00e4tzen", "tokens": ["Auf", "Man\u00df\u00b7felds", "sei\u00b7ne", "Macht", "in", "Wet\u00b7te\u00b7rau\u00b7schen", "Pl\u00e4t\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "PPOSAT", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Gelagert und verpftegt/ gewaltig anzus\u00e4tzen.", "tokens": ["Ge\u00b7la\u00b7gert", "und", "ver\u00b7pf\u00b7tegt", "/", "ge\u00b7wal\u00b7tig", "an\u00b7zu\u00b7s\u00e4t\u00b7zen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVFIN", "$(", "ADJD", "VVIZU", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.28": {"text": "So bald als sich sein Feind jhm in die Augen gab", "tokens": ["So", "bald", "als", "sich", "sein", "Feind", "jhm", "in", "die", "Au\u00b7gen", "gab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "KOUS", "PRF", "PPOSAT", "NN", "PPER", "APPR", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Schlug er bey Michelbach zwey Regimenter ab.", "tokens": ["Schlug", "er", "bey", "Mi\u00b7chel\u00b7bach", "zwey", "Re\u00b7gi\u00b7men\u00b7ter", "ab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NE", "CARD", "NN", "PTKVZ", "$."], "meter": "+-+-+-+----+", "measure": "unknown.measure.penta"}, "line.30": {"text": "Es wurd\u2019 auch ein Qnartier bey W\u00e4chterbach geschlagen/", "tokens": ["Es", "wurd'", "auch", "ein", "Qnar\u00b7tier", "bey", "W\u00e4ch\u00b7ter\u00b7bach", "ge\u00b7schla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "APPR", "NE", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "und eine gro\u00dfe Zahl von F\u00fchrern/ Volck und Wagen", "tokens": ["und", "ei\u00b7ne", "gro\u00b7\u00dfe", "Zahl", "von", "F\u00fch\u00b7rern", "/", "Volck", "und", "Wa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "APPR", "NN", "$(", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "Gefangen eingebracht. Di\u00df trieb den Mann- ins Feld", "tokens": ["Ge\u00b7fan\u00b7gen", "ein\u00b7ge\u00b7bracht", ".", "Di\u00df", "trieb", "den", "Mann", "ins", "Feld"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVPP", "$.", "PDS", "VVFIN", "ART", "TRUNC", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Von langer Hungers Noth und ungeheurer K\u00e4lt", "tokens": ["Von", "lan\u00b7ger", "Hun\u00b7gers", "Noth", "und", "un\u00b7ge\u00b7heu\u00b7rer", "K\u00e4lt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Ermattet/ \u00fcbern Meyn bey Aschenburg/ zu gehen/", "tokens": ["Er\u00b7mat\u00b7tet", "/", "\u00fc\u00b7bern", "Meyn", "bey", "A\u00b7schen\u00b7burg", "/", "zu", "ge\u00b7hen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVPP", "$(", "ADV", "PPOSAT", "APPR", "NE", "$(", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "und da so lang verwahrt/ bi\u00df Rettuug kam/ zu stehen.", "tokens": ["und", "da", "so", "lang", "ver\u00b7wahrt", "/", "bi\u00df", "Ret\u00b7tu\u00b7ug", "kam", "/", "zu", "ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADJD", "VVPP", "$(", "APPR", "NN", "VVFIN", "$(", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.36": {"text": "Worauf auch alle Macht/ die noch am Necker war/", "tokens": ["Wo\u00b7rauf", "auch", "al\u00b7le", "Macht", "/", "die", "noch", "am", "Ne\u00b7cker", "war", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "PIAT", "NN", "$(", "ART", "ADV", "APPRART", "NN", "VAFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.37": {"text": "Sich nach dem Meyn und Reyhn und Man\u00dffelds seiner", "tokens": ["Sich", "nach", "dem", "Meyn", "und", "Reyhn", "und", "Man\u00df\u00b7felds", "sei\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "NN", "KON", "NN", "KON", "NN", "PPOSAT"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.38": {"text": "Schar", "tokens": ["Schar"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.39": {"text": "Begab/ des Weymars Zug und neues Gl\u00fcck zu br\u00e4chen.", "tokens": ["Be\u00b7gab", "/", "des", "Wey\u00b7mars", "Zug", "und", "neu\u00b7es", "Gl\u00fcck", "zu", "br\u00e4\u00b7chen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$(", "ART", "NN", "NN", "KON", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Es gieng auch zimlich an/ wie aus dem fernern spr\u00e4chen", "tokens": ["Es", "gieng", "auch", "zim\u00b7lich", "an", "/", "wie", "aus", "dem", "fer\u00b7nern", "spr\u00e4\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PTKVZ", "$(", "KOKOM", "APPR", "ART", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Wol sol vernommen seyn. Ob schon noch gr\u00f6sre Macht", "tokens": ["Wol", "sol", "ver\u00b7nom\u00b7men", "seyn", ".", "Ob", "schon", "noch", "gr\u00f6s\u00b7re", "Macht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "VVPP", "VAINF", "$.", "KOUS", "ADV", "ADV", "ADJA", "NN"], "meter": "++-+-+-+-+-+", "measure": "unknown.measure.septa"}, "line.42": {"text": "Durch einen Cardinal aus Franckreich war gebracht/", "tokens": ["Durch", "ei\u00b7nen", "Car\u00b7di\u00b7nal", "aus", "Fran\u00b7ck\u00b7reich", "war", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NE", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.43": {"text": "Des Hertzog Bernhards Heer zu st\u00e4rcken/ und den Gegner", "tokens": ["Des", "Hert\u00b7zog", "Bern\u00b7hards", "Heer", "zu", "st\u00e4r\u00b7cken", "/", "und", "den", "Geg\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NE", "NE", "NN", "PTKZU", "VVINF", "$(", "KON", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.44": {"text": "Gewaltig ob zu seyn/ war doch des Sieges-Segner", "tokens": ["Ge\u00b7wal\u00b7tig", "ob", "zu", "seyn", "/", "war", "doch", "des", "Sie\u00b7ges\u00b7Seg\u00b7ner"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "KOUS", "PTKZU", "VAINF", "$(", "VAFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.45": {"text": "Dem allen abgethan. Sie musten \u00fcber Reyhn/", "tokens": ["Dem", "al\u00b7len", "ab\u00b7ge\u00b7than", ".", "Sie", "mus\u00b7ten", "\u00fc\u00b7ber", "Reyhn", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVPP", "$.", "PPER", "VMFIN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.46": {"text": "und nahmen sie daselbst das St\u00e4dtlein Bingen ein/", "tokens": ["und", "nah\u00b7men", "sie", "da\u00b7selbst", "das", "St\u00e4dt\u00b7lein", "Bin\u00b7gen", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PAV", "ART", "NN", "NN", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.47": {"text": "Meyntz/ Creutznach/ Oppenheim und Franckenthal zu sch\u00fc-", "tokens": ["Meyntz", "/", "Creutz\u00b7nach", "/", "Op\u00b7pen\u00b7heim", "und", "Fran\u00b7cken\u00b7thal", "zu", "sch\u00fc"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$(", "NE", "$(", "NE", "KON", "NN", "APPR", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.48": {"text": "So kam hergegen Worms (da unl\u00e4ngst noch ein sitzen", "tokens": ["So", "kam", "her\u00b7ge\u00b7gen", "Worms", "(", "da", "un\u00b7l\u00e4ngst", "noch", "ein", "sit\u00b7zen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "NN", "$(", "ADV", "ADV", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.49": {"text": "Von weisen leuthen war/ zu rathen/ wie der Krieg", "tokens": ["Von", "wei\u00b7sen", "leu\u00b7then", "war", "/", "zu", "ra\u00b7then", "/", "wie", "der", "Krieg"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VAFIN", "$(", "PTKZU", "VVINF", "$(", "KOKOM", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.50": {"text": "Dem K\u00e4yser anzuthun) in unsers K\u00e4ysers Sieg.", "tokens": ["Dem", "K\u00e4y\u00b7ser", "an\u00b7zu\u00b7thun", ")", "in", "un\u00b7sers", "K\u00e4y\u00b7sers", "Sieg", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$(", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.51": {"text": "Dem K\u00e4ysers-lutern folgt\u2019/ ob schon nach langem Fechten/", "tokens": ["Dem", "K\u00e4y\u00b7ser\u00b7slu\u00b7tern", "folgt'", "/", "ob", "schon", "nach", "lan\u00b7gem", "Fech\u00b7ten", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "KOUS", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.52": {"text": "Und muste Hatzfeldt hier bey seinen eignen Knechten", "tokens": ["Und", "mus\u00b7te", "Hatz\u00b7feldt", "hier", "bey", "sei\u00b7nen", "eig\u00b7nen", "Knech\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "ADV", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.53": {"text": "Gewalt thun/ diese Stadt gewaltig\u2019 anzugehn/", "tokens": ["Ge\u00b7walt", "thun", "/", "die\u00b7se", "Stadt", "ge\u00b7wal\u00b7tig'", "an\u00b7zu\u00b7gehn", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "$(", "PDAT", "NN", "ADJD", "VVIZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.54": {"text": "Man w\u00fcrgte/ was man hatt\u2019 in Waffen angesehn.", "tokens": ["Man", "w\u00fcrg\u00b7te", "/", "was", "man", "hatt'", "in", "Waf\u00b7fen", "an\u00b7ge\u00b7sehn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "$(", "PWS", "PIS", "VAFIN", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.55": {"text": "Hierauf begunte man auch Meyntz zu \u00fcberkommen/", "tokens": ["Hier\u00b7auf", "be\u00b7gun\u00b7te", "man", "auch", "Meyntz", "zu", "\u00fc\u00b7ber\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIS", "ADV", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.56": {"text": "Valette aber hatt\u2019 es gl\u00fccklich unternommen.", "tokens": ["Va\u00b7let\u00b7te", "a\u00b7ber", "hatt'", "es", "gl\u00fcck\u00b7lich", "un\u00b7ter\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "VAFIN", "PPER", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.57": {"text": "Er kam mit Macht d\u00e4hin/ des Man\u00dffelds seine Macht/", "tokens": ["Er", "kam", "mit", "Macht", "d\u00e4\u00b7hin", "/", "des", "Man\u00df\u00b7felds", "sei\u00b7ne", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "PTKVZ", "$(", "ART", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.58": {"text": "Die diese Stadt umgab/ in offentliche Schlacht", "tokens": ["Die", "die\u00b7se", "Stadt", "um\u00b7gab", "/", "in", "of\u00b7fent\u00b7li\u00b7che", "Schlacht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "PDAT", "NN", "VVFIN", "$(", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+---+", "measure": "unknown.measure.penta"}, "line.59": {"text": "Zu zwingen/ aber sie gieng nach des Gallas Scharen/", "tokens": ["Zu", "zwin\u00b7gen", "/", "a\u00b7ber", "sie", "gieng", "nach", "des", "Gal\u00b7las", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "PPER", "VVFIN", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.60": {"text": "Die l\u00e4ngst den Reyn bey Worms in guten Schantzen wa-", "tokens": ["Die", "l\u00e4ngst", "den", "Reyn", "bey", "Worms", "in", "gu\u00b7ten", "Schant\u00b7zen", "wa"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ART", "NN", "APPR", "NN", "APPR", "ADJA", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.61": {"text": "Und nun eugte sich am Meyn ein andrer Krieg.", "tokens": ["Und", "nun", "eug\u00b7te", "sich", "am", "Meyn", "ein", "an\u00b7drer", "Krieg", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PRF", "APPRART", "PPOSAT", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.62": {"text": "Es lenckte Franckfurt sich nach seines K\u00e4ysers Sieg", "tokens": ["Es", "lenck\u00b7te", "Fran\u00b7ck\u00b7furt", "sich", "nach", "sei\u00b7nes", "K\u00e4y\u00b7sers", "Sieg"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "PRF", "APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.63": {"text": "und lie\u00df von Schweden ab/ wor\u00fcber sich die Schweden/", "tokens": ["und", "lie\u00df", "von", "Schwe\u00b7den", "ab", "/", "wo\u00b7r\u00fc\u00b7ber", "sich", "die", "Schwe\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "NE", "PTKVZ", "$(", "PWAV", "PRF", "ART", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.64": {"text": "In Sachsenhausen fest/ mit vielen gro\u00dfen Reden", "tokens": ["In", "Sach\u00b7sen\u00b7hau\u00b7sen", "fest", "/", "mit", "vie\u00b7len", "gro\u00b7\u00dfen", "Re\u00b7den"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "PTKVZ", "$(", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.65": {"text": "Beklagten. Es gebrach an Kraut/ an loht und Brod/", "tokens": ["Be\u00b7klag\u00b7ten", ".", "Es", "ge\u00b7brach", "an", "Kraut", "/", "an", "loht", "und", "Brod", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "VVFIN", "APPR", "NN", "$(", "APPR", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.66": {"text": "und darum suchten sie/ da\u00df Franckfurt/ solche Noth", "tokens": ["und", "da\u00b7rum", "such\u00b7ten", "sie", "/", "da\u00df", "Fran\u00b7ck\u00b7furt", "/", "sol\u00b7che", "Noth"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["KON", "PAV", "VVFIN", "PPER", "$(", "KOUS", "NE", "$(", "PIAT", "NN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.67": {"text": "Betrachtend/ helffen m\u00f6cht/ um jhrer lo\u00df zu kommen.", "tokens": ["Be\u00b7trach\u00b7tend", "/", "helf\u00b7fen", "m\u00f6cht", "/", "um", "jhrer", "lo\u00df", "zu", "kom\u00b7men", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "VVINF", "VMFIN", "$(", "APPR", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+----+-+-", "measure": "unknown.measure.tetra"}, "line.68": {"text": "Wir haben/ sagte sie/ uns g\u00e4ntzlich vorgenommen", "tokens": ["Wir", "ha\u00b7ben", "/", "sag\u00b7te", "sie", "/", "uns", "g\u00e4ntz\u00b7lich", "vor\u00b7ge\u00b7nom\u00b7men"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "$(", "VVFIN", "PPER", "$(", "PPER", "ADJD", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.69": {"text": "Von Sachsenhausen ab und zur Armee zu gehn/", "tokens": ["Von", "Sach\u00b7sen\u00b7hau\u00b7sen", "ab", "und", "zur", "Ar\u00b7mee", "zu", "gehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PTKVZ", "KON", "APPRART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.70": {"text": "Weil keine Hilff/ so wir beschossen seyn/ zu sehn.", "tokens": ["Weil", "kei\u00b7ne", "Hilff", "/", "so", "wir", "be\u00b7schos\u00b7sen", "seyn", "/", "zu", "sehn", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "$(", "ADV", "PPER", "VVPP", "VAINF", "$(", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.71": {"text": "Verhelfft uns dann hiemit/ da\u00df wir auf unsern Wegen", "tokens": ["Ver\u00b7helfft", "uns", "dann", "hie\u00b7mit", "/", "da\u00df", "wir", "auf", "un\u00b7sern", "We\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "PAV", "$(", "KOUS", "PPER", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.72": {"text": "Dem Feinde/ der uns trifft/ uns k\u00f6nnen widerlegen.", "tokens": ["Dem", "Fein\u00b7de", "/", "der", "uns", "trifft", "/", "uns", "k\u00f6n\u00b7nen", "wi\u00b7der\u00b7le\u00b7gen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$(", "PRELS", "PPER", "VVFIN", "$(", "PPER", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.73": {"text": "Wol/ Franckfurt schaffte Raht. Sie aber/ die den Raht", "tokens": ["Wol", "/", "Fran\u00b7ck\u00b7furt", "schaff\u00b7te", "Raht", ".", "Sie", "a\u00b7ber", "/", "die", "den", "Raht"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$(", "NE", "ADJA", "NN", "$.", "PPER", "ADV", "$(", "ART", "ART", "NN"], "meter": "+---+-+-+-+-+", "measure": "dactylic.init"}, "line.74": {"text": "Bekamen/ schickten sich zu einer andern That/", "tokens": ["Be\u00b7ka\u00b7men", "/", "schick\u00b7ten", "sich", "zu", "ei\u00b7ner", "an\u00b7dern", "That", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "VVFIN", "PRF", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.75": {"text": "und kehrten das Gesch\u00fctz nach Franckfurts festen Mauern/", "tokens": ["und", "kehr\u00b7ten", "das", "Ge\u00b7sch\u00fctz", "nach", "Fran\u00b7ck\u00b7furts", "fes\u00b7ten", "Mau\u00b7ern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "NE", "ADJA", "NN", "$("], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.76": {"text": "Damit so kam der Wein von Freindschafft zu versauern.", "tokens": ["Da\u00b7mit", "so", "kam", "der", "Wein", "von", "Freind\u00b7schafft", "zu", "ver\u00b7sau\u00b7ern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "ART", "NN", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.77": {"text": "Was kunte Franckfurt mehr als nur dergleichen Werck", "tokens": ["Was", "kun\u00b7te", "Fran\u00b7ck\u00b7furt", "mehr", "als", "nur", "derg\u00b7lei\u00b7chen", "Werck"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VMFIN", "NE", "PIAT", "KOKOM", "ADV", "PIS", "NN"], "meter": "-+-+----+-+-+", "measure": "unknown.measure.penta"}, "line.78": {"text": "Beginnen/ wie es that. Es brauchte seiner St\u00e4rck/", "tokens": ["Be\u00b7gin\u00b7nen", "/", "wie", "es", "that", ".", "Es", "brauch\u00b7te", "sei\u00b7ner", "St\u00e4rck", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "PWAV", "PPER", "VVFIN", "$.", "PPER", "VVFIN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.79": {"text": "und bot/ was man entbot. Es kam zu offnem sch\u00fcssen/", "tokens": ["und", "bot", "/", "was", "man", "ent\u00b7bot", ".", "Es", "kam", "zu", "off\u00b7nem", "sch\u00fcs\u00b7sen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$(", "PWS", "PIS", "VVFIN", "$.", "PPER", "VVFIN", "APPR", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.80": {"text": "Und haben beyde St\u00e4dt' hiedurch viel leiden m\u00fcssen.", "tokens": ["Und", "ha\u00b7ben", "bey\u00b7de", "St\u00e4dt'", "hie\u00b7durch", "viel", "lei\u00b7den", "m\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIAT", "NN", "PAV", "ADV", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.81": {"text": "Damit erweckte man die K\u00e4yserliche Macht/", "tokens": ["Da\u00b7mit", "er\u00b7weck\u00b7te", "man", "die", "K\u00e4y\u00b7ser\u00b7li\u00b7che", "Macht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIS", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.82": {"text": "und wurd\u2019 ein gro\u00dfes Theil hierum in Stand gebracht.", "tokens": ["und", "wurd'", "ein", "gro\u00b7\u00dfes", "Theil", "hie\u00b7rum", "in", "Stand", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "ADJA", "NN", "PAV", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.83": {"text": "Die Schweden wehrten sich so viel sie immer kanten/", "tokens": ["Die", "Schwe\u00b7den", "wehr\u00b7ten", "sich", "so", "viel", "sie", "im\u00b7mer", "kan\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "PRF", "ADV", "ADV", "PPER", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.84": {"text": "Bi\u00df da\u00df die K\u00e4ysrischen zum st\u00fcrmen fertig stundten.", "tokens": ["Bi\u00df", "da\u00df", "die", "K\u00e4y\u00b7sri\u00b7schen", "zum", "st\u00fcr\u00b7men", "fer\u00b7tig", "stund\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "ART", "NN", "APPRART", "VVFIN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.85": {"text": "Da gaben sie dem Ort in des Bel\u00e4grers Hand/", "tokens": ["Da", "ga\u00b7ben", "sie", "dem", "Ort", "in", "des", "Be\u00b7l\u00e4g\u00b7rers", "Hand", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.86": {"text": "und wolten nach dem Platz Gustavusburg genannt.", "tokens": ["und", "wol\u00b7ten", "nach", "dem", "Platz", "Gus\u00b7ta\u00b7vus\u00b7burg", "ge\u00b7nannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "ART", "NN", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.87": {"text": "Man stundt es jhnen zu. Sie zogen. Schau/ da kamen", "tokens": ["Man", "stundt", "es", "jh\u00b7nen", "zu", ".", "Sie", "zo\u00b7gen", ".", "Schau", "/", "da", "ka\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "PPER", "PTKVZ", "$.", "PPER", "VVFIN", "$.", "NN", "$(", "ADV", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.88": {"text": "lamboyens V\u00f6lcker/ die sie alle r\u00fcckwertz nahmen/", "tokens": ["lam\u00b7bo\u00b7yens", "V\u00f6l\u00b7cker", "/", "die", "sie", "al\u00b7le", "r\u00fcck\u00b7wertz", "nah\u00b7men", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "$(", "PRELS", "PPER", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.89": {"text": "Die meisten nahmen Dienst. Hiemit vergolte man/", "tokens": ["Die", "meis\u00b7ten", "nah\u00b7men", "Dienst", ".", "Hie\u00b7mit", "ver\u00b7gol\u00b7te", "man", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "NN", "$.", "ADV", "VVFIN", "PIS", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.90": {"text": "Was von denselbigen an Franckfurt war gethan.", "tokens": ["Was", "von", "den\u00b7sel\u00b7bi\u00b7gen", "an", "Fran\u00b7ck\u00b7furt", "war", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PDS", "APPR", "NE", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-++-+", "measure": "unknown.measure.septa"}, "line.91": {"text": "Als Gallas sah/ wie sch\u00f6n es jhnen wolte gl\u00fccken/", "tokens": ["Als", "Gal\u00b7las", "sah", "/", "wie", "sch\u00f6n", "es", "jh\u00b7nen", "wol\u00b7te", "gl\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "VVFIN", "$(", "PWAV", "ADJD", "PPER", "PPER", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.92": {"text": "Mit Einnahm vieler St\u00e4dt\u2019/ umgab er auch Sarbr\u00fccken.", "tokens": ["Mit", "Ein\u00b7nahm", "vie\u00b7ler", "St\u00e4dt'", "/", "um\u00b7gab", "er", "auch", "Sar\u00b7br\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PIAT", "NN", "$(", "VVFIN", "PPER", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.93": {"text": "Der Ort hielt tapffer aus/ doch endlich ohn\u2019 Entsatz", "tokens": ["Der", "Ort", "hielt", "tapf\u00b7fer", "aus", "/", "doch", "end\u00b7lich", "ohn'", "Ent\u00b7satz"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADJD", "PTKVZ", "$(", "ADV", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.94": {"text": "Erm\u00fcdet/ lie\u00df er nach und gab dem Gallas Platz.", "tokens": ["Er\u00b7m\u00fc\u00b7det", "/", "lie\u00df", "er", "nach", "und", "gab", "dem", "Gal\u00b7las", "Platz", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "VVFIN", "PPER", "APPR", "KON", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.95": {"text": "Di\u00df alles \u00fcberlegt/ und wol da bey erwogen", "tokens": ["Di\u00df", "al\u00b7les", "\u00fc\u00b7ber\u00b7legt", "/", "und", "wol", "da", "bey", "er\u00b7wo\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "VVPP", "$(", "KON", "ADV", "ADV", "APPR", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.96": {"text": "(was gro\u00dfe Hungers Noth das Land hatt\u2019 \u00fcberzogen/", "tokens": ["(", "was", "gro\u00b7\u00dfe", "Hun\u00b7gers", "Noth", "das", "Land", "hatt'", "\u00fc\u00b7berz\u00b7o\u00b7gen", "/"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "ADJA", "NN", "NN", "ART", "NN", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.97": {"text": "So/ da\u00df man Wurtzeln/ Gra\u00df/ und noch viel \u00e4rgers a\u00df/", "tokens": ["So", "/", "da\u00df", "man", "Wurt\u00b7zeln", "/", "Gra\u00df", "/", "und", "noch", "viel", "\u00e4r\u00b7gers", "a\u00df", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "KOUS", "PIS", "NN", "$(", "NN", "$(", "KON", "ADV", "ADV", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.98": {"text": "Wor\u00fcber mancher Knecht der rechten Pflicht verga\u00df", "tokens": ["Wo\u00b7r\u00fc\u00b7ber", "man\u00b7cher", "Knecht", "der", "rech\u00b7ten", "Pflicht", "ver\u00b7ga\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PIAT", "NN", "ART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.99": {"text": "und von der Fahnen blieb/ vom Hunger nicht zu sterben)", "tokens": ["und", "von", "der", "Fah\u00b7nen", "blieb", "/", "vom", "Hun\u00b7ger", "nicht", "zu", "ster\u00b7ben", ")"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "$(", "APPRART", "NN", "PTKNEG", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.100": {"text": "Brach Hertzog Bernhard auf und lie\u00df die Burg verderben", "tokens": ["Brach", "Hert\u00b7zog", "Bern\u00b7hard", "auf", "und", "lie\u00df", "die", "Burg", "ver\u00b7der\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "NE", "PTKVZ", "KON", "VVFIN", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.101": {"text": "Die nach Gustavus hie\u00df/ verst\u00e4rckte Meyntz und gieng", "tokens": ["Die", "nach", "Gus\u00b7ta\u00b7vus", "hie\u00df", "/", "ver\u00b7st\u00e4rck\u00b7te", "Meyntz", "und", "gieng"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "APPR", "NE", "VVFIN", "$(", "VVFIN", "NE", "KON", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.102": {"text": "Auf tausend K\u00e4ysrische/ die er halb schlug/ halb fieng.", "tokens": ["Auf", "tau\u00b7send", "K\u00e4y\u00b7sri\u00b7sche", "/", "die", "er", "halb", "schlug", "/", "halb", "fi\u00b7eng", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "$(", "PRELS", "PPER", "ADJD", "VVFIN", "$(", "ADJD", "VVFIN", "$."], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.103": {"text": "Auf dieses st\u00e4llt\u2019 er sich nach Coblontz zu zugehen/", "tokens": ["Auf", "die\u00b7ses", "st\u00e4llt'", "er", "sich", "nach", "Cob\u00b7lontz", "zu", "zu\u00b7ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "VVFIN", "PPER", "PRF", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.104": {"text": "Da es im Grunde war nach Franckreich angesehen.", "tokens": ["Da", "es", "im", "Grun\u00b7de", "war", "nach", "Fran\u00b7ck\u00b7reich", "an\u00b7ge\u00b7se\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "VAFIN", "APPR", "NE", "VVPP", "$."], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.105": {"text": "Es brachte diese List so viel/ da\u00df er den Sprung/", "tokens": ["Es", "brach\u00b7te", "die\u00b7se", "List", "so", "viel", "/", "da\u00df", "er", "den", "Sprung", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PDAT", "NN", "ADV", "ADV", "$(", "KOUS", "PPER", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.106": {"text": "Vor Gallas nahm/ der selbst nach diesem Wege rung.", "tokens": ["Vor", "Gal\u00b7las", "nahm", "/", "der", "selbst", "nach", "die\u00b7sem", "We\u00b7ge", "rung", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VVFIN", "$(", "ART", "ADV", "APPR", "PDAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.107": {"text": "Doch es blieb viel im Stich/ und sonderlich an St\u00fccken/", "tokens": ["Doch", "es", "blieb", "viel", "im", "Stich", "/", "und", "son\u00b7der\u00b7lich", "an", "St\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "APPRART", "NN", "$(", "KON", "ADJD", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.108": {"text": "Die man begrub und viel nach Coblentz muste schicken/", "tokens": ["Die", "man", "be\u00b7grub", "und", "viel", "nach", "Cob\u00b7lentz", "mus\u00b7te", "schi\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "KON", "ADV", "APPR", "NE", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.109": {"text": "Eh sie der Feind bek\u00e4m. Es setzte Gallas nach.", "tokens": ["Eh", "sie", "der", "Feind", "be\u00b7k\u00e4m", ".", "Es", "setz\u00b7te", "Gal\u00b7las", "nach", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PTKVZ", "$.", "PPER", "VVFIN", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.110": {"text": "Sein Vorsatz aber war zum ersten viel zu schwach.", "tokens": ["Sein", "Vor\u00b7satz", "a\u00b7ber", "war", "zum", "ers\u00b7ten", "viel", "zu", "schwach", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "VAFIN", "APPRART", "ADJA", "ADV", "PTKA", "ADJD", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.111": {"text": "Man wandte sich und gab demselben so ein Zeich en/", "tokens": ["Man", "wand\u00b7te", "sich", "und", "gab", "dem\u00b7sel\u00b7ben", "so", "ein", "Zeich", "en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PRF", "KON", "VVFIN", "PDAT", "ADV", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.112": {"text": "Da\u00df er zu klagen hatt\u2019. Ein solches zu vergleichen/", "tokens": ["Da\u00df", "er", "zu", "kla\u00b7gen", "hatt'", ".", "Ein", "sol\u00b7ches", "zu", "ver\u00b7glei\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKZU", "VVINF", "VAFIN", "$.", "ART", "PIS", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.113": {"text": "Satzt er noch st\u00e4rcker nach bi\u00df einen Tag von Metz/", "tokens": ["Satzt", "er", "noch", "st\u00e4r\u00b7cker", "nach", "bi\u00df", "ei\u00b7nen", "Tag", "von", "Metz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADJD", "APPR", "APPR", "ART", "NN", "APPR", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.114": {"text": "Da gab es wiederum ein greuliches Gefetz/", "tokens": ["Da", "gab", "es", "wie\u00b7de\u00b7rum", "ein", "greu\u00b7li\u00b7ches", "Ge\u00b7fetz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.115": {"text": "und blieben heyderseits viel tapfere Soldaten.", "tokens": ["und", "blie\u00b7ben", "hey\u00b7der\u00b7seits", "viel", "tap\u00b7fe\u00b7re", "Sol\u00b7da\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.116": {"text": "Hierauf ist ", "tokens": ["Hier\u00b7auf", "ist"], "token_info": ["word", "word"], "pos": ["PAV", "VAFIN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.117": {"text": "Bey welchem Orte sich di\u00df Scharmuziern begab.", "tokens": ["Bey", "wel\u00b7chem", "Or\u00b7te", "sich", "di\u00df", "Schar\u00b7mu\u00b7zi\u00b7ern", "be\u00b7gab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "PRF", "PDS", "NN", "VVFIN", "$."], "meter": "+--+-+-+-+--+", "measure": "iambic.hexa.invert"}, "line.118": {"text": "Er nahm auch ", "tokens": ["Er", "nahm", "auch"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV"], "meter": "-+-", "measure": "amphibrach.single"}, "line.119": {"text": "Vor Homberg aber wolt\u2019 es jhm nicht wol gelingen/", "tokens": ["Vor", "Hom\u00b7berg", "a\u00b7ber", "wolt'", "es", "jhm", "nicht", "wol", "ge\u00b7lin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADV", "VMFIN", "PPER", "PPER", "PTKNEG", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.120": {"text": "Und wust er solchen Ort auf di\u00dfmal nicht zu zwingen.", "tokens": ["Und", "wust", "er", "sol\u00b7chen", "Ort", "auf", "di\u00df\u00b7mal", "nicht", "zu", "zwin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PIAT", "NN", "APPR", "ADV", "PTKNEG", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.121": {"text": "Es sorgte Bernhard sehr da\u00df Gallas seine Macht", "tokens": ["Es", "sorg\u00b7te", "Bern\u00b7hard", "sehr", "da\u00df", "Gal\u00b7las", "sei\u00b7ne", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NE", "ADV", "KOUS", "NE", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.122": {"text": "Mit der von Lothringen wurd\u2019 in ein Heer gebracht/", "tokens": ["Mit", "der", "von", "Loth\u00b7rin\u00b7gen", "wurd'", "in", "ein", "Heer", "ge\u00b7bracht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "APPR", "NE", "VAFIN", "APPR", "ART", "NN", "VVPP", "$("], "meter": "+--+--+--+-+", "measure": "dactylic.tri.plus"}, "line.123": {"text": "Daher er sich also vertheilte/ diesen beyden", "tokens": ["Da\u00b7her", "er", "sich", "al\u00b7so", "ver\u00b7theil\u00b7te", "/", "die\u00b7sen", "bey\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PAV", "PPER", "PRF", "ADV", "VVFIN", "$(", "PDAT", "PIAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.124": {"text": "Den Weg zur Einigung mit Vorthel abzuschneyden.", "tokens": ["Den", "Weg", "zur", "Ei\u00b7ni\u00b7gung", "mit", "Vor\u00b7thel", "ab\u00b7zu\u00b7schney\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "APPR", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.125": {"text": "Als aber niemand kam/ erwie\u00df er sich drey Tag", "tokens": ["Als", "a\u00b7ber", "nie\u00b7mand", "kam", "/", "er\u00b7wie\u00df", "er", "sich", "drey", "Tag"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PIS", "VVFIN", "$(", "VVFIN", "PPER", "PRF", "CARD", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.126": {"text": "Jm offnen und entbot dem Gallas einen Schlag.", "tokens": ["Jm", "off\u00b7nen", "und", "ent\u00b7bot", "dem", "Gal\u00b7las", "ei\u00b7nen", "Schlag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "KON", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.127": {"text": "Er aber wolte sich hierzu nicht gro\u00df verstehen.", "tokens": ["Er", "a\u00b7ber", "wol\u00b7te", "sich", "hier\u00b7zu", "nicht", "gro\u00df", "ver\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VMFIN", "PRF", "PAV", "PTKNEG", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.128": {"text": "Es war jhm lieber hin nach Hagenau zu gehen/", "tokens": ["Es", "war", "jhm", "lie\u00b7ber", "hin", "nach", "Ha\u00b7ge\u00b7nau", "zu", "ge\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ADV", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.129": {"text": "Das er belagerte/ jedennoch nicht gewann/", "tokens": ["Das", "er", "be\u00b7la\u00b7ger\u00b7te", "/", "je\u00b7den\u00b7noch", "nicht", "ge\u00b7wann", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "PPER", "VVFIN", "$(", "ADV", "PTKNEG", "VVFIN", "$("], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.130": {"text": "und kostete der Ort ein etlich hundert Mann/", "tokens": ["und", "kos\u00b7te\u00b7te", "der", "Ort", "ein", "et\u00b7lich", "hun\u00b7dert", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ART", "ADJD", "CARD", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.131": {"text": "Durch die aus Hagenau/ bald dort/ bald da erschlagen.", "tokens": ["Durch", "die", "aus", "Ha\u00b7ge\u00b7nau", "/", "bald", "dort", "/", "bald", "da", "er\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "APPR", "NE", "$(", "ADV", "ADV", "$(", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.132": {"text": "Es war nun in der Zeit von Winterlichen Tagen/", "tokens": ["Es", "war", "nun", "in", "der", "Zeit", "von", "Win\u00b7ter\u00b7li\u00b7chen", "Ta\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "ART", "NN", "APPR", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.133": {"text": "We\u00dfwegen Gallassich um einen Ort beward/", "tokens": ["We\u00df\u00b7we\u00b7gen", "Gal\u00b7las\u00b7sich", "um", "ei\u00b7nen", "Ort", "be\u00b7ward", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "APPR", "ART", "NN", "NE", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.134": {"text": "Sich und sein Voick/ das sehr durch ", "tokens": ["Sich", "und", "sein", "Voick", "/", "das", "sehr", "durch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PRF", "KON", "PPOSAT", "NN", "$(", "PDS", "ADV", "APPR"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.135": {"text": "Zu retten welches dann viel Oerter an dem Reyhne/", "tokens": ["Zu", "ret\u00b7ten", "wel\u00b7ches", "dann", "viel", "O\u00b7er\u00b7ter", "an", "dem", "Reyh\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "PRELS", "ADV", "PIAT", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.136": {"text": "Auch in der Weiterau und an dem krummen Meyhne/", "tokens": ["Auch", "in", "der", "Wei\u00b7te\u00b7rau", "und", "an", "dem", "krum\u00b7men", "Meyh\u00b7ne", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "KON", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.137": {"text": "Bevor des Reiches Kern/ das W\u00fcrtenberger land/", "tokens": ["Be\u00b7vor", "des", "Rei\u00b7ches", "Kern", "/", "das", "W\u00fcr\u00b7ten\u00b7ber\u00b7ger", "land", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.138": {"text": "Noch eins wie vor betraff. Besehet nun den Stand", "tokens": ["Noch", "eins", "wie", "vor", "be\u00b7traff", ".", "Be\u00b7se\u00b7het", "nun", "den", "Stand"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "PIS", "KOKOM", "APPR", "NE", "$.", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.139": {"text": "Vom \u00fcber-alten Meyntz/ mit Schweden angef\u00fcllet.", "tokens": ["Vom", "\u00fc\u00b7ber\u00b7al\u00b7ten", "Meyntz", "/", "mit", "Schwe\u00b7den", "an\u00b7ge\u00b7f\u00fcl\u00b7let", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$(", "APPR", "NE", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.140": {"text": "Es gieng ein tausend Mann/ vom Nebel starck verh\u00fcllet/", "tokens": ["Es", "gieng", "ein", "tau\u00b7send", "Mann", "/", "vom", "Ne\u00b7bel", "starck", "ver\u00b7h\u00fcl\u00b7let", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "CARD", "NN", "$(", "APPRART", "NN", "ADJD", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.141": {"text": "Vor die Gustanus-Burg vom Feinde neu bew\u00e4llt", "tokens": ["Vor", "die", "Gu\u00b7sta\u00b7nus\u00b7Burg", "vom", "Fein\u00b7de", "neu", "be\u00b7w\u00e4llt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPRART", "NN", "ADJD", "VVFIN"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.142": {"text": "und zimlich starck besetzt. Doch es war so best\u00e4llt/", "tokens": ["und", "zim\u00b7lich", "starck", "be\u00b7setzt", ".", "Doch", "es", "war", "so", "be\u00b7st\u00e4llt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VVPP", "$.", "KON", "PPER", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.143": {"text": "Da\u00df man es eingewann/ eh es der Feind versp\u00fchrte/", "tokens": ["Da\u00df", "man", "es", "ein\u00b7ge\u00b7wann", "/", "eh", "es", "der", "Feind", "ver\u00b7sp\u00fchr\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "$(", "KOUS", "PPER", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.144": {"text": "Worauf man Stuck und Volck nach Meyntz gefangen", "tokens": ["Wo\u00b7rauf", "man", "Stuck", "und", "Volck", "nach", "Meyntz", "ge\u00b7fan\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PIS", "NN", "KON", "NN", "APPR", "NE", "VVPP"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.145": {"text": "f\u00fchrte/", "tokens": ["f\u00fchr\u00b7te", "/"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.146": {"text": "und alle Wercke so vergleichte/ da\u00df kein Mann", "tokens": ["und", "al\u00b7le", "Wer\u00b7cke", "so", "ver\u00b7gleich\u00b7te", "/", "da\u00df", "kein", "Mann"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "ADV", "VVFIN", "$(", "KOUS", "PIAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.147": {"text": "Von solcher Burg nunmehr was gro\u00dfes sehen kan.", "tokens": ["Von", "sol\u00b7cher", "Burg", "nun\u00b7mehr", "was", "gro\u00b7\u00dfes", "se\u00b7hen", "kan", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ADV", "PWS", "ADJA", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.148": {"text": "Hergegen setzte man das Franckenthal in Schrancken/", "tokens": ["Her\u00b7ge\u00b7gen", "setz\u00b7te", "man", "das", "Fran\u00b7cken\u00b7thal", "in", "Schran\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIS", "ART", "NN", "APPR", "NN", "$("], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.149": {"text": "und \u00fcberkam den Platz fast \u00fcber die Gedancken.", "tokens": ["und", "\u00fc\u00b7ber\u00b7kam", "den", "Platz", "fast", "\u00fc\u00b7ber", "die", "Ge\u00b7dan\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.150": {"text": "Graf Man\u00dffeld zog hinein/ der Wein\u00dfyeim aber ab/", "tokens": ["Graf", "Man\u00df\u00b7feld", "zog", "hin\u00b7ein", "/", "der", "Wein\u00b7\u00df\u00b7yeim", "a\u00b7ber", "ab", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "VVFIN", "PTKVZ", "$(", "ART", "NN", "ADV", "PTKVZ", "$("], "meter": "--+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.151": {"text": "Es kam in Monats frist zu einer ", "tokens": ["Es", "kam", "in", "Mo\u00b7nats", "frist", "zu", "ei\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "NN", "APPR", "ART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.152": {"text": "Dergleichen wolte man mit Hanau auch beginnen/", "tokens": ["Derg\u00b7lei\u00b7chen", "wol\u00b7te", "man", "mit", "Ha\u00b7nau", "auch", "be\u00b7gin\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PIS", "APPR", "NE", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.153": {"text": "Di\u00df aber war ein Oet sehr schwerlich zu gewinnen.", "tokens": ["Di\u00df", "a\u00b7ber", "war", "ein", "O\u00b7et", "sehr", "schwer\u00b7lich", "zu", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VAFIN", "ART", "NN", "ADV", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.154": {"text": "Man brauchte gro\u00dfe Macht und eine lange Zeit.", "tokens": ["Man", "brauch\u00b7te", "gro\u00b7\u00dfe", "Macht", "und", "ei\u00b7ne", "lan\u00b7ge", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADJA", "NN", "KON", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.155": {"text": "Die aber Ramsayh dort durch seine Tapferkeit", "tokens": ["Die", "a\u00b7ber", "Ram\u00b7sayh", "dort", "durch", "sei\u00b7ne", "Tap\u00b7fer\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "NN", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.156": {"text": "Seyr nichtig hat gemacht. Den Ort mit Macht zu haben/", "tokens": ["Seyr", "nich\u00b7tig", "hat", "ge\u00b7macht", ".", "Den", "Ort", "mit", "Macht", "zu", "ha\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "ADJD", "VAFIN", "VVPP", "$.", "ART", "NN", "APPR", "NN", "PTKZU", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.157": {"text": "So say man drey Armeen vor solchem sich vergraben.", "tokens": ["So", "say", "man", "drey", "Ar\u00b7me\u00b7en", "vor", "sol\u00b7chem", "sich", "ver\u00b7gra\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "CARD", "NN", "APPR", "PIAT", "PRF", "VVPP", "$."], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.158": {"text": "Da war Lamboyens Macht/ da war des Man\u00dffelds Heer/", "tokens": ["Da", "war", "Lam\u00b7bo\u00b7yens", "Macht", "/", "da", "war", "des", "Man\u00df\u00b7felds", "Heer", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "NN", "$(", "ADV", "VAFIN", "ART", "NN", "NN", "$("], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.159": {"text": "und was ", "tokens": ["und", "was"], "token_info": ["word", "word"], "pos": ["KON", "PWS"], "meter": "-+", "measure": "iambic.single"}, "line.160": {"text": "Als nur des Hessen Macht und Le\u00dflen Heer zum wachen/", "tokens": ["Als", "nur", "des", "Hes\u00b7sen", "Macht", "und", "Le\u00df\u00b7len", "Heer", "zum", "wa\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "KON", "NN", "NN", "APPRART", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.161": {"text": "umb der Belagerung ein kurtzes Ziel zu machen.", "tokens": ["umb", "der", "Be\u00b7la\u00b7ge\u00b7rung", "ein", "kurt\u00b7zes", "Ziel", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.162": {"text": "Sie zogen wieder ab/ geschlagen m\u00fcd und matt/", "tokens": ["Sie", "zo\u00b7gen", "wie\u00b7der", "ab", "/", "ge\u00b7schla\u00b7gen", "m\u00fcd", "und", "matt", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKVZ", "$(", "VVPP", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.163": {"text": "und lie\u00dfen diesen Ort an diesen/ der es hatt\u2019.", "tokens": ["und", "lie\u00b7\u00dfen", "die\u00b7sen", "Ort", "an", "die\u00b7sen", "/", "der", "es", "hatt'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PDAT", "NN", "APPR", "PDAT", "$(", "PRELS", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.164": {"text": "Hergegen wurde dort Ameneburg von Hessen", "tokens": ["Her\u00b7ge\u00b7gen", "wur\u00b7de", "dort", "A\u00b7me\u00b7ne\u00b7burg", "von", "Hes\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "ADV", "NE", "APPR", "NE"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.165": {"text": "Belagert und es blieb auch Hirschfeld unvergessen/", "tokens": ["Be\u00b7la\u00b7gert", "und", "es", "blieb", "auch", "Hirschfeld", "un\u00b7ver\u00b7ges\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "PPER", "VVFIN", "ADV", "NN", "ADJD", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.166": {"text": "Man nahm sie beyde weg. Ein mehrers thaten sie", "tokens": ["Man", "nahm", "sie", "bey\u00b7de", "weg", ".", "Ein", "meh\u00b7rers", "tha\u00b7ten", "sie"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "PIS", "PTKVZ", "$.", "ART", "PIS", "VVFIN", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.167": {"text": "Der G\u00f6\u00dfischen Armee/ und offt mit schlechter M\u00fch/", "tokens": ["Der", "G\u00f6\u00b7\u00dfisc\u00b7hen", "Ar\u00b7mee", "/", "und", "offt", "mit", "schlech\u00b7ter", "M\u00fch", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "KON", "ADV", "APPR", "ADJA", "NN", "$("], "meter": "-+---+-+-+-+", "measure": "dactylic.init"}, "line.168": {"text": "So/ da\u00df sie endlich sich geiwungen must erheben", "tokens": ["So", "/", "da\u00df", "sie", "end\u00b7lich", "sich", "gei\u00b7wun\u00b7gen", "must", "er\u00b7he\u00b7ben"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "$(", "KOUS", "PPER", "ADV", "PRF", "VVINF", "VMFIN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.169": {"text": "und alles Hessen-Land um das von Francken geben.", "tokens": ["und", "al\u00b7les", "Hes\u00b7sen\u00b7Land", "um", "das", "von", "Fran\u00b7cken", "ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "APPR", "ART", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.170": {"text": "Der Hatzfeld folgte nach. ", "tokens": ["Der", "Hatz\u00b7feld", "folg\u00b7te", "nach", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.171": {"text": "Brach Gallas und sein Volck vor Hunger wieder auff/", "tokens": ["Brach", "Gal\u00b7las", "und", "sein", "Volck", "vor", "Hun\u00b7ger", "wie\u00b7der", "auff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "PPOSAT", "NN", "APPR", "NN", "ADV", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.172": {"text": "Gieng mit dem Lothringer/ Lamboyen/ Jsolanen/", "tokens": ["Gieng", "mit", "dem", "Loth\u00b7rin\u00b7ger", "/", "Lam\u00b7bo\u00b7yen", "/", "Jso\u00b7la\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "$(", "NN", "$(", "NE", "$("], "meter": "+--+--+-+-+-", "measure": "dactylic.di.plus"}, "line.173": {"text": "Mit Piccolomini/ Forgatzen und de Granen/", "tokens": ["Mit", "Pic\u00b7co\u00b7lo\u00b7mi\u00b7ni", "/", "For\u00b7gat\u00b7zen", "und", "de", "Gra\u00b7nen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "NN", "KON", "NE", "NE", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.174": {"text": "Ein viertzig tausend starck/ stracks in Burgundien ein.", "tokens": ["Ein", "viert\u00b7zig", "tau\u00b7send", "starck", "/", "stracks", "in", "Bur\u00b7gun\u00b7di\u00b7en", "ein", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "CARD", "NN", "$(", "ADV", "APPR", "NE", "PTKVZ", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.175": {"text": "Und war der erste Platz/ den er bekam/ Fontein.", "tokens": ["Und", "war", "der", "ers\u00b7te", "Platz", "/", "den", "er", "be\u00b7kam", "/", "Fon\u00b7tein", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "ADJA", "NN", "$(", "PRELS", "PPER", "VVFIN", "$(", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.176": {"text": "Hier folgte Mirebeau/ Dyon war Zweifels ohne", "tokens": ["Hier", "folg\u00b7te", "Mi\u00b7re\u00b7be\u00b7au", "/", "Dy\u00b7on", "war", "Zwei\u00b7fels", "oh\u00b7ne"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NE", "$(", "NE", "VAFIN", "NN", "APPR"], "meter": "-+-+---+--+-+-", "measure": "iambic.penta.relaxed"}, "line.177": {"text": "Be\u00e4ngstet/ noch vielmehr die Stadt ", "tokens": ["Be\u00b7\u00e4ngs\u00b7tet", "/", "noch", "viel\u00b7mehr", "die", "Stadt"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["VVPP", "$(", "ADV", "ADV", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.178": {"text": "Die man sehr hart beschlo\u00df mit zwantzig tausend Mann.", "tokens": ["Die", "man", "sehr", "hart", "be\u00b7schlo\u00df", "mit", "zwant\u00b7zig", "tau\u00b7send", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADV", "ADJD", "VVFIN", "APPR", "CARD", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.179": {"text": "Man scho\u00df und griff den Ort mit gro\u00dfem Eufer an.", "tokens": ["Man", "scho\u00df", "und", "griff", "den", "Ort", "mit", "gro\u00b7\u00dfem", "Eu\u00b7fer", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "KON", "VVFIN", "ART", "NN", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.180": {"text": "Die Frantzen/ diesen Ort nicht gern verlassend/ sehen", "tokens": ["Die", "Frant\u00b7zen", "/", "die\u00b7sen", "Ort", "nicht", "gern", "ver\u00b7las\u00b7send", "/", "se\u00b7hen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "$(", "PDAT", "NN", "PTKNEG", "ADV", "VVPP", "$(", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.181": {"text": "um Mittel diesem Heer mit Ernst zu widerstehen.", "tokens": ["um", "Mit\u00b7tel", "die\u00b7sem", "Heer", "mit", "Ernst", "zu", "wi\u00b7der\u00b7ste\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PDAT", "NN", "APPR", "NE", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.182": {"text": "Zu schlagen war nicht Raht/ jhr Heer war viel zu klein/", "tokens": ["Zu", "schla\u00b7gen", "war", "nicht", "Raht", "/", "jhr", "Heer", "war", "viel", "zu", "klein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VAFIN", "PTKNEG", "NN", "$(", "PPOSAT", "NN", "VAFIN", "ADV", "PTKA", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.183": {"text": "Es must\u2019 ein ander Werck hier vorgenommen seyn.", "tokens": ["Es", "must'", "ein", "an\u00b7der", "Werck", "hier", "vor\u00b7ge\u00b7nom\u00b7men", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "ADV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.184": {"text": "Der Ort war fest genug/ der Mangel war an Leuthen/", "tokens": ["Der", "Ort", "war", "fest", "ge\u00b7nug", "/", "der", "Man\u00b7gel", "war", "an", "Leu\u00b7then", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "ADV", "$(", "ART", "NN", "VAFIN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.185": {"text": "und zwar an einem H\u00e4upt/ das in dergleichen Streiten", "tokens": ["und", "zwar", "an", "ei\u00b7nem", "H\u00e4upt", "/", "das", "in", "derg\u00b7lei\u00b7chen", "Strei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "$(", "ART", "APPR", "PIS", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.186": {"text": "Erfahren. Lang bedacht/ sprach man dem Rantzau zu/", "tokens": ["Er\u00b7fah\u00b7ren", ".", "Lang", "be\u00b7dacht", "/", "sprach", "man", "dem", "Rant\u00b7zau", "zu", "/"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "VVPP", "$(", "VVFIN", "PIS", "ART", "NN", "PTKZU", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.187": {"text": "Und hielt der Frantzen Sinn Burgundjens gantze Ruh", "tokens": ["Und", "hielt", "der", "Frant\u00b7zen", "Sinn", "Bur\u00b7gund\u00b7jens", "gant\u00b7ze", "Ruh"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "NN", "NE", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.188": {"text": "In dieses Mannes Hand ergeben. Er voll Wunden/", "tokens": ["In", "die\u00b7ses", "Man\u00b7nes", "Hand", "er\u00b7ge\u00b7ben", ".", "Er", "voll", "Wun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "NN", "VVPP", "$.", "PPER", "ADJD", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.189": {"text": "Die er vor Dole kriegt\u2019/ und noch fast unverbunden/", "tokens": ["Die", "er", "vor", "Do\u00b7le", "kriegt'", "/", "und", "noch", "fast", "un\u00b7ver\u00b7bun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "VVFIN", "$(", "KON", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.190": {"text": "Gieng jhr Begehren ein kam \u00fcber den Morast", "tokens": ["Gieng", "jhr", "Be\u00b7geh\u00b7ren", "ein", "kam", "\u00fc\u00b7ber", "den", "Mo\u00b7rast"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "PTKVZ", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.191": {"text": "Zwerg weaig in die Stadt und milderte die Last/", "tokens": ["Zwerg", "we\u00b7aig", "in", "die", "Stadt", "und", "mil\u00b7der\u00b7te", "die", "Last", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "APPR", "ART", "NN", "KON", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.192": {"text": "In dem er Morgens fr\u00fch sich aus den Pforten machte/", "tokens": ["In", "dem", "er", "Mor\u00b7gens", "fr\u00fch", "sich", "aus", "den", "Pfor\u00b7ten", "mach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADV", "ADJD", "PRF", "APPR", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.193": {"text": "und mit geringer Hilff den Feind vom Stande brachte/", "tokens": ["und", "mit", "ge\u00b7rin\u00b7ger", "Hilff", "den", "Feind", "vom", "Stan\u00b7de", "brach\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "NN", "ART", "NN", "APPRART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.194": {"text": "Zum schnellen fliehen trieb/ acht hundert Mann erschlug/", "tokens": ["Zum", "schnel\u00b7len", "flie\u00b7hen", "trieb", "/", "acht", "hun\u00b7dert", "Mann", "er\u00b7schlug", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "VVINF", "VVFIN", "$(", "CARD", "CARD", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.195": {"text": "Die St\u00fcck eroberte/ den Brand ins Pulver trug.", "tokens": ["Die", "St\u00fcck", "er\u00b7o\u00b7ber\u00b7te", "/", "den", "Brand", "ins", "Pul\u00b7ver", "trug", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.196": {"text": "Damit war diese Stadt befreyet/ und die Feinde", "tokens": ["Da\u00b7mit", "war", "die\u00b7se", "Stadt", "be\u00b7fre\u00b7yet", "/", "und", "die", "Fein\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PDAT", "NN", "VVFIN", "$(", "KON", "ART", "NN"], "meter": "-+---+-+-+-+-", "measure": "dactylic.init"}, "line.197": {"text": "Zerstoben. Gallas und der Lothringer/ zween Freinde/", "tokens": ["Zer\u00b7sto\u00b7ben", ".", "Gal\u00b7las", "und", "der", "Loth\u00b7rin\u00b7ger", "/", "zween", "Frein\u00b7de", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$.", "NN", "KON", "ART", "NN", "$(", "VVFIN", "NN", "$("], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.198": {"text": "Geriethen in den Zwist und meynte jener recht/", "tokens": ["Ge\u00b7rie\u00b7then", "in", "den", "Zwist", "und", "meyn\u00b7te", "je\u00b7ner", "recht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "KON", "VVFIN", "PDAT", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.199": {"text": "Es hielte Gallas sich in seinem Ampte schlecht.", "tokens": ["Es", "hiel\u00b7te", "Gal\u00b7las", "sich", "in", "sei\u00b7nem", "Amp\u00b7te", "schlecht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "PRF", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.200": {"text": "Er h\u00e4tte so verst\u00e4rckt vielmehr verrichten k\u00f6nnen/", "tokens": ["Er", "h\u00e4t\u00b7te", "so", "ver\u00b7st\u00e4rckt", "viel\u00b7mehr", "ver\u00b7rich\u00b7ten", "k\u00f6n\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVFIN", "ADV", "VVINF", "VMINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.201": {"text": "Er m\u00fcste seinem Herrn/ dem K\u00e4yser/ wenig g\u00f6nnen", "tokens": ["Er", "m\u00fcs\u00b7te", "sei\u00b7nem", "Herrn", "/", "dem", "K\u00e4y\u00b7ser", "/", "we\u00b7nig", "g\u00f6n\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VMFIN", "PPOSAT", "NN", "$(", "ART", "NN", "$(", "PIS", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.202": {"text": "Wahr ists/ es war nicht mehr mit dieser gro\u00dfen Macht", "tokens": ["Wahr", "ists", "/", "es", "war", "nicht", "mehr", "mit", "die\u00b7ser", "gro\u00b7\u00dfen", "Macht"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "$(", "PPER", "VAFIN", "PTKNEG", "ADV", "APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.203": {"text": "Gethan/ als nur das Land in gro\u00dfen Brand gebracht.", "tokens": ["Ge\u00b7than", "/", "als", "nur", "das", "Land", "in", "gro\u00b7\u00dfen", "Brand", "ge\u00b7bracht", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "KOKOM", "ADV", "ART", "NN", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.204": {"text": "Damit so gieng er durch. Von viertzig tausend Seelen", "tokens": ["Da\u00b7mit", "so", "gieng", "er", "durch", ".", "Von", "viert\u00b7zig", "tau\u00b7send", "See\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PAV", "ADV", "VVFIN", "PPER", "PTKVZ", "$.", "APPR", "CARD", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.205": {"text": "Vermochte man mit M\u00fch die H\u00e4lffte nicht zu z\u00e4hlen.", "tokens": ["Ver\u00b7moch\u00b7te", "man", "mit", "M\u00fch", "die", "H\u00e4lff\u00b7te", "nicht", "zu", "z\u00e4h\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "NN", "ART", "NN", "PTKNEG", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.206": {"text": "Die gro\u00dfen Wasserg\u00fc\u00df\u2019 und b\u00f6se Zeit vom Jahr", "tokens": ["Die", "gro\u00b7\u00dfen", "Was\u00b7ser\u00b7g\u00fc\u00df'", "und", "b\u00f6\u00b7se", "Zeit", "vom", "Jahr"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "KON", "ADJA", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.207": {"text": "Ergriffen jhn so schnell/ da\u00df es kaum m\u00f6glich war", "tokens": ["Er\u00b7grif\u00b7fen", "jhn", "so", "schnell", "/", "da\u00df", "es", "kaum", "m\u00f6g\u00b7lich", "war"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "ADJD", "$(", "KOUS", "PPER", "ADV", "ADJD", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.208": {"text": "Von dannen lo\u00df zu seyn. Die meisten Karr- und Wagen", "tokens": ["Von", "dan\u00b7nen", "lo\u00df", "zu", "seyn", ".", "Die", "meis\u00b7ten", "Karr", "und", "Wa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "ADJD", "PTKZU", "VAINF", "$.", "ART", "PIAT", "TRUNC", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.209": {"text": "Besteckten in dem Koth. Ein jeder muste tragen", "tokens": ["Be\u00b7steck\u00b7ten", "in", "dem", "Koth", ".", "Ein", "je\u00b7der", "mus\u00b7te", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN", "$.", "ART", "PIS", "VMFIN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.210": {"text": "So viel jhm m\u00f6glich war. So kahl kam man davon.", "tokens": ["So", "viel", "jhm", "m\u00f6g\u00b7lich", "war", ".", "So", "kahl", "kam", "man", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "ADJD", "VAFIN", "$.", "ADV", "ADJD", "VVFIN", "PIS", "PAV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.211": {"text": "Auch blieben \u00fcber di\u00df die St\u00fcck in ", "tokens": ["Auch", "blie\u00b7ben", "\u00fc\u00b7ber", "di\u00df", "die", "St\u00fcck", "in"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "PDS", "ART", "NN", "APPR"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.212": {"text": "Und doch galt Gallas viel/ als ein Soldat von Rathen/", "tokens": ["Und", "doch", "galt", "Gal\u00b7las", "viel", "/", "als", "ein", "Sol\u00b7dat", "von", "Ra\u00b7then", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "NE", "ADV", "$(", "KOUS", "ART", "NN", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.213": {"text": "Er blieb auch lebenslang in unsers K\u00e4ysers Gnaden.", "tokens": ["Er", "blieb", "auch", "le\u00b7bens\u00b7lang", "in", "un\u00b7sers", "K\u00e4y\u00b7sers", "Gna\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "APPR", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.214": {"text": "Noch gr\u00f6\u00dfer aber war die Gnade/ die Pari\u00df/", "tokens": ["Noch", "gr\u00f6\u00b7\u00dfer", "a\u00b7ber", "war", "die", "Gna\u00b7de", "/", "die", "Pa\u00b7ri\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "VAFIN", "ART", "NN", "$(", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.215": {"text": "Voraus sein gro\u00dfer Herr/ anjetzt an dier bewie\u00df/", "tokens": ["Vo\u00b7raus", "sein", "gro\u00b7\u00dfer", "Herr", "/", "an\u00b7jetzt", "an", "dier", "be\u00b7wie\u00df", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$(", "ADV", "APPR", "PPER", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.216": {"text": "An dier/ du gro\u00dfer F\u00fcrst von Weymar. Deine Thaten", "tokens": ["An", "dier", "/", "du", "gro\u00b7\u00dfer", "F\u00fcrst", "von", "Wey\u00b7mar", ".", "Dei\u00b7ne", "Tha\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PDAT", "$(", "PPER", "ADJA", "NN", "APPR", "NE", "$.", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.217": {"text": "Die waren durch den Neyd selbselbst dahin gerathen.", "tokens": ["Die", "wa\u00b7ren", "durch", "den", "Neyd", "selb\u00b7selbst", "da\u00b7hin", "ge\u00b7ra\u00b7then", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "ART", "NN", "ADV", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.218": {"text": "Du warest wert und lieb. Was nur beehren mag", "tokens": ["Du", "wa\u00b7rest", "wert", "und", "lieb", ".", "Was", "nur", "be\u00b7eh\u00b7ren", "mag"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "KON", "ADJD", "$.", "PWS", "ADV", "VVINF", "VMFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.219": {"text": "Das brachte dieser Hof zu deiner Ehr\u2019 an Tag.", "tokens": ["Das", "brach\u00b7te", "die\u00b7ser", "Hof", "zu", "dei\u00b7ner", "Ehr'", "an", "Tag", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PDAT", "NN", "APPR", "PPOSAT", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.220": {"text": "Ich schweige die Geschenck und theuere Jubelen/", "tokens": ["Ich", "schwei\u00b7ge", "die", "Ge\u00b7schenck", "und", "theu\u00b7e\u00b7re", "Ju\u00b7be\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "KON", "ADJA", "NN", "$("], "meter": "-+-+-+-+--+--", "measure": "iambic.penta.relaxed"}, "line.221": {"text": "Gold/ Volck und was man dier noch weiter zu lie\u00df z\u00e4hlen/", "tokens": ["Gold", "/", "Volck", "und", "was", "man", "dier", "noch", "wei\u00b7ter", "zu", "lie\u00df", "z\u00e4h\u00b7len", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "NN", "KON", "PWS", "PIS", "ADV", "ADV", "ADV", "PTKZU", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.222": {"text": "Als ein verdientes Werck. Auf dieses machtestu", "tokens": ["Als", "ein", "ver\u00b7dien\u00b7tes", "Werck", ".", "Auf", "die\u00b7ses", "mach\u00b7tes\u00b7tu"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN", "$.", "APPR", "PDAT", "VVFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.223": {"text": "Dich wieder in das Feld. Die h\u00f6fsche Lust und Ruh", "tokens": ["Dich", "wie\u00b7der", "in", "das", "Feld", ".", "Die", "h\u00f6f\u00b7sche", "Lust", "und", "Ruh"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "APPR", "ART", "NN", "$.", "ART", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.224": {"text": "War dir nicht angebohrn. Du hattest deine Jugend", "tokens": ["War", "dir", "nicht", "an\u00b7ge\u00b7bohrn", ".", "Du", "hat\u00b7test", "dei\u00b7ne", "Ju\u00b7gend"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PTKNEG", "VVPP", "$.", "PPER", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.225": {"text": "und gr\u00f6\u00dfre Jahre Zeit allein zur Helden-Tugend", "tokens": ["und", "gr\u00f6\u00df\u00b7re", "Jah\u00b7re", "Zeit", "al\u00b7lein", "zur", "Hel\u00b7den\u00b7Tu\u00b7gend"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJA", "NN", "NN", "ADV", "APPRART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.226": {"text": "Gew\u00f6hnet/ welche mehr nach blancken Waffen schaut/", "tokens": ["Ge\u00b7w\u00f6h\u00b7net", "/", "wel\u00b7che", "mehr", "nach", "blan\u00b7cken", "Waf\u00b7fen", "schaut", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "PRELS", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.227": {"text": "Als was der zarte Hof f\u00fcr Dantz-ger\u00fcste baut.", "tokens": ["Als", "was", "der", "zar\u00b7te", "Hof", "f\u00fcr", "Dant\u00b7zge\u00b7r\u00fcs\u00b7te", "baut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWS", "ART", "ADJA", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.228": {"text": "Es war dein nechster Weg nach deinen Krieges-Scharen/", "tokens": ["Es", "war", "dein", "nechs\u00b7ter", "Weg", "nach", "dei\u00b7nen", "Krie\u00b7ge\u00b7sScha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.229": {"text": "Die jetzund in dem Zug nach Elsas-Zabern waren/", "tokens": ["Die", "je\u00b7tzund", "in", "dem", "Zug", "nach", "El\u00b7sas\u00b7Z\u00b7abern", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ART", "NN", "APPR", "NE", "VAFIN", "$("], "meter": "-+---+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.230": {"text": "Das sich in Monats Frist in deine H\u00e4nde gab.", "tokens": ["Das", "sich", "in", "Mo\u00b7nats", "Frist", "in", "dei\u00b7ne", "H\u00e4n\u00b7de", "gab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PRF", "APPR", "NN", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.231": {"text": "Hierauf so schlugestu ein Theil von Feinden ab/", "tokens": ["Hier\u00b7auf", "so", "schlu\u00b7ge\u00b7stu", "ein", "Theil", "von", "Fein\u00b7den", "ab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "ART", "NN", "APPR", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.232": {"text": "Die sich um Blanckenberg/ Blamont und Ramberts weiler", "tokens": ["Die", "sich", "um", "Blan\u00b7cken\u00b7berg", "/", "Bla\u00b7mont", "und", "Ram\u00b7berts", "wei\u00b7ler"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "PRF", "APPR", "NE", "$(", "NN", "KON", "NE", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.233": {"text": "Jm Lande Lothringen mit Gallas/ jhrem Pfeiler/", "tokens": ["Jm", "Lan\u00b7de", "Loth\u00b7rin\u00b7gen", "mit", "Gal\u00b7las", "/", "jhrem", "Pfei\u00b7ler", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NN", "NE", "APPR", "NE", "$(", "PPOSAT", "NN", "$("], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.234": {"text": "Bezeigten. Welches dann die drey besagten Pl\u00e4tz\u2019", "tokens": ["Be\u00b7zeig\u00b7ten", ".", "Wel\u00b7ches", "dann", "die", "drey", "be\u00b7sag\u00b7ten", "Pl\u00e4tz'"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "PWS", "ADV", "ART", "CARD", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.235": {"text": "Auch selbst dahin bezwung/ das Weymarsche Gesetz", "tokens": ["Auch", "selbst", "da\u00b7hin", "be\u00b7zwung", "/", "das", "Wey\u00b7mar\u00b7sche", "Ge\u00b7setz"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "PAV", "NN", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.236": {"text": "Zu lernen. Diesem nach gieng er/ die Winter-Plagen/", "tokens": ["Zu", "ler\u00b7nen", ".", "Die\u00b7sem", "nach", "gieng", "er", "/", "die", "Win\u00b7ter\u00b7Pla\u00b7gen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$.", "PDAT", "APPR", "VVFIN", "PPER", "$(", "ART", "NN", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.237": {"text": "Die nun beginneten/ was b\u00e4sser zu ertragen/", "tokens": ["Die", "nun", "be\u00b7gin\u00b7ne\u00b7ten", "/", "was", "b\u00e4s\u00b7ser", "zu", "er\u00b7tra\u00b7gen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVFIN", "$(", "PWS", "ADJD", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.238": {"text": "Vom Feld\u2019 und satzte sich in guten St\u00e4dten ein.", "tokens": ["Vom", "Feld'", "und", "satz\u00b7te", "sich", "in", "gu\u00b7ten", "St\u00e4d\u00b7ten", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "VVFIN", "PRF", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.239": {"text": "Jm Fr\u00fchling ruffte man jhn wieder nach der Sain", "tokens": ["Jm", "Fr\u00fch\u00b7ling", "ruff\u00b7te", "man", "jhn", "wie\u00b7der", "nach", "der", "Sain"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "PPER", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.240": {"text": "Ins m\u00e4chtige Paris/ demselben mitzutheilen", "tokens": ["Ins", "m\u00e4ch\u00b7ti\u00b7ge", "Pa\u00b7ris", "/", "dem\u00b7sel\u00b7ben", "mit\u00b7zu\u00b7thei\u00b7len"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPRART", "ADJA", "NE", "$(", "PDAT", "VVIZU"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.241": {"text": "Was man beschlossen hatt\u2019. Er/ sonder viel verweilen/", "tokens": ["Was", "man", "be\u00b7schlos\u00b7sen", "hatt'", ".", "Er", "/", "son\u00b7der", "viel", "ver\u00b7wei\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "VVPP", "VAFIN", "$.", "PPER", "$(", "KON", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.242": {"text": "Kam hin und wieder her. Kaum/ da\u00df jhn seine Schar", "tokens": ["Kam", "hin", "und", "wie\u00b7der", "her", ".", "Kaum", "/", "da\u00df", "jhn", "sei\u00b7ne", "Schar"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "PTKVZ", "KON", "ADV", "PTKVZ", "$.", "ADV", "$(", "KOUS", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.243": {"text": "Empfieng/ erblickte man die Feinds Parthey/ da war", "tokens": ["Emp\u00b7fi\u00b7eng", "/", "er\u00b7blick\u00b7te", "man", "die", "Feinds", "Par\u00b7they", "/", "da", "war"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["NE", "$(", "VVFIN", "PIS", "ART", "NN", "NN", "$(", "ADV", "VAFIN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.244": {"text": "Der F\u00fcrst von Lothringen mit vielen andern Hauffen/", "tokens": ["Der", "F\u00fcrst", "von", "Loth\u00b7rin\u00b7gen", "mit", "vie\u00b7len", "an\u00b7dern", "Hauf\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NE", "APPR", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.245": {"text": "Das Vesontiner Feld mit Menschen-Blut zu tauffen.", "tokens": ["Das", "Ve\u00b7son\u00b7ti\u00b7ner", "Feld", "mit", "Men\u00b7schen\u00b7Blut", "zu", "tauf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.246": {"text": "Der Sieg war aber so/ da\u00df Carls seine Macht", "tokens": ["Der", "Sieg", "war", "a\u00b7ber", "so", "/", "da\u00df", "Carls", "sei\u00b7ne", "Macht"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "$(", "KOUS", "NE", "PPOSAT", "NN"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.247": {"text": "Geschlagen und er selbst kaum wurd\u2019 hinweg gebracht.", "tokens": ["Ge\u00b7schla\u00b7gen", "und", "er", "selbst", "kaum", "wurd'", "hin\u00b7weg", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "PPER", "ADV", "ADV", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.248": {"text": "So nah stundt jhm sein Tod. Worauf der Feind- Zer-", "tokens": ["So", "nah", "stundt", "jhm", "sein", "Tod", ".", "Wo\u00b7rauf", "der", "Fein\u00b7d", "Zer"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVFIN", "PPER", "PPOSAT", "NN", "$.", "PAV", "ART", "TRUNC", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.249": {"text": "Sich nach dem Reyhne zog/ und bald bey Wittenweyer", "tokens": ["Sich", "nach", "dem", "Reyh\u00b7ne", "zog", "/", "und", "bald", "bey", "Wit\u00b7ten\u00b7we\u00b7yer"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "APPR", "ART", "NN", "VVFIN", "$(", "KON", "ADV", "APPR", "NE"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.250": {"text": "und Reinau eine Br\u00fcck erbauet\u2019/ \u00fcber Reyhn", "tokens": ["und", "Rei\u00b7nau", "ei\u00b7ne", "Br\u00fcck", "er\u00b7bau\u00b7et'", "/", "\u00fc\u00b7ber", "Reyhn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "NN", "ART", "NN", "VVFIN", "$(", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.251": {"text": "Zu gehn/ und muste sie auch bald umschantzet seyn.", "tokens": ["Zu", "gehn", "/", "und", "mus\u00b7te", "sie", "auch", "bald", "um\u00b7schant\u00b7zet", "seyn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "KON", "VMFIN", "PPER", "ADV", "ADV", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.252": {"text": "Di\u00df alles war verbracht den Feinden gro\u00df zum Schaden.", "tokens": ["Di\u00df", "al\u00b7les", "war", "ver\u00b7bracht", "den", "Fein\u00b7den", "gro\u00df", "zum", "Scha\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VAFIN", "VVPP", "ART", "NN", "ADJD", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.253": {"text": "Daher sich Jean de Wert/ ein Au\u00dfbund der Soldaten/", "tokens": ["Da\u00b7her", "sich", "Jean", "de", "Wert", "/", "ein", "Au\u00df\u00b7bund", "der", "Sol\u00b7da\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PRF", "NE", "NE", "NE", "$(", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.254": {"text": "Sich bald dahin begab und einen Anfall that/", "tokens": ["Sich", "bald", "da\u00b7hin", "be\u00b7gab", "und", "ei\u00b7nen", "An\u00b7fall", "that", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "PAV", "VVFIN", "KON", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.255": {"text": "Der aber disesmal ihm nur geschadet hat.", "tokens": ["Der", "a\u00b7ber", "di\u00b7ses\u00b7mal", "ihm", "nur", "ge\u00b7scha\u00b7det", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "PPER", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.256": {"text": "Es blieb ein zimlich Volck Di\u00df alles unerwogen/", "tokens": ["Es", "blieb", "ein", "zim\u00b7lich", "Volck", "Di\u00df", "al\u00b7les", "un\u00b7er\u00b7wo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJD", "NN", "PDS", "PIS", "ADJD", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.257": {"text": "Kam er zum andernmal mit Macht hinan gezogen/", "tokens": ["Kam", "er", "zum", "an\u00b7dern\u00b7mal", "mit", "Macht", "hi\u00b7nan", "ge\u00b7zo\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPRART", "ADV", "APPR", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.258": {"text": "und sah zum andernmal/ wie jhn sein Gl\u00fcck betrog.", "tokens": ["und", "sah", "zum", "an\u00b7dern\u00b7mal", "/", "wie", "jhn", "sein", "Gl\u00fcck", "be\u00b7trog", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "ADV", "$(", "PWAV", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.259": {"text": "Worauf der Weymar-Held vor manche Schl\u00f6sser zog/", "tokens": ["Wo\u00b7rauf", "der", "Wey\u00b7ma\u00b7rHeld", "vor", "man\u00b7che", "Schl\u00f6s\u00b7ser", "zog", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "APPR", "PIAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.260": {"text": "und bald besiegete zum Vortheil seiner Scharen/", "tokens": ["und", "bald", "be\u00b7sie\u00b7ge\u00b7te", "zum", "Vor\u00b7theil", "sei\u00b7ner", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "APPRART", "NN", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.261": {"text": "Weil solche Pl\u00e4tze voll von Lebens Mitteln waren.", "tokens": ["Weil", "sol\u00b7che", "Pl\u00e4t\u00b7ze", "voll", "von", "Le\u00b7bens", "Mit\u00b7teln", "wa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "ADJD", "APPR", "NN", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.262": {"text": "Hierzwischen machte sich der wolbenahmte Held/", "tokens": ["Hier\u00b7zwi\u00b7schen", "mach\u00b7te", "sich", "der", "wol\u00b7be\u00b7nahm\u00b7te", "Held", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PRF", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.263": {"text": "De Wert/ zum drittenmal sehr m\u00e4chtig in das Feld/", "tokens": ["De", "Wert", "/", "zum", "drit\u00b7ten\u00b7mal", "sehr", "m\u00e4ch\u00b7tig", "in", "das", "Feld", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$(", "APPRART", "ADV", "ADV", "ADJD", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.264": {"text": "Die wolverwahrte Br\u00fcck\u2019 an Reynau zu best\u00fcrmen/", "tokens": ["Die", "wol\u00b7ver\u00b7wahr\u00b7te", "Br\u00fcck", "an", "Rey\u00b7nau", "zu", "be\u00b7st\u00fcr\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "NE", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.265": {"text": "Doch er wurd\u2019 itzt/ wie vor/ durch m\u00e4chtiges beschirmen", "tokens": ["Doch", "er", "wurd'", "itzt", "/", "wie", "vor", "/", "durch", "m\u00e4ch\u00b7ti\u00b7ges", "be\u00b7schir\u00b7men"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ADV", "$(", "KOKOM", "APPR", "$(", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.266": {"text": "Des Weymarischen Volcks/ seher h\u00e4\u00dflich abgesetzt/", "tokens": ["Des", "Wey\u00b7ma\u00b7ri\u00b7schen", "Volcks", "/", "se\u00b7her", "h\u00e4\u00df\u00b7lich", "ab\u00b7ge\u00b7setzt", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ADV", "ADJD", "VVPP", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.267": {"text": "Jhm selber wurd\u2019 hiedurch ein Wangentheil verletzt/", "tokens": ["Jhm", "sel\u00b7ber", "wurd'", "hie\u00b7durch", "ein", "Wan\u00b7gen\u00b7theil", "ver\u00b7letzt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VAFIN", "PAV", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.268": {"text": "Und hat er diesesmal vier hundert Mann/ 2. St\u00fccke", "tokens": ["Und", "hat", "er", "die\u00b7ses\u00b7mal", "vier", "hun\u00b7dert", "Mann", "/", "2.", "St\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "ordinal", "word"], "pos": ["KON", "VAFIN", "PPER", "ADV", "CARD", "CARD", "NN", "$(", "ADJA", "NN"], "meter": "-+-+--++--+-", "measure": "iambic.penta.relaxed"}, "line.269": {"text": "und mehrers eingeb\u00fc\u00dft. H\u00f6rt aber was f\u00fcr Gl\u00fccke", "tokens": ["und", "meh\u00b7rers", "ein\u00b7ge\u00b7b\u00fc\u00dft", ".", "H\u00f6rt", "a\u00b7ber", "was", "f\u00fcr", "Gl\u00fc\u00b7cke"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VVPP", "$.", "NE", "KON", "PWS", "APPR", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.270": {"text": "Sich jhm hierauf erwie\u00df. Der Weymar-Held brach auff", "tokens": ["Sich", "jhm", "hier\u00b7auf", "er\u00b7wie\u00df", ".", "Der", "Wey\u00b7ma\u00b7rHeld", "brach", "auff"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "PPER", "PAV", "VVFIN", "$.", "ART", "NN", "VVFIN", "APPR"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.271": {"text": "und war der Reuterey fast-Wind-geschwinder Lauff", "tokens": ["und", "war", "der", "Reu\u00b7te\u00b7rey", "fast\u00b7Win\u00b7dge\u00b7schwin\u00b7der", "Lauff"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.272": {"text": "Gerad nach Ensi\u00dfheim. Di\u00df gl\u00fccklich eingenommen/", "tokens": ["Ge\u00b7rad", "nach", "En\u00b7si\u00df\u00b7heim", ".", "Di\u00df", "gl\u00fcck\u00b7lich", "ein\u00b7ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "$.", "PDS", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.273": {"text": "Sah man das gantze Heer hin in das Suntgau kommen/", "tokens": ["Sah", "man", "das", "gant\u00b7ze", "Heer", "hin", "in", "das", "Sunt\u00b7gau", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "ADJA", "NN", "ADV", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.274": {"text": "Da es den Winter durch sich zu verpflegen blieb.", "tokens": ["Da", "es", "den", "Win\u00b7ter", "durch", "sich", "zu", "ver\u00b7pfle\u00b7gen", "blieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "PRF", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.275": {"text": "Einmal nach M\u00fch in Ruh/ ist jedem Menschen lieb.", "tokens": ["Ein\u00b7mal", "nach", "M\u00fch", "in", "Ruh", "/", "ist", "je\u00b7dem", "Men\u00b7schen", "lieb", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "APPR", "NN", "$(", "VAFIN", "PIAT", "NN", "ADJD", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.276": {"text": "Di\u00df nahm de Wert in acht/ und sah da\u00df man den Frantzen", "tokens": ["Di\u00df", "nahm", "de", "Wert", "in", "acht", "/", "und", "sah", "da\u00df", "man", "den", "Frant\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "NE", "NE", "APPR", "CARD", "$(", "KON", "VVFIN", "KOUS", "PIS", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.277": {"text": "Die offt-best\u00fcrmte Br\u00fcck mit allen jhren Schatzen", "tokens": ["Die", "offt\u00b7be\u00b7st\u00fcrm\u00b7te", "Br\u00fcck", "mit", "al\u00b7len", "jhren", "Schat\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "PIAT", "PPOSAT", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.278": {"text": "Vertrauet hatt\u2019. Er kam/ that einen Sturm darauff/", "tokens": ["Ver\u00b7trau\u00b7et", "hatt'", ".", "Er", "kam", "/", "that", "ei\u00b7nen", "Sturm", "dar\u00b7auff", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "VAFIN", "$.", "PPER", "VVFIN", "$(", "VVFIN", "ART", "NN", "PAV", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.279": {"text": "und trieb das Frantzen-Volck sehr schleunig auf den Lauff/", "tokens": ["und", "trieb", "das", "Frant\u00b7zen\u00b7Volck", "sehr", "schleu\u00b7nig", "auf", "den", "Lauff", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "ADV", "ADJD", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.280": {"text": "Gewann den Ort/ den er von stunden an lie\u00df schleiffen/", "tokens": ["Ge\u00b7wann", "den", "Ort", "/", "den", "er", "von", "stun\u00b7den", "an", "lie\u00df", "schleif\u00b7fen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "$(", "PRELS", "PPER", "APPR", "ADJA", "APPR", "VVFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.281": {"text": "Aus Sorge da\u00df sein Feind/ jhn wieder anzugreiffen/", "tokens": ["Aus", "Sor\u00b7ge", "da\u00df", "sein", "Feind", "/", "jhn", "wie\u00b7der", "an\u00b7zu\u00b7greif\u00b7fen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KOUS", "PPOSAT", "NN", "$(", "PPER", "ADV", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.282": {"text": "Nicht leichtlich ruhen w\u00fcrd. In dem di\u00df hier verlieff/", "tokens": ["Nicht", "leicht\u00b7lich", "ru\u00b7hen", "w\u00fcrd", ".", "In", "dem", "di\u00df", "hier", "ver\u00b7lieff", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "VVINF", "VAFIN", "$.", "APPR", "ART", "PDS", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.283": {"text": "That dort der W\u00e4lsche Mann/ ", "tokens": ["That", "dort", "der", "W\u00e4l\u00b7sche", "Mann", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.284": {"text": "Auf des von Weymars Macht/ in Meynung gro\u00dfe Thaten", "tokens": ["Auf", "des", "von", "Wey\u00b7mars", "Macht", "/", "in", "Mey\u00b7nung", "gro\u00b7\u00dfe", "Tha\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "APPR", "NE", "NN", "$(", "APPR", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.285": {"text": "Zu w\u00fcrcken/ welches jhm nicht b\u00e4sser hat gerathen", "tokens": ["Zu", "w\u00fcr\u00b7cken", "/", "wel\u00b7ches", "jhm", "nicht", "b\u00e4s\u00b7ser", "hat", "ge\u00b7ra\u00b7then"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "$(", "PWS", "PPER", "PTKNEG", "ADJD", "VAFIN", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.286": {"text": "Als dem von Lothringen/ der hier Gesellschafft that.", "tokens": ["Als", "dem", "von", "Loth\u00b7rin\u00b7gen", "/", "der", "hier", "Ge\u00b7sell\u00b7schafft", "that", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "APPR", "NE", "$(", "ART", "ADV", "NN", "VVFIN", "$."], "meter": "+--+--+--+-+", "measure": "dactylic.tri.plus"}, "line.287": {"text": "Der L\u00f6u vernichtete des Fuchsen List und Raht.", "tokens": ["Der", "L\u00f6u", "ver\u00b7nich\u00b7te\u00b7te", "des", "Fuch\u00b7sen", "List", "und", "Raht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.288": {"text": "Die Sonne kam/ des Frosts sein Regiment zu schw\u00e4chen/", "tokens": ["Die", "Son\u00b7ne", "kam", "/", "des", "Frosts", "sein", "Re\u00b7gi\u00b7ment", "zu", "schw\u00e4\u00b7chen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "ART", "NN", "PPOSAT", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.289": {"text": "Di\u00df mahnete den Held von Weymar aufzubr\u00e4chen/", "tokens": ["Di\u00df", "mah\u00b7ne\u00b7te", "den", "Held", "von", "Wey\u00b7mar", "auf\u00b7zu\u00b7br\u00e4\u00b7chen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPR", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.290": {"text": "Was wichtigers zu thun. Es war in stiller Nacht/", "tokens": ["Was", "wich\u00b7ti\u00b7gers", "zu", "thun", ".", "Es", "war", "in", "stil\u00b7ler", "Nacht", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "PTKZU", "VVINF", "$.", "PPER", "VAFIN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.291": {"text": "Da er sich \u00fcber Reyhn mit aller seiner Macht", "tokens": ["Da", "er", "sich", "\u00fc\u00b7ber", "Reyhn", "mit", "al\u00b7ler", "sei\u00b7ner", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "APPR", "NN", "APPR", "PIAT", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.292": {"text": "Nach Seckingen begab. Di\u00df schleunig eingenommen/", "tokens": ["Nach", "Se\u00b7ckin\u00b7gen", "be\u00b7gab", ".", "Di\u00df", "schleu\u00b7nig", "ein\u00b7ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$.", "PDS", "ADJD", "VVPP", "$("], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.293": {"text": "Ist er nach Bicken hin/ nicht weit von Basel kommen.", "tokens": ["Ist", "er", "nach", "Bi\u00b7cken", "hin", "/", "nicht", "weit", "von", "Ba\u00b7sel", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "PTKVZ", "$(", "PTKNEG", "ADJD", "APPR", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.294": {"text": "Hier auf nach Lauffenburg/ Er kam bey beyden ein/", "tokens": ["Hier", "auf", "nach", "Lauf\u00b7fen\u00b7burg", "/", "Er", "kam", "bey", "bey\u00b7den", "ein", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "APPR", "NE", "$(", "PPER", "VVFIN", "APPR", "PIAT", "ART", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.295": {"text": "und wolt\u2019 auch \u00fcber di\u00df im festen Reinfeld seyn.", "tokens": ["und", "wolt'", "auch", "\u00fc\u00b7ber", "di\u00df", "im", "fes\u00b7ten", "Re\u00b7in\u00b7feld", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "APPR", "PDS", "APPRART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.296": {"text": "Er satzte sich hierum/ beschlo\u00df es mit besch\u00fc\u00dfen.", "tokens": ["Er", "satz\u00b7te", "sich", "hie\u00b7rum", "/", "be\u00b7schlo\u00df", "es", "mit", "be\u00b7sch\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PAV", "$(", "VVFIN", "PPER", "APPR", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.297": {"text": "Als die bel\u00e4gerte sich nichts bewegen lie\u00dfen/", "tokens": ["Als", "die", "be\u00b7l\u00e4\u00b7ger\u00b7te", "sich", "nichts", "be\u00b7we\u00b7gen", "lie\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "VVFIN", "PRF", "PIS", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.298": {"text": "Scho\u00df er viel Feuer ein/ und qu\u00e4lte Tag und Nacht.", "tokens": ["Scho\u00df", "er", "viel", "Feu\u00b7er", "ein", "/", "und", "qu\u00e4l\u00b7te", "Tag", "und", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PIAT", "NN", "ART", "$(", "KON", "ADJA", "NN", "KON", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.299": {"text": "Di\u00df alles reitzete die K\u00e4yserliche Macht", "tokens": ["Di\u00df", "al\u00b7les", "reit\u00b7ze\u00b7te", "die", "K\u00e4y\u00b7ser\u00b7li\u00b7che", "Macht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "PIS", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.300": {"text": "Dem Ort Entsatz zu thun. Sie kam mit gro\u00dfen Scharen/", "tokens": ["Dem", "Ort", "Ent\u00b7satz", "zu", "thun", ".", "Sie", "kam", "mit", "gro\u00b7\u00dfen", "Scha\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PTKZU", "VVINF", "$.", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.301": {"text": "Worbey de Wert/ Lamboy und der Savelli waren/", "tokens": ["Wor\u00b7bey", "de", "Wert", "/", "Lam\u00b7boy", "und", "der", "Sa\u00b7vel\u00b7li", "wa\u00b7ren", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "NE", "$(", "NE", "KON", "ART", "NE", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.302": {"text": "Sverreuter/ Enckefort/ Goliz/ Neuneck/ F\u00fcrstenbergk/", "tokens": ["Sver\u00b7reu\u00b7ter", "/", "En\u00b7cke\u00b7fort", "/", "Go\u00b7liz", "/", "Neu\u00b7neck", "/", "F\u00fcrs\u00b7ten\u00b7bergk", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "NE", "$(", "NE", "$(", "NN", "$("], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.303": {"text": "Die legten alle Hand an die Entsatzungs Werck.", "tokens": ["Die", "leg\u00b7ten", "al\u00b7le", "Hand", "an", "die", "Ent\u00b7sat\u00b7zungs", "Werck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIAT", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.304": {"text": "Der Weymar diese Macht verm\u00e4rckend/ lie\u00df die Schan-", "tokens": ["Der", "Wey\u00b7mar", "die\u00b7se", "Macht", "ver\u00b7m\u00e4r\u00b7ckend", "/", "lie\u00df", "die", "Schan"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "PDAT", "NN", "VVPP", "$(", "VVFIN", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.305": {"text": "Gieng in das freye Feld und f\u00e4llte seine Lantzen", "tokens": ["Gieng", "in", "das", "frey\u00b7e", "Feld", "und", "f\u00e4ll\u00b7te", "sei\u00b7ne", "Lant\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "ADJA", "NN", "KON", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.306": {"text": "Den Feinden in die Stirn. Es gieng zu einer Schlacht/", "tokens": ["Den", "Fein\u00b7den", "in", "die", "Stirn", ".", "Es", "gieng", "zu", "ei\u00b7ner", "Schlacht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NN", "$.", "PPER", "VVFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.307": {"text": "Die dauerte so lang/ bi\u00df da\u00df die tunckle Nacht", "tokens": ["Die", "dau\u00b7er\u00b7te", "so", "lang", "/", "bi\u00df", "da\u00df", "die", "tunck\u00b7le", "Nacht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "ADJD", "$(", "APPR", "KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.308": {"text": "Das Metzeln unternam. Noch kunte man nicht sagen", "tokens": ["Das", "Met\u00b7zeln", "un\u00b7ter\u00b7nam", ".", "Noch", "kun\u00b7te", "man", "nicht", "sa\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "$.", "ADV", "VMFIN", "PIS", "PTKNEG", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.309": {"text": "Wer ", "tokens": ["Wer"], "token_info": ["word"], "pos": ["PWS"], "meter": "-", "measure": "single.down"}, "line.310": {"text": "und auch zur neuen Schlacht. Kein C", "tokens": ["und", "auch", "zur", "neu\u00b7en", "Schlacht", ".", "Kein", "C"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "ADV", "APPRART", "ADJA", "NN", "$.", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.311": {"text": "Pompejus/ Sapio/ Gambriv/ und die so viel", "tokens": ["Pom\u00b7pe\u00b7jus", "/", "Sa\u00b7pio", "/", "Gam\u00b7briv", "/", "und", "die", "so", "viel"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "$(", "NE", "$(", "NN", "$(", "KON", "ART", "ADV", "ADV"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.312": {"text": "Von Thaten kuntig sind/ vermochten so zu streiten", "tokens": ["Von", "Tha\u00b7ten", "kun\u00b7tig", "sind", "/", "ver\u00b7moch\u00b7ten", "so", "zu", "strei\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "ADJD", "VAFIN", "$(", "VVFIN", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.313": {"text": "Als man auf diesen Tag von diesen beyden Seiten", "tokens": ["Als", "man", "auf", "die\u00b7sen", "Tag", "von", "die\u00b7sen", "bey\u00b7den", "Sei\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "PDAT", "NN", "APPR", "PDAT", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.314": {"text": "Ein grimmig fechten sah. Wir sahen eine Schar", "tokens": ["Ein", "grim\u00b7mig", "fech\u00b7ten", "sah", ".", "Wir", "sa\u00b7hen", "ei\u00b7ne", "Schar"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "VVINF", "VVFIN", "$.", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.315": {"text": "Von L\u00f6uen/ derer Zorn ein flammend Feuer war.", "tokens": ["Von", "L\u00f6u\u00b7en", "/", "de\u00b7rer", "Zorn", "ein", "flam\u00b7mend", "Feu\u00b7er", "war", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$(", "PDS", "NN", "ART", "ADJD", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.316": {"text": "Der wolte Reinfelds Hilff/ der dessen Herscher hei\u00dfen/", "tokens": ["Der", "wol\u00b7te", "Re\u00b7in\u00b7felds", "Hilff", "/", "der", "des\u00b7sen", "Her\u00b7scher", "hei\u00b7\u00dfen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "NE", "NE", "$(", "ART", "PRELAT", "NN", "VVINF", "$("], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.317": {"text": "Es sparte keiner nichts. Nach einem langen schmei\u00dfen", "tokens": ["Es", "spar\u00b7te", "kei\u00b7ner", "nichts", ".", "Nach", "ei\u00b7nem", "lan\u00b7gen", "schmei\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIS", "PIS", "$.", "APPR", "ART", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.318": {"text": "und w\u00fcrgen fiel der Sieg ", "tokens": ["und", "w\u00fcr\u00b7gen", "fiel", "der", "Sieg"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVINF", "VVFIN", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.319": {"text": "Da lagen tausend Feind\u2019 in einer langen Ruh/", "tokens": ["Da", "la\u00b7gen", "tau\u00b7send", "Feind'", "in", "ei\u00b7ner", "lan\u00b7gen", "Ruh", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "CARD", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.320": {"text": "Vier tausend musten sich gefangen lassen nehmen/", "tokens": ["Vier", "tau\u00b7send", "mus\u00b7ten", "sich", "ge\u00b7fan\u00b7gen", "las\u00b7sen", "neh\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "CARD", "VMFIN", "PRF", "ADJD", "VVINF", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.321": {"text": "Und Reinfeld muste sich hierauf auch bald bequemen/", "tokens": ["Und", "Re\u00b7in\u00b7feld", "mus\u00b7te", "sich", "hier\u00b7auf", "auch", "bald", "be\u00b7que\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VMFIN", "PRF", "PAV", "ADV", "ADV", "ADJA", "$("], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.322": {"text": "Weil Muth und Macht erlag. Wie/ war auch wol ein", "tokens": ["Weil", "Muth", "und", "Macht", "er\u00b7lag", ".", "Wie", "/", "war", "auch", "wol", "ein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "KON", "NN", "VVFIN", "$.", "PWAV", "$(", "VAFIN", "ADV", "ADV", "ART"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.323": {"text": "Held", "tokens": ["Held"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.324": {"text": "Von diesem gantzen Heer/ der weiter in das Feld", "tokens": ["Von", "die\u00b7sem", "gant\u00b7zen", "Heer", "/", "der", "wei\u00b7ter", "in", "das", "Feld"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$(", "ART", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.325": {"text": "Zu gehen m\u00e4chtig war? Sie wurden mit einander", "tokens": ["Zu", "ge\u00b7hen", "m\u00e4ch\u00b7tig", "war", "?", "Sie", "wur\u00b7den", "mit", "ein\u00b7an\u00b7der"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "ADJD", "VAFIN", "$.", "PPER", "VAFIN", "APPR", "PRF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.326": {"text": "(lamboyen au\u00dfgesetzt) dem S\u00e4chschen Alexander", "tokens": ["(", "lam\u00b7bo\u00b7yen", "au\u00df\u00b7ge\u00b7setzt", ")", "dem", "S\u00e4ch\u00b7schen", "A\u00b7lex\u00b7an\u00b7der"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "VVINF", "VVPP", "$(", "ART", "NN", "NE"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.327": {"text": "Gefangen eingebracht. De Wert und Enckefort", "tokens": ["Ge\u00b7fan\u00b7gen", "ein\u00b7ge\u00b7bracht", ".", "De", "Wert", "und", "En\u00b7cke\u00b7fort"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "VVPP", "$.", "NE", "NE", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.328": {"text": "Die musten nach Paris/ und weiter an ein Ort", "tokens": ["Die", "mus\u00b7ten", "nach", "Pa\u00b7ris", "/", "und", "wei\u00b7ter", "an", "ein", "Ort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VMFIN", "APPR", "NE", "$(", "KON", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.329": {"text": "Das wol bewahret ist/ ein Heyl des Horns zu werden/", "tokens": ["Das", "wol", "be\u00b7wah\u00b7ret", "ist", "/", "ein", "Heyl", "des", "Horns", "zu", "wer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "VVPP", "VAFIN", "$(", "ART", "NN", "ART", "NN", "PTKZU", "VAINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.330": {"text": "Savelli aber drung mit List durch die Beschwerden/", "tokens": ["Sa\u00b7vel\u00b7li", "a\u00b7ber", "drung", "mit", "List", "durch", "die", "Be\u00b7schwer\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "ADV", "APPR", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.331": {"text": "Die die Gefangenschafft mag geben. Di\u00df gethan/", "tokens": ["Die", "die", "Ge\u00b7fan\u00b7gen\u00b7schafft", "mag", "ge\u00b7ben", ".", "Di\u00df", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "VMFIN", "VVINF", "$.", "PDS", "VVPP", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.332": {"text": "Fieng man mit Retelen/ mit Neu- und Friedburg an.", "tokens": ["Fi\u00b7eng", "man", "mit", "Re\u00b7te\u00b7len", "/", "mit", "Neu", "und", "Fried\u00b7burg", "an", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "NN", "$(", "APPR", "TRUNC", "KON", "NE", "PTKVZ", "$."], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.333": {"text": "Sie aber sonder Macht/ so gro\u00dfer Macht zu steuern/", "tokens": ["Sie", "a\u00b7ber", "son\u00b7der", "Macht", "/", "so", "gro\u00b7\u00dfer", "Macht", "zu", "steu\u00b7ern", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADJA", "NN", "$(", "ADV", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.334": {"text": "Ergaben sich. Was Post war dieses bey den Beyern", "tokens": ["Er\u00b7ga\u00b7ben", "sich", ".", "Was", "Post", "war", "die\u00b7ses", "bey", "den", "Be\u00b7yern"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "$.", "PWS", "NN", "VAFIN", "PDAT", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.335": {"text": "und noch viel mehr zu Wien? Hieran war nicht genug/", "tokens": ["und", "noch", "viel", "mehr", "zu", "Wi\u00b7en", "?", "Hie\u00b7ran", "war", "nicht", "ge\u00b7nug", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "ADV", "APPR", "NE", "$.", "PAV", "VAFIN", "PTKNEG", "ADV", "$("], "meter": "--+--+--+-+-+", "measure": "anapaest.tri.plus"}, "line.336": {"text": "Man that nicht lang hierauf noch einen gr\u00f6\u00dfern Zug/", "tokens": ["Man", "that", "nicht", "lang", "hier\u00b7auf", "noch", "ei\u00b7nen", "gr\u00f6\u00b7\u00dfern", "Zug", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PTKNEG", "ADJD", "PAV", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.337": {"text": "Den drey Armeen nicht vermochten abzuwenden.", "tokens": ["Den", "drey", "Ar\u00b7me\u00b7en", "nicht", "ver\u00b7moch\u00b7ten", "ab\u00b7zu\u00b7wen\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "CARD", "NN", "PTKNEG", "VVFIN", "VVIZU", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.338": {"text": "Halt ein ", "tokens": ["Halt", "ein"], "token_info": ["word", "word"], "pos": ["VVIMP", "ART"], "meter": "-+", "measure": "iambic.single"}, "line.339": {"text": "Stehstu im schreiben mir hinf\u00fcro nochmals bey/", "tokens": ["Steh\u00b7stu", "im", "schrei\u00b7ben", "mir", "hin\u00b7f\u00fc\u00b7ro", "noch\u00b7mals", "bey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPRART", "VVFIN", "PPER", "ADV", "ADV", "APPR", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.340": {"text": "So sage wie es dort mit den Banirschen sey.", "tokens": ["So", "sa\u00b7ge", "wie", "es", "dort", "mit", "den", "Ba\u00b7nir\u00b7schen", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "KOKOM", "PPER", "ADV", "APPR", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}