{"textgrid.poem.65163": {"metadata": {"author": {"name": "Paoli, Betty", "birth": "N.A.", "death": "N.A."}, "title": "4.", "genre": "verse", "period": "N.A.", "pub_year": 1854, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbdas nenn' ich eine Kinderzucht!", "tokens": ["\u00bb", "das", "nenn'", "ich", "ei\u00b7ne", "Kin\u00b7der\u00b7zucht", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PDS", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das w\u00e4re mir die rechte Liebe,", "tokens": ["Das", "w\u00e4\u00b7re", "mir", "die", "rech\u00b7te", "Lie\u00b7be", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die alles zu entschuld'gen sucht,", "tokens": ["Die", "al\u00b7les", "zu", "ent\u00b7schuld'\u00b7gen", "sucht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was immer auch ihr Abgott triebe!", "tokens": ["Was", "im\u00b7mer", "auch", "ihr", "Ab\u00b7gott", "trie\u00b7be", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADV", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wie lang noch, und das Tierchen h\u00e4lt", "tokens": ["Wie", "lang", "noch", ",", "und", "das", "Tier\u00b7chen", "h\u00e4lt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "ADV", "$,", "KON", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sich f\u00fcr den Mittelpunkt der Welt!\u00ab", "tokens": ["Sich", "f\u00fcr", "den", "Mit\u00b7tel\u00b7punkt", "der", "Welt", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PRF", "APPR", "ART", "NN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Dein Vater sprach's, der grimme Mann!", "tokens": ["Dein", "Va\u00b7ter", "sprach's", ",", "der", "grim\u00b7me", "Mann", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NE", "$,", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Predigt war zu ", "tokens": ["Die", "Pre\u00b7digt", "war", "zu"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "APPR"], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Weil ich, da du in Acht und Bann,", "tokens": ["Weil", "ich", ",", "da", "du", "in", "Acht", "und", "Bann", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "KOUS", "PPER", "APPR", "CARD", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zu laut Partei f\u00fcr dich genommen.", "tokens": ["Zu", "laut", "Par\u00b7tei", "f\u00fcr", "dich", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "NN", "APPR", "PPER", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Mir ward dabei ganz schw\u00fcl und hei\u00df, \u2013", "tokens": ["Mir", "ward", "da\u00b7bei", "ganz", "schw\u00fcl", "und", "hei\u00df", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PAV", "ADV", "ADJD", "KON", "ADJD", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ich sagte nichts und duckte leis.", "tokens": ["Ich", "sag\u00b7te", "nichts", "und", "duck\u00b7te", "leis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "KON", "VVFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Dein M\u00fctterlein nahm's nicht so arg,", "tokens": ["Dein", "M\u00fct\u00b7ter\u00b7lein", "nahm's", "nicht", "so", "arg", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NE", "PTKNEG", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mild klang das Wort der Guten, Sch\u00f6nen!", "tokens": ["Mild", "klang", "das", "Wort", "der", "Gu\u00b7ten", ",", "Sch\u00f6\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "VVFIN", "ART", "NN", "ART", "NN", "$,", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbdas Leben ist mit Liebe karg, \u2013", "tokens": ["\u00bb", "das", "Le\u00b7ben", "ist", "mit", "Lie\u00b7be", "karg", ",", "\u2013"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "APPR", "NN", "VVFIN", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mag sie des Kindes Stirne kr\u00f6nen!", "tokens": ["Mag", "sie", "des", "Kin\u00b7des", "Stir\u00b7ne", "kr\u00f6\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Thut sie zu viel, das Weltgebraus", "tokens": ["Thut", "sie", "zu", "viel", ",", "das", "Welt\u00b7ge\u00b7braus"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "PTKA", "PIS", "$,", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Gleicht's einst durch manch' Zuwenig aus.\u00ab", "tokens": ["Gleicht's", "einst", "durch", "man\u00b7ch'", "Zu\u00b7we\u00b7nig", "aus", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "ADV", "APPR", "PIAT", "NN", "PTKVZ", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Er drauf: \u00bbEin wunderlicher Schlu\u00df!", "tokens": ["Er", "drauf", ":", "\u00bb", "Ein", "wun\u00b7der\u00b7li\u00b7cher", "Schlu\u00df", "!"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "$.", "$(", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Weil rauhe Pfade zu beschreiten,", "tokens": ["Weil", "rau\u00b7he", "Pfa\u00b7de", "zu", "be\u00b7schrei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Soll durch Verw\u00f6hnung man den Fu\u00df,", "tokens": ["Soll", "durch", "Ver\u00b7w\u00f6h\u00b7nung", "man", "den", "Fu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "NN", "PIS", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So meinst du, darauf vorbereiten!", "tokens": ["So", "meinst", "du", ",", "da\u00b7rauf", "vor\u00b7be\u00b7rei\u00b7ten", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$,", "PAV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wie kalt die Welt, wie ungelind,", "tokens": ["Wie", "kalt", "die", "Welt", ",", "wie", "un\u00b7ge\u00b7lind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ART", "NN", "$,", "PWAV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "F\u00fchlt doppelt das verzog'ne Kind!\u00ab", "tokens": ["F\u00fchlt", "dop\u00b7pelt", "das", "ver\u00b7zo\u00b7g'\u00b7ne", "Kind", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "ADJD", "ART", "ADJA", "NN", "$.", "$("], "meter": "-----+--+", "measure": "iambic.di.chol"}}, "stanza.5": {"line.1": {"text": "Still l\u00e4chelnd blickt' ich vor mich hin.", "tokens": ["Still", "l\u00e4\u00b7chelnd", "blickt'", "ich", "vor", "mich", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "VVFIN", "PPER", "APPR", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dich zu verzieh'n, mein liebes Leben!", "tokens": ["Dich", "zu", "ver\u00b7zieh'n", ",", "mein", "lie\u00b7bes", "Le\u00b7ben", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "PTKZU", "VVINF", "$,", "PPOSAT", "ADJA", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "H\u00e4tt' ich so Schlimmes auch im Sinn,", "tokens": ["H\u00e4tt'", "ich", "so", "Schlim\u00b7mes", "auch", "im", "Sinn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nicht Zeit w\u00e4r' mir dazu gegeben.", "tokens": ["Nicht", "Zeit", "w\u00e4r'", "mir", "da\u00b7zu", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "NN", "VAFIN", "PPER", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dein Morgen- ist mein Abendrot, \u2013", "tokens": ["Dein", "Mor\u00b7gen", "ist", "mein", "A\u00b7ben\u00b7drot", ",", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "TRUNC", "VAFIN", "PPOSAT", "NN", "$,", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Eh' du verzogen, bin ich tot.", "tokens": ["Eh'", "du", "ver\u00b7zo\u00b7gen", ",", "bin", "ich", "tot", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "$,", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}