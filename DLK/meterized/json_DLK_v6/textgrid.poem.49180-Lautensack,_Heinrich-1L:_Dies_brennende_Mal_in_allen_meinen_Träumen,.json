{"textgrid.poem.49180": {"metadata": {"author": {"name": "Lautensack, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "1L: Dies brennende Mal in allen meinen Tr\u00e4umen,", "genre": "verse", "period": "N.A.", "pub_year": 1900, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Dies brennende Mal in allen meinen Tr\u00e4umen,", "tokens": ["Dies", "bren\u00b7nen\u00b7de", "Mal", "in", "al\u00b7len", "mei\u00b7nen", "Tr\u00e4u\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJA", "NN", "APPR", "PIAT", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "vom Tod selbst eingebrannt, aus Niedertracht!", "tokens": ["vom", "Tod", "selbst", "ein\u00b7ge\u00b7brannt", ",", "aus", "Nie\u00b7der\u00b7tracht", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "VVPP", "$,", "APPR", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Dies brennende Mal in allen meinen Tr\u00e4umen:", "tokens": ["Dies", "bren\u00b7nen\u00b7de", "Mal", "in", "al\u00b7len", "mei\u00b7nen", "Tr\u00e4u\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJA", "NN", "APPR", "PIAT", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "\u2013 \u2013 \u2013 \u2013", "tokens": ["\u2013", "\u2013", "\u2013", "\u2013"], "token_info": ["punct", "punct", "punct", "punct"], "pos": ["$(", "$(", "$(", "$("]}}, "stanza.3": {"line.1": {"text": "Noch wagte ich es nicht, danach zu fragen,", "tokens": ["Noch", "wag\u00b7te", "ich", "es", "nicht", ",", "da\u00b7nach", "zu", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "PTKNEG", "$,", "PAV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "ob dieser Schein, der umgeht, Hand zu Hand \u2013 \u2013", "tokens": ["ob", "die\u00b7ser", "Schein", ",", "der", "um\u00b7geht", ",", "Hand", "zu", "Hand", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PDAT", "NN", "$,", "PRELS", "VVFIN", "$,", "NN", "APPR", "NN", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Noch wagte ich es nicht, danach zu fragen,", "tokens": ["Noch", "wag\u00b7te", "ich", "es", "nicht", ",", "da\u00b7nach", "zu", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "PTKNEG", "$,", "PAV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "ob dies Papier denn einzuwechseln w\u00e4re", "tokens": ["ob", "dies", "Pa\u00b7pier", "denn", "ein\u00b7zu\u00b7wech\u00b7seln", "w\u00e4\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDS", "NN", "KON", "VVINF", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "f\u00fcr just so viel als wie bar Geld, bar Geld \u2013 \u2013", "tokens": ["f\u00fcr", "just", "so", "viel", "als", "wie", "bar", "Geld", ",", "bar", "Geld", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["APPR", "ADV", "ADV", "ADV", "KOUS", "KOKOM", "ADJD", "NN", "$,", "ADJD", "NN", "$(", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "ob dies Papier denn einzuwechseln w\u00e4re?", "tokens": ["ob", "dies", "Pa\u00b7pier", "denn", "ein\u00b7zu\u00b7wech\u00b7seln", "w\u00e4\u00b7re", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "NN", "KON", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Ich glaub' noch immer: 's ist ein \u00fcbler Streich,", "tokens": ["Ich", "glaub'", "noch", "im\u00b7mer", ":", "'s", "ist", "ein", "\u00fcb\u00b7ler", "Streich", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "dir angetan, aus jenen \u00bbSchieber\u00ab-Kreisen ...", "tokens": ["dir", "an\u00b7ge\u00b7tan", ",", "aus", "je\u00b7nen", "\u00bb", "Schie\u00b7ber", "\u00ab", "..."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVPP", "$,", "APPR", "PDAT", "$(", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "ich glaub' noch immer: 's ist ein \u00fcbler Streich.", "tokens": ["ich", "glaub'", "noch", "im\u00b7mer", ":", "'s", "ist", "ein", "\u00fcb\u00b7ler", "Streich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "$.", "PPER", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}}, "stanza.6": {"line.1": {"text": "Du kannst nicht sein wie die, die Frauen lieben!", "tokens": ["Du", "kannst", "nicht", "sein", "wie", "die", ",", "die", "Frau\u00b7en", "lie\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VAINF", "KOKOM", "ART", "$,", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Dein Auge leugnet's, und dein Mund, dein Haar!", "tokens": ["Dein", "Au\u00b7ge", "leug\u00b7net's", ",", "und", "dein", "Mund", ",", "dein", "Haar", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$,", "KON", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Du kannst nicht sein wie die, die Frauen lieben,", "tokens": ["Du", "kannst", "nicht", "sein", "wie", "die", ",", "die", "Frau\u00b7en", "lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PTKNEG", "VAINF", "KOKOM", "ART", "$,", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "du w\u00e4rst sonst nicht unendlich Weib geblieben!", "tokens": ["du", "w\u00e4rst", "sonst", "nicht", "un\u00b7end\u00b7lich", "Weib", "ge\u00b7blie\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "ADJD", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "dein Aug' ist wahr! dein Mund ist wahr! dein Haar ...", "tokens": ["dein", "Aug'", "ist", "wahr", "!", "dein", "Mund", "ist", "wahr", "!", "dein", "Haar", "..."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADJD", "$.", "PPOSAT", "NN", "VAFIN", "ADJD", "$.", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "du w\u00e4rst sonst nicht unendlich Weib geblieben!", "tokens": ["du", "w\u00e4rst", "sonst", "nicht", "un\u00b7end\u00b7lich", "Weib", "ge\u00b7blie\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "ADJD", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "\u2013 \u2013 \u2013 \u2013", "tokens": ["\u2013", "\u2013", "\u2013", "\u2013"], "token_info": ["punct", "punct", "punct", "punct"], "pos": ["$(", "$(", "$(", "$("]}}, "stanza.9": {"line.1": {"text": "so: wie der Abglanz deiner nackten F\u00fc\u00dfe,", "tokens": ["so", ":", "wie", "der", "Ab\u00b7glanz", "dei\u00b7ner", "nack\u00b7ten", "F\u00fc\u00b7\u00dfe", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PWAV", "ART", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "wenn sie am Abend (wei\u00dft du noch?) hinliefen,", "tokens": ["wenn", "sie", "am", "A\u00b7bend", "(", "wei\u00dft", "du", "noch", "?", ")", "hin\u00b7lie\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "$(", "VVFIN", "PPER", "ADV", "$.", "$(", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "so: wie der Abglanz deiner nackten F\u00fc\u00dfe", "tokens": ["so", ":", "wie", "der", "Ab\u00b7glanz", "dei\u00b7ner", "nack\u00b7ten", "F\u00fc\u00b7\u00dfe"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "$.", "PWAV", "ART", "NN", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.10": {"line.1": {"text": "dem n\u00e4chsten Morgen erst ganz Glanz und S\u00fc\u00dfe", "tokens": ["dem", "n\u00e4chs\u00b7ten", "Mor\u00b7gen", "erst", "ganz", "Glanz", "und", "S\u00fc\u00b7\u00dfe"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ADV", "ADV", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "aufdr\u00fcckte, da die Blumen noch alle schliefen ...", "tokens": ["auf\u00b7dr\u00fcck\u00b7te", ",", "da", "die", "Blu\u00b7men", "noch", "al\u00b7le", "schlie\u00b7fen", "..."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "ART", "NN", "ADV", "PIS", "VVFIN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "dem n\u00e4chsten Morgen erst ganz Glanz und S\u00fc\u00dfe!", "tokens": ["dem", "n\u00e4chs\u00b7ten", "Mor\u00b7gen", "erst", "ganz", "Glanz", "und", "S\u00fc\u00b7\u00dfe", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "ADV", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "und wie dein Haar den gelben Flimmer auslieh,", "tokens": ["und", "wie", "dein", "Haar", "den", "gel\u00b7ben", "Flim\u00b7mer", "aus\u00b7lieh", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPOSAT", "NN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "da\u00df sich der neue Tag fein schm\u00fccken konnte!", "tokens": ["da\u00df", "sich", "der", "neu\u00b7e", "Tag", "fein", "schm\u00fc\u00b7cken", "konn\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "ADJA", "NN", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "oh, wie dein Haar den gelben Flimmer auslieh,", "tokens": ["oh", ",", "wie", "dein", "Haar", "den", "gel\u00b7ben", "Flim\u00b7mer", "aus\u00b7lieh", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PWAV", "PPOSAT", "NN", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "+--+-+-+--+", "measure": "iambic.penta.invert"}}, "stanza.12": {"line.1": {"text": "da\u00df sich die ganze, ganze Welt draus sonnte!", "tokens": ["da\u00df", "sich", "die", "gan\u00b7ze", ",", "gan\u00b7ze", "Welt", "draus", "sonn\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "ADJA", "$,", "ADJA", "NN", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "\u2013 \u2013 \u2013 \u2013", "tokens": ["\u2013", "\u2013", "\u2013", "\u2013"], "token_info": ["punct", "punct", "punct", "punct"], "pos": ["$(", "$(", "$(", "$("]}, "line.3": {"text": "\u2013 \u2013 \u2013 \u2013", "tokens": ["\u2013", "\u2013", "\u2013", "\u2013"], "token_info": ["punct", "punct", "punct", "punct"], "pos": ["$(", "$(", "$(", "$("]}}, "stanza.13": {"line.1": {"text": "Es ist nicht! Kann nicht sein! ich darf's nicht glauben!", "tokens": ["Es", "ist", "nicht", "!", "Kann", "nicht", "sein", "!", "ich", "dar\u00b7f's", "nicht", "glau\u00b7ben", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "$.", "VMFIN", "PTKNEG", "VAINF", "$.", "PPER", "PAV", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.2": {"text": "und ob ich hunderttausend Tr\u00e4nen wein' ...", "tokens": ["und", "ob", "ich", "hun\u00b7dert\u00b7tau\u00b7send", "Tr\u00e4\u00b7nen", "wein'", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "CARD", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "es ist nicht! Kann nicht sein! ich darf's nicht glauben!", "tokens": ["es", "ist", "nicht", "!", "Kann", "nicht", "sein", "!", "ich", "dar\u00b7f's", "nicht", "glau\u00b7ben", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "$.", "VMFIN", "PTKNEG", "VAINF", "$.", "PPER", "PAV", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.14": {"line.1": {"text": "und meine Tr\u00e4ume sollen mir nichts l\u00fcgen \u2013", "tokens": ["und", "mei\u00b7ne", "Tr\u00e4u\u00b7me", "sol\u00b7len", "mir", "nichts", "l\u00fc\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VMFIN", "PPER", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "und sein sie millionenmal vom Tod \u2013", "tokens": ["und", "sein", "sie", "mil\u00b7li\u00b7o\u00b7nen\u00b7mal", "vom", "Tod", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "PPER", "ADV", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und meine Tr\u00e4ume sollen mir nicht mehr l\u00fcgen \u2013", "tokens": ["und", "mei\u00b7ne", "Tr\u00e4u\u00b7me", "sol\u00b7len", "mir", "nicht", "mehr", "l\u00fc\u00b7gen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VMFIN", "PPER", "PTKNEG", "ADV", "VVFIN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.15": {"line.1": {"text": "dein Leib ist Brot, dein Leib ist heilig Brot,", "tokens": ["dein", "Leib", "ist", "Brot", ",", "dein", "Leib", "ist", "hei\u00b7lig", "Brot", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "NN", "$,", "PPOSAT", "NN", "VAFIN", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "und deine K\u00fcsse sind wie Kypernwein,", "tokens": ["und", "dei\u00b7ne", "K\u00fcs\u00b7se", "sind", "wie", "Ky\u00b7pern\u00b7wein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "KOKOM", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und \u2013 \u2013 also harr' ich, wurzelnd tief im Glauben,", "tokens": ["und", "\u2013", "\u2013", "al\u00b7so", "harr'", "ich", ",", "wur\u00b7zelnd", "tief", "im", "Glau\u00b7ben", ","], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$(", "$(", "ADV", "VAFIN", "PPER", "$,", "VAFIN", "ADJD", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.16": {"line.1": {"text": "so wie M\u00e4rtyrer einst zutiefst im Herrn ...", "tokens": ["so", "wie", "M\u00e4r\u00b7ty\u00b7rer", "einst", "zu\u00b7tiefst", "im", "Herrn", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "NN", "ADV", "VVFIN", "APPRART", "NN", "$("], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}}, "stanza.17": {"line.1": {"text": "Am Himmel meiner Seele steht ein Stern ...", "tokens": ["Am", "Him\u00b7mel", "mei\u00b7ner", "See\u00b7le", "steht", "ein", "Stern", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}