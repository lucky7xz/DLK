{"textgrid.poem.49702": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Resignation", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es gibt noch Leute, die sich qu\u00e4len,", "tokens": ["Es", "gibt", "noch", "Leu\u00b7te", ",", "die", "sich", "qu\u00e4\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "NN", "$,", "PRELS", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Aus denen sich die Frage ringt:", "tokens": ["Aus", "de\u00b7nen", "sich", "die", "Fra\u00b7ge", "ringt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PRF", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie wird der Deutsche n\u00e4chstens w\u00e4hlen?", "tokens": ["Wie", "wird", "der", "Deut\u00b7sche", "n\u00e4chs\u00b7tens", "w\u00e4h\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie wird das, was die Urne bringt?", "tokens": ["Wie", "wird", "das", ",", "was", "die", "Ur\u00b7ne", "bringt", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PDS", "$,", "PRELS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Die Guten! Wie sie immer hoffen!", "tokens": ["Die", "Gu\u00b7ten", "!", "Wie", "sie", "im\u00b7mer", "hof\u00b7fen", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PWAV", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie macht sie doch ein jedesmal", "tokens": ["Wie", "macht", "sie", "doch", "ein", "je\u00b7des\u00b7mal"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "ADV", "ART", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Ausfall neuerdings betroffen!", "tokens": ["Der", "Aus\u00b7fall", "neu\u00b7er\u00b7dings", "be\u00b7trof\u00b7fen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als w\u00e4r' er anders, wie normal!", "tokens": ["Als", "w\u00e4r'", "er", "an\u00b7ders", ",", "wie", "nor\u00b7mal", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "ADV", "$,", "PWAV", "ADV", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "Wir wissen doch von Adam Riese,", "tokens": ["Wir", "wis\u00b7sen", "doch", "von", "A\u00b7dam", "Rie\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df zwei mal zwei gleich vieren z\u00e4hlt.", "tokens": ["Da\u00df", "zwei", "mal", "zwei", "gleich", "vie\u00b7ren", "z\u00e4hlt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "CARD", "ADV", "CARD", "ADV", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und eine Wahrheit fest wie diese", "tokens": ["Und", "ei\u00b7ne", "Wahr\u00b7heit", "fest", "wie", "die\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "PTKVZ", "KOKOM", "PDS"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ist, da\u00df man immer Schwarze w\u00e4hlt.", "tokens": ["Ist", ",", "da\u00df", "man", "im\u00b7mer", "Schwar\u00b7ze", "w\u00e4hlt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PIS", "ADV", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Das Faktum l\u00e4\u00dft sich nicht bestreiten,", "tokens": ["Das", "Fak\u00b7tum", "l\u00e4\u00dft", "sich", "nicht", "be\u00b7strei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auch wenn es noch so bitter schmeckt.", "tokens": ["Auch", "wenn", "es", "noch", "so", "bit\u00b7ter", "schmeckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "ADV", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch hat das \u00dcbel gute Seiten:", "tokens": ["Doch", "hat", "das", "\u00dc\u00b7bel", "gu\u00b7te", "Sei\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es ruhet nicht auf Intellekt.", "tokens": ["Es", "ru\u00b7het", "nicht", "auf", "In\u00b7tel\u00b7lekt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Man mu\u00df die Sache recht verstehen;", "tokens": ["Man", "mu\u00df", "die", "Sa\u00b7che", "recht", "ver\u00b7ste\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "ART", "NN", "ADJD", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sie ist nicht b\u00f6se, ist nicht gut.", "tokens": ["Sie", "ist", "nicht", "b\u00f6\u00b7se", ",", "ist", "nicht", "gut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "$,", "VAFIN", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Deutsche will zur Urne gehen,", "tokens": ["Der", "Deut\u00b7sche", "will", "zur", "Ur\u00b7ne", "ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So wie man das Gewohnte tut.", "tokens": ["So", "wie", "man", "das", "Ge\u00b7wohn\u00b7te", "tut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "PIS", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wer hofft, da\u00df es noch anders w\u00fcrde,", "tokens": ["Wer", "hofft", ",", "da\u00df", "es", "noch", "an\u00b7ders", "w\u00fcr\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "PPER", "ADV", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der t\u00e4uscht sich hier, wie \u00fcberall.", "tokens": ["Der", "t\u00e4uscht", "sich", "hier", ",", "wie", "\u00fc\u00b7be\u00b7rall", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PRF", "ADV", "$,", "PWAV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Schafe suchen ihre H\u00fcrde,", "tokens": ["Die", "Scha\u00b7fe", "su\u00b7chen", "ih\u00b7re", "H\u00fcr\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Das Rindvieh suchet seinen Stall.", "tokens": ["Das", "Rind\u00b7vieh", "su\u00b7chet", "sei\u00b7nen", "Stall", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}