{"textgrid.poem.42986": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Auf gro\u00dfen Pl\u00e4tzen in den St\u00e4dten", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Auf gro\u00dfen Pl\u00e4tzen in den St\u00e4dten", "tokens": ["Auf", "gro\u00b7\u00dfen", "Pl\u00e4t\u00b7zen", "in", "den", "St\u00e4d\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "M\u00e4sten sich Taubenschw\u00e4rme.", "tokens": ["M\u00e4s\u00b7ten", "sich", "Tau\u00b7ben\u00b7schw\u00e4r\u00b7me", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PRF", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Es gehen knurrend manchmal Ged\u00e4rme", "tokens": ["Es", "ge\u00b7hen", "knur\u00b7rend", "manch\u00b7mal", "Ge\u00b7d\u00e4r\u00b7me"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "ADV", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Vorbei, die nur ein solch Federvieh", "tokens": ["Vor\u00b7bei", ",", "die", "nur", "ein", "solch", "Fe\u00b7der\u00b7vieh"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "PRELS", "ADV", "ART", "PIAT", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Gar zu gern und gebraten h\u00e4tten.", "tokens": ["Gar", "zu", "gern", "und", "ge\u00b7bra\u00b7ten", "h\u00e4t\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKA", "ADV", "KON", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Man erziehe rechtzeitig sein Kind", "tokens": ["Man", "er\u00b7zie\u00b7he", "recht\u00b7zei\u00b7tig", "sein", "Kind"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADJD", "PPOSAT", "NN"], "meter": "+-+-++--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Zu der Liebe zu allen Tieren.", "tokens": ["Zu", "der", "Lie\u00b7be", "zu", "al\u00b7len", "Tie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "PIAT", "NN", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Kinder, die sch\u00f6n angezogen sind,", "tokens": ["Kin\u00b7der", ",", "die", "sch\u00f6n", "an\u00b7ge\u00b7zo\u00b7gen", "sind", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "ADJD", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Sollen mit reichgekleideten M\u00fcttern", "tokens": ["Sol\u00b7len", "mit", "reich\u00b7ge\u00b7klei\u00b7de\u00b7ten", "M\u00fct\u00b7tern"], "token_info": ["word", "word", "word", "word"], "pos": ["VMFIN", "APPR", "ADJA", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Tauben \u00f6ffentlich h\u00e4tscheln und f\u00fcttern", "tokens": ["Tau\u00b7ben", "\u00f6f\u00b7fent\u00b7lich", "h\u00e4t\u00b7scheln", "und", "f\u00fct\u00b7tern"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "VVINF", "KON", "VVINF"], "meter": "+-----+-+-", "measure": "dactylic.init"}, "line.6": {"text": "Und sich dabei", "tokens": ["Und", "sich", "da\u00b7bei"], "token_info": ["word", "word", "word"], "pos": ["KON", "PRF", "PAV"], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Neckisch und lieblich photographieren", "tokens": ["Ne\u00b7ckisch", "und", "lieb\u00b7lich", "pho\u00b7to\u00b7gra\u00b7phie\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "ADJD", "VVINF"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.8": {"text": "Lassen. \u2013 Spatzen sind vogelfrei.", "tokens": ["Las\u00b7sen", ".", "\u2013", "Spat\u00b7zen", "sind", "vo\u00b7gel\u00b7frei", "."], "token_info": ["word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "$(", "NN", "VAFIN", "ADJD", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}}, "stanza.3": {"line.1": {"text": "Ich habe vor markusplatzigen Tauben", "tokens": ["Ich", "ha\u00b7be", "vor", "mar\u00b7kus\u00b7plat\u00b7zi\u00b7gen", "Tau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "ADJA", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Etwas Angst wegen meines Hutes.", "tokens": ["Et\u00b7was", "Angst", "we\u00b7gen", "mei\u00b7nes", "Hu\u00b7tes", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Ich kann mir nicht viele H\u00fcte erlauben.", "tokens": ["Ich", "kann", "mir", "nicht", "vie\u00b7le", "H\u00fc\u00b7te", "er\u00b7lau\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PTKNEG", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ich w\u00fcnsche den Photographen nur Gutes", "tokens": ["Ich", "w\u00fcn\u00b7sche", "den", "Pho\u00b7to\u00b7gra\u00b7phen", "nur", "Gu\u00b7tes"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADV", "NN"], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und den M\u00fcttern auf der Parade \u2013", "tokens": ["Und", "den", "M\u00fct\u00b7tern", "auf", "der", "Pa\u00b7ra\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Nicht ihrem Kind \u2013", "tokens": ["Nicht", "ih\u00b7rem", "Kind", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKNEG", "PPOSAT", "NN", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "All das, wof\u00fcr meine H\u00fcte zu schade", "tokens": ["All", "das", ",", "wo\u00b7f\u00fcr", "mei\u00b7ne", "H\u00fc\u00b7te", "zu", "scha\u00b7de"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PIAT", "PDS", "$,", "PWAV", "PPOSAT", "NN", "PTKZU", "VVFIN"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.8": {"text": "Sind.", "tokens": ["Sind", "."], "token_info": ["word", "punct"], "pos": ["VAFIN", "$."], "meter": "-", "measure": "single.down"}}}}}