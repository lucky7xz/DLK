{"textgrid.poem.52973": {"metadata": {"author": {"name": "Seume, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wenn wir am Rand des Lebens stehen,", "genre": "verse", "period": "N.A.", "pub_year": 1786, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wenn wir am Rand des Lebens stehen,", "tokens": ["Wenn", "wir", "am", "Rand", "des", "Le\u00b7bens", "ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und alles, was die Erde h\u00e4lt,", "tokens": ["Und", "al\u00b7les", ",", "was", "die", "Er\u00b7de", "h\u00e4lt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Rund um uns her zusammen f\u00e4llt,", "tokens": ["Rund", "um", "uns", "her", "zu\u00b7sam\u00b7men", "f\u00e4llt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PPER", "ADV", "ADV", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Wenn Kronen mit dem Bettelstab vergehen;", "tokens": ["Wenn", "Kro\u00b7nen", "mit", "dem", "Bet\u00b7tel\u00b7stab", "ver\u00b7ge\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Wenn Herrn von weiten weiten Reichen,", "tokens": ["Wenn", "Herrn", "von", "wei\u00b7ten", "wei\u00b7ten", "Rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die gestern noch mit ihrer Riesenhand", "tokens": ["Die", "ge\u00b7stern", "noch", "mit", "ih\u00b7rer", "Rie\u00b7sen\u00b7hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADV", "ADV", "APPR", "PPOSAT", "NN"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Den Orient und Occident umspannt,", "tokens": ["Den", "O\u00b7rient", "und", "Oc\u00b7ci\u00b7dent", "um\u00b7spannt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Heut ihrem letzten Sclaven gleichen;", "tokens": ["Heut", "ih\u00b7rem", "letz\u00b7ten", "Scla\u00b7ven", "glei\u00b7chen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Wenn eitler Weisheit Dunst zerst\u00e4ubt,", "tokens": ["Wenn", "eit\u00b7ler", "Weis\u00b7heit", "Dunst", "zer\u00b7st\u00e4ubt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und von den Hypothesenkr\u00fccken,", "tokens": ["Und", "von", "den", "Hy\u00b7po\u00b7the\u00b7sen\u00b7kr\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der gr\u00f6\u00dften K\u00f6pfe Meisterst\u00fccken,", "tokens": ["Der", "gr\u00f6\u00df\u00b7ten", "K\u00f6p\u00b7fe", "Meis\u00b7ter\u00b7st\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Kaum noch ein Splitter \u00fcbrig bleibt;", "tokens": ["Kaum", "noch", "ein", "Split\u00b7ter", "\u00fcb\u00b7rig", "bleibt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Wenn tiefe tiefe Dunkelheit", "tokens": ["Wenn", "tie\u00b7fe", "tie\u00b7fe", "Dun\u00b7kel\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ADJA", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Des Sinnes Ohnmacht schwer umh\u00fcllet,", "tokens": ["Des", "Sin\u00b7nes", "Ohn\u00b7macht", "schwer", "um\u00b7h\u00fcl\u00b7let", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und Ein Gedanke nur die Seele f\u00fcllet,", "tokens": ["Und", "Ein", "Ge\u00b7dan\u00b7ke", "nur", "die", "See\u00b7le", "f\u00fcl\u00b7let", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "An Gott und Nichts und Ewigkeit:", "tokens": ["An", "Gott", "und", "Nichts", "und", "E\u00b7wig\u00b7keit", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Dann, dann ist Eine gute That,", "tokens": ["Dann", ",", "dann", "ist", "Ei\u00b7ne", "gu\u00b7te", "That", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "ADV", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im Sinn des Testaments gethan,", "tokens": ["Im", "Sinn", "des", "Tes\u00b7ta\u00b7ments", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein be\u00dfrer Pa\u00df zur unbekannten Bahn,", "tokens": ["Ein", "be\u00df\u00b7rer", "Pa\u00df", "zur", "un\u00b7be\u00b7kann\u00b7ten", "Bahn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Als aller Pfarrer Attestat.", "tokens": ["Als", "al\u00b7ler", "Pfar\u00b7rer", "At\u00b7tes\u00b7tat", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}