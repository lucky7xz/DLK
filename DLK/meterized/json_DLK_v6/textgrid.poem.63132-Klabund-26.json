{"textgrid.poem.63132": {"metadata": {"author": {"name": "Klabund", "birth": "N.A.", "death": "N.A."}, "title": "26", "genre": "verse", "period": "N.A.", "pub_year": 1909, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wo ist Flora, gebt Bescheid,", "tokens": ["Wo", "ist", "Flo\u00b7ra", ",", "gebt", "Be\u00b7scheid", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "NE", "$,", "VVFIN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Deren Br\u00fcste Rom entbrannten...?", "tokens": ["De\u00b7ren", "Br\u00fcs\u00b7te", "Rom", "ent\u00b7brann\u00b7ten", "...", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "NE", "VVFIN", "$(", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Archipiada weilt so weit", "tokens": ["Ar\u00b7chi\u00b7pi\u00b7a\u00b7da", "weilt", "so", "weit"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit der holdesten Verwandten...", "tokens": ["Mit", "der", "hol\u00b7des\u00b7ten", "Ver\u00b7wand\u00b7ten", "..."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$("], "meter": "--+---+-", "measure": "anapaest.init"}, "line.5": {"text": "Echo, Wogenruferin \u2013", "tokens": ["E\u00b7cho", ",", "Wo\u00b7gen\u00b7ru\u00b7fe\u00b7rin", "\u2013"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "PWAV", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Scham und Sch\u00f6nheit schritt zur Bahre \u2013", "tokens": ["Scham", "und", "Sch\u00f6n\u00b7heit", "schritt", "zur", "Bah\u00b7re", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VVFIN", "APPRART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Alle, alle sind dahin", "tokens": ["Al\u00b7le", ",", "al\u00b7le", "sind", "da\u00b7hin"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PIS", "$,", "PIS", "VAFIN", "PAV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Wie der Schnee vom vorigen Jahre...", "tokens": ["Wie", "der", "Schnee", "vom", "vo\u00b7ri\u00b7gen", "Jah\u00b7re", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPRART", "ADJA", "NN", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Heloise, Amors Sklavin,", "tokens": ["He\u00b7loi\u00b7se", ",", "A\u00b7mors", "Skla\u00b7vin", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NE", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Deren Liebster ein Eunuch...", "tokens": ["De\u00b7ren", "Liebs\u00b7ter", "ein", "Eu\u00b7nuch", "..."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "ART", "NN", "$("], "meter": "+-+-++-", "measure": "unknown.measure.tetra"}, "line.3": {"text": "M\u00f6nch- und Menschenelend traf ihn,", "tokens": ["M\u00f6n\u00b7ch", "und", "Men\u00b7sche\u00b7ne\u00b7lend", "traf", "ihn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["TRUNC", "KON", "NN", "VVFIN", "PPER", "$,"], "meter": "+--+--++-", "measure": "dactylic.di.plus"}, "line.4": {"text": "Und der Seufzer schwoll zum Fluch.", "tokens": ["Und", "der", "Seuf\u00b7zer", "schwoll", "zum", "Fluch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJD", "APPRART", "NN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.5": {"text": "Und die Buridan geliebt \u2013", "tokens": ["Und", "die", "Bu\u00b7ri\u00b7dan", "ge\u00b7liebt", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVPP", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Fische h\u00fcpften Totent\u00e4nze \u2013", "tokens": ["Fi\u00b7sche", "h\u00fcpf\u00b7ten", "To\u00b7ten\u00b7t\u00e4n\u00b7ze", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Alle, alle sind zerstiebt", "tokens": ["Al\u00b7le", ",", "al\u00b7le", "sind", "zer\u00b7stiebt"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PIS", "$,", "PIS", "VAFIN", "VVPP"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Wie der Schnee vom letzten Lenze...", "tokens": ["Wie", "der", "Schnee", "vom", "letz\u00b7ten", "Len\u00b7ze", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPRART", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Blanche! Sirene! Die den leisen", "tokens": ["Blan\u00b7che", "!", "Si\u00b7re\u00b7ne", "!", "Die", "den", "lei\u00b7sen"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$.", "NN", "$.", "ART", "ART", "ADJA"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Leib wie Liliensichel schwang!", "tokens": ["Leib", "wie", "Li\u00b7li\u00b7en\u00b7si\u00b7chel", "schwang", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "NE", "VVFIN", "$."], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Berthe, die wir als m\u00e4nnlich preisen", "tokens": ["Bert\u00b7he", ",", "die", "wir", "als", "m\u00e4nn\u00b7lich", "prei\u00b7sen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "$,", "PRELS", "PPER", "KOUS", "ADJD", "VVFIN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Und Jeanne d'Arc von Orleans,", "tokens": ["Und", "Jean\u00b7ne", "d'\u00b7A\u00b7rc", "von", "Or\u00b7le\u00b7ans", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "APPR", "NE", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Die zum feurigen Gebet", "tokens": ["Die", "zum", "feu\u00b7ri\u00b7gen", "Ge\u00b7bet"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "APPRART", "ADJA", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.6": {"text": "England schleift am heiligen Haare \u2013", "tokens": ["En\u00b7gland", "schleift", "am", "hei\u00b7li\u00b7gen", "Haa\u00b7re", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "APPRART", "ADJA", "NN", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.7": {"text": "Alle, alle sind verweht", "tokens": ["Al\u00b7le", ",", "al\u00b7le", "sind", "ver\u00b7weht"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PIS", "$,", "PIS", "VAFIN", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Wie der Schnee vom vorigen Jahre...", "tokens": ["Wie", "der", "Schnee", "vom", "vo\u00b7ri\u00b7gen", "Jah\u00b7re", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPRART", "ADJA", "NN", "$("], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Frage nimmer: Schmerz zuviel hing", "tokens": ["Fra\u00b7ge", "nim\u00b7mer", ":", "Schmerz", "zu\u00b7viel", "hing"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NN", "ADV", "$.", "NN", "PIS", "VVFIN"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Traubenschwer im Herzen inn...", "tokens": ["Trau\u00b7ben\u00b7schwer", "im", "Her\u00b7zen", "in\u00b7n.", ".."], "token_info": ["word", "word", "word", "abbreviation", "punct"], "pos": ["NN", "APPRART", "NN", "PTKVZ", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Alle, alle sind dahin", "tokens": ["Al\u00b7le", ",", "al\u00b7le", "sind", "da\u00b7hin"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["PIS", "$,", "PIS", "VAFIN", "PAV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie der Schnee vom letzten Fr\u00fchling...", "tokens": ["Wie", "der", "Schnee", "vom", "letz\u00b7ten", "Fr\u00fch\u00b7ling", "..."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPRART", "ADJA", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}