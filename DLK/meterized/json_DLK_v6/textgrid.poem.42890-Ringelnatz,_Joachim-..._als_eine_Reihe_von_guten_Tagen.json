{"textgrid.poem.42890": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "... als eine Reihe von guten Tagen", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.85", "nl:0.14"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wir wollen uns wieder mal zanken,", "tokens": ["Wir", "wol\u00b7len", "uns", "wie\u00b7der", "mal", "zan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Auf etwas hacken wie Raben,", "tokens": ["Auf", "et\u00b7was", "ha\u00b7cken", "wie", "Ra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVINF", "KOKOM", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Da\u00df unsre zufriednen Gedanken", "tokens": ["Da\u00df", "uns\u00b7re", "zu\u00b7fried\u00b7nen", "Ge\u00b7dan\u00b7ken"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Eine Ablenkung haben.", "tokens": ["Ei\u00b7ne", "Ab\u00b7len\u00b7kung", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.2": {"line.1": {"text": "Wir wollen irgendein harmloses Wort", "tokens": ["Wir", "wol\u00b7len", "ir\u00b7gend\u00b7ein", "harm\u00b7lo\u00b7ses", "Wort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Entstellen,", "tokens": ["Ent\u00b7stel\u00b7len", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Dann uns verleumden und zum Tort", "tokens": ["Dann", "uns", "ver\u00b7leum\u00b7den", "und", "zum", "Tort"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VVINF", "KON", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Etwas tun; das schl\u00e4gt dann Wellen.", "tokens": ["Et\u00b7was", "tun", ";", "das", "schl\u00e4gt", "dann", "Wel\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "$.", "PDS", "VVFIN", "ADV", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Wir wollen dritte aufzuhetzen", "tokens": ["Wir", "wol\u00b7len", "drit\u00b7te", "auf\u00b7zu\u00b7het\u00b7zen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Versuchen,", "tokens": ["Ver\u00b7su\u00b7chen", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Dann unsere Freundschaft verfluchen,", "tokens": ["Dann", "un\u00b7se\u00b7re", "Freund\u00b7schaft", "ver\u00b7flu\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Einmal sogar ein Messer wetzen,", "tokens": ["Ein\u00b7mal", "so\u00b7gar", "ein", "Mes\u00b7ser", "wet\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Dann aber uns \u2013 in Blickweite \u2013", "tokens": ["Dann", "a\u00b7ber", "uns", "\u2013", "in", "Blick\u00b7wei\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "$(", "APPR", "NN", "$("], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.6": {"text": "Auseinander zusammensetzen,", "tokens": ["Aus\u00b7ein\u00b7an\u00b7der", "zu\u00b7sam\u00b7men\u00b7set\u00b7zen", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.7": {"text": "Um superior jedem weiteren Streite", "tokens": ["Um", "su\u00b7pe\u00b7ri\u00b7or", "je\u00b7dem", "wei\u00b7te\u00b7ren", "Strei\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUI", "APPR", "PIAT", "ADJA", "NN"], "meter": "-+---+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Auszuweichen;", "tokens": ["Aus\u00b7zu\u00b7wei\u00b7chen", ";"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "Mit dem Schwur beiseite:", "tokens": ["Mit", "dem", "Schwur", "bei\u00b7sei\u00b7te", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.10": {"text": "Uns nimmermehr zu vergleichen.", "tokens": ["Uns", "nim\u00b7mer\u00b7mehr", "zu", "ver\u00b7glei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Dann wollen wir, jeder mit Ungeduld,", "tokens": ["Dann", "wol\u00b7len", "wir", ",", "je\u00b7der", "mit", "Un\u00b7ge\u00b7duld", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "$,", "PIS", "APPR", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Ein paar N\u00e4chte schlecht tr\u00e4umen,", "tokens": ["Ein", "paar", "N\u00e4ch\u00b7te", "schlecht", "tr\u00e4u\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "ADJD", "VVINF", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Dann heimlich eine gewisse Schuld", "tokens": ["Dann", "heim\u00b7lich", "ei\u00b7ne", "ge\u00b7wis\u00b7se", "Schuld"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "ART", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Dem anderen einr\u00e4umen,", "tokens": ["Dem", "an\u00b7de\u00b7ren", "ein\u00b7r\u00e4u\u00b7men", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Dann l\u00e4cheln, dann seufzen, dann st\u00f6hnen,", "tokens": ["Dann", "l\u00e4\u00b7cheln", ",", "dann", "seuf\u00b7zen", ",", "dann", "st\u00f6h\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "ADV", "VVINF", "$,", "ADV", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "Dann pl\u00f6tzlich uns gr\u00fcndlich bezechen,", "tokens": ["Dann", "pl\u00f6tz\u00b7lich", "uns", "gr\u00fcnd\u00b7lich", "be\u00b7ze\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+--+-+--", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Dann von dem verg\u00e4nglichen, wundersch\u00f6nen", "tokens": ["Dann", "von", "dem", "ver\u00b7g\u00e4ng\u00b7li\u00b7chen", ",", "wun\u00b7der\u00b7sch\u00f6\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "APPR", "ART", "ADJA", "$,", "ADJA"], "meter": "----+--+-+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Leben sprechen.", "tokens": ["Le\u00b7ben", "spre\u00b7chen", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "Und dann uns wieder einmal vers\u00f6hnen.", "tokens": ["Und", "dann", "uns", "wie\u00b7der", "ein\u00b7mal", "ver\u00b7s\u00f6h\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPER", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}}}}}