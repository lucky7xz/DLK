{"dta.poem.9767": {"metadata": {"author": {"name": "Hofmannswaldau, Christian Hofmann von", "birth": "N.A.", "death": "N.A."}, "title": "Auff eine hochzeit.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1697", "urn": "urn:nbn:de:kobv:b4-200905199377", "language": ["de:0.71", "en:0.28"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Man fragt mich was die ehe sey?", "tokens": ["Man", "fragt", "mich", "was", "die", "e\u00b7he", "sey", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "PRELS", "ART", "KOUS", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das kan ich warlich keinem sagen;", "tokens": ["Das", "kan", "ich", "war\u00b7lich", "kei\u00b7nem", "sa\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Denn d\u00e4cht ich gleich den kopff entzwey/", "tokens": ["Denn", "d\u00e4cht", "ich", "gleich", "den", "kopff", "ent\u00b7zwey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So k\u00f6nt ich doch in vierzehn tagen/", "tokens": ["So", "k\u00f6nt", "ich", "doch", "in", "vier\u00b7zehn", "ta\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "CARD", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Und w\u00e4r ich auch nur gantz allein/", "tokens": ["Und", "w\u00e4r", "ich", "auch", "nur", "gantz", "al\u00b7lein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADV", "ADV", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Nicht mit mir selber einig seyn.", "tokens": ["Nicht", "mit", "mir", "sel\u00b7ber", "ei\u00b7nig", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "PPER", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Man lese vor und hinter sich/", "tokens": ["Man", "le\u00b7se", "vor", "und", "hin\u00b7ter", "sich", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PTKVZ", "KON", "APPR", "PRF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die ehe ist und bleibet ehe;", "tokens": ["Die", "e\u00b7he", "ist", "und", "blei\u00b7bet", "e\u00b7he", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "KOUS", "VAFIN", "KON", "VVFIN", "KOUS", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und ist es so gar wunderlich/", "tokens": ["Und", "ist", "es", "so", "gar", "wun\u00b7der\u00b7lich", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df ich es selber nicht verstehe/", "tokens": ["Da\u00df", "ich", "es", "sel\u00b7ber", "nicht", "ver\u00b7ste\u00b7he", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "PTKNEG", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wie zwey und eins/ und eins und zwey", "tokens": ["Wie", "zwey", "und", "eins", "/", "und", "eins", "und", "zwey"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "CARD", "KON", "PIS", "$(", "KON", "PIS", "KON", "CARD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Nur eins und doch gedoppelt sey.", "tokens": ["Nur", "eins", "und", "doch", "ge\u00b7dop\u00b7pelt", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "KON", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Das ist ein seltzam einmahl ein/", "tokens": ["Das", "ist", "ein", "selt\u00b7zam", "ein\u00b7mahl", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJD", "ADV", "ART", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wann zwey auff eins gerechnet werden.", "tokens": ["Wann", "zwey", "auff", "eins", "ge\u00b7rech\u00b7net", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "CARD", "APPR", "PIS", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Soll di\u00df in aller einnahm seyn/", "tokens": ["Soll", "di\u00df", "in", "al\u00b7ler", "ein\u00b7nahm", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PDS", "APPR", "PIAT", "VVFIN", "VAINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So ist kein besser ding auff erden:", "tokens": ["So", "ist", "kein", "bes\u00b7ser", "ding", "auff", "er\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIAT", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Doch das beh\u00e4lt die eh vor sich/", "tokens": ["Doch", "das", "be\u00b7h\u00e4lt", "die", "eh", "vor", "sich", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "ART", "KOUS", "APPR", "PRF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Drum ists so wunder-wunderlich.", "tokens": ["Drum", "ists", "so", "wun\u00b7der\u00b7wun\u00b7der\u00b7lich", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Das capital ist bey dem mann/", "tokens": ["Das", "ca\u00b7pi\u00b7tal", "ist", "bey", "dem", "mann", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab ich wohl ehmahls h\u00f6ren sagen;", "tokens": ["Hab", "ich", "wohl", "eh\u00b7mahls", "h\u00f6\u00b7ren", "sa\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "ADV", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und was an renten fallen kan/", "tokens": ["Und", "was", "an", "ren\u00b7ten", "fal\u00b7len", "kan", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "APPR", "ADJA", "VVINF", "VMFIN", "$("], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Das m\u00fcsse dann das weibgen tragen.", "tokens": ["Das", "m\u00fcs\u00b7se", "dann", "das", "weib\u00b7gen", "tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ADV", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich wei\u00df es nicht/ es steht dahin/", "tokens": ["Ich", "wei\u00df", "es", "nicht", "/", "es", "steht", "da\u00b7hin", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "$(", "PPER", "VVFIN", "PAV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ich bleibe noch auff meinem sinn.", "tokens": ["Ich", "blei\u00b7be", "noch", "auff", "mei\u00b7nem", "sinn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Zwey hertzen sollen eines seyn/", "tokens": ["Zwey", "hert\u00b7zen", "sol\u00b7len", "ei\u00b7nes", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VMFIN", "PIS", "VAINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So/ spricht man/ sey die eh vollkommen.", "tokens": ["So", "/", "spricht", "man", "/", "sey", "die", "eh", "voll\u00b7kom\u00b7men", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "VVFIN", "PIS", "$(", "VAFIN", "ART", "KOUS", "ADJD", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Di\u00df will mir gantz und gar nicht ein/", "tokens": ["Di\u00df", "will", "mir", "gantz", "und", "gar", "nicht", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PPER", "ADV", "KON", "ADV", "PTKNEG", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df eine mu\u00df ja seyn genommen:", "tokens": ["Da\u00df", "ei\u00b7ne", "mu\u00df", "ja", "seyn", "ge\u00b7nom\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VMFIN", "ADV", "PPOSAT", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So eins des diebstals sich beschwert/", "tokens": ["So", "eins", "des", "diebs\u00b7tals", "sich", "be\u00b7schwert", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "ART", "NN", "PRF", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So ist das andre henckens werth.", "tokens": ["So", "ist", "das", "and\u00b7re", "hen\u00b7ckens", "werth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Und treten dann gleich zwey in eins/", "tokens": ["Und", "tre\u00b7ten", "dann", "gleich", "zwey", "in", "eins", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "CARD", "APPR", "PIS", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So ist die sache nicht gehoben:", "tokens": ["So", "ist", "die", "sa\u00b7che", "nicht", "ge\u00b7ho\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "ADJA", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Denn wie man saget/ eins ist keins;", "tokens": ["Denn", "wie", "man", "sa\u00b7get", "/", "eins", "ist", "keins", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PIS", "VVFIN", "$(", "PIS", "VAFIN", "PIAT", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und sind dann zwey in eins geschoben/", "tokens": ["Und", "sind", "dann", "zwey", "in", "eins", "ge\u00b7scho\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "CARD", "APPR", "PIS", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So m\u00fcssen beydes zwey und ein", "tokens": ["So", "m\u00fcs\u00b7sen", "bey\u00b7des", "zwey", "und", "ein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "CARD", "KON", "ART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Nicht mehr als keins und nichtes seyn.", "tokens": ["Nicht", "mehr", "als", "keins", "und", "nich\u00b7tes", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIAT", "KOKOM", "PIAT", "KON", "PIS", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Auch hat kein theil ein gantzes hertz;", "tokens": ["Auch", "hat", "kein", "theil", "ein", "gant\u00b7zes", "hertz", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIAT", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Denn ist es ja bey einem jeden/", "tokens": ["Denn", "ist", "es", "ja", "bey", "ei\u00b7nem", "je\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "APPR", "ART", "PIAT", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "So ist ihr lieben nur ein schertz/", "tokens": ["So", "ist", "ihr", "lie\u00b7ben", "nur", "ein", "schertz", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVFIN", "ADV", "ART", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und ist kein theil damit zu frieden/", "tokens": ["Und", "ist", "kein", "theil", "da\u00b7mit", "zu", "frie\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PIAT", "NN", "PAV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Hat liebe keinen falschen schein/", "tokens": ["Hat", "lie\u00b7be", "kei\u00b7nen", "fal\u00b7schen", "schein", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVFIN", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "So mu\u00df das hertz getheilet seyn.", "tokens": ["So", "mu\u00df", "das", "hertz", "ge\u00b7thei\u00b7let", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ist aber denn das hertz getheilt/", "tokens": ["Ist", "a\u00b7ber", "denn", "das", "hertz", "ge\u00b7theilt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wird jedes nur ein halbes tragen/", "tokens": ["Wird", "je\u00b7des", "nur", "ein", "hal\u00b7bes", "tra\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADV", "ART", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie w\u00fcrde denn der bruch geheilt?", "tokens": ["Wie", "w\u00fcr\u00b7de", "denn", "der", "bruch", "ge\u00b7heilt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da m\u00f6cht ein kalter brand zuschlagen.", "tokens": ["Da", "m\u00f6cht", "ein", "kal\u00b7ter", "brand", "zu\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ein halbes hertz kan nicht bestehn/", "tokens": ["Ein", "hal\u00b7bes", "hertz", "kan", "nicht", "be\u00b7stehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Es m\u00fcste denn auff steltzen gehn.", "tokens": ["Es", "m\u00fcs\u00b7te", "denn", "auff", "stelt\u00b7zen", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "APPR", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "So kan man ein verliebtes hertz", "tokens": ["So", "kan", "man", "ein", "ver\u00b7lieb\u00b7tes", "hertz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PIS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht doppelt/ auch nicht einfach nennen/", "tokens": ["Nicht", "dop\u00b7pelt", "/", "auch", "nicht", "ein\u00b7fach", "nen\u00b7nen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$(", "ADV", "PTKNEG", "ADV", "VVINF", "$("], "meter": "--+-+-+--", "measure": "anapaest.init"}, "line.3": {"text": "Nicht halb/ nicht gantz/ und ohne schertz/", "tokens": ["Nicht", "halb", "/", "nicht", "gantz", "/", "und", "oh\u00b7ne", "schertz", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$(", "PTKNEG", "ADV", "$(", "KON", "APPR", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn ich die warheit sol bekennen/", "tokens": ["Wenn", "ich", "die", "war\u00b7heit", "sol", "be\u00b7ken\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "So denck ich wohl und sag es frey/", "tokens": ["So", "denck", "ich", "wohl", "und", "sag", "es", "frey", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "PPER", "ADV", "KON", "VVFIN", "PPER", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Da\u00df gar kein hertz in beyden sey.", "tokens": ["Da\u00df", "gar", "kein", "hertz", "in", "bey\u00b7den", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "NN", "APPR", "PIAT", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Soll aber das das hertze seyn/", "tokens": ["Soll", "a\u00b7ber", "das", "das", "hert\u00b7ze", "seyn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "ART", "NN", "VAINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das stets so zappelt und sich reget?", "tokens": ["Das", "stets", "so", "zap\u00b7pelt", "und", "sich", "re\u00b7get", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ADV", "ADJD", "KON", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "So bild ich mir wohl g\u00e4ntzlich ein/", "tokens": ["So", "bild", "ich", "mir", "wohl", "g\u00e4ntz\u00b7lich", "ein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "PPER", "PPER", "ADV", "ADJD", "ART", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn erst der puls der liebe schl\u00e4get/", "tokens": ["Wenn", "erst", "der", "puls", "der", "lie\u00b7be", "schl\u00e4\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "ART", "ADJA", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Da\u00df die verliebten insgemein", "tokens": ["Da\u00df", "die", "ver\u00b7lieb\u00b7ten", "ins\u00b7ge\u00b7mein"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sonst nichts als lauter hertze seyn.", "tokens": ["Sonst", "nichts", "als", "lau\u00b7ter", "hert\u00b7ze", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "KOKOM", "PIAT", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Herr br\u00e4utigam was d\u00fcncket euch/", "tokens": ["Herr", "br\u00e4u\u00b7ti\u00b7gam", "was", "d\u00fcn\u00b7cket", "euch", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "PWS", "VVFIN", "PPER", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Jhr habt das lehrgeld nun gegeben/", "tokens": ["Ihr", "habt", "das", "lehr\u00b7geld", "nun", "ge\u00b7ge\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PDS", "VVFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Seyd ihr euch beyd am hertzen gleich/", "tokens": ["Seyd", "ihr", "euch", "beyd", "am", "hert\u00b7zen", "gleich", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PPER", "PPER", "PIS", "APPRART", "NN", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die ihr zusammen sollet leben?", "tokens": ["Die", "ihr", "zu\u00b7sam\u00b7men", "sol\u00b7let", "le\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wo nicht/ so saget mir dabey/", "tokens": ["Wo", "nicht", "/", "so", "sa\u00b7get", "mir", "da\u00b7bey", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PTKNEG", "$(", "ADV", "VVFIN", "PPER", "PAV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Bey welchem theil das meiste sey.", "tokens": ["Bey", "wel\u00b7chem", "theil", "das", "meis\u00b7te", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PWAT", "NN", "ART", "ADJA", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Doch halt ich/ wi\u00dft ihrs selbst noch nicht/", "tokens": ["Doch", "halt", "ich", "/", "wi\u00dft", "ihrs", "selbst", "noch", "nicht", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "$(", "VVFIN", "PPER", "ADV", "ADV", "PTKNEG", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Drum wird es auff erfahrung stehen/", "tokens": ["Drum", "wird", "es", "auff", "er\u00b7fah\u00b7rung", "ste\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "APPR", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Jhr solt noch heut vors liebs-gericht/", "tokens": ["Ihr", "solt", "noch", "heut", "vors", "liebs\u00b7ge\u00b7richt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADV", "APPRART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da solt ihr eure part versehen/", "tokens": ["Da", "solt", "ihr", "eu\u00b7re", "part", "ver\u00b7se\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich wette/ wenn ich gleich verliehr/", "tokens": ["Ich", "wet\u00b7te", "/", "wenn", "ich", "gleich", "ver\u00b7liehr", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "KOUS", "PPER", "ADV", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die liebste hat mehr hertz als ihr.", "tokens": ["Die", "liebs\u00b7te", "hat", "mehr", "hertz", "als", "ihr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "PIAT", "NN", "KOUS", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}