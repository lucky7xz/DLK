{"textgrid.poem.26397": {"metadata": {"author": {"name": "Dauthendey, Max", "birth": "N.A.", "death": "N.A."}, "title": "1L: \u00bbja wirklich,\u00ab sprach da Eine", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbja wirklich,\u00ab sprach da Eine", "tokens": ["\u00bb", "ja", "wirk\u00b7lich", ",", "\u00ab", "sprach", "da", "Ei\u00b7ne"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word"], "pos": ["$(", "ADV", "ADJD", "$,", "$(", "VVFIN", "ADV", "ART"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "\u00bbnichts ist vor Lieb' gefeit,", "tokens": ["\u00bb", "nichts", "ist", "vor", "Lieb'", "ge\u00b7feit", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PIS", "VAFIN", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Herrn Heinz liebten einst reine", "tokens": ["Herrn", "Heinz", "lieb\u00b7ten", "einst", "rei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NE", "VVFIN", "ADV", "ADJA"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Zwilling zur Veilchenzeit.", "tokens": ["Zwil\u00b7ling", "zur", "Veil\u00b7chen\u00b7zeit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.2": {"line.1": {"text": "Und da gibt's nichts zu lachen,", "tokens": ["Und", "da", "gibt's", "nichts", "zu", "la\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PIS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Die Lieb ist wundersam.", "tokens": ["Die", "Lieb", "ist", "wun\u00b7der\u00b7sam", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und Liebe konnt' es machen,", "tokens": ["Und", "Lie\u00b7be", "konnt'", "es", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da\u00df Totes wiederkam.\u00ab", "tokens": ["Da\u00df", "To\u00b7tes", "wie\u00b7der\u00b7kam", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "punct"], "pos": ["KOUS", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Die sieben andern Ammen", "tokens": ["Die", "sie\u00b7ben", "an\u00b7dern", "Am\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "CARD", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Mu\u00dften ans Herz sich fassen.", "tokens": ["Mu\u00df\u00b7ten", "ans", "Herz", "sich", "fas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPRART", "NN", "PRF", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Sie r\u00fcckten eng zusammen", "tokens": ["Sie", "r\u00fcck\u00b7ten", "eng", "zu\u00b7sam\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und stellten fort die Tassen.", "tokens": ["Und", "stell\u00b7ten", "fort", "die", "Tas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKVZ", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "\u00bbaurora und Alice,\u00ab", "tokens": ["\u00bb", "au\u00b7ro\u00b7ra", "und", "A\u00b7li\u00b7ce", ",", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "NE", "KON", "NE", "$,", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "So hie\u00df ein Zwillingspaar.", "tokens": ["So", "hie\u00df", "ein", "Zwil\u00b7lings\u00b7paar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "So \u00e4hnlich waren diese,", "tokens": ["So", "\u00e4hn\u00b7lich", "wa\u00b7ren", "die\u00b7se", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "PDS", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Es fehlte dran kein Haar.", "tokens": ["Es", "fehl\u00b7te", "dran", "kein", "Haar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PAV", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Durch einen Blumenladen", "tokens": ["Durch", "ei\u00b7nen", "Blu\u00b7men\u00b7la\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Ern\u00e4hrten sie sich keusch.", "tokens": ["Er\u00b7n\u00e4hr\u00b7ten", "sie", "sich", "keusch", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wie Rosen still auf Drahten", "tokens": ["Wie", "Ro\u00b7sen", "still", "auf", "Drah\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "ADJD", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Lebten sie ohn' Ger\u00e4usch.", "tokens": ["Leb\u00b7ten", "sie", "ohn'", "Ge\u00b7r\u00e4usch", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.6": {"line.1": {"text": "Heinz kam zur Rosenhecke", "tokens": ["Heinz", "kam", "zur", "Ro\u00b7sen\u00b7he\u00b7cke"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Hinter den Ladenpult,", "tokens": ["Hin\u00b7ter", "den", "La\u00b7den\u00b7pult", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "Und nur zum Ankaufszwecke", "tokens": ["Und", "nur", "zum", "An\u00b7kaufs\u00b7zwe\u00b7cke"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "APPRART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Empfing man ihn mit Huld.", "tokens": ["Emp\u00b7fing", "man", "ihn", "mit", "Huld", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Es wagten nie die Damen", "tokens": ["Es", "wag\u00b7ten", "nie", "die", "Da\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Die K\u00e4ufer anzusehn.", "tokens": ["Die", "K\u00e4u\u00b7fer", "an\u00b7zu\u00b7sehn", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIZU", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Selbst wenn Ausl\u00e4nder kamen,", "tokens": ["Selbst", "wenn", "Aus\u00b7l\u00e4n\u00b7der", "ka\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "NN", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Blieb jede schamrot stehn.", "tokens": ["Blieb", "je\u00b7de", "scham\u00b7rot", "stehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "ADJD", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Sie sahen nur auf H\u00e4nde,", "tokens": ["Sie", "sa\u00b7hen", "nur", "auf", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Man schlug nie auf den Blick,", "tokens": ["Man", "schlug", "nie", "auf", "den", "Blick", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Erkannten ohn' Umst\u00e4nde", "tokens": ["Er\u00b7kann\u00b7ten", "ohn'", "Um\u00b7st\u00e4n\u00b7de"], "token_info": ["word", "word", "word"], "pos": ["NN", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Den Mensch am Handmimik.", "tokens": ["Den", "Mensch", "am", "Hand\u00b7mi\u00b7mik", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Konnten durch Handschuh lesen,", "tokens": ["Konn\u00b7ten", "durch", "Hand\u00b7schuh", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "NN", "VVINF", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.2": {"text": "Was jeder K\u00e4ufer denkt,", "tokens": ["Was", "je\u00b7der", "K\u00e4u\u00b7fer", "denkt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wenn sie dem obern Wesen", "tokens": ["Wenn", "sie", "dem", "o\u00b7bern", "We\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Auch keinen Blick geschenkt.", "tokens": ["Auch", "kei\u00b7nen", "Blick", "ge\u00b7schenkt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Sah'n nur den kleinen Finger", "tokens": ["Sah'n", "nur", "den", "klei\u00b7nen", "Fin\u00b7ger"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und wu\u00dften es sogleich:", "tokens": ["Und", "wu\u00df\u00b7ten", "es", "sog\u00b7leich", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Trotz Diamantendinger", "tokens": ["Trotz", "Di\u00b7a\u00b7man\u00b7ten\u00b7din\u00b7ger"], "token_info": ["word", "word"], "pos": ["APPR", "NN"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Sind Menschen doch nicht reich.", "tokens": ["Sind", "Men\u00b7schen", "doch", "nicht", "reich", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Sie m\u00fcssen noch was haben,", "tokens": ["Sie", "m\u00fcs\u00b7sen", "noch", "was", "ha\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "PWS", "VAFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Noch ein besondres Air.", "tokens": ["Noch", "ein", "be\u00b7sond\u00b7res", "Air", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Denn sonst, bei allen Gaben,", "tokens": ["Denn", "sonst", ",", "bei", "al\u00b7len", "Ga\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sind ihre H\u00e4nde leer.", "tokens": ["Sind", "ih\u00b7re", "H\u00e4n\u00b7de", "leer", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Bei Heinzens H\u00e4nde fielen", "tokens": ["Bei", "Hein\u00b7zens", "H\u00e4n\u00b7de", "fie\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NE", "NN", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Sie beide fast zur Wand.", "tokens": ["Sie", "bei\u00b7de", "fast", "zur", "Wand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIS", "ADV", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie war ganz ohne Schwielen", "tokens": ["Sie", "war", "ganz", "oh\u00b7ne", "Schwie\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und doch die Schicksalshand.", "tokens": ["Und", "doch", "die", "Schick\u00b7sals\u00b7hand", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Zum ersten Male tauten", "tokens": ["Zum", "ers\u00b7ten", "Ma\u00b7le", "tau\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Die beiden Damen auf.", "tokens": ["Die", "bei\u00b7den", "Da\u00b7men", "auf", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und ihre Blicke blauten", "tokens": ["Und", "ih\u00b7re", "Bli\u00b7cke", "blau\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Heinz bis zum Hals hinauf.", "tokens": ["Heinz", "bis", "zum", "Hals", "hin\u00b7auf", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Von Beiden die Aurora,", "tokens": ["Von", "Bei\u00b7den", "die", "Au\u00b7ro\u00b7ra", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "ART", "NE", "$,"], "meter": "-+-+++-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Sie ward besonders rot.", "tokens": ["Sie", "ward", "be\u00b7son\u00b7ders", "rot", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Was fl\u00fcstert ihr ins Ohr da:", "tokens": ["Was", "fl\u00fcs\u00b7tert", "ihr", "ins", "Ohr", "da", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Der Mann, der bringt den Tod!", "tokens": ["Der", "Mann", ",", "der", "bringt", "den", "Tod", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "Wogegen die Alice", "tokens": ["Wo\u00b7ge\u00b7gen", "die", "A\u00b7li\u00b7ce"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "ART", "NE"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Den Heinzen fast anblickt.", "tokens": ["Den", "Hein\u00b7zen", "fast", "an\u00b7blickt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und eben es war diese,", "tokens": ["Und", "e\u00b7ben", "es", "war", "die\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PPER", "VAFIN", "PDS", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Die dann am Heinz erstickt.", "tokens": ["Die", "dann", "am", "Heinz", "er\u00b7stickt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Herr Heinz kauft hundert Rosen", "tokens": ["Herr", "Heinz", "kauft", "hun\u00b7dert", "Ro\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NE", "VVFIN", "CARD", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und alle ohne Draht.", "tokens": ["Und", "al\u00b7le", "oh\u00b7ne", "Draht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Weil er die Stengellosen", "tokens": ["Weil", "er", "die", "Sten\u00b7gel\u00b7lo\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Von je verachtet hat.", "tokens": ["Von", "je", "ver\u00b7ach\u00b7tet", "hat", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Herr Heinz kam jeden Morgen", "tokens": ["Herr", "Heinz", "kam", "je\u00b7den", "Mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "NE", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und kaufte wie verr\u00fcckt,", "tokens": ["Und", "kauf\u00b7te", "wie", "ver\u00b7r\u00fcckt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "KOKOM", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ihm taten Freunde borgen,", "tokens": ["Ihm", "ta\u00b7ten", "Freun\u00b7de", "bor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Weil's Geben sie entz\u00fcckt.", "tokens": ["Weil's", "Ge\u00b7ben", "sie", "ent\u00b7z\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "PPER", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.18": {"line.1": {"text": "Im Herbste, wo die Veilchen", "tokens": ["Im", "Herbs\u00b7te", ",", "wo", "die", "Veil\u00b7chen"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "NN", "$,", "PWAV", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "In zweiter Bl\u00fcte stehn,", "tokens": ["In", "zwei\u00b7ter", "Bl\u00fc\u00b7te", "stehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da mu\u00dft nach einem Weilchen", "tokens": ["Da", "mu\u00dft", "nach", "ei\u00b7nem", "Weil\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Ein Zwilling einsam stehn.", "tokens": ["Ein", "Zwil\u00b7ling", "ein\u00b7sam", "stehn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Denn Heinz, er hat's entschieden:", "tokens": ["Denn", "Heinz", ",", "er", "hat's", "ent\u00b7schie\u00b7den", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NE", "$,", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Er n\u00e4hm Fr\u00e4ulein Auror'.", "tokens": ["Er", "n\u00e4hm", "Fr\u00e4u\u00b7lein", "Au\u00b7ror'", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "NE", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Ihm schien die mehr zu sieden,", "tokens": ["Ihm", "schien", "die", "mehr", "zu", "sie\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "PIS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und kam ihm w\u00e4rmer vor.", "tokens": ["Und", "kam", "ihm", "w\u00e4r\u00b7mer", "vor", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.20": {"line.1": {"text": "Alice stand im Laden", "tokens": ["A\u00b7li\u00b7ce", "stand", "im", "La\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "VVFIN", "APPRART", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "Am Sonntag Nachmittag,", "tokens": ["Am", "Sonn\u00b7tag", "Nach\u00b7mit\u00b7tag", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Heut' ging die Stadt zum Baden,", "tokens": ["Heut'", "ging", "die", "Stadt", "zum", "Ba\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Herbstglut am Himmel lag.", "tokens": ["Herbst\u00b7glut", "am", "Him\u00b7mel", "lag", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "VVFIN", "$."], "meter": "++-+-+", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Sie mochte nicht mal denken", "tokens": ["Sie", "moch\u00b7te", "nicht", "mal", "den\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "An das geringste Bad.", "tokens": ["An", "das", "ge\u00b7rings\u00b7te", "Bad", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Konnt den Gedank nicht lenken", "tokens": ["Konnt", "den", "Ge\u00b7dank", "nicht", "len\u00b7ken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "ART", "NN", "PTKNEG", "VVINF"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.4": {"text": "Von Heinz, den sie nicht hat.", "tokens": ["Von", "Heinz", ",", "den", "sie", "nicht", "hat", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "PRELS", "PPER", "PTKNEG", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.22": {"line.1": {"text": "Sie ist schon l\u00e4ngst entschlossen,", "tokens": ["Sie", "ist", "schon", "l\u00e4ngst", "ent\u00b7schlos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und heute wird's getan,", "tokens": ["Und", "heu\u00b7te", "wird's", "ge\u00b7tan", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie f\u00fcllt sich einen gro\u00dfen", "tokens": ["Sie", "f\u00fcllt", "sich", "ei\u00b7nen", "gro\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ART", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Waschkorb mit Veilchen an.", "tokens": ["Waschkorb", "mit", "Veil\u00b7chen", "an", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "PTKVZ", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.23": {"line.1": {"text": "Sie ist mit ihrem wei\u00dfen", "tokens": ["Sie", "ist", "mit", "ih\u00b7rem", "wei\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "PPOSAT", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Firmungsgewand geschm\u00fcckt.", "tokens": ["Fir\u00b7mungs\u00b7ge\u00b7wand", "ge\u00b7schm\u00fcckt", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "Tat Tr\u00e4nen stolz verbei\u00dfen", "tokens": ["Tat", "Tr\u00e4\u00b7nen", "stolz", "ver\u00b7bei\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "NN", "ADJD", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und hat sich tief geb\u00fcckt.", "tokens": ["Und", "hat", "sich", "tief", "ge\u00b7b\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PRF", "ADJD", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.24": {"line.1": {"text": "Im Waschkorb zu ersticken", "tokens": ["Im", "Waschkorb", "zu", "er\u00b7sti\u00b7cken"], "token_info": ["word", "word", "word", "word"], "pos": ["APPRART", "NN", "PTKZU", "VVINF"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Sucht sie durch Veilchen Ruh.", "tokens": ["Sucht", "sie", "durch", "Veil\u00b7chen", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ein Talglicht bl\u00f6d von Blicken", "tokens": ["Ein", "Talg\u00b7licht", "bl\u00f6d", "von", "Bli\u00b7cken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sieht ihr mit Tr\u00e4nen zu.", "tokens": ["Sieht", "ihr", "mit", "Tr\u00e4\u00b7nen", "zu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.25": {"line.1": {"text": "Auf einen wei\u00dfen Bogen", "tokens": ["Auf", "ei\u00b7nen", "wei\u00b7\u00dfen", "Bo\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Schrieb sie es vorher hin:", "tokens": ["Schrieb", "sie", "es", "vor\u00b7her", "hin", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbaurora, bin betragen,", "tokens": ["\u00bb", "au\u00b7ro\u00b7ra", ",", "bin", "be\u00b7tra\u00b7gen", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct"], "pos": ["$(", "NE", "$,", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Weil ich Dein Zwilling bin.\u00ab", "tokens": ["Weil", "ich", "Dein", "Zwil\u00b7ling", "bin", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.26": {"line.1": {"text": "Mit Heinz kommt heim Aurora", "tokens": ["Mit", "Heinz", "kommt", "heim", "Au\u00b7ro\u00b7ra"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "VVFIN", "NE", "NE"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und sucht im Ladenraum:", "tokens": ["Und", "sucht", "im", "La\u00b7den\u00b7raum", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbalic' war doch zuvor da!", "tokens": ["\u00bb", "a\u00b7lic'", "war", "doch", "zu\u00b7vor", "da", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VAFIN", "ADV", "ADV", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Jetzt sieht man sie ja kaum.\u00ab", "tokens": ["Jetzt", "sieht", "man", "sie", "ja", "kaum", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PPER", "ADV", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.27": {"line.1": {"text": "Die Talglichttr\u00e4nen stanken,", "tokens": ["Die", "Talg\u00b7licht\u00b7tr\u00e4\u00b7nen", "stan\u00b7ken", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Das Licht war eben aus.", "tokens": ["Das", "Licht", "war", "e\u00b7ben", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Es raucht noch in Gedanken \u2013", "tokens": ["Es", "raucht", "noch", "in", "Ge\u00b7dan\u00b7ken", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Aurora schlich hinaus.", "tokens": ["Au\u00b7ro\u00b7ra", "schlich", "hin\u00b7aus", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.28": {"line.1": {"text": "\u00bbach, Heinz, komm doch mal n\u00e4her,", "tokens": ["\u00bb", "ach", ",", "Heinz", ",", "komm", "doch", "mal", "n\u00e4\u00b7her", ","], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "$,", "NE", "$,", "VVFIN", "ADV", "ADV", "ADJD", "$,"], "meter": "+-+-++-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Ich glaub', es ist wer tot,", "tokens": ["Ich", "glaub'", ",", "es", "ist", "wer", "tot", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VAFIN", "PWS", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Es riecht nach Leichen eher", "tokens": ["Es", "riecht", "nach", "Lei\u00b7chen", "e\u00b7her"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "ADV"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Als wie nach Rosenrot.\u00ab", "tokens": ["Als", "wie", "nach", "Ro\u00b7sen\u00b7rot", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "KOKOM", "APPR", "NE", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.29": {"line.1": {"text": "Heinz kommt ganz in Gedanken,", "tokens": ["Heinz", "kommt", "ganz", "in", "Ge\u00b7dan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Zum Veilchenkorbe hin,", "tokens": ["Zum", "Veil\u00b7chen\u00b7kor\u00b7be", "hin", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "F\u00fchlt seine Kniee wanken", "tokens": ["F\u00fchlt", "sei\u00b7ne", "Kni\u00b7ee", "wan\u00b7ken"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und sagt: \u00bbEs liegt wer drin.\u00ab", "tokens": ["Und", "sagt", ":", "\u00bb", "Es", "liegt", "wer", "drin", ".", "\u00ab"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "$.", "$(", "PPER", "VVFIN", "PWS", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.30": {"line.1": {"text": "Tief unter blauen Veilchen", "tokens": ["Tief", "un\u00b7ter", "blau\u00b7en", "Veil\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Lag die Alice wei\u00df,", "tokens": ["Lag", "die", "A\u00b7li\u00b7ce", "wei\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NE", "VVFIN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Sie zuckte noch ein Weilchen", "tokens": ["Sie", "zuck\u00b7te", "noch", "ein", "Weil\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und starb dann schnell mit Flei\u00df.", "tokens": ["Und", "starb", "dann", "schnell", "mit", "Flei\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADJD", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.31": {"line.1": {"text": "Untr\u00f6stlich war Aurora,", "tokens": ["Un\u00b7tr\u00f6st\u00b7lich", "war", "Au\u00b7ro\u00b7ra", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Herr Heinzen schluchzt mit Macht.", "tokens": ["Herr", "Hein\u00b7zen", "schluchzt", "mit", "Macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ein Licht war noch zurvor da,", "tokens": ["Ein", "Licht", "war", "noch", "zur\u00b7vor", "da", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "ADV", "$,"], "meter": "-+--+--", "measure": "iambic.di.relaxed"}, "line.4": {"text": "Und jetzt war's still und Nacht.", "tokens": ["Und", "jetzt", "wa\u00b7r's", "still", "und", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADJD", "KON", "NN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.32": {"line.1": {"text": "Und noch nach langen Jahren", "tokens": ["Und", "noch", "nach", "lan\u00b7gen", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Sieht man den Heinzen viel,", "tokens": ["Sieht", "man", "den", "Hein\u00b7zen", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "ADV", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "Mit seltsamen Gebahren", "tokens": ["Mit", "selt\u00b7sa\u00b7men", "Ge\u00b7bah\u00b7ren"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ADJA", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Zur Veilchenzeit oft still,", "tokens": ["Zur", "Veil\u00b7chen\u00b7zeit", "oft", "still", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.33": {"line.1": {"text": "In eine Hand voll Veilchen", "tokens": ["In", "ei\u00b7ne", "Hand", "voll", "Veil\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADJD", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Den Kopf hineingesteckt,", "tokens": ["Den", "Kopf", "hin\u00b7ein\u00b7ge\u00b7steckt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Das treibt er so ein Weilchen,", "tokens": ["Das", "treibt", "er", "so", "ein", "Weil\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Bis ihn Aurora weckt.", "tokens": ["Bis", "ihn", "Au\u00b7ro\u00b7ra", "weckt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "NE", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.34": {"line.1": {"text": "\u00bbich wollte es nur f\u00fchlen\u00ab", "tokens": ["\u00bb", "ich", "woll\u00b7te", "es", "nur", "f\u00fch\u00b7len", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "PPER", "ADV", "VVINF", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Spricht Heinz dann lebensm\u00fcd,", "tokens": ["Spricht", "Heinz", "dann", "le\u00b7bens\u00b7m\u00fcd", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ADV", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbob Veilchen wirklich k\u00fchlen,", "tokens": ["\u00bb", "ob", "Veil\u00b7chen", "wirk\u00b7lich", "k\u00fch\u00b7len", ","], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "KOUS", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Wenn's Blut im Herzen gl\u00fcht.\u00ab", "tokens": ["Wenn's", "Blut", "im", "Her\u00b7zen", "gl\u00fcht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "NN", "APPRART", "NN", "VVFIN", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.35": {"line.1": {"text": "Heut' sitzt am Sarge diese", "tokens": ["Heut'", "sitzt", "am", "Sar\u00b7ge", "die\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "PDAT"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Aurora, und sie weint,", "tokens": ["Au\u00b7ro\u00b7ra", ",", "und", "sie", "weint", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "KON", "PPER", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Denkt: gl\u00fccklich ist Alice,", "tokens": ["Denkt", ":", "gl\u00fcck\u00b7lich", "ist", "A\u00b7li\u00b7ce", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "ADJD", "VAFIN", "NE", "$,"], "meter": "-+--+--", "measure": "iambic.di.relaxed"}, "line.4": {"text": "Jetzt kriegt sie meinen Freund.", "tokens": ["Jetzt", "kriegt", "sie", "mei\u00b7nen", "Freund", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.36": {"line.1": {"text": "Und seufzend streut Aurora", "tokens": ["Und", "seuf\u00b7zend", "streut", "Au\u00b7ro\u00b7ra"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADJD", "VVFIN", "NE"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Ihm Parmaveilchen hin:", "tokens": ["Ihm", "Par\u00b7ma\u00b7veil\u00b7chen", "hin", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbach w\u00e4rst Du wie zuvor da,", "tokens": ["\u00bb", "ach", "w\u00e4rst", "Du", "wie", "zu\u00b7vor", "da", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "XY", "VAFIN", "PPER", "KOKOM", "ADV", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Weil ich noch lebend bin!\u00ab", "tokens": ["Weil", "ich", "noch", "le\u00b7bend", "bin", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.37": {"line.1": {"text": "\u00bbjetzo\u00ab, schlo\u00df hier die Amme,", "tokens": ["\u00bb", "jet\u00b7zo", "\u00ab", ",", "schlo\u00df", "hier", "die", "Am\u00b7me", ","], "token_info": ["punct", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$(", "$,", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.2": {"text": "\u00bbwill ich Kaffee einschenken.", "tokens": ["\u00bb", "will", "ich", "Kaf\u00b7fee", "ein\u00b7schen\u00b7ken", "."], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VMFIN", "PPER", "NN", "VVIZU", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Lieb ist 'ne wundersame", "tokens": ["Lieb", "ist", "'ne", "wun\u00b7der\u00b7sa\u00b7me"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VAFIN", "ART", "ADJA"], "meter": "++-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Sache und gibt zu denken.\u00ab \u2013", "tokens": ["Sa\u00b7che", "und", "gibt", "zu", "den\u00b7ken", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["NN", "KON", "VVFIN", "PTKZU", "VVINF", "$.", "$(", "$("], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}}}}