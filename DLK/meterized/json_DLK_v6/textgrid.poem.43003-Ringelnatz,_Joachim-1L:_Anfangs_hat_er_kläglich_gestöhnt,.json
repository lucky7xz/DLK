{"textgrid.poem.43003": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Anfangs hat er kl\u00e4glich gest\u00f6hnt,", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Anfangs hat er kl\u00e4glich gest\u00f6hnt,", "tokens": ["An\u00b7fangs", "hat", "er", "kl\u00e4g\u00b7lich", "ge\u00b7st\u00f6hnt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "VVPP", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Denn er war zuvor in der K\u00fcche", "tokens": ["Denn", "er", "war", "zu\u00b7vor", "in", "der", "K\u00fc\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ADV", "APPR", "ART", "NN"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Kartoffelsch\u00e4ler und andre Ger\u00fcche", "tokens": ["Kar\u00b7tof\u00b7fel\u00b7sch\u00e4\u00b7ler", "und", "and\u00b7re", "Ge\u00b7r\u00fc\u00b7che"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "KON", "ADJA", "NN"], "meter": "---+--+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Von daher gew\u00f6hnt.", "tokens": ["Von", "da\u00b7her", "ge\u00b7w\u00f6hnt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PAV", "VVPP", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Er ist ebenso dumm wie faul.", "tokens": ["Er", "ist", "e\u00b7ben\u00b7so", "dumm", "wie", "faul", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "KOKOM", "ADJD", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Er \u00f6ffnet die T\u00fcren zu den Aborten,", "tokens": ["Er", "\u00f6ff\u00b7net", "die", "T\u00fc\u00b7ren", "zu", "den", "Ab\u00b7or\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+-+--", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und nach kurzen, bl\u00f6dsinnigen Worten", "tokens": ["Und", "nach", "kur\u00b7zen", ",", "bl\u00f6d\u00b7sin\u00b7ni\u00b7gen", "Wor\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KON", "APPR", "ADJA", "$,", "ADJA", "NN"], "meter": "--+--+--+-", "measure": "anapaest.tri.plus"}, "line.4": {"text": "\u00dcber das Wetter h\u00e4lt er das Maul.", "tokens": ["\u00dc\u00b7ber", "das", "Wet\u00b7ter", "h\u00e4lt", "er", "das", "Maul", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}}, "stanza.3": {"line.1": {"text": "Nie ist er freundlich. Dennoch verehren", "tokens": ["Nie", "ist", "er", "freund\u00b7lich", ".", "Den\u00b7noch", "ver\u00b7eh\u00b7ren"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "$.", "ADV", "VVFIN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ihn manche sehr;", "tokens": ["Ihn", "man\u00b7che", "sehr", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "PIS", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Besonders die, die ihm hinterher", "tokens": ["Be\u00b7son\u00b7ders", "die", ",", "die", "ihm", "hin\u00b7ter\u00b7her"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ART", "$,", "PRELS", "PPER", "ADV"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Handt\u00fccher stehlen und Nagelscheren.", "tokens": ["Hand\u00b7t\u00fc\u00b7cher", "steh\u00b7len", "und", "Na\u00b7gel\u00b7sche\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "KON", "NE", "$."], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}}, "stanza.4": {"line.1": {"text": "Ich wei\u00df nicht, warum ich mich vor ihm geniere.", "tokens": ["Ich", "wei\u00df", "nicht", ",", "wa\u00b7rum", "ich", "mich", "vor", "ihm", "ge\u00b7nie\u00b7re", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$,", "PWAV", "PPER", "PRF", "APPR", "PPER", "ADJA", "$."], "meter": "-+--+-++--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Er l\u00e4\u00dft mir niemals zum Waschen Zeit,", "tokens": ["Er", "l\u00e4\u00dft", "mir", "nie\u00b7mals", "zum", "Wa\u00b7schen", "Zeit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "APPRART", "NN", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und durch seinen Geiz in bezug auf Papiere", "tokens": ["Und", "durch", "sei\u00b7nen", "Geiz", "in", "be\u00b7zug", "auf", "Pa\u00b7pie\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "PPOSAT", "NN", "APPR", "NN", "APPR", "NN"], "meter": "--+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Geriet ich schon oft in Verlegenheit.", "tokens": ["Ge\u00b7riet", "ich", "schon", "oft", "in", "Ver\u00b7le\u00b7gen\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "ADV", "APPR", "NN", "$."], "meter": "-+-+++-+-+", "measure": "zehnsilber"}}, "stanza.5": {"line.1": {"text": "Im Grunde \u00e4rgert's ihn, wenn man seine", "tokens": ["Im", "Grun\u00b7de", "\u00e4r\u00b7gert's", "ihn", ",", "wenn", "man", "sei\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "$,", "KOUS", "PIS", "PPOSAT"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ger\u00e4te benutzt.", "tokens": ["Ge\u00b7r\u00e4\u00b7te", "be\u00b7nutzt", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.3": {"text": "Obwohl er niemals, auch nicht mal zum Scheine,", "tokens": ["Ob\u00b7wohl", "er", "nie\u00b7mals", ",", "auch", "nicht", "mal", "zum", "Schei\u00b7ne", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "$,", "ADV", "PTKNEG", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Daran etwas putzt.", "tokens": ["Da\u00b7ran", "et\u00b7was", "putzt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PAV", "PIS", "VVFIN", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.6": {"line.1": {"text": "\u00bbgedenket des Alten,", "tokens": ["\u00bb", "ge\u00b7den\u00b7ket", "des", "Al\u00b7ten", ","], "token_info": ["punct", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Denn er mu\u00df alles reine halten!\u00ab", "tokens": ["Denn", "er", "mu\u00df", "al\u00b7les", "rei\u00b7ne", "hal\u00b7ten", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPER", "VMFIN", "PIAT", "ADJA", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Schreibt er mit Seife, Frechheit und Ruhe", "tokens": ["Schreibt", "er", "mit", "Sei\u00b7fe", ",", "Frech\u00b7heit", "und", "Ru\u00b7he"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "NN", "$,", "NN", "KON", "NN"], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Jeden Morgen gro\u00df an den Spiegel.", "tokens": ["Je\u00b7den", "Mor\u00b7gen", "gro\u00df", "an", "den", "Spie\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADJD", "APPR", "ART", "NN", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Und dabei hat dieser Schweinigel", "tokens": ["Und", "da\u00b7bei", "hat", "die\u00b7ser", "Schwei\u00b7ni\u00b7gel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PAV", "VAFIN", "PDAT", "NN"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.6": {"text": "So ein vornehm nerv\u00f6ses Getue,", "tokens": ["So", "ein", "vor\u00b7nehm", "ner\u00b7v\u00f6\u00b7ses", "Ge\u00b7tue", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.7": {"text": "Das jeden zwingt, ihm viel Trinkgeld zu geben,", "tokens": ["Das", "je\u00b7den", "zwingt", ",", "ihm", "viel", "Trink\u00b7geld", "zu", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "$,", "PPER", "PIAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Und er z\u00e4hlt immer gleich nach, wieviel. \u2013 \u2013", "tokens": ["Und", "er", "z\u00e4hlt", "im\u00b7mer", "gleich", "nach", ",", "wie\u00b7viel", ".", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "punct", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADV", "PTKVZ", "$,", "VVFIN", "$.", "$(", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.7": {"line.1": {"text": "Ja, so ein bequemes, geldbringendes Leben", "tokens": ["Ja", ",", "so", "ein", "be\u00b7que\u00b7mes", ",", "geld\u00b7brin\u00b7gen\u00b7des", "Le\u00b7ben"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["PTKANT", "$,", "ADV", "ART", "ADJA", "$,", "ADJA", "NN"], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Zu f\u00fchren, das w\u00e4re wohl jedermanns Ziel.", "tokens": ["Zu", "f\u00fch\u00b7ren", ",", "das", "w\u00e4\u00b7re", "wohl", "je\u00b7der\u00b7manns", "Ziel", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PDS", "VAFIN", "ADV", "PIAT", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}}}}}