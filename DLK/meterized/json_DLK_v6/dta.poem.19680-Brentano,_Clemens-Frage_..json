{"dta.poem.19680": {"metadata": {"author": {"name": "Brentano, Clemens", "birth": "N.A.", "death": "N.A."}, "title": "Frage .", "genre": "Lyrik", "period": "N.A.", "pub_year": "1808", "urn": "urn:nbn:de:kobv:b4-20090519168", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Gr\u00fc\u00df dich Gott mein Schmidt!             ", "tokens": ["Gr\u00fc\u00df", "dich", "Gott", "mein", "Schmidt", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "NN", "PPOSAT", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Dank dir Gott mein Schmidt!", "tokens": ["Dank", "dir", "Gott", "mein", "Schmidt", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "NN", "PPOSAT", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Mein Schmidt, wo streichst du her?", "tokens": ["Mein", "Schmidt", ",", "wo", "streichst", "du", "her", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWAV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Da\u00df deine Schuhe so staubig,", "tokens": ["Da\u00df", "dei\u00b7ne", "Schu\u00b7he", "so", "stau\u00b7big", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Dein Haar so krausig, dein Bart auf beiden Backen herausf\u00e4hrt", "tokens": ["Dein", "Haar", "so", "krau\u00b7sig", ",", "dein", "Bart", "auf", "bei\u00b7den", "Ba\u00b7cken", "her\u00b7aus\u00b7f\u00e4hrt"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV", "ADJD", "$,", "PPOSAT", "NN", "APPR", "PIAT", "NN", "VVPP"], "meter": "-+-+--+-+-+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Wie ein zweischneidig Schlachtschwerdt.", "tokens": ["Wie", "ein", "zwei\u00b7schnei\u00b7dig", "Schlacht\u00b7schwerdt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Du hast eine feine meisterliche Art,", "tokens": ["Du", "hast", "ei\u00b7ne", "fei\u00b7ne", "meis\u00b7ter\u00b7li\u00b7che", "Art", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.6": {"text": "Einen feinen meisterlichen Bart,", "tokens": ["Ei\u00b7nen", "fei\u00b7nen", "meis\u00b7ter\u00b7li\u00b7chen", "Bart", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.7": {"text": "Eine feine meisterliche Gestalt,", "tokens": ["Ei\u00b7ne", "fei\u00b7ne", "meis\u00b7ter\u00b7li\u00b7che", "Ge\u00b7stalt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.8": {"text": "Du bist weder zu jung noch zu alt.", "tokens": ["Du", "bist", "we\u00b7der", "zu", "jung", "noch", "zu", "alt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "KON", "PTKA", "ADJD", "ADV", "PTKA", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.9": {"text": "Mein Schmidt bist du Meister gewesen,", "tokens": ["Mein", "Schmidt", "bist", "du", "Meis\u00b7ter", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "NN", "VAPP", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.10": {"text": "Oder denkst du noch mit der Zeit Meister zu werden?", "tokens": ["O\u00b7der", "denkst", "du", "noch", "mit", "der", "Zeit", "Meis\u00b7ter", "zu", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPR", "ART", "NN", "NN", "PTKZU", "VAINF", "$."], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Mein Schmidt, ich streich daher \u00fcbers Land,", "tokens": ["Mein", "Schmidt", ",", "ich", "streich", "da\u00b7her", "\u00fc\u00b7bers", "Land", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPER", "PRF", "PAV", "APPRART", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Wie der Krebs \u00fcbern Sand,", "tokens": ["Wie", "der", "Krebs", "\u00fc\u00b7bern", "Sand", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Wie der Fisch \u00fcbers Meer,", "tokens": ["Wie", "der", "Fisch", "\u00fc\u00b7bers", "Meer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPRART", "NN", "$,"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Da\u00df ich mich junger Hufschmidt auch ern\u00e4hr.", "tokens": ["Da\u00df", "ich", "mich", "jun\u00b7ger", "Huf\u00b7schmidt", "auch", "er\u00b7n\u00e4hr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADJA", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Mein Schmidt ich bin nicht Meister gewesen,", "tokens": ["Mein", "Schmidt", "ich", "bin", "nicht", "Meis\u00b7ter", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPER", "VAFIN", "PTKNEG", "NN", "VAPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Ich denk aber mit der Zeit noch Meister zu werden,", "tokens": ["Ich", "denk", "a\u00b7ber", "mit", "der", "Zeit", "noch", "Meis\u00b7ter", "zu", "wer\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "NN", "ADV", "NN", "PTKZU", "VAINF", "$,"], "meter": "-+--+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Ist es gleich nicht hier,", "tokens": ["Ist", "es", "gleich", "nicht", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "PTKNEG", "ADV", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.8": {"text": "So ist es anderswo schier,", "tokens": ["So", "ist", "es", "an\u00b7ders\u00b7wo", "schier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADJD", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.9": {"text": "Wenn es gleich ist eine Meile von dem Ring,", "tokens": ["Wenn", "es", "gleich", "ist", "ei\u00b7ne", "Mei\u00b7le", "von", "dem", "Ring", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VAFIN", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.10": {"text": "Da der Hund \u00fcber Zaun springt,", "tokens": ["Da", "der", "Hund", "\u00fc\u00b7ber", "Zaun", "springt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "--++-+-", "measure": "anapaest.init"}, "line.11": {"text": "Da ist auch gut Meister zu werden.", "tokens": ["Da", "ist", "auch", "gut", "Meis\u00b7ter", "zu", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADJD", "NN", "PTKZU", "VAINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.5": {"line.1": {"text": "Mein Schmidt, wie thust du dich nennen,", "tokens": ["Mein", "Schmidt", ",", "wie", "thust", "du", "dich", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWAV", "VVFIN", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Wenn du hier und anderswo auf der Gesellen Herberge", "tokens": ["Wenn", "du", "hier", "und", "an\u00b7ders\u00b7wo", "auf", "der", "Ge\u00b7sel\u00b7len", "Her\u00b7ber\u00b7ge"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "KON", "ADV", "APPR", "ART", "NN", "NN"], "meter": "--+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "kommst,", "tokens": ["kommst", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.4": {"text": "Die Gesellen Lade offen steht,", "tokens": ["Die", "Ge\u00b7sel\u00b7len", "La\u00b7de", "of\u00b7fen", "steht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "ADJD", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "B\u00fcchse, Briefe, Siegel, Geld und Gut drinnen", "tokens": ["B\u00fcch\u00b7se", ",", "Brie\u00b7fe", ",", "Sie\u00b7gel", ",", "Geld", "und", "Gut", "drin\u00b7nen"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "KON", "ADJD", "ADV"], "meter": "+-+-+-+-++-", "measure": "unknown.measure.hexa"}, "line.6": {"text": "Und draussen herum liegen, g\u00fcnstige Meister und Gesellen,", "tokens": ["Und", "draus\u00b7sen", "he\u00b7rum", "lie\u00b7gen", ",", "g\u00fcns\u00b7ti\u00b7ge", "Meis\u00b7ter", "und", "Ge\u00b7sel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "APZR", "VVFIN", "$,", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.7": {"text": "Jung und alt um den Tisch herum sitzen, und halten eine", "tokens": ["Jung", "und", "alt", "um", "den", "Tisch", "he\u00b7rum", "sit\u00b7zen", ",", "und", "hal\u00b7ten", "ei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "KON", "ADJD", "APPR", "ART", "NN", "APZR", "VVINF", "$,", "KON", "VVFIN", "ART"], "meter": "+-+--+--+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.8": {"text": "feine stille Umfrage,", "tokens": ["fei\u00b7ne", "stil\u00b7le", "Um\u00b7fra\u00b7ge", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+--", "measure": "unknown.measure.tri"}, "line.9": {"text": "Gleich wie jetzt und allhier geschiehet?", "tokens": ["Gleich", "wie", "jetzt", "und", "all\u00b7hier", "ge\u00b7schie\u00b7het", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOKOM", "ADV", "KON", "ADV", "VVPP", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "Mein Schmidt, ich thu mich nennen,", "tokens": ["Mein", "Schmidt", ",", "ich", "thu", "mich", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPER", "VVFIN", "PPER", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Ferdinand Silbernagel, das ehrliche Blut,", "tokens": ["Fer\u00b7di\u00b7nand", "Sil\u00b7ber\u00b7na\u00b7gel", ",", "das", "ehr\u00b7li\u00b7che", "Blut", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ART", "ADJA", "NN", "$,"], "meter": "+--+-+--+--+", "measure": "iambic.penta.invert"}, "line.3": {"text": "Dem Essen und Trinken wohl thut,", "tokens": ["Dem", "Es\u00b7sen", "und", "Trin\u00b7ken", "wohl", "thut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "ADV", "VVFIN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Essen und Trinken hat mich ern\u00e4hrt,", "tokens": ["Es\u00b7sen", "und", "Trin\u00b7ken", "hat", "mich", "er\u00b7n\u00e4hrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VAFIN", "PPER", "VVFIN", "$,"], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Dar\u00fcber hab ich manchen sch\u00f6nen Pfenning verzehrt,", "tokens": ["Da\u00b7r\u00fc\u00b7ber", "hab", "ich", "man\u00b7chen", "sch\u00f6\u00b7nen", "Pfen\u00b7ning", "ver\u00b7zehrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PIAT", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.6": {"text": "All mein Vaters Gut,", "tokens": ["All", "mein", "Va\u00b7ters", "Gut", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "PPOSAT", "NN", "ADJD", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "Bis auf einen alten Filzhut,", "tokens": ["Bis", "auf", "ei\u00b7nen", "al\u00b7ten", "Filz\u00b7hut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.8": {"text": "Der liegt in der K\u00f6niglichen See- und Handlungs-Stadt", "tokens": ["Der", "liegt", "in", "der", "K\u00f6\u00b7nig\u00b7li\u00b7chen", "See", "und", "Hand\u00b7lungs\u00b7Stadt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "APPR", "ART", "NN", "TRUNC", "KON", "NN"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "Danzig,", "tokens": ["Dan\u00b7zig", ","], "token_info": ["word", "punct"], "pos": ["NE", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.10": {"text": "Unter des Herrn Vaters Dach;", "tokens": ["Un\u00b7ter", "des", "Herrn", "Va\u00b7ters", "Dach", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.11": {"text": "Wenn ich aber vor\u00fcbergeh,", "tokens": ["Wenn", "ich", "a\u00b7ber", "vor\u00b7\u00fc\u00b7ber\u00b7geh", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VVFIN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.12": {"text": "So mu\u00df ich seiner lachen,", "tokens": ["So", "mu\u00df", "ich", "sei\u00b7ner", "la\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPOSAT", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.13": {"text": "Er ist mir weder zu gut noch zu b\u00f6s,", "tokens": ["Er", "ist", "mir", "we\u00b7der", "zu", "gut", "noch", "zu", "b\u00f6s", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "KON", "PTKA", "ADJD", "ADV", "PTKA", "ADJD", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Da\u00df ich ihn nicht mag l\u00f6sen, mein Schmidt wilst du ihn l\u00f6sen,", "tokens": ["Da\u00df", "ich", "ihn", "nicht", "mag", "l\u00f6\u00b7sen", ",", "mein", "Schmidt", "wilst", "du", "ihn", "l\u00f6\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "PTKNEG", "VMFIN", "VVINF", "$,", "PPOSAT", "NN", "VMFIN", "PPER", "PPER", "VVINF", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.15": {"text": "So will ich dir auch 3 Heller zur Beisteuer schenken.", "tokens": ["So", "will", "ich", "dir", "auch", "3", "Hel\u00b7ler", "zur", "Bei\u00b7steu\u00b7er", "schen\u00b7ken", "."], "token_info": ["word", "word", "word", "word", "word", "number", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "ADV", "CARD", "NN", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Mein Schmidt, bedanke mich deines alten Filzhuts,", "tokens": ["Mein", "Schmidt", ",", "be\u00b7dan\u00b7ke", "mich", "dei\u00b7nes", "al\u00b7ten", "Filz\u00b7huts", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VVFIN", "PPER", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Ich habe selbst einen der ist nicht gut.", "tokens": ["Ich", "ha\u00b7be", "selbst", "ei\u00b7nen", "der", "ist", "nicht", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "ART", "VAFIN", "PTKNEG", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Aber Ferdinand Silbernagel ist wohl ein feiner Name,", "tokens": ["A\u00b7ber", "Fer\u00b7di\u00b7nand", "Sil\u00b7ber\u00b7na\u00b7gel", "ist", "wohl", "ein", "fei\u00b7ner", "Na\u00b7me", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "NE", "VAFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "--+--+-+--+-+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Er ist wohl 100 Reichsthaler mehr als ein fauler Apfel", "tokens": ["Er", "ist", "wohl", "100", "Reichst\u00b7ha\u00b7ler", "mehr", "als", "ein", "fau\u00b7ler", "Ap\u00b7fel"], "token_info": ["word", "word", "word", "number", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "CARD", "NN", "PIAT", "KOKOM", "ART", "ADJA", "NN"], "meter": "-+-+---+-+-+-", "measure": "unknown.measure.penta"}, "line.5": {"text": "einen Pfenning werth,", "tokens": ["ei\u00b7nen", "Pfen\u00b7ning", "werth", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.6": {"text": "Denselben nimmt man und wirft ihn zum Fenster hinaus,", "tokens": ["Den\u00b7sel\u00b7ben", "nimmt", "man", "und", "wirft", "ihn", "zum", "Fens\u00b7ter", "hin\u00b7aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PIS", "KON", "VVFIN", "PPER", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+--+--+--+", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Da kommt wohl ein grober, toller, voller Bauer mit sei-", "tokens": ["Da", "kommt", "wohl", "ein", "gro\u00b7ber", ",", "tol\u00b7ler", ",", "vol\u00b7ler", "Bau\u00b7er", "mit", "sei"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "ART", "ADJA", "$,", "ADJA", "$,", "ADJA", "NN", "APPR", "TRUNC"], "meter": "----+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.8": {"text": "nen gro\u00dfen Hanrey Stefeln", "tokens": ["nen", "gro\u00b7\u00dfen", "Han\u00b7rey", "Ste\u00b7feln"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "ADJA", "NN", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Und bricht wohl 99 mahl den Hals dar\u00fcber,", "tokens": ["Und", "bricht", "wohl", "99", "mahl", "den", "Hals", "da\u00b7r\u00fc\u00b7ber", ","], "token_info": ["word", "word", "word", "number", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "CARD", "ADV", "ART", "NN", "PAV", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Und spricht nicht einmal ho ho!", "tokens": ["Und", "spricht", "nicht", "ein\u00b7mal", "ho", "ho", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "ADV", "ITJ", "ITJ", "$."], "meter": "+-+-+--", "measure": "unknown.measure.tri"}, "line.11": {"text": "Aber dich und deinen ehrlichen Namen wollen wir hier", "tokens": ["A\u00b7ber", "dich", "und", "dei\u00b7nen", "ehr\u00b7li\u00b7chen", "Na\u00b7men", "wol\u00b7len", "wir", "hier"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "KON", "PPOSAT", "ADJA", "NN", "VMFIN", "PPER", "ADV"], "meter": "+-+-+-+--+-+--+", "measure": "trochaic.septa.relaxed"}, "line.12": {"text": "behalten,", "tokens": ["be\u00b7hal\u00b7ten", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.13": {"text": "Er ist auch wohl behaltens werth.", "tokens": ["Er", "ist", "auch", "wohl", "be\u00b7hal\u00b7tens", "werth", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Mein Schmidt, wo hast du ihn bekommen?", "tokens": ["Mein", "Schmidt", ",", "wo", "hast", "du", "ihn", "be\u00b7kom\u00b7men", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWAV", "VAFIN", "PPER", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Hast du ihn ersungen oder hast du ihn ersprungen,", "tokens": ["Hast", "du", "ihn", "er\u00b7sun\u00b7gen", "o\u00b7der", "hast", "du", "ihn", "er\u00b7sprun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "VVPP", "KON", "VAFIN", "PPER", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.16": {"text": "Oder hast du ihn bey sch\u00f6nen Jungfern bekommen?", "tokens": ["O\u00b7der", "hast", "du", "ihn", "bey", "sch\u00f6\u00b7nen", "Jung\u00b7fern", "be\u00b7kom\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPER", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "+-+-+-+-+--+-", "measure": "hexameter"}}, "stanza.8": {"line.1": {"text": "Mein Schmidt, ich konte wohl singen,", "tokens": ["Mein", "Schmidt", ",", "ich", "kon\u00b7te", "wohl", "sin\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPER", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Ich konte wohl springen,", "tokens": ["Ich", "kon\u00b7te", "wohl", "sprin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Ich konte wohl mit sch\u00f6nen Jungfern umgehen, das alles", "tokens": ["Ich", "kon\u00b7te", "wohl", "mit", "sch\u00f6\u00b7nen", "Jung\u00b7fern", "um\u00b7ge\u00b7hen", ",", "das", "al\u00b7les"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VMFIN", "ADV", "APPR", "ADJA", "NN", "VVINF", "$,", "PRELS", "PIS"], "meter": "-+-+-+-+--+--+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "wollte nichts helfen,", "tokens": ["woll\u00b7te", "nichts", "hel\u00b7fen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "VVINF", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Ich muste meinen ehrlichen Namen um ein frei Wochlohn", "tokens": ["Ich", "mus\u00b7te", "mei\u00b7nen", "ehr\u00b7li\u00b7chen", "Na\u00b7men", "um", "ein", "frei", "Woch\u00b7lohn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPOSAT", "ADJA", "NN", "APPR", "ART", "ADJD", "NN"], "meter": "-+-+-+--+---+-+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "kaufen,", "tokens": ["kau\u00b7fen", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Das Wochlohn wollte nicht recken,", "tokens": ["Das", "Woch\u00b7lohn", "woll\u00b7te", "nicht", "re\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Ich muste die Mutterpfennige und das Trinkgeld auch", "tokens": ["Ich", "mus\u00b7te", "die", "Mut\u00b7ter\u00b7pfen\u00b7ni\u00b7ge", "und", "das", "Trink\u00b7geld", "auch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "NN", "KON", "ART", "NN", "ADV"], "meter": "-+--+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.9": {"text": "drein stecken.", "tokens": ["drein", "ste\u00b7cken", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.9": {"line.1": {"text": "Mein Schmidt, in welcher Stadt oder Markflecken", "tokens": ["Mein", "Schmidt", ",", "in", "wel\u00b7cher", "Stadt", "o\u00b7der", "Mark\u00b7fle\u00b7cken"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "APPR", "PWAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Sind dir solch edle Wohlthaten wiederfahren?", "tokens": ["Sind", "dir", "solch", "ed\u00b7le", "Wohlt\u00b7ha\u00b7ten", "wie\u00b7der\u00b7fah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIAT", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.10": {"line.1": {"text": "Mein Schmidt, in der K\u00f6niglichen See- und Handlungs-", "tokens": ["Mein", "Schmidt", ",", "in", "der", "K\u00f6\u00b7nig\u00b7li\u00b7chen", "See", "und", "Hand\u00b7lungs"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "APPR", "ART", "NN", "TRUNC", "KON", "TRUNC"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Stadt Danzig,", "tokens": ["Stadt", "Dan\u00b7zig", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "NE", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.3": {"text": "Da man mehr Gersten zu Bier m\u00e4lzt,", "tokens": ["Da", "man", "mehr", "Gers\u00b7ten", "zu", "Bier", "m\u00e4lzt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PIAT", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Als man Silber und Gold schmelzt.", "tokens": ["Als", "man", "Sil\u00b7ber", "und", "Gold", "schmelzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "NN", "KON", "NN", "VVFIN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.11": {"line.1": {"text": "Mein Schmidt, kanst du mir nicht zwei oder drei nennen,", "tokens": ["Mein", "Schmidt", ",", "kanst", "du", "mir", "nicht", "zwei", "o\u00b7der", "drei", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VMFIN", "PPER", "PPER", "PTKNEG", "CARD", "KON", "CARD", "VVINF", "$,"], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.2": {"text": "Damit ich dich und deinen ehrlichen Namen m\u00f6g erkennen?", "tokens": ["Da\u00b7mit", "ich", "dich", "und", "dei\u00b7nen", "ehr\u00b7li\u00b7chen", "Na\u00b7men", "m\u00f6g", "er\u00b7ken\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "KON", "PPOSAT", "ADJA", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}}, "stanza.12": {"line.1": {"text": "Mein Schmidt, ich kan sie dir wohl nennen,", "tokens": ["Mein", "Schmidt", ",", "ich", "kan", "sie", "dir", "wohl", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPER", "VMFIN", "PPER", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn du sie nur th\u00e4test erkennen;", "tokens": ["Wenn", "du", "sie", "nur", "th\u00e4\u00b7test", "er\u00b7ken\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "VVFIN", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Es ist da bey gewesen Gotthelf Springinsfeld, Andreas", "tokens": ["Es", "ist", "da", "bey", "ge\u00b7we\u00b7sen", "Got\u00b7thelf", "Sprin\u00b7gins\u00b7feld", ",", "A\u00b7ndre\u00b7as"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "VAPP", "CARD", "NN", "$,", "NE"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.4": {"text": "Silbernagel, Gottlob Trifteisen,", "tokens": ["Sil\u00b7ber\u00b7na\u00b7gel", ",", "Gott\u00b7lob", "Trif\u00b7tei\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.5": {"text": "Mit diesen dreien kan ichs bezeugen und beweisen", "tokens": ["Mit", "die\u00b7sen", "drei\u00b7en", "kan", "ichs", "be\u00b7zeu\u00b7gen", "und", "be\u00b7wei\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "CARD", "VMFIN", "PIS", "VVFIN", "KON", "VVINF"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Und ist es dir nicht genug,", "tokens": ["Und", "ist", "es", "dir", "nicht", "ge\u00b7nug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PPER", "PTKNEG", "ADV", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "So bin Ferdinand Silbernagel der vierte", "tokens": ["So", "bin", "Fer\u00b7di\u00b7nand", "Sil\u00b7ber\u00b7na\u00b7gel", "der", "vier\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "NE", "NN", "ART", "ADJA"], "meter": "--+--+-+--+-", "measure": "anapaest.di.plus"}, "line.8": {"text": "Und andere gute Gesellen mehr,", "tokens": ["Und", "an\u00b7de\u00b7re", "gu\u00b7te", "Ge\u00b7sel\u00b7len", "mehr", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "ADV", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.9": {"text": "Die ich nicht alle herz\u00e4hlen kann.", "tokens": ["Die", "ich", "nicht", "al\u00b7le", "her\u00b7z\u00e4h\u00b7len", "kann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PTKNEG", "PIS", "VVINF", "VMFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.13": {"line.1": {"text": "Mein Schmidt, war es dir nicht leid,", "tokens": ["Mein", "Schmidt", ",", "war", "es", "dir", "nicht", "leid", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VAFIN", "PPER", "PPER", "PTKNEG", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df es deren so viel waren?", "tokens": ["Da\u00df", "es", "de\u00b7ren", "so", "viel", "wa\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PDS", "ADV", "ADV", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Mein Schmidt es war mir nicht leid,", "tokens": ["Mein", "Schmidt", "es", "war", "mir", "nicht", "leid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPER", "VAFIN", "PPER", "PTKNEG", "ADJD", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Da\u00df es ihrer so viel waren,", "tokens": ["Da\u00df", "es", "ih\u00b7rer", "so", "viel", "wa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "ADV", "ADV", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Es war mir leid,", "tokens": ["Es", "war", "mir", "leid", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.4": {"text": "Das du und deine gute Neben-Gesellen nicht auch dabei", "tokens": ["Das", "du", "und", "dei\u00b7ne", "gu\u00b7te", "Ne\u00b7ben\u00b7Ge\u00b7sel\u00b7len", "nicht", "auch", "da\u00b7bei"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "KON", "PPOSAT", "ADJA", "NN", "PTKNEG", "ADV", "PAV"], "meter": "-+-+-+-+--+--+-+", "measure": "iambic.septa.relaxed"}, "line.5": {"text": "waren,", "tokens": ["wa\u00b7ren", ","], "token_info": ["word", "punct"], "pos": ["VAFIN", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.6": {"text": "Da\u00df die Stube oben so voll wie unten, und unten so", "tokens": ["Da\u00df", "die", "Stu\u00b7be", "o\u00b7ben", "so", "voll", "wie", "un\u00b7ten", ",", "und", "un\u00b7ten", "so"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV", "ADV", "ADJD", "KOKOM", "ADV", "$,", "KON", "ADV", "ADV"], "meter": "--+-+--+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "voll wie oben,", "tokens": ["voll", "wie", "o\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KOKOM", "ADV", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.8": {"text": "Und h\u00e4tten einander zum Fenster hinaus getrunken,", "tokens": ["Und", "h\u00e4t\u00b7ten", "ein\u00b7an\u00b7der", "zum", "Fens\u00b7ter", "hin\u00b7aus", "ge\u00b7trun\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADV", "APPRART", "NN", "APZR", "VVPP", "$,"], "meter": "-+--+--+--+-+-", "measure": "amphibrach.tetra.plus"}, "line.9": {"text": "Und zum Kachelofen wieder herein,", "tokens": ["Und", "zum", "Ka\u00b7chel\u00b7o\u00b7fen", "wie\u00b7der", "her\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "ADV", "PTKVZ", "$,"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.10": {"text": "Der Kopf h\u00e4tte doch allezeit der vorderste must sein.", "tokens": ["Der", "Kopf", "h\u00e4t\u00b7te", "doch", "al\u00b7le\u00b7zeit", "der", "vor\u00b7ders\u00b7te", "must", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}}, "stanza.15": {"line.1": {"text": "Mein Schmidt, was w\u00e4re dir mit meinem Kopfschaden", "tokens": ["Mein", "Schmidt", ",", "was", "w\u00e4\u00b7re", "dir", "mit", "mei\u00b7nem", "Kopf\u00b7scha\u00b7den"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "PWS", "VAFIN", "PPER", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-+--", "measure": "unknown.measure.penta"}, "line.2": {"text": "gedient gewesen?", "tokens": ["ge\u00b7dient", "ge\u00b7we\u00b7sen", "?"], "token_info": ["word", "word", "punct"], "pos": ["VVPP", "VAPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "W\u00e4re es nicht besser gewesen,", "tokens": ["W\u00e4\u00b7re", "es", "nicht", "bes\u00b7ser", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADJD", "VAPP", "$,"], "meter": "+---+--+-", "measure": "trochaic.tri.relaxed"}, "line.4": {"text": "Wir w\u00e4ren gewesen zu K\u00f6lln am Rhein,", "tokens": ["Wir", "w\u00e4\u00b7ren", "ge\u00b7we\u00b7sen", "zu", "K\u00f6lln", "am", "Rhein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VAPP", "APPR", "NE", "APPRART", "NE", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Und h\u00e4tten einander zugetrunken 24 Kannen Bier oder", "tokens": ["Und", "h\u00e4t\u00b7ten", "ein\u00b7an\u00b7der", "zu\u00b7ge\u00b7trun\u00b7ken", "24", "Kan\u00b7nen", "Bier", "o\u00b7der"], "token_info": ["word", "word", "word", "word", "number", "word", "word", "word"], "pos": ["KON", "VAFIN", "ADV", "VVPP", "CARD", "NN", "NN", "KON"], "meter": "-+--+-+-+-+-+-+", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "Wein.", "tokens": ["Wein", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Indessen scheid ich von dir, und du von mir,", "tokens": ["In\u00b7des\u00b7sen", "scheid", "ich", "von", "dir", ",", "und", "du", "von", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PPER", "$,", "KON", "PPER", "APPR", "PPER", "$,"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "Und ich werde dich hinfort nicht fragen mehr.", "tokens": ["Und", "ich", "wer\u00b7de", "dich", "hin\u00b7fort", "nicht", "fra\u00b7gen", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "PPER", "ADV", "PTKNEG", "VVFIN", "ADV", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}}}}