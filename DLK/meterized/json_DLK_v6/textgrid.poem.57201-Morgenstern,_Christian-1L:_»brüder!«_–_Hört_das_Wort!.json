{"textgrid.poem.57201": {"metadata": {"author": {"name": "Morgenstern, Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: \u00bbbr\u00fcder!\u00ab \u2013 H\u00f6rt das Wort!", "genre": "verse", "period": "N.A.", "pub_year": 1892, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbbr\u00fcder!\u00ab \u2013 H\u00f6rt das Wort!", "tokens": ["\u00bb", "br\u00fc\u00b7der", "!", "\u00ab", "\u2013", "H\u00f6rt", "das", "Wort", "!"], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$.", "$(", "$(", "VVIMP", "ART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Soll's ein Wort nur bleiben?", "tokens": ["Soll's", "ein", "Wort", "nur", "blei\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "ADV", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Soll's nicht Fr\u00fcchte treiben", "tokens": ["Soll's", "nicht", "Fr\u00fcch\u00b7te", "trei\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "PTKNEG", "NN", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "fort und fort?", "tokens": ["fort", "und", "fort", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PTKVZ", "KON", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.2": {"line.1": {"text": "Oft erscholl der Schwur!", "tokens": ["Oft", "er\u00b7scholl", "der", "Schwur", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Ward auch oft gehalten \u2013", "tokens": ["Ward", "auch", "oft", "ge\u00b7hal\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "VVPP", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "doch in engem, alten", "tokens": ["doch", "in", "en\u00b7gem", ",", "al\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ADV", "APPR", "ADJA", "$,", "ADJA"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Sinne nur.", "tokens": ["Sin\u00b7ne", "nur", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "ADV", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "O sein neuer Sinn!", "tokens": ["O", "sein", "neu\u00b7er", "Sinn", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Lernt ihn doch erkennen!", "tokens": ["Lernt", "ihn", "doch", "er\u00b7ken\u00b7nen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "La\u00dft doch hei\u00df ihn brennen", "tokens": ["La\u00dft", "doch", "hei\u00df", "ihn", "bren\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVIMP", "ADV", "ADJD", "PPER", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "durch euch hin!", "tokens": ["durch", "euch", "hin", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PTKVZ", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "Allen Bruder sein!", "tokens": ["Al\u00b7len", "Bru\u00b7der", "sein", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VAINF", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Allen helfen, dienen!", "tokens": ["Al\u00b7len", "hel\u00b7fen", ",", "die\u00b7nen", "!"], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PIS", "VVINF", "$,", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Ist, seit ER erschienen,", "tokens": ["Ist", ",", "seit", "Er", "er\u00b7schie\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Ziel allein!", "tokens": ["Ziel", "al\u00b7lein", "!"], "token_info": ["word", "word", "punct"], "pos": ["NN", "ADV", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.5": {"line.1": {"text": "Auch dem B\u00f6sewicht,", "tokens": ["Auch", "dem", "B\u00f6\u00b7se\u00b7wicht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "der uns widerstrebet!", "tokens": ["der", "uns", "wi\u00b7der\u00b7stre\u00b7bet", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "PPER", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Er auch ward gewebet", "tokens": ["Er", "auch", "ward", "ge\u00b7we\u00b7bet"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "ADV", "VAFIN", "VVPP"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "einst aus Licht.", "tokens": ["einst", "aus", "Licht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.6": {"line.1": {"text": "\u00bbliebt das B\u00f6se \u2013 gut!\u00ab", "tokens": ["\u00bb", "liebt", "das", "B\u00f6\u00b7se", "\u2013", "gut", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "ART", "NN", "$(", "ADJD", "$.", "$("], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "lehren tiefe Seelen.", "tokens": ["leh\u00b7ren", "tie\u00b7fe", "See\u00b7len", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Lernt am Hasse st\u00e4hlen \u2013", "tokens": ["Lernt", "am", "Has\u00b7se", "st\u00e4h\u00b7len", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "VVINF", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Liebesmut!", "tokens": ["Lie\u00b7bes\u00b7mut", "!"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.7": {"line.1": {"text": "\u00bbbr\u00fcder!\u00ab \u2013 H\u00f6rt das Wort!", "tokens": ["\u00bb", "br\u00fc\u00b7der", "!", "\u00ab", "\u2013", "H\u00f6rt", "das", "Wort", "!"], "token_info": ["punct", "word", "punct", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$.", "$(", "$(", "VVIMP", "ART", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Da\u00df es Wahrheit werde \u2013", "tokens": ["Da\u00df", "es", "Wahr\u00b7heit", "wer\u00b7de", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VAFIN", "$("], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "und dereinst die Erde", "tokens": ["und", "de\u00b7reinst", "die", "Er\u00b7de"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Gottes Ort.", "tokens": ["Got\u00b7tes", "Ort", "."], "token_info": ["word", "word", "punct"], "pos": ["NN", "NN", "$."], "meter": "+-+", "measure": "trochaic.di"}}}}}