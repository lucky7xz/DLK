{"textgrid.poem.48286": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "Die Br\u00fcck' am Tay", "genre": "verse", "period": "N.A.", "pub_year": 1858, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbwann treffen wir drei wieder zusamm?\u00ab", "tokens": ["\u00bb", "wann", "tref\u00b7fen", "wir", "drei", "wie\u00b7der", "zu\u00b7samm", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPER", "CARD", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "\u00bbum die siebente Stund', am Br\u00fcckendamm.\u00ab", "tokens": ["\u00bb", "um", "die", "sie\u00b7ben\u00b7te", "Stund'", ",", "am", "Br\u00fc\u00b7cken\u00b7damm", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "APPR", "ART", "ADJA", "NN", "$,", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "\u00bbam Mittelpfeiler.\u00ab", "tokens": ["\u00bb", "am", "Mit\u00b7tel\u00b7pfei\u00b7ler", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "APPRART", "NN", "$.", "$("], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "\u00bbich l\u00f6sche die Flamm.\u00ab", "tokens": ["\u00bb", "ich", "l\u00f6\u00b7sche", "die", "Flamm", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.2": {"text": "\u00bbich mit.\u00ab", "tokens": ["\u00bb", "ich", "mit", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "PTKVZ", "$.", "$("], "meter": "-+", "measure": "iambic.single"}}, "stanza.3": {"line.1": {"text": "\u00bbich komme vom Norden her.\u00ab", "tokens": ["\u00bb", "ich", "kom\u00b7me", "vom", "Nor\u00b7den", "her", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "APPRART", "NN", "PTKVZ", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "\u00bbund ich vom S\u00fcden.\u00ab", "tokens": ["\u00bb", "und", "ich", "vom", "S\u00fc\u00b7den", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KON", "PPER", "APPRART", "NN", "$.", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.3": {"text": "\u00bbund ich vom Meer.\u00ab", "tokens": ["\u00bb", "und", "ich", "vom", "Meer", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KON", "PPER", "APPRART", "NN", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "\u00bbhei, das gibt einen Ringelreihn,", "tokens": ["\u00bb", "hei", ",", "das", "gibt", "ei\u00b7nen", "Rin\u00b7gel\u00b7reihn", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKVZ", "$,", "PDS", "VVFIN", "ART", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Und die Br\u00fccke mu\u00df in den Grund hinein.\u00ab", "tokens": ["Und", "die", "Br\u00fc\u00b7cke", "mu\u00df", "in", "den", "Grund", "hin\u00b7ein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "VMFIN", "APPR", "ART", "NN", "PTKVZ", "$.", "$("], "meter": "--+-++-+-+", "measure": "anapaest.init"}}, "stanza.5": {"line.1": {"text": "\u00bbund der Zug, der in die Br\u00fccke tritt", "tokens": ["\u00bb", "und", "der", "Zug", ",", "der", "in", "die", "Br\u00fc\u00b7cke", "tritt"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "KON", "ART", "NN", "$,", "PRELS", "APPR", "ART", "NN", "VVFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Um die siebente Stund'?\u00ab", "tokens": ["Um", "die", "sie\u00b7ben\u00b7te", "Stund'", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KOUI", "ART", "ADJA", "NN", "$.", "$("], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.3": {"text": "\u00bbei, der mu\u00df mit.\u00ab", "tokens": ["\u00bb", "ei", ",", "der", "mu\u00df", "mit", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ITJ", "$,", "ART", "VMFIN", "APPR", "$.", "$("], "meter": "+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "\u00bbmu\u00df mit.\u00ab", "tokens": ["\u00bb", "mu\u00df", "mit", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "VMFIN", "APPR", "$.", "$("], "meter": "-+", "measure": "iambic.single"}}, "stanza.6": {"line.1": {"text": "\u00bbtand, Tand", "tokens": ["\u00bb", "tand", ",", "Tand"], "token_info": ["punct", "word", "punct", "word"], "pos": ["$(", "VVFIN", "$,", "NN"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Ist das Gebilde von Menschenhand!\u00ab", "tokens": ["Ist", "das", "Ge\u00b7bil\u00b7de", "von", "Men\u00b7schen\u00b7hand", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ART", "NN", "APPR", "NN", "$.", "$("], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.7": {"line.1": {"text": "Auf der ", "tokens": ["Auf", "der"], "token_info": ["word", "word"], "pos": ["APPR", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.2": {"text": "Alle Fenster sehen nach S\u00fcden aus,", "tokens": ["Al\u00b7le", "Fens\u00b7ter", "se\u00b7hen", "nach", "S\u00fc\u00b7den", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "APPR", "NN", "PTKVZ", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und die Br\u00fccknersleut' ohne Rast und Ruh", "tokens": ["Und", "die", "Br\u00fc\u00b7ck\u00b7ners\u00b7leut'", "oh\u00b7ne", "Rast", "und", "Ruh"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Und in Bangen sehen nach S\u00fcden zu,", "tokens": ["Und", "in", "Ban\u00b7gen", "se\u00b7hen", "nach", "S\u00fc\u00b7den", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VVFIN", "APPR", "NN", "PTKVZ", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Sehen und warten, ob nicht ein Licht", "tokens": ["Se\u00b7hen", "und", "war\u00b7ten", ",", "ob", "nicht", "ein", "Licht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "KON", "VVINF", "$,", "KOUS", "PTKNEG", "ART", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.6": {"text": "\u00dcbers Wasser hin \u00bbIch komme\u00ab spricht,", "tokens": ["\u00dc\u00b7bers", "Was\u00b7ser", "hin", "\u00bb", "Ich", "kom\u00b7me", "\u00ab", "spricht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "NN", "PTKVZ", "$(", "PPER", "VVFIN", "$(", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.7": {"text": "\u00bbich komme, trotz Nacht und Sturmesflug,", "tokens": ["\u00bb", "ich", "kom\u00b7me", ",", "trotz", "Nacht", "und", "Stur\u00b7mes\u00b7flug", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "$,", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.8": {"line.1": {"text": "Und der Br\u00fcckner jetzt: \u00bbIch seh' einen Schein", "tokens": ["Und", "der", "Br\u00fcck\u00b7ner", "jetzt", ":", "\u00bb", "Ich", "seh'", "ei\u00b7nen", "Schein"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "ADV", "$.", "$(", "PPER", "VVFIN", "ART", "NN"], "meter": "--+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Am anderen Ufer. Das mu\u00df er sein.", "tokens": ["Am", "an\u00b7de\u00b7ren", "U\u00b7fer", ".", "Das", "mu\u00df", "er", "sein", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$.", "PDS", "VMFIN", "PPER", "VAINF", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Nun, Mutter, weg mit dem bangen Traum,", "tokens": ["Nun", ",", "Mut\u00b7ter", ",", "weg", "mit", "dem", "ban\u00b7gen", "Traum", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "NN", "$,", "ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Unser Johnie kommt und will seinen Baum,", "tokens": ["Un\u00b7ser", "Joh\u00b7nie", "kommt", "und", "will", "sei\u00b7nen", "Baum", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "KON", "VMFIN", "PPOSAT", "NN", "$,"], "meter": "+---+-+--+", "measure": "iambic.tetra.chol"}, "line.5": {"text": "Und was noch am Baume von Lichtern ist,", "tokens": ["Und", "was", "noch", "am", "Bau\u00b7me", "von", "Lich\u00b7tern", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "APPRART", "NN", "APPR", "NN", "VAFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Z\u00fcnd' alles an wie zum heiligen Christ,", "tokens": ["Z\u00fcnd'", "al\u00b7les", "an", "wie", "zum", "hei\u00b7li\u00b7gen", "Christ", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "PTKVZ", "KOKOM", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Der will heuer ", "tokens": ["Der", "will", "heu\u00b7er"], "token_info": ["word", "word", "word"], "pos": ["PDS", "VMFIN", "ADV"], "meter": "-+--", "measure": "dactylic.init"}, "line.8": {"text": "Und in elf Minuten ist er herein.\u00ab", "tokens": ["Und", "in", "elf", "Mi\u00b7nu\u00b7ten", "ist", "er", "her\u00b7ein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "APPR", "CARD", "NN", "VAFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Und es war der Zug. Am ", "tokens": ["Und", "es", "war", "der", "Zug", ".", "Am"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["KON", "PPER", "VAFIN", "ART", "NN", "$.", "APPRART"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": "Keucht er vorbei jetzt gegen den Sturm,", "tokens": ["Keucht", "er", "vor\u00b7bei", "jetzt", "ge\u00b7gen", "den", "Sturm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Und Johnie spricht: \u00bbDie Br\u00fccke noch!", "tokens": ["Und", "Joh\u00b7nie", "spricht", ":", "\u00bb", "Die", "Br\u00fc\u00b7cke", "noch", "!"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "$.", "$(", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Aber was tut es, wir zwingen es doch.", "tokens": ["A\u00b7ber", "was", "tut", "es", ",", "wir", "zwin\u00b7gen", "es", "doch", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "PPER", "ADV", "$."], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.5": {"text": "Ein fester Kessel, ein doppelter Dampf,", "tokens": ["Ein", "fes\u00b7ter", "Kes\u00b7sel", ",", "ein", "dop\u00b7pel\u00b7ter", "Dampf", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Die bleiben Sieger in solchem Kampf.", "tokens": ["Die", "blei\u00b7ben", "Sie\u00b7ger", "in", "sol\u00b7chem", "Kampf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Und wie's auch rast und ringt und rennt,", "tokens": ["Und", "wie's", "auch", "rast", "und", "ringt", "und", "rennt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADJD", "KON", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Wir kriegen es unter, das Element.", "tokens": ["Wir", "krie\u00b7gen", "es", "un\u00b7ter", ",", "das", "E\u00b7le\u00b7ment", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "$,", "ART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.10": {"line.1": {"text": "Und unser Stolz ist unsre Br\u00fcck';", "tokens": ["Und", "un\u00b7ser", "Stolz", "ist", "uns\u00b7re", "Br\u00fcck", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ich lache, denk' ich an fr\u00fcher zur\u00fcck,", "tokens": ["Ich", "la\u00b7che", ",", "denk'", "ich", "an", "fr\u00fc\u00b7her", "zu\u00b7r\u00fcck", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "VVFIN", "PPER", "APPR", "ADJD", "PTKVZ", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "An all den Jammer und all die Not", "tokens": ["An", "all", "den", "Jam\u00b7mer", "und", "all", "die", "Not"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "ART", "NN", "KON", "PIAT", "ART", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mit dem elend alten Schifferboot;", "tokens": ["Mit", "dem", "e\u00b7lend", "al\u00b7ten", "Schif\u00b7fer\u00b7boot", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJD", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Wie manche liebe Christfestnacht", "tokens": ["Wie", "man\u00b7che", "lie\u00b7be", "Christ\u00b7fest\u00b7nacht"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Hab' ich im F\u00e4hrhaus zugebracht", "tokens": ["Hab'", "ich", "im", "F\u00e4hr\u00b7haus", "zu\u00b7ge\u00b7bracht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "PPER", "APPRART", "NN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Und sah unsrer Fenster lichten Schein", "tokens": ["Und", "sah", "uns\u00b7rer", "Fens\u00b7ter", "lich\u00b7ten", "Schein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Und z\u00e4hlte und konnte nicht dr\u00fcben sein.\u00ab", "tokens": ["Und", "z\u00e4hl\u00b7te", "und", "konn\u00b7te", "nicht", "dr\u00fc\u00b7ben", "sein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "KON", "VMFIN", "PTKNEG", "ADV", "VAINF", "$.", "$("], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.11": {"line.1": {"text": "Auf der Norderseite, das Br\u00fcckenhaus \u2013", "tokens": ["Auf", "der", "Nor\u00b7der\u00b7sei\u00b7te", ",", "das", "Br\u00fc\u00b7cken\u00b7haus", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "ART", "NN", "$("], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Alle Fenster sehen nach S\u00fcden aus,", "tokens": ["Al\u00b7le", "Fens\u00b7ter", "se\u00b7hen", "nach", "S\u00fc\u00b7den", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "APPR", "NN", "PTKVZ", "$,"], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und die Br\u00fccknersleut' ohne Rast und Ruh", "tokens": ["Und", "die", "Br\u00fc\u00b7ck\u00b7ners\u00b7leut'", "oh\u00b7ne", "Rast", "und", "Ruh"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Und in Bangen sehen nach S\u00fcden zu;", "tokens": ["Und", "in", "Ban\u00b7gen", "se\u00b7hen", "nach", "S\u00fc\u00b7den", "zu", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VVFIN", "APPR", "NN", "PTKVZ", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Denn w\u00fctender wurde der Winde Spiel,", "tokens": ["Denn", "w\u00fc\u00b7ten\u00b7der", "wur\u00b7de", "der", "Win\u00b7de", "Spiel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "Und jetzt, als ob Feuer vom Himmel fiel',", "tokens": ["Und", "jetzt", ",", "als", "ob", "Feu\u00b7er", "vom", "Him\u00b7mel", "fiel'", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "KOKOM", "KOUS", "NN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Ergl\u00fcht es in niederschie\u00dfender Pracht", "tokens": ["Er\u00b7gl\u00fcht", "es", "in", "nie\u00b7der\u00b7schie\u00b7\u00dfen\u00b7der", "Pracht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "APPR", "ADJA", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "\u00dcberm Wasser unten ... Und wieder ist Nacht.", "tokens": ["\u00dc\u00b7berm", "Was\u00b7ser", "un\u00b7ten", "...", "Und", "wie\u00b7der", "ist", "Nacht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "$(", "KON", "ADV", "VAFIN", "NN", "$."], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "\u00bbwann treffen wir drei wieder zusamm?\u00ab", "tokens": ["\u00bb", "wann", "tref\u00b7fen", "wir", "drei", "wie\u00b7der", "zu\u00b7samm", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPER", "CARD", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "\u00bbum Mitternacht, am Bergeskamm.\u00ab", "tokens": ["\u00bb", "um", "Mit\u00b7ter\u00b7nacht", ",", "am", "Ber\u00b7ges\u00b7kamm", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "APPR", "NN", "$,", "APPRART", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbauf dem hohen Moor, am Erlenstamm.\u00ab", "tokens": ["\u00bb", "auf", "dem", "ho\u00b7hen", "Moor", ",", "am", "Er\u00b7len\u00b7stamm", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "APPR", "ART", "ADJA", "NN", "$,", "APPRART", "NN", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.13": {"line.1": {"text": "\u00bbich komme.\u00ab", "tokens": ["\u00bb", "ich", "kom\u00b7me", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "$.", "$("], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "\u00bb Ich mit.\u00ab", "tokens": ["\u00bb", "Ich", "mit", ".", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "PTKVZ", "$.", "$("], "meter": "-+", "measure": "iambic.single"}, "line.3": {"text": "\u00bbich nenn' euch die Zahl.\u00ab", "tokens": ["\u00bb", "ich", "nenn'", "euch", "die", "Zahl", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "ART", "NN", "$.", "$("], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "\u00bbund ich die Namen.\u00ab", "tokens": ["\u00bb", "und", "ich", "die", "Na\u00b7men", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KON", "PPER", "ART", "NN", "$.", "$("], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "\u00bbund ich die Qual.\u00ab", "tokens": ["\u00bb", "und", "ich", "die", "Qual", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KON", "PPER", "ART", "NN", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "\u00bbhei!", "tokens": ["\u00bb", "hei", "!"], "token_info": ["punct", "word", "punct"], "pos": ["$(", "PTKVZ", "$."], "meter": "+", "measure": "single.up"}, "line.7": {"text": "Wie Splitter brach das Geb\u00e4lk entzwei.\u00ab", "tokens": ["Wie", "Split\u00b7ter", "brach", "das", "Ge\u00b7b\u00e4lk", "ent\u00b7zwei", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "\u00bbtand, Tand", "tokens": ["\u00bb", "tand", ",", "Tand"], "token_info": ["punct", "word", "punct", "word"], "pos": ["$(", "VVFIN", "$,", "NN"], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "Ist das Gebilde von Menschenhand.\u00ab", "tokens": ["Ist", "das", "Ge\u00b7bil\u00b7de", "von", "Men\u00b7schen\u00b7hand", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "ART", "NN", "APPR", "NN", "$.", "$("], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}}}}