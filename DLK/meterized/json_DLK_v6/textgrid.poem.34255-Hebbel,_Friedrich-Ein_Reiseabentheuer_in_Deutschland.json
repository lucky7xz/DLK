{"textgrid.poem.34255": {"metadata": {"author": {"name": "Hebbel, Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "Ein Reiseabentheuer in Deutschland", "genre": "verse", "period": "N.A.", "pub_year": 1838, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Es flog in X. mein Hut mir ab,", "tokens": ["Es", "flog", "in", "X.", "mein", "Hut", "mir", "ab", ","], "token_info": ["word", "word", "word", "abbreviation", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NE", "PPOSAT", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Nat\u00fcrlich \u00fcber die Gr\u00e4nze,", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", "\u00fc\u00b7ber", "die", "Gr\u00e4n\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Und als ich, ihn wieder zu holen, lief,", "tokens": ["Und", "als", "ich", ",", "ihn", "wie\u00b7der", "zu", "ho\u00b7len", ",", "lief", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "$,", "PPER", "ADV", "PTKZU", "VVINF", "$,", "VVFIN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Da gab's vertrackte T\u00e4nze.", "tokens": ["Da", "gab's", "ver\u00b7track\u00b7te", "T\u00e4n\u00b7ze", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Ich durfte den Deutschen Nachbarstaat", "tokens": ["Ich", "durf\u00b7te", "den", "Deut\u00b7schen", "Nach\u00b7bar\u00b7staat"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Nicht ohne Pa\u00df betreten,", "tokens": ["Nicht", "oh\u00b7ne", "Pa\u00df", "be\u00b7tre\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und da ich blo\u00df spatzieren ging,", "tokens": ["Und", "da", "ich", "blo\u00df", "spat\u00b7zie\u00b7ren", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "ADV", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So hatt' ich mir keinen erbeten.", "tokens": ["So", "hatt'", "ich", "mir", "kei\u00b7nen", "er\u00b7be\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "PIAT", "ADJA", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.3": {"line.1": {"text": "Das that ich nun, auch wurde ich", "tokens": ["Das", "that", "ich", "nun", ",", "auch", "wur\u00b7de", "ich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "$,", "ADV", "VAFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In Gnaden damit versehen,", "tokens": ["In", "Gna\u00b7den", "da\u00b7mit", "ver\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PAV", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Doch war's um meinen armen Hut", "tokens": ["Doch", "wa\u00b7r's", "um", "mei\u00b7nen", "ar\u00b7men", "Hut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Trotz alledem geschehen.", "tokens": ["Trotz", "al\u00b7le\u00b7dem", "ge\u00b7sche\u00b7hen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PIS", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Der war schon l\u00e4ngst im dritten Staat", "tokens": ["Der", "war", "schon", "l\u00e4ngst", "im", "drit\u00b7ten", "Staat"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und blieb auch dort nicht liegen,", "tokens": ["Und", "blieb", "auch", "dort", "nicht", "lie\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ihm lie\u00df der schadenfrohe Wind", "tokens": ["Ihm", "lie\u00df", "der", "scha\u00b7den\u00b7fro\u00b7he", "Wind"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Dutzend noch durchfliegen.", "tokens": ["Ein", "Dut\u00b7zend", "noch", "durch\u00b7flie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Was half mir nun der gute Pa\u00df,", "tokens": ["Was", "half", "mir", "nun", "der", "gu\u00b7te", "Pa\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den ich in X. genommen?", "tokens": ["Den", "ich", "in", "X.", "ge\u00b7nom\u00b7men", "?"], "token_info": ["word", "word", "word", "abbreviation", "word", "punct"], "pos": ["NE", "PPER", "APPR", "NE", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Zehn neue braucht' ich in Einem Tag,", "tokens": ["Zehn", "neu\u00b7e", "braucht'", "ich", "in", "Ei\u00b7nem", "Tag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da war nicht nachzukommen.", "tokens": ["Da", "war", "nicht", "nach\u00b7zu\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "VVIZU", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Ich kaufte mir einen andern Hut,", "tokens": ["Ich", "kauf\u00b7te", "mir", "ei\u00b7nen", "an\u00b7dern", "Hut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Der Meister aber erw\u00e4hlte", "tokens": ["Der", "Meis\u00b7ter", "a\u00b7ber", "er\u00b7w\u00e4hl\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VVFIN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Den Wiener Congre\u00df zum Schutzpatron,", "tokens": ["Den", "Wie\u00b7ner", "Con\u00b7gre\u00df", "zum", "Schutz\u00b7pat\u00b7ron", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Als ich mein Schicksal erz\u00e4hlte.", "tokens": ["Als", "ich", "mein", "Schick\u00b7sal", "er\u00b7z\u00e4hl\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}}}}