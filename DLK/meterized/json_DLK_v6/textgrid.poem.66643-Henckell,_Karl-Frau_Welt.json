{"textgrid.poem.66643": {"metadata": {"author": {"name": "Henckell, Karl", "birth": "N.A.", "death": "N.A."}, "title": "Frau Welt", "genre": "verse", "period": "N.A.", "pub_year": 1896, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Frau Welt beschlo\u00df, nicht mehr zu ", "tokens": ["Frau", "Welt", "be\u00b7schlo\u00df", ",", "nicht", "mehr", "zu"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "NN", "VVFIN", "$,", "PTKNEG", "ADV", "PTKZU"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Der Flu\u00df der Dinge schafft ihr Pein.", "tokens": ["Der", "Flu\u00df", "der", "Din\u00b7ge", "schafft", "ihr", "Pein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sie grollte: \u00bbAllem schl\u00e4gt die Stunde,", "tokens": ["Sie", "groll\u00b7te", ":", "\u00bb", "Al\u00b7lem", "schl\u00e4gt", "die", "Stun\u00b7de", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PIS", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nur ich geh' nimmermehr zugrunde.", "tokens": ["Nur", "ich", "geh'", "nim\u00b7mer\u00b7mehr", "zu\u00b7grun\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "ADV", "ADJA", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Was sich auch wandelt f\u00fcr und f\u00fcr,", "tokens": ["Was", "sich", "auch", "wan\u00b7delt", "f\u00fcr", "und", "f\u00fcr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "ADV", "VVFIN", "APPR", "KON", "APPR", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nur mir winkt keine Ausgangst\u00fcr.", "tokens": ["Nur", "mir", "winkt", "kei\u00b7ne", "Aus\u00b7gangs\u00b7t\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Das ist ein ewiges Geflute \u2013", "tokens": ["Das", "ist", "ein", "e\u00b7wi\u00b7ges", "Ge\u00b7flu\u00b7te", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Unheimlich schier wird mir zumute.", "tokens": ["Un\u00b7heim\u00b7lich", "schier", "wird", "mir", "zu\u00b7mu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "VAFIN", "PPER", "VVFIN", "$."], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Wo harrt mein Grab, wo find' ich Ruh?", "tokens": ["Wo", "harrt", "mein", "Grab", ",", "wo", "find'", "ich", "Ruh", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPOSAT", "NN", "$,", "PWAV", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Im Gange bleib' ich immerzu.", "tokens": ["Im", "Gan\u00b7ge", "bleib'", "ich", "im\u00b7mer\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Ist denn kein Doktor aufzutreiben,", "tokens": ["Ist", "denn", "kein", "Dok\u00b7tor", "auf\u00b7zu\u00b7trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PIAT", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mir Weltarsenik zu verschreiben?\u00ab", "tokens": ["Mir", "Welt\u00b7ar\u00b7se\u00b7nik", "zu", "ver\u00b7schrei\u00b7ben", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "NN", "PTKZU", "VVINF", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.7": {"line.1": {"text": "Sie raufte sich ihr Nebelhaar,", "tokens": ["Sie", "rauf\u00b7te", "sich", "ihr", "Ne\u00b7bel\u00b7haar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das w\u00e4hrte zehn Millionen Jahr.", "tokens": ["Das", "w\u00e4hr\u00b7te", "zehn", "Mil\u00b7lion\u00b7en", "Jahr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "CARD", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Sie bi\u00df sich auf die Himmelslippen,", "tokens": ["Sie", "bi\u00df", "sich", "auf", "die", "Him\u00b7mels\u00b7lip\u00b7pen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie schlug sich auf die H\u00f6llenrippen,", "tokens": ["Sie", "schlug", "sich", "auf", "die", "H\u00f6l\u00b7len\u00b7rip\u00b7pen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Indes sie: \u00bbWeh, Welt! Weh, Welt!\u00ab sang.", "tokens": ["In\u00b7des", "sie", ":", "\u00bb", "Weh", ",", "Welt", "!", "Weh", ",", "Welt", "!", "\u00ab", "sang", "."], "token_info": ["word", "word", "punct", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "punct", "word", "punct"], "pos": ["KOUS", "PPER", "$.", "$(", "NN", "$,", "NN", "$.", "NN", "$,", "NN", "$.", "$(", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das dauerte Milliarden lang.", "tokens": ["Das", "dau\u00b7er\u00b7te", "Mil\u00b7li\u00b7ar\u00b7den", "lang", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.10": {"line.1": {"text": "Wie sie auch aufstie\u00df mit den F\u00fc\u00dfen,", "tokens": ["Wie", "sie", "auch", "auf\u00b7stie\u00df", "mit", "den", "F\u00fc\u00b7\u00dfen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Herr Nichts vermied, sie zu begr\u00fc\u00dfen.", "tokens": ["Herr", "Nichts", "ver\u00b7mied", ",", "sie", "zu", "be\u00b7gr\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PIS", "VVFIN", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Matt sank aufs Sofa sie zur\u00fcck:", "tokens": ["Matt", "sank", "aufs", "So\u00b7fa", "sie", "zu\u00b7r\u00fcck", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPRART", "NN", "PPER", "PTKVZ", "$."], "meter": "-+-+---+", "measure": "unknown.measure.tri"}, "line.2": {"text": "\u00bbich bin und bleib ein Schelmenst\u00fcck.", "tokens": ["\u00bb", "ich", "bin", "und", "bleib", "ein", "Schel\u00b7men\u00b7st\u00fcck", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "KON", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Bin schon so gr\u00e4\u00dflich alt geworden", "tokens": ["Bin", "schon", "so", "gr\u00e4\u00df\u00b7lich", "alt", "ge\u00b7wor\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ADV", "ADJD", "ADJD", "VAPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und darf mich nicht mal selbst vermorden.", "tokens": ["Und", "darf", "mich", "nicht", "mal", "selbst", "ver\u00b7mor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "PTKNEG", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Reicht keiner den Erl\u00f6sungstrank,", "tokens": ["Reicht", "kei\u00b7ner", "den", "Er\u00b7l\u00f6\u00b7sungs\u00b7trank", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Werd' ich vor Tiefsinn geisteskrank ...\u00ab", "tokens": ["Werd'", "ich", "vor", "Tief\u00b7sinn", "geis\u00b7tes\u00b7krank", "...", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "ADJD", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Frau Welt beschlo\u00df, zu resignieren", "tokens": ["Frau", "Welt", "be\u00b7schlo\u00df", ",", "zu", "re\u00b7sig\u00b7nie\u00b7ren"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NN", "NN", "VVFIN", "$,", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und Schopenhauer zu studieren.", "tokens": ["Und", "Scho\u00b7pen\u00b7hau\u00b7er", "zu", "stu\u00b7die\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}