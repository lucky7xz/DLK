{"textgrid.poem.32890": {"metadata": {"author": {"name": "Wei\u00dfe, Christian Felix", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ich trank, und Chloe trank mit mir,", "genre": "verse", "period": "N.A.", "pub_year": 1765, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich trank, und Chloe trank mit mir,", "tokens": ["Ich", "trank", ",", "und", "Chloe", "trank", "mit", "mir", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KON", "NE", "VVFIN", "APPR", "PPER", "$,"], "meter": "-+-++-+", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Gleich war der Gott der Lieb auch hier:", "tokens": ["Gleich", "war", "der", "Gott", "der", "Lieb", "auch", "hier", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ART", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ach! seufzte Chloe, sieh! schon st\u00f6rt er unsre Freuden,", "tokens": ["Ach", "!", "seufz\u00b7te", "Chloe", ",", "sieh", "!", "schon", "st\u00f6rt", "er", "uns\u00b7re", "Freu\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$.", "VVFIN", "NE", "$,", "VVIMP", "$.", "ADV", "VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Hasch ihn, wir wollen ihm die Fl\u00fcgel gleich beschneiden.", "tokens": ["Hasch", "ihn", ",", "wir", "wol\u00b7len", "ihm", "die", "Fl\u00fc\u00b7gel", "gleich", "be\u00b7schnei\u00b7den", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "$,", "PPER", "VMFIN", "PPER", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Nein, sagt ich, da k\u00f6nnt er noch fliehn:", "tokens": ["Nein", ",", "sagt", "ich", ",", "da", "k\u00f6nnt", "er", "noch", "fliehn", ":"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "VVFIN", "PPER", "$,", "ADV", "VVFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Die Fl\u00fcgel wachsen: la\u00df uns ihn,", "tokens": ["Die", "Fl\u00fc\u00b7gel", "wach\u00b7sen", ":", "la\u00df", "uns", "ihn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$.", "VVIMP", "PPER", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Den kleinen B\u00f6sewicht, eh er entflieht, ersticken!", "tokens": ["Den", "klei\u00b7nen", "B\u00f6\u00b7se\u00b7wicht", ",", "eh", "er", "ent\u00b7flieht", ",", "er\u00b7sti\u00b7cken", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "KOUS", "PPER", "VVFIN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Nicht wahr? so kann er uns doch weiter nicht ber\u00fccken.", "tokens": ["Nicht", "wahr", "?", "so", "kann", "er", "uns", "doch", "wei\u00b7ter", "nicht", "be\u00b7r\u00fc\u00b7cken", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PTKVZ", "$.", "ADV", "VMFIN", "PPER", "PPER", "ADV", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Wir haschten: eh man sichs versah,", "tokens": ["Wir", "haschten", ":", "eh", "man", "sichs", "ver\u00b7sah", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "KOUS", "PIS", "PIS", "VVFIN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "War er bald dort, bald wieder da:", "tokens": ["War", "er", "bald", "dort", ",", "bald", "wie\u00b7der", "da", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "$,", "ADV", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und als ich ihn einmal recht fest zu halten dachte,", "tokens": ["Und", "als", "ich", "ihn", "ein\u00b7mal", "recht", "fest", "zu", "hal\u00b7ten", "dach\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "PPER", "ADV", "ADV", "ADJD", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Floh er in unser Herz; wir seufzten, und er lachte!", "tokens": ["Floh", "er", "in", "un\u00b7ser", "Herz", ";", "wir", "seufz\u00b7ten", ",", "und", "er", "lach\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$.", "PPER", "VVFIN", "$,", "KON", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}