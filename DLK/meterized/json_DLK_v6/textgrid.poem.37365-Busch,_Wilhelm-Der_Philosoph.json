{"textgrid.poem.37365": {"metadata": {"author": {"name": "Busch, Wilhelm", "birth": "N.A.", "death": "N.A."}, "title": "Der Philosoph", "genre": "verse", "period": "N.A.", "pub_year": 1870, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Philosoph von ernster Art,", "tokens": ["Ein", "Phi\u00b7lo\u00b7soph", "von", "erns\u00b7ter", "Art", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der sprach und strich sich seinen Bart:", "tokens": ["Der", "sprach", "und", "strich", "sich", "sei\u00b7nen", "Bart", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "KON", "ADJD", "PRF", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ich lache nie. Ich lieb es nicht,", "tokens": ["Ich", "la\u00b7che", "nie", ".", "Ich", "lieb", "es", "nicht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$.", "PPER", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mein ehrenwertes Angesicht", "tokens": ["Mein", "eh\u00b7ren\u00b7wer\u00b7tes", "An\u00b7ge\u00b7sicht"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durch Z\u00e4hnefletschen zu entstellen", "tokens": ["Durch", "Z\u00e4h\u00b7ne\u00b7flet\u00b7schen", "zu", "ent\u00b7stel\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und n\u00e4rrisch wie ein Hund zu bellen;", "tokens": ["Und", "n\u00e4r\u00b7risch", "wie", "ein", "Hund", "zu", "bel\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "KOKOM", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich lieb es nicht durch ein Gemecker", "tokens": ["Ich", "lieb", "es", "nicht", "durch", "ein", "Ge\u00b7me\u00b7cker"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Zu zeigen, da\u00df ich Witzentdecker;", "tokens": ["Zu", "zei\u00b7gen", ",", "da\u00df", "ich", "Wit\u00b7zent\u00b7de\u00b7cker", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "KOUS", "PPER", "NN", "$."], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Ich brauche nicht durch Wertvergleichen", "tokens": ["Ich", "brau\u00b7che", "nicht", "durch", "Wert\u00b7ver\u00b7glei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Mit andern mich herauszustreichen,", "tokens": ["Mit", "an\u00b7dern", "mich", "her\u00b7aus\u00b7zu\u00b7strei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Um zu ermessen, was ich bin,", "tokens": ["Um", "zu", "er\u00b7mes\u00b7sen", ",", "was", "ich", "bin", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VVINF", "$,", "PWS", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Denn dieses wei\u00df ich ohnehin.", "tokens": ["Denn", "die\u00b7ses", "wei\u00df", "ich", "oh\u00b7ne\u00b7hin", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Das Lachen will ich \u00fcberlassen", "tokens": ["Das", "La\u00b7chen", "will", "ich", "\u00fc\u00b7ber\u00b7las\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den minder hochbegabten Klassen.", "tokens": ["Den", "min\u00b7der", "hoch\u00b7be\u00b7gab\u00b7ten", "Klas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ist einer ohne Selbstvertraun", "tokens": ["Ist", "ei\u00b7ner", "oh\u00b7ne", "Selbst\u00b7ver\u00b7traun"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ART", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In Gegenwart von sch\u00f6nen Fraun,", "tokens": ["In", "Ge\u00b7gen\u00b7wart", "von", "sch\u00f6\u00b7nen", "Fraun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So da\u00df sie ihn als faden Gecken", "tokens": ["So", "da\u00df", "sie", "ihn", "als", "fa\u00b7den", "Ge\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "PPER", "KOUS", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Abfahren lassen oder necken,", "tokens": ["Ab\u00b7fah\u00b7ren", "las\u00b7sen", "o\u00b7der", "ne\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "KON", "VVINF", "$,"], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.7": {"text": "Und f\u00fchlt er drob geheimen Groll", "tokens": ["Und", "f\u00fchlt", "er", "drob", "ge\u00b7hei\u00b7men", "Groll"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und wei\u00df nicht, was er sagen soll,", "tokens": ["Und", "wei\u00df", "nicht", ",", "was", "er", "sa\u00b7gen", "soll", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "$,", "PWS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Dann schwebt mit Recht auf seinen Z\u00fcgen", "tokens": ["Dann", "schwebt", "mit", "Recht", "auf", "sei\u00b7nen", "Z\u00fc\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.10": {"text": "Ein unaussprechliches Vergn\u00fcgen.", "tokens": ["Ein", "un\u00b7aus\u00b7sprech\u00b7li\u00b7ches", "Ver\u00b7gn\u00fc\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Und hat er Kursverlust erlitten,", "tokens": ["Und", "hat", "er", "Kurs\u00b7ver\u00b7lust", "er\u00b7lit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist er moralisch ausgeglitten,", "tokens": ["Ist", "er", "mo\u00b7ra\u00b7lisch", "aus\u00b7ge\u00b7glit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVFIN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "So gibt es Leute, die doch immer", "tokens": ["So", "gibt", "es", "Leu\u00b7te", ",", "die", "doch", "im\u00b7mer"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NN", "$,", "PRELS", "ADV", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Noch d\u00fcmmer sind als er und schlimmer,", "tokens": ["Noch", "d\u00fcm\u00b7mer", "sind", "als", "er", "und", "schlim\u00b7mer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "KOUS", "PPER", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Und hat er etwa krumme Beine,", "tokens": ["Und", "hat", "er", "et\u00b7wa", "krum\u00b7me", "Bei\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "So gibt's noch kr\u00fcmmere als seine.", "tokens": ["So", "gibt's", "noch", "kr\u00fcm\u00b7me\u00b7re", "als", "sei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "PIS", "KOKOM", "PPOSAT", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Er tr\u00f6stet sich und lacht dar\u00fcber", "tokens": ["Er", "tr\u00f6s\u00b7tet", "sich", "und", "lacht", "da\u00b7r\u00fc\u00b7ber"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "KON", "VVFIN", "PAV"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und denkt: Da bin ich mir doch lieber.", "tokens": ["Und", "denkt", ":", "Da", "bin", "ich", "mir", "doch", "lie\u00b7ber", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$.", "ADV", "VAFIN", "PPER", "PPER", "ADV", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Den Teufel la\u00df ich aus dem Spiele.", "tokens": ["Den", "Teu\u00b7fel", "la\u00df", "ich", "aus", "dem", "Spie\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVIMP", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auch sonst noch lachen ihrer viele,", "tokens": ["Auch", "sonst", "noch", "la\u00b7chen", "ih\u00b7rer", "vie\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "VVFIN", "PPOSAT", "PIAT", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Besonders jene ewig Heitern,", "tokens": ["Be\u00b7son\u00b7ders", "je\u00b7ne", "e\u00b7wig", "Hei\u00b7tern", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die unbewu\u00dft den Mund erweitern,", "tokens": ["Die", "un\u00b7be\u00b7wu\u00dft", "den", "Mund", "er\u00b7wei\u00b7tern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Die, sozusagen, auserkoren", "tokens": ["Die", ",", "so\u00b7zu\u00b7sa\u00b7gen", ",", "au\u00b7ser\u00b7ko\u00b7ren"], "token_info": ["word", "punct", "word", "punct", "word"], "pos": ["ART", "$,", "ADV", "$,", "VVIZU"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Zum Lachen bis an beide Ohren.", "tokens": ["Zum", "La\u00b7chen", "bis", "an", "bei\u00b7de", "Oh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Sie freuen sich mit Weib und Kind", "tokens": ["Sie", "freu\u00b7en", "sich", "mit", "Weib", "und", "Kind"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Schon blo\u00df, weil sie vorhanden sind.", "tokens": ["Schon", "blo\u00df", ",", "weil", "sie", "vor\u00b7han\u00b7den", "sind", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "$,", "KOUS", "PPER", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Ich dahingegen, der ich sitze", "tokens": ["Ich", "da\u00b7hin\u00b7ge\u00b7gen", ",", "der", "ich", "sit\u00b7ze"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "PAV", "$,", "PRELS", "PPER", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auf der Betrachtung h\u00f6chster Spitze,", "tokens": ["Auf", "der", "Be\u00b7trach\u00b7tung", "h\u00f6chs\u00b7ter", "Spit\u00b7ze", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Weit \u00fcber allem Was und Wie,", "tokens": ["Weit", "\u00fc\u00b7ber", "al\u00b7lem", "Was", "und", "Wie", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PIS", "PWS", "KON", "PWAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich bin f\u00fcr mich und lache nie.", "tokens": ["Ich", "bin", "f\u00fcr", "mich", "und", "la\u00b7che", "nie", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "KON", "VVFIN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}