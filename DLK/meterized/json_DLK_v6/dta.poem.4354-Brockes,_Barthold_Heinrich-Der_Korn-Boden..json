{"dta.poem.4354": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Der Korn-Boden.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Indem ich neulich, mit Vergn\u00fcgen, auf dem Getrayde-", "tokens": ["In\u00b7dem", "ich", "neu\u00b7lich", ",", "mit", "Ver\u00b7gn\u00fc\u00b7gen", ",", "auf", "dem", "Ge\u00b7tray\u00b7de"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADV", "$,", "APPR", "NN", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Boden stand,", "tokens": ["Bo\u00b7den", "stand", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Und ihn, ob gleich derselbige von einer ungemeinen L\u00e4nge,", "tokens": ["Und", "ihn", ",", "ob", "gleich", "der\u00b7sel\u00b7bi\u00b7ge", "von", "ei\u00b7ner", "un\u00b7ge\u00b7mei\u00b7nen", "L\u00e4n\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "$,", "KOUS", "ADV", "PDS", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-+-+-", "measure": "iambic.octa.plus"}, "line.4": {"text": "Dennoch von Rocken, Weizen, Gersten und Habern in", "tokens": ["Den\u00b7noch", "von", "Ro\u00b7cken", ",", "Wei\u00b7zen", ",", "Gers\u00b7ten", "und", "Ha\u00b7bern", "in"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "APPR", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "APPR"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "recht schwehrer Menge,", "tokens": ["recht", "schweh\u00b7rer", "Men\u00b7ge", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Theils selbst gebaut, und theils zum Zehnden geliefert,", "tokens": ["Theils", "selbst", "ge\u00b7baut", ",", "und", "theils", "zum", "Zehn\u00b7den", "ge\u00b7lie\u00b7fert", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VVPP", "$,", "KON", "ADV", "APPRART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "angef\u00fcllet fand;", "tokens": ["an\u00b7ge\u00b7f\u00fcl\u00b7let", "fand", ";"], "token_info": ["word", "word", "punct"], "pos": ["VVPP", "VVFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.8": {"text": "Verehrt\u2019 ich, meinen Pflichten nach, zuerst den Geber", "tokens": ["Ver\u00b7ehrt'", "ich", ",", "mei\u00b7nen", "Pflich\u00b7ten", "nach", ",", "zu\u00b7erst", "den", "Ge\u00b7ber"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "PPOSAT", "NN", "PTKVZ", "$,", "ADV", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "dieser Gabe,", "tokens": ["die\u00b7ser", "Ga\u00b7be", ","], "token_info": ["word", "word", "punct"], "pos": ["PDAT", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.10": {"text": "Von Dem ich dieses, wie wir alles, als ein Geschenk em-", "tokens": ["Von", "Dem", "ich", "die\u00b7ses", ",", "wie", "wir", "al\u00b7les", ",", "als", "ein", "Ge\u00b7schenk", "em"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "PPER", "PDS", "$,", "PWAV", "PPER", "PIS", "$,", "KOUS", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.11": {"text": "pfangen habe,", "tokens": ["pfan\u00b7gen", "ha\u00b7be", ","], "token_info": ["word", "word", "punct"], "pos": ["VVPP", "VAFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.12": {"text": "Bewunderte die Zeugungs-Kraft, die GOtt dem Saamen", "tokens": ["Be\u00b7wun\u00b7der\u00b7te", "die", "Zeu\u00b7gungs\u00b7Kraft", ",", "die", "Gott", "dem", "Saa\u00b7men"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "NN", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "eingesenket,", "tokens": ["ein\u00b7ge\u00b7sen\u00b7ket", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.14": {"text": "Bewunderte die Kraft der Erde, den Sonnen-Schein,", "tokens": ["Be\u00b7wun\u00b7der\u00b7te", "die", "Kraft", "der", "Er\u00b7de", ",", "den", "Son\u00b7nen\u00b7Schein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "-+---+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.15": {"text": "Luft, Regen, Wind,", "tokens": ["Luft", ",", "Re\u00b7gen", ",", "Wind", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "++-+", "measure": "iambic.di"}, "line.16": {"text": "Derselben ordentlichen Wechsel, die alle dazu n\u00f6htig sind;", "tokens": ["Der\u00b7sel\u00b7ben", "or\u00b7dent\u00b7li\u00b7chen", "Wech\u00b7sel", ",", "die", "al\u00b7le", "da\u00b7zu", "n\u00f6h\u00b7tig", "sind", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "ADJA", "NN", "$,", "PRELS", "PIS", "PAV", "ADJD", "VAFIN", "$."], "meter": "-+--+--+--+-+-+-+", "measure": "amphibrach.tetra.plus"}, "line.17": {"text": "Jmgleichen, da\u00df auch uns dazu so viel Verstand und Kraft", "tokens": ["Jm\u00b7glei\u00b7chen", ",", "da\u00df", "auch", "uns", "da\u00b7zu", "so", "viel", "Ver\u00b7stand", "und", "Kraft"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$,", "KOUS", "ADV", "PPER", "PAV", "ADV", "PIAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.18": {"text": "geschenket,", "tokens": ["ge\u00b7schen\u00b7ket", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.19": {"text": "So Saat, als Acker zu besorgen, der Saat Kraft, unser", "tokens": ["So", "Saat", ",", "als", "A\u00b7cker", "zu", "be\u00b7sor\u00b7gen", ",", "der", "Saat", "Kraft", ",", "un\u00b7ser"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["ADV", "NN", "$,", "KOUS", "NN", "PTKZU", "VVINF", "$,", "ART", "NN", "NN", "$,", "PPOSAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.20": {"text": "Fleisch zu n\u00e4hren,", "tokens": ["Fleisch", "zu", "n\u00e4h\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.21": {"text": "Und \u00fcberdem, zu unserm Nutzen, so Segen-reich sich zu", "tokens": ["Und", "\u00fc\u00b7ber\u00b7dem", ",", "zu", "un\u00b7serm", "Nut\u00b7zen", ",", "so", "Se\u00b7gen\u00b7reich", "sich", "zu"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PAV", "$,", "APPR", "PPOSAT", "NN", "$,", "ADV", "ADJD", "PRF", "APPR"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.22": {"text": "vermehren.", "tokens": ["ver\u00b7meh\u00b7ren", "."], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.2": {"line.1": {"text": "Nachhero fiel mir, da mein Blick noch einst den Haufen", "tokens": ["Nach\u00b7he\u00b7ro", "fiel", "mir", ",", "da", "mein", "Blick", "noch", "einst", "den", "Hau\u00b7fen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "KOUS", "PPOSAT", "NN", "ADV", "ADV", "ART", "NN"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "\u00fcberlief,", "tokens": ["\u00fc\u00b7berl\u00b7ief", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Die baldige Ver\u00e4nderung, der es wird unterworfen seyn,", "tokens": ["Die", "bal\u00b7di\u00b7ge", "Ver\u00b7\u00e4n\u00b7de\u00b7rung", ",", "der", "es", "wird", "un\u00b7ter\u00b7wor\u00b7fen", "seyn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VAFIN", "VVPP", "VAINF", "$,"], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}, "line.4": {"text": "Und die so sonderlich, als n\u00fctzlich, von dieser Menge Korn,", "tokens": ["Und", "die", "so", "son\u00b7der\u00b7lich", ",", "als", "n\u00fctz\u00b7lich", ",", "von", "die\u00b7ser", "Men\u00b7ge", "Korn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADV", "ADJD", "$,", "KOUS", "ADJD", "$,", "APPR", "PDAT", "NN", "NN", "$,"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.5": {"text": "mir ein.", "tokens": ["mir", "ein", "."], "token_info": ["word", "word", "punct"], "pos": ["PPER", "PTKVZ", "$."], "meter": "-+", "measure": "iambic.single"}, "line.6": {"text": "Daher ich, dieser Ordnung Weisheit erwegend, bey mir", "tokens": ["Da\u00b7her", "ich", ",", "die\u00b7ser", "Ord\u00b7nung", "Weis\u00b7heit", "er\u00b7we\u00b7gend", ",", "bey", "mir"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PAV", "PPER", "$,", "PDAT", "NN", "NN", "VVPP", "$,", "APPR", "PPER"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.7": {"text": "selber rief:", "tokens": ["sel\u00b7ber", "rief", ":"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": "Welch ein Bewunderns-wehrter Cirkel! Was, nach so", "tokens": ["Welch", "ein", "Be\u00b7wun\u00b7derns\u00b7wehr\u00b7ter", "Cir\u00b7kel", "!", "Was", ",", "nach", "so"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["PIAT", "ART", "NN", "NN", "$.", "PWS", "$,", "APPR", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "emsigem Bem\u00fchen,", "tokens": ["em\u00b7si\u00b7gem", "Be\u00b7m\u00fc\u00b7hen", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+---+-", "measure": "dactylic.init"}, "line.3": {"text": "Der Landmann in dem Stand gewesen, dem Schoo\u00df der", "tokens": ["Der", "Land\u00b7mann", "in", "dem", "Stand", "ge\u00b7we\u00b7sen", ",", "dem", "Schoo\u00df", "der"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN", "VAPP", "$,", "ART", "NN", "ART"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Erden zu entziehen,", "tokens": ["Er\u00b7den", "zu", "ent\u00b7zie\u00b7hen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "Was hier nunmehr zu Hauf geliefert, und, wohl bedeckt,", "tokens": ["Was", "hier", "nun\u00b7mehr", "zu", "Hauf", "ge\u00b7lie\u00b7fert", ",", "und", ",", "wohl", "be\u00b7deckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "ADV", "ADV", "APPR", "NN", "VVPP", "$,", "KON", "$,", "ADV", "VVPP", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "in Ruhe liegt,", "tokens": ["in", "Ru\u00b7he", "liegt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "Wird bald von neuem aufgemessen, zum M\u00fcller und zum", "tokens": ["Wird", "bald", "von", "neu\u00b7em", "auf\u00b7ge\u00b7mes\u00b7sen", ",", "zum", "M\u00fcl\u00b7ler", "und", "zum"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "APPR", "ADJA", "VVPP", "$,", "APPRART", "NE", "KON", "APPRART"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Becker kommen,", "tokens": ["Be\u00b7cker", "kom\u00b7men", ","], "token_info": ["word", "word", "punct"], "pos": ["NE", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.9": {"text": "Und wenn es vieler tausend M\u00fcnde, zum Nutzen und zur", "tokens": ["Und", "wenn", "es", "vie\u00b7ler", "tau\u00b7send", "M\u00fcn\u00b7de", ",", "zum", "Nut\u00b7zen", "und", "zur"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPER", "PIAT", "CARD", "NN", "$,", "APPRART", "NN", "KON", "APPRART"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "Lust, gef\u00fcllet,", "tokens": ["Lust", ",", "ge\u00b7f\u00fcl\u00b7let", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "Und theils im Magen uns den Hunger, theils auch, im", "tokens": ["Und", "theils", "im", "Ma\u00b7gen", "uns", "den", "Hun\u00b7ger", ",", "theils", "auch", ",", "im"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["KON", "ADV", "APPRART", "NN", "PPER", "ART", "NN", "$,", "ADV", "ADV", "$,", "APPRART"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Bier, den Durst gestillet,", "tokens": ["Bier", ",", "den", "Durst", "ge\u00b7stil\u00b7let", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "NN", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.13": {"text": "Da sie, statt abgegangner Theilchen, an neuen Theilchen", "tokens": ["Da", "sie", ",", "statt", "ab\u00b7ge\u00b7gang\u00b7ner", "Theil\u00b7chen", ",", "an", "neu\u00b7en", "Theil\u00b7chen"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "$,", "KOUI", "ADJA", "NN", "$,", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.14": {"text": "zugenommen;", "tokens": ["zu\u00b7ge\u00b7nom\u00b7men", ";"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.15": {"text": "Wird alles wiederum getrennt, vermischt, und gr\u00f6\u00dften-", "tokens": ["Wird", "al\u00b7les", "wie\u00b7de\u00b7rum", "ge\u00b7trennt", ",", "ver\u00b7mischt", ",", "und", "gr\u00f6\u00df\u00b7ten"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["VAFIN", "PIS", "ADV", "VVPP", "$,", "VVPP", "$,", "KON", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "theils der Erden,", "tokens": ["theils", "der", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.17": {"text": "Zur abermahligen Bereitung von neuem einverleibet wer-", "tokens": ["Zur", "a\u00b7ber\u00b7mah\u00b7li\u00b7gen", "Be\u00b7rei\u00b7tung", "von", "neu\u00b7em", "ein\u00b7ver\u00b7lei\u00b7bet", "wer"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "APPR", "ADJA", "VVFIN", "TRUNC"], "meter": "-+-+-+-+--+-+-+-+", "measure": "iambic.octa.plus.relaxed"}, "line.18": {"text": "den,", "tokens": ["den", ","], "token_info": ["word", "punct"], "pos": ["ART", "$,"], "meter": "-", "measure": "single.down"}, "line.19": {"text": "Um, nach der wunderbaren Ordnung, in allen uns bekann-", "tokens": ["Um", ",", "nach", "der", "wun\u00b7der\u00b7ba\u00b7ren", "Ord\u00b7nung", ",", "in", "al\u00b7len", "uns", "be\u00b7kann"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUI", "$,", "APPR", "ART", "ADJA", "NN", "$,", "APPR", "PIS", "PPER", "TRUNC"], "meter": "-+-+-+----+-+-+", "measure": "unknown.measure.hexa"}, "line.20": {"text": "ten Dingen,", "tokens": ["ten", "Din\u00b7gen", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.21": {"text": "In seinem unverr\u00fcckten Wechsel, den grossen Kreis-Lauf", "tokens": ["In", "sei\u00b7nem", "un\u00b7ver\u00b7r\u00fcck\u00b7ten", "Wech\u00b7sel", ",", "den", "gros\u00b7sen", "Kreis\u00b7Lauf"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+--+-++", "measure": "iambic.septa.relaxed"}, "line.22": {"text": "zu vollbringen.", "tokens": ["zu", "voll\u00b7brin\u00b7gen", "."], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.23": {"text": "Ich dachte den Partikeln nach, aus welchen Fleisch und", "tokens": ["Ich", "dach\u00b7te", "den", "Par\u00b7ti\u00b7keln", "nach", ",", "aus", "wel\u00b7chen", "Fleisch", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "PTKVZ", "$,", "APPR", "PWAT", "NN", "KON"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.24": {"text": "Blut formiert,", "tokens": ["Blut", "for\u00b7miert", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.25": {"text": "Ob etwan etwas Geistiges damit verbunden, welches nur", "tokens": ["Ob", "et\u00b7wan", "et\u00b7was", "Geis\u00b7ti\u00b7ges", "da\u00b7mit", "ver\u00b7bun\u00b7den", ",", "wel\u00b7ches", "nur"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "ADV", "PIAT", "NN", "PAV", "VVPP", "$,", "PRELS", "ADV"], "meter": "++-+-+-+-+-+-+-+", "measure": "unknown.measure.octa.plus"}, "line.26": {"text": "Allein in Fleisch sich zu verkehren, und, nach den Regeln", "tokens": ["Al\u00b7lein", "in", "Fleisch", "sich", "zu", "ver\u00b7keh\u00b7ren", ",", "und", ",", "nach", "den", "Re\u00b7geln"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["ADV", "APPR", "NN", "PRF", "PTKZU", "VVINF", "$,", "KON", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.27": {"text": "der Natur,", "tokens": ["der", "Na\u00b7tur", ","], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$,"], "meter": "--+", "measure": "anapaest.init"}, "line.28": {"text": "Das Thier-Reich zu ern\u00e4hren t\u00fcchtig. Aufs wenigste wird", "tokens": ["Das", "Thier\u00b7Reich", "zu", "er\u00b7n\u00e4h\u00b7ren", "t\u00fcch\u00b7tig", ".", "Aufs", "we\u00b7nigs\u00b7te", "wird"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "PTKZU", "VVINF", "ADJD", "$.", "FM.la", "FM.la", "FM.la"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.29": {"text": "man sie k\u00f6nnen,", "tokens": ["man", "sie", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "PPER", "VMFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.30": {"text": "Wofern nicht wirklich Geistigkeiten, doch ganz besondre", "tokens": ["Wo\u00b7fern", "nicht", "wirk\u00b7lich", "Geis\u00b7tig\u00b7kei\u00b7ten", ",", "doch", "ganz", "be\u00b7sond\u00b7re"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PTKNEG", "ADJD", "NN", "$,", "ADV", "ADV", "ADJA"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.31": {"text": "Kr\u00e4fte nennen,", "tokens": ["Kr\u00e4f\u00b7te", "nen\u00b7nen", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.4": {"line.1": {"text": "Wovon wir uns zwar vielerley von neuem vorzustellen", "tokens": ["Wo\u00b7von", "wir", "uns", "zwar", "vie\u00b7ler\u00b7ley", "von", "neu\u00b7em", "vor\u00b7zu\u00b7stel\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "PRF", "ADV", "ADV", "APPR", "ADJA", "VVINF"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.2": {"text": "taugen;", "tokens": ["tau\u00b7gen", ";"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Jedoch, wenn wir die Wahrheit sagen, sind sie nicht", "tokens": ["Je\u00b7doch", ",", "wenn", "wir", "die", "Wahr\u00b7heit", "sa\u00b7gen", ",", "sind", "sie", "nicht"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "$,", "KOUS", "PPER", "ART", "NN", "VVINF", "$,", "VAFIN", "PPER", "PTKNEG"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "minder unsern Augen,", "tokens": ["min\u00b7der", "un\u00b7sern", "Au\u00b7gen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PPOSAT", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "So wohl, als wie vorher, verborgen. Denn, wenn wir", "tokens": ["So", "wohl", ",", "als", "wie", "vor\u00b7her", ",", "ver\u00b7bor\u00b7gen", ".", "Denn", ",", "wenn", "wir"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word"], "pos": ["ADV", "ADV", "$,", "KOUS", "KOKOM", "ADV", "$,", "VVPP", "$.", "KON", "$,", "KOUS", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "auch dieselben gleich", "tokens": ["auch", "die\u00b7sel\u00b7ben", "gleich"], "token_info": ["word", "word", "word"], "pos": ["ADV", "PDAT", "ADV"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.7": {"text": "In fein\u2019 und gr\u00f6bere zertheilen, die gr\u00f6bere dem Pflanzen-", "tokens": ["In", "fein'", "und", "gr\u00f6\u00b7be\u00b7re", "zer\u00b7thei\u00b7len", ",", "die", "gr\u00f6\u00b7be\u00b7re", "dem", "Pflan\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ADJD", "KON", "ADJA", "VVINF", "$,", "ART", "ADJA", "ART", "TRUNC"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.8": {"text": "Reich,", "tokens": ["Reich", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+", "measure": "single.up"}, "line.9": {"text": "Die feinere dem Reich der Thiere blo\u00df zuzueignen uns", "tokens": ["Die", "fei\u00b7ne\u00b7re", "dem", "Reich", "der", "Thie\u00b7re", "blo\u00df", "zu\u00b7zu\u00b7eig\u00b7nen", "uns"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "ART", "NN", "ART", "NN", "ADV", "VVIZU", "PPER"], "meter": "-+-+-+-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.10": {"text": "bem\u00fchn;", "tokens": ["be\u00b7m\u00fchn", ";"], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "-+", "measure": "iambic.single"}, "line.11": {"text": "So wird man doch aus aller Theilung bey weiten nicht", "tokens": ["So", "wird", "man", "doch", "aus", "al\u00b7ler", "Thei\u00b7lung", "bey", "wei\u00b7ten", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIS", "ADV", "APPR", "PIAT", "NN", "APPR", "ADJA", "PTKNEG"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "den Nutzen ziehn,", "tokens": ["den", "Nut\u00b7zen", "ziehn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.13": {"text": "Als wie wohl die Gelehrten meynen, weil eben eine solche", "tokens": ["Als", "wie", "wohl", "die", "Ge\u00b7lehr\u00b7ten", "mey\u00b7nen", ",", "weil", "e\u00b7ben", "ei\u00b7ne", "sol\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "ADV", "ART", "NN", "VVFIN", "$,", "KOUS", "ADV", "ART", "PIAT"], "meter": "--+--+-+--+-+-+-", "measure": "anapaest.di.plus"}, "line.14": {"text": "Kraft", "tokens": ["Kraft"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.15": {"text": "Uns keine gr\u00f6\u00dfre Wahrheit zeiget, als die verborgne Ei-", "tokens": ["Uns", "kei\u00b7ne", "gr\u00f6\u00df\u00b7re", "Wahr\u00b7heit", "zei\u00b7get", ",", "als", "die", "ver\u00b7borg\u00b7ne", "Ei"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "PIAT", "ADJA", "NN", "VVFIN", "$,", "KOUS", "ART", "ADJA", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.16": {"text": "genschaft,", "tokens": ["gen\u00b7schaft", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.17": {"text": "Die man am Stagyrit verlacht. Doch kommt von allen", "tokens": ["Die", "man", "am", "Sta\u00b7gy\u00b7rit", "ver\u00b7lacht", ".", "Doch", "kommt", "von", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "PIS", "APPRART", "NN", "VVPP", "$.", "KON", "VVFIN", "APPR", "PIAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "diesen mir", "tokens": ["die\u00b7sen", "mir"], "token_info": ["word", "word"], "pos": ["PDAT", "PPER"], "meter": "+-+", "measure": "trochaic.di"}, "line.19": {"text": "Der Wahrheit am gem\u00e4ssesten und am begreiflichsten noch", "tokens": ["Der", "Wahr\u00b7heit", "am", "ge\u00b7m\u00e4s\u00b7ses\u00b7ten", "und", "am", "be\u00b7greif\u00b7lichs\u00b7ten", "noch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPRART", "ADJA", "KON", "APPRART", "ADJA", "ADV"], "meter": "-+-+-+---+-+--+", "measure": "iambic.hexa.chol"}, "line.20": {"text": "f\u00fcr,", "tokens": ["f\u00fcr", ","], "token_info": ["word", "punct"], "pos": ["APPR", "$,"], "meter": "-", "measure": "single.down"}, "line.21": {"text": "Da\u00df im Getrayde solche Theile, die gleichsam einerley", "tokens": ["Da\u00df", "im", "Ge\u00b7tray\u00b7de", "sol\u00b7che", "Thei\u00b7le", ",", "die", "gleich\u00b7sam", "ei\u00b7ner\u00b7ley"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "APPRART", "NN", "PIAT", "NN", "$,", "PRELS", "ADJD", "PIAT"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.22": {"text": "Figur", "tokens": ["Fi\u00b7gur"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.23": {"text": "Mit unsers Blutes Theilen haben, und etwan einerley", "tokens": ["Mit", "un\u00b7sers", "Blu\u00b7tes", "Thei\u00b7len", "ha\u00b7ben", ",", "und", "et\u00b7wan", "ei\u00b7ner\u00b7ley"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "NN", "VAFIN", "$,", "KON", "ADV", "PIAT"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.24": {"text": "Natur", "tokens": ["Na\u00b7tur"], "token_info": ["word"], "pos": ["NN"], "meter": "-+", "measure": "iambic.single"}, "line.25": {"text": "Und Harmonie mit ihnen hegen, wodurch sie Zung\u2019 und", "tokens": ["Und", "Har\u00b7mo\u00b7nie", "mit", "ih\u00b7nen", "he\u00b7gen", ",", "wo\u00b7durch", "sie", "Zung'", "und"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "NN", "APPR", "PPER", "VVFIN", "$,", "PWAV", "PPER", "NN", "KON"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.26": {"text": "Mund beh\u00e4glich", "tokens": ["Mund", "be\u00b7h\u00e4g\u00b7lich"], "token_info": ["word", "word"], "pos": ["NN", "ADJD"], "meter": "+-+-", "measure": "trochaic.di"}, "line.27": {"text": "Und angenehm im Schmecken sind; dem Magen eben-", "tokens": ["Und", "an\u00b7ge\u00b7nehm", "im", "Schme\u00b7cken", "sind", ";", "dem", "Ma\u00b7gen", "e\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADJD", "APPRART", "NN", "VAFIN", "$.", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "falls ertr\u00e4glich", "tokens": ["falls", "er\u00b7tr\u00e4g\u00b7lich"], "token_info": ["word", "word"], "pos": ["ADV", "ADJD"], "meter": "+-+-", "measure": "trochaic.di"}, "line.29": {"text": "Nicht minder unsern innern Theilen, als Adern, Nerven,", "tokens": ["Nicht", "min\u00b7der", "un\u00b7sern", "in\u00b7nern", "Thei\u00b7len", ",", "als", "A\u00b7dern", ",", "Ner\u00b7ven", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PTKNEG", "ADV", "PPOSAT", "ADJA", "NN", "$,", "KOUS", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.30": {"text": "Dr\u00fcsen, Blut,", "tokens": ["Dr\u00fc\u00b7sen", ",", "Blut", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADV", "$,", "NN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.31": {"text": "Womit, der Theile Gleichheit halber, vermuhtlich sich zu-", "tokens": ["Wo\u00b7mit", ",", "der", "Thei\u00b7le", "Gleich\u00b7heit", "hal\u00b7ber", ",", "ver\u00b7muht\u00b7lich", "sich", "zu"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "$,", "ART", "NN", "NN", "APPO", "$,", "ADJD", "PRF", "TRUNC"], "meter": "+--+-+-+--+--+", "measure": "iambic.hexa.invert"}, "line.32": {"text": "sammen thut,", "tokens": ["sam\u00b7men", "thut", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVFIN", "$,"], "meter": "+--", "measure": "dactylic.init"}}, "stanza.5": {"line.1": {"text": "Vereint, und das, was fehlt, ersetzet. Ich dachte ferner,", "tokens": ["Ver\u00b7eint", ",", "und", "das", ",", "was", "fehlt", ",", "er\u00b7set\u00b7zet", ".", "Ich", "dach\u00b7te", "fer\u00b7ner", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KON", "PDS", "$,", "PWS", "VVFIN", "$,", "VVFIN", "$.", "PPER", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "da der Mist", "tokens": ["da", "der", "Mist"], "token_info": ["word", "word", "word"], "pos": ["KOUS", "ART", "NN"], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "Zur D\u00fcngung und zum Wachsthum selber fast unent-", "tokens": ["Zur", "D\u00fcn\u00b7gung", "und", "zum", "Wach\u00b7sthum", "sel\u00b7ber", "fast", "un\u00b7ent"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "KON", "APPRART", "NN", "ADV", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "behrlich n\u00f6htig ist,", "tokens": ["behr\u00b7lich", "n\u00f6h\u00b7tig", "ist", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "VAFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "Und dieser aus dem Thier-Reich stammt, ob die\u00df uns nicht", "tokens": ["Und", "die\u00b7ser", "aus", "dem", "Thier\u00b7Reich", "stammt", ",", "ob", "die\u00df", "uns", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PDS", "APPR", "ART", "NN", "VVFIN", "$,", "KOUS", "PDS", "PPER", "PTKNEG"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "vor Augen leget,", "tokens": ["vor", "Au\u00b7gen", "le\u00b7get", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.7": {"text": "Da\u00df ja fast gar auf welche Weise, das Korn so gleiche Theile", "tokens": ["Da\u00df", "ja", "fast", "gar", "auf", "wel\u00b7che", "Wei\u00b7se", ",", "das", "Korn", "so", "glei\u00b7che", "Thei\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ADV", "ADV", "APPR", "PWAT", "NN", "$,", "ART", "NN", "ADV", "ADJA", "NN"], "meter": "-+--+--+--+-+-+-", "measure": "amphibrach.tetra.plus"}, "line.8": {"text": "heget", "tokens": ["he\u00b7get"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+-", "measure": "trochaic.single"}, "line.9": {"text": "Mit unsern und der Thiere C\u00f6rpern. Mir fiel nachher noch", "tokens": ["Mit", "un\u00b7sern", "und", "der", "Thie\u00b7re", "C\u00f6r\u00b7pern", ".", "Mir", "fiel", "nach\u00b7her", "noch"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "KON", "ART", "NN", "NE", "$.", "PPER", "VVFIN", "ADV", "ADV"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.10": {"text": "ferner bey,", "tokens": ["fer\u00b7ner", "bey", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "PTKVZ", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.11": {"text": "Wie sehr des grossen Sch\u00f6pfers Weisheit hierinn noch zu", "tokens": ["Wie", "sehr", "des", "gros\u00b7sen", "Sch\u00f6p\u00b7fers", "Weis\u00b7heit", "hie\u00b7rinn", "noch", "zu"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ART", "ADJA", "NN", "NN", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "bewundern sey,", "tokens": ["be\u00b7wun\u00b7dern", "sey", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VAFIN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.13": {"text": "Da\u00df, da das Vieh sich selbst zu helfen und sich zu n\u00e4hren", "tokens": ["Da\u00df", ",", "da", "das", "Vieh", "sich", "selbst", "zu", "hel\u00b7fen", "und", "sich", "zu", "n\u00e4h\u00b7ren"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "$,", "KOUS", "ART", "NN", "PRF", "ADV", "PTKZU", "VVINF", "KON", "PRF", "PTKZU", "VVINF"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.14": {"text": "nicht geschickt,", "tokens": ["nicht", "ge\u00b7schickt", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.15": {"text": "Sich ihre Nahrung, Gras und Kraut, von selbst fast aus", "tokens": ["Sich", "ih\u00b7re", "Nah\u00b7rung", ",", "Gras", "und", "Kraut", ",", "von", "selbst", "fast", "aus"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "PPOSAT", "NN", "$,", "NN", "KON", "NN", "$,", "APPR", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "der Erden dr\u00fcckt;", "tokens": ["der", "Er\u00b7den", "dr\u00fcckt", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.17": {"text": "Da wir hingegen unsre Kost, durch Flei\u00df und Arbeit,", "tokens": ["Da", "wir", "hin\u00b7ge\u00b7gen", "uns\u00b7re", "Kost", ",", "durch", "Flei\u00df", "und", "Ar\u00b7beit", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PPOSAT", "NN", "$,", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "finden k\u00f6nnen:", "tokens": ["fin\u00b7den", "k\u00f6n\u00b7nen", ":"], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.19": {"text": "So hat Er uns dazu so vieles, die k\u00fcnstliche gelenke", "tokens": ["So", "hat", "Er", "uns", "da\u00b7zu", "so", "vie\u00b7les", ",", "die", "k\u00fcnst\u00b7li\u00b7che", "ge\u00b7len\u00b7ke"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PRF", "PAV", "ADV", "PIS", "$,", "ART", "ADJA", "NN"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.20": {"text": "Hand,", "tokens": ["Hand", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+", "measure": "single.up"}, "line.21": {"text": "Holz, Eisen, Seile, Pferd\u2019 und Ochsen, zumahl den sinnenden", "tokens": ["Holz", ",", "Ei\u00b7sen", ",", "Sei\u00b7le", ",", "Pferd'", "und", "Och\u00b7sen", ",", "zu\u00b7mahl", "den", "sin\u00b7nen\u00b7den"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+--+-+--", "measure": "iambic.hexa.relaxed"}, "line.22": {"text": "Verstand,", "tokens": ["Ver\u00b7stand", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "-+", "measure": "iambic.single"}, "line.23": {"text": "Zu unserer Besch\u00e4ftigung, und andrer Absicht wollen g\u00f6n-", "tokens": ["Zu", "un\u00b7se\u00b7rer", "Be\u00b7sch\u00e4f\u00b7ti\u00b7gung", ",", "und", "an\u00b7drer", "Ab\u00b7sicht", "wol\u00b7len", "g\u00f6n"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "$,", "KON", "ADJA", "NN", "VMFIN", "TRUNC"], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}, "line.24": {"text": "nen.", "tokens": ["nen", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-", "measure": "single.down"}, "line.25": {"text": "Denn scheinet gleich, beym ersten Anblick, die Arbeit m\u00fch-", "tokens": ["Denn", "schei\u00b7net", "gleich", ",", "beym", "ers\u00b7ten", "An\u00b7blick", ",", "die", "Ar\u00b7beit", "m\u00fch"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "$,", "APPRART", "ADJA", "NN", "$,", "ART", "NN", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.26": {"text": "sam, saur und schwehr", "tokens": ["sam", ",", "saur", "und", "schwehr"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["ADJD", "$,", "ADJD", "KON", "ADJD"], "meter": "-+-+", "measure": "iambic.di"}, "line.27": {"text": "Ist die\u00df doch lange nicht so schlimm, als wenn die Mensch-", "tokens": ["Ist", "die\u00df", "doch", "lan\u00b7ge", "nicht", "so", "schlimm", ",", "als", "wenn", "die", "Men\u00b7sch"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PDS", "ADV", "ADV", "PTKNEG", "ADV", "ADJD", "$,", "KOKOM", "KOUS", "ART", "TRUNC"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.28": {"text": "heit m\u00fc\u00dfig w\u00e4r;", "tokens": ["heit", "m\u00fc\u00b7\u00dfig", "w\u00e4r", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "VAFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.29": {"text": "Indem, wenn hier auf dieser Welt der Mensch von keine", "tokens": ["In\u00b7dem", ",", "wenn", "hier", "auf", "die\u00b7ser", "Welt", "der", "Mensch", "von", "kei\u00b7ne"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "$,", "KOUS", "ADV", "APPR", "PDAT", "NN", "ART", "NN", "APPR", "PIAT"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Arbeit w\u00fc\u00dfte,", "tokens": ["Ar\u00b7beit", "w\u00fc\u00df\u00b7te", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.31": {"text": "Ein allgemeiner M\u00fc\u00dfiggang das Leben wirklich meh", "tokens": ["Ein", "all\u00b7ge\u00b7mei\u00b7ner", "M\u00fc\u00b7\u00dfig\u00b7gang", "das", "Le\u00b7ben", "wirk\u00b7lich", "meh"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "ADJD", "NE"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.32": {"text": "verdrie\u00dflich,", "tokens": ["ver\u00b7drie\u00df\u00b7lich", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "-+-", "measure": "amphibrach.single"}}, "stanza.6": {"line.1": {"text": "Zum Bande der Geselligkeit viel minder n\u00fctzlich und er-", "tokens": ["Zum", "Ban\u00b7de", "der", "Ge\u00b7sel\u00b7lig\u00b7keit", "viel", "min\u00b7der", "n\u00fctz\u00b7lich", "und", "er"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "ART", "NN", "ADV", "ADV", "ADJD", "KON", "TRUNC"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.2": {"text": "sprie\u00dflich,", "tokens": ["sprie\u00df\u00b7lich", ","], "token_info": ["word", "punct"], "pos": ["ADJD", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Und kurz: den Zustand unsrer Welt gewi\u00df viel schlimmer", "tokens": ["Und", "kurz", ":", "den", "Zu\u00b7stand", "uns\u00b7rer", "Welt", "ge\u00b7wi\u00df", "viel", "schlim\u00b7mer"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADJD", "$.", "ART", "NN", "PPOSAT", "NN", "ADV", "ADV", "ADJD"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "machen m\u00fc\u00dfte.", "tokens": ["ma\u00b7chen", "m\u00fc\u00df\u00b7te", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Von der Bewegung, der Gewohnheit, wodurch sie ihre", "tokens": ["Von", "der", "Be\u00b7we\u00b7gung", ",", "der", "Ge\u00b7wohn\u00b7heit", ",", "wo\u00b7durch", "sie", "ih\u00b7re"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "ART", "NN", "$,", "PWAV", "PPER", "PPOSAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "M\u00fch' ertragen,", "tokens": ["M\u00fch'", "er\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.7": {"text": "Und der Bewunderns-wehrten Ordnung, worinn sie stehn,", "tokens": ["Und", "der", "Be\u00b7wun\u00b7derns\u00b7wehr\u00b7ten", "Ord\u00b7nung", ",", "wo\u00b7rinn", "sie", "stehn", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$,", "PWAV", "PPER", "VVINF", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "nicht einst zu sagen,", "tokens": ["nicht", "einst", "zu", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.9": {"text": "So da\u00df man von der Arbeit selber kann eine klare Probe", "tokens": ["So", "da\u00df", "man", "von", "der", "Ar\u00b7beit", "sel\u00b7ber", "kann", "ei\u00b7ne", "kla\u00b7re", "Pro\u00b7be"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PIS", "APPR", "ART", "NN", "ADV", "VMFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-++-+-+-", "measure": "unknown.measure.octa.plus"}, "line.10": {"text": "geben,", "tokens": ["ge\u00b7ben", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.11": {"text": "Sie sey so n\u00f6htig und so n\u00fctze, als unser Brod, ja selbst", "tokens": ["Sie", "sey", "so", "n\u00f6h\u00b7tig", "und", "so", "n\u00fct\u00b7ze", ",", "als", "un\u00b7ser", "Brod", ",", "ja", "selbst"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "KON", "ADV", "VVFIN", "$,", "KOUS", "PPOSAT", "NN", "$,", "ADV", "ADV"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.12": {"text": "das Leben.", "tokens": ["das", "Le\u00b7ben", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+-", "measure": "amphibrach.single"}, "line.13": {"text": "Es scheint, der Sch\u00f6pfer habe hier die Faulheit, die", "tokens": ["Es", "scheint", ",", "der", "Sch\u00f6p\u00b7fer", "ha\u00b7be", "hier", "die", "Faul\u00b7heit", ",", "die"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "$,", "ART", "NN", "VAFIN", "ADV", "ART", "NN", "$,", "PRELS"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "uns wirklich plaget,", "tokens": ["uns", "wirk\u00b7lich", "pla\u00b7get", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.15": {"text": "In diesem grossen Werk, dem Land-Bau, durch die Noht-", "tokens": ["In", "die\u00b7sem", "gros\u00b7sen", "Werk", ",", "dem", "Lan\u00b7dBau", ",", "durch", "die", "Noht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,", "ART", "NN", "$,", "APPR", "ART", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "wendigkeit verjaget.", "tokens": ["wen\u00b7dig\u00b7keit", "ver\u00b7ja\u00b7get", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.17": {"text": "Und ob gleich Er nur blo\u00df allein das, was wir s\u00e4en, l\u00e4", "tokens": ["Und", "ob", "gleich", "Er", "nur", "blo\u00df", "al\u00b7lein", "das", ",", "was", "wir", "s\u00e4\u00b7en", ",", "l\u00e4"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["KON", "KOUS", "ADV", "PPER", "ADV", "ADV", "ADV", "PDS", "$,", "PRELS", "PPER", "VVFIN", "$,", "XY"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.18": {"text": "gedeyn;", "tokens": ["ge\u00b7deyn", ";"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+", "measure": "iambic.single"}, "line.19": {"text": "So scheint, Er wolle Seinen Segen im Schatten unsrer", "tokens": ["So", "scheint", ",", "Er", "wol\u00b7le", "Sei\u00b7nen", "Se\u00b7gen", "im", "Schat\u00b7ten", "uns\u00b7rer"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "$,", "PPER", "VMFIN", "PPOSAT", "NN", "APPRART", "NN", "PPOSAT"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.20": {"text": "M\u00fch' verstecken,", "tokens": ["M\u00fch'", "ver\u00b7ste\u00b7cken", ","], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVINF", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.21": {"text": "Und lieber durch der Menschen Arbeit die wahre Segens-", "tokens": ["Und", "lie\u00b7ber", "durch", "der", "Men\u00b7schen", "Ar\u00b7beit", "die", "wah\u00b7re", "Se\u00b7gens"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ART", "NN", "NN", "ART", "ADJA", "TRUNC"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.22": {"text": "Quell' bedecken,", "tokens": ["Quell'", "be\u00b7de\u00b7cken", ","], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.23": {"text": "Als uns die F\u00fclle Seiner G\u00fcter unmittelbar, wie sonst,", "tokens": ["Als", "uns", "die", "F\u00fcl\u00b7le", "Sei\u00b7ner", "G\u00fc\u00b7ter", "un\u00b7mit\u00b7tel\u00b7bar", ",", "wie", "sonst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "PPOSAT", "NN", "ADJD", "$,", "PWAV", "ADV", "$,"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.24": {"text": "zu schenken,", "tokens": ["zu", "schen\u00b7ken", ","], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.25": {"text": "Da\u00df wir uns nur in den Morast des M\u00fc\u00dfiggangs nicht", "tokens": ["Da\u00df", "wir", "uns", "nur", "in", "den", "Mo\u00b7rast", "des", "M\u00fc\u00b7\u00dfig\u00b7gangs", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PRF", "ADV", "APPR", "ART", "NN", "ART", "NN", "PTKNEG"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "m\u00f6gten senken;", "tokens": ["m\u00f6g\u00b7ten", "sen\u00b7ken", ";"], "token_info": ["word", "word", "punct"], "pos": ["VMFIN", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.27": {"text": "Indem, durch Arbeit und Bewegung, zugleich von Krank-", "tokens": ["In\u00b7dem", ",", "durch", "Ar\u00b7beit", "und", "Be\u00b7we\u00b7gung", ",", "zu\u00b7gleich", "von", "Krank"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "$,", "APPR", "NN", "KON", "NN", "$,", "ADV", "APPR", "TRUNC"], "meter": "+--+-+-+--+-+", "measure": "iambic.hexa.invert"}, "line.28": {"text": "heit und Beschwehrden,", "tokens": ["heit", "und", "Be\u00b7schwehr\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "KON", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.29": {"text": "Nebst vielen sonst gewissen Lastern, wir wunderbar befreyet", "tokens": ["Nebst", "vie\u00b7len", "sonst", "ge\u00b7wis\u00b7sen", "Las\u00b7tern", ",", "wir", "wun\u00b7der\u00b7bar", "be\u00b7fre\u00b7yet"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "PIAT", "ADV", "ADJA", "NN", "$,", "PPER", "ADJD", "VVFIN"], "meter": "-+-+-+-+--+-+--+", "measure": "iambic.septa.relaxed"}, "line.30": {"text": "werden.", "tokens": ["wer\u00b7den", "."], "token_info": ["word", "punct"], "pos": ["VAINF", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.31": {"text": "Ich sehe ferner auch den Segen, den man Jhm nie verdan-", "tokens": ["Ich", "se\u00b7he", "fer\u00b7ner", "auch", "den", "Se\u00b7gen", ",", "den", "man", "Jhm", "nie", "ver\u00b7dan"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "ART", "NN", "$,", "PRELS", "PIS", "PPER", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.32": {"text": "ken kann,", "tokens": ["ken", "kann", ","], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VMFIN", "$,"], "meter": "-+", "measure": "iambic.single"}}, "stanza.7": {"line.1": {"text": "Da\u00df, durch den Kreis von dem Entstehn und dem Vergehen,", "tokens": ["Da\u00df", ",", "durch", "den", "Kreis", "von", "dem", "Ent\u00b7stehn", "und", "dem", "Ver\u00b7ge\u00b7hen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "APPR", "ART", "NN", "APPR", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "unsre Erde,", "tokens": ["uns\u00b7re", "Er\u00b7de", ","], "token_info": ["word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Vermittelst Sonne, Luft und Regen, an Kr\u00e4ften nie er-", "tokens": ["Ver\u00b7mit\u00b7telst", "Son\u00b7ne", ",", "Luft", "und", "Re\u00b7gen", ",", "an", "Kr\u00e4f\u00b7ten", "nie", "er"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "NN", "$,", "NN", "KON", "NN", "$,", "APPR", "NN", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "sch\u00f6pfet werde,", "tokens": ["sch\u00f6p\u00b7fet", "wer\u00b7de", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "VAFIN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Als ein von einer weisen Allmacht so eingerichtet Wunder", "tokens": ["Als", "ein", "von", "ei\u00b7ner", "wei\u00b7sen", "All\u00b7macht", "so", "ein\u00b7ge\u00b7rich\u00b7tet", "Wun\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "APPR", "ART", "ADJA", "NN", "ADV", "VVPP", "NN"], "meter": "-+-+-+--+-+-+-+-", "measure": "iambic.septa.relaxed"}, "line.6": {"text": "an.", "tokens": ["an", "."], "token_info": ["word", "punct"], "pos": ["PTKVZ", "$."], "meter": "+", "measure": "single.up"}}, "stanza.8": {"line.1": {"text": "Wenn wir demnach als Menschen lebten, und recht als", "tokens": ["Wenn", "wir", "dem\u00b7nach", "als", "Men\u00b7schen", "leb\u00b7ten", ",", "und", "recht", "als"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PPER", "PAV", "KOUS", "NN", "VVFIN", "$,", "KON", "ADJD", "KOKOM"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "wahre Menschen d\u00e4chten;", "tokens": ["wah\u00b7re", "Men\u00b7schen", "d\u00e4ch\u00b7ten", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "So w\u00fcrden wir ein wenig mehr, als wie das Vieh, die", "tokens": ["So", "w\u00fcr\u00b7den", "wir", "ein", "we\u00b7nig", "mehr", ",", "als", "wie", "das", "Vieh", ",", "die"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "PIS", "ADV", "$,", "KOUS", "KOKOM", "ART", "NN", "$,", "PRELS"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wunder-Weise,", "tokens": ["Wun\u00b7der\u00b7Wei\u00b7se", ","], "token_info": ["word", "punct"], "pos": ["NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.5": {"text": "Wie eigentlich das Korn ger\u00e4ht, dem Sch\u00f6pfer der Natur", "tokens": ["Wie", "ei\u00b7gent\u00b7lich", "das", "Korn", "ge\u00b7r\u00e4ht", ",", "dem", "Sch\u00f6p\u00b7fer", "der", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ART", "NN", "VVPP", "$,", "ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.6": {"text": "zum Preise,", "tokens": ["zum", "Prei\u00b7se", ","], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$,"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "Betrachten, Seine weise Huld bewundern; ja ich wei\u00df, wir", "tokens": ["Be\u00b7trach\u00b7ten", ",", "Sei\u00b7ne", "wei\u00b7se", "Huld", "be\u00b7wun\u00b7dern", ";", "ja", "ich", "wei\u00df", ",", "wir"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["NN", "$,", "PPOSAT", "ADJA", "NN", "VVINF", "$.", "ADV", "PPER", "VVFIN", "$,", "PPER"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.8": {"text": "br\u00e4chten", "tokens": ["br\u00e4ch\u00b7ten"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+-", "measure": "trochaic.single"}, "line.9": {"text": "Jhm die ger\u00fchrten Seelen selbst zum Opfer, nebst dem", "tokens": ["Jhm", "die", "ge\u00b7r\u00fchr\u00b7ten", "See\u00b7len", "selbst", "zum", "Op\u00b7fer", ",", "nebst", "dem"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "ART", "ADJA", "NN", "ADV", "APPRART", "NN", "$,", "APPR", "ART"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "ernsten Willen,", "tokens": ["erns\u00b7ten", "Wil\u00b7len", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "Was Jhm mi\u00dff\u00e4llig nicht zu thun, was Jhm gef\u00e4llig", "tokens": ["Was", "Jhm", "mi\u00df\u00b7f\u00e4l\u00b7lig", "nicht", "zu", "thun", ",", "was", "Jhm", "ge\u00b7f\u00e4l\u00b7lig"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "PPER", "ADJD", "PTKNEG", "PTKZU", "VVINF", "$,", "PWS", "PPER", "ADJD"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "zu erf\u00fcllen.", "tokens": ["zu", "er\u00b7f\u00fcl\u00b7len", "."], "token_info": ["word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}}}}}