{"textgrid.poem.52130": {"metadata": {"author": {"name": "Czepko von Reigersfeld, Daniel", "birth": "N.A.", "death": "N.A."}, "title": "50.", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Weg mit der Satyra und aller Tichterey,", "tokens": ["Weg", "mit", "der", "Sa\u00b7ty\u00b7ra", "und", "al\u00b7ler", "Tich\u00b7te\u00b7rey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "KON", "PIAT", "NN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Hier geht der Laster Sturm und Castalis vorbey:", "tokens": ["Hier", "geht", "der", "Las\u00b7ter", "Sturm", "und", "Cas\u00b7ta\u00b7lis", "vor\u00b7bey", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NN", "KON", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Es ist nicht noth durchsehn der andern Leute Leben,", "tokens": ["Es", "ist", "nicht", "noth", "durch\u00b7sehn", "der", "an\u00b7dern", "Leu\u00b7te", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "NN", "VVFIN", "ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Weil viel Gefahr und Schuld aus unserm ist zu heben.", "tokens": ["Weil", "viel", "Ge\u00b7fahr", "und", "Schuld", "aus", "un\u00b7serm", "ist", "zu", "he\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "KON", "NN", "APPR", "PPOSAT", "VAFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Ein ieder, wenn er sich zu Bette hat gelegt,", "tokens": ["Ein", "ie\u00b7der", ",", "wenn", "er", "sich", "zu", "Bet\u00b7te", "hat", "ge\u00b7legt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "$,", "KOUS", "PPER", "PRF", "APPR", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Den Wachsstock ausgel\u00f6scht, sich nichts im Hause regt:", "tokens": ["Den", "Wachs\u00b7stock", "aus\u00b7ge\u00b7l\u00f6scht", ",", "sich", "nichts", "im", "Hau\u00b7se", "regt", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,", "PRF", "PIS", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Erforsche nur von Sich, was er den Tag begangen,", "tokens": ["Er\u00b7for\u00b7sche", "nur", "von", "Sich", ",", "was", "er", "den", "Tag", "be\u00b7gan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "PRF", "$,", "PWS", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Er wird Bericht genung, mehr als ihm lieb, empfangen.", "tokens": ["Er", "wird", "Be\u00b7richt", "ge\u00b7nung", ",", "mehr", "als", "ihm", "lieb", ",", "emp\u00b7fan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "NN", "ADV", "$,", "ADV", "KOUS", "PPER", "ADJD", "$,", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Wann sein Gewi\u00dfen Ihm die Seinen stellet hin,", "tokens": ["Wann", "sein", "Ge\u00b7wi\u00b7\u00dfen", "Ihm", "die", "Sei\u00b7nen", "stel\u00b7let", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "PPER", "ART", "PPOSS", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Darff Andrer Fehler er nicht durch die Hechel ziehn:", "tokens": ["Darff", "A\u00b7ndrer", "Feh\u00b7ler", "er", "nicht", "durch", "die", "He\u00b7chel", "ziehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADJA", "NN", "PPER", "PTKNEG", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nun die Gewalt sol auch bey mir ink\u00fcnfftig gelten,", "tokens": ["Nun", "die", "Ge\u00b7walt", "sol", "auch", "bey", "mir", "in\u00b7k\u00fcnff\u00b7tig", "gel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VMFIN", "ADV", "APPR", "PPER", "ADJD", "VVINF", "$,"], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.4": {"text": "Ich wil von Nacht auff Nacht mein Hertz und Leben schelten:", "tokens": ["Ich", "wil", "von", "Nacht", "auff", "Nacht", "mein", "Hertz", "und", "Le\u00b7ben", "schel\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "APPR", "NN", "APPR", "NN", "PPOSAT", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Wil sagen: Diesesmahl, geb ich dir noch Geh\u00f6r,", "tokens": ["Wil", "sa\u00b7gen", ":", "Die\u00b7ses\u00b7mahl", ",", "geb", "ich", "dir", "noch", "Ge\u00b7h\u00f6r", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VVINF", "$.", "NN", "$,", "VVFIN", "PPER", "PPER", "ADV", "NN", "$,"], "meter": "+--+--+-++-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Verzeih' ich dir die Schuld: thu es hinfort nicht mehr:", "tokens": ["Ver\u00b7zeih'", "ich", "dir", "die", "Schuld", ":", "thu", "es", "hin\u00b7fort", "nicht", "mehr", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PPER", "ART", "NN", "$.", "VVFIN", "PPER", "ADV", "PTKNEG", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Folgst du mein Leser mir, so wirst du frey von N\u00f6then,", "tokens": ["Folgst", "du", "mein", "Le\u00b7ser", "mir", ",", "so", "wirst", "du", "frey", "von", "N\u00f6\u00b7then", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "PPER", "$,", "ADV", "VAFIN", "PPER", "ADJD", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Darffst keiner Satyra, das lerne vom Poeten.", "tokens": ["Darffst", "kei\u00b7ner", "Sa\u00b7ty\u00b7ra", ",", "das", "ler\u00b7ne", "vom", "Po\u00b7et\u00b7en", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PIAT", "NN", "$,", "PDS", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}