{"textgrid.poem.33235": {"metadata": {"author": {"name": "Canitz, Friedrich Rudolph Ludwig von", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wer Lust zu lieben hat, geb es selbst zu erkennen;", "genre": "verse", "period": "N.A.", "pub_year": 1676, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wer Lust zu lieben hat, geb es selbst zu erkennen;", "tokens": ["Wer", "Lust", "zu", "lie\u00b7ben", "hat", ",", "geb", "es", "selbst", "zu", "er\u00b7ken\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "PTKZU", "VVINF", "VAFIN", "$,", "VVFIN", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+--+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Doch wann er frey heraus gesagt,", "tokens": ["Doch", "wann", "er", "frey", "he\u00b7raus", "ge\u00b7sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADJD", "PTKVZ", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was ihn f\u00fcr eine Regung plagt,", "tokens": ["Was", "ihn", "f\u00fcr", "ei\u00b7ne", "Re\u00b7gung", "plagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So mu\u00df man seinen Schwur auch keinen Meineyd nennen.", "tokens": ["So", "mu\u00df", "man", "sei\u00b7nen", "Schwur", "auch", "kei\u00b7nen", "Mei\u00b7neyd", "nen\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "PPOSAT", "VVFIN", "ADV", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+--+--", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Man trau ihm auf sein Wort, es gehe recht von Hertzen.", "tokens": ["Man", "trau", "ihm", "auf", "sein", "Wort", ",", "es", "ge\u00b7he", "recht", "von", "Hert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$,", "PPER", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ein ungegr\u00fcndeter unbilliger Verdacht,", "tokens": ["Ein", "un\u00b7ge\u00b7gr\u00fcn\u00b7de\u00b7ter", "un\u00b7bil\u00b7li\u00b7ger", "Ver\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Der endlich die Gedult der Buhler m\u00fcde macht,", "tokens": ["Der", "end\u00b7lich", "die", "Ge\u00b7dult", "der", "Buh\u00b7ler", "m\u00fc\u00b7de", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "ART", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Kan ein gewonnen Hertz offt liederlich verschertzen.", "tokens": ["Kan", "ein", "ge\u00b7won\u00b7nen", "Hertz", "offt", "lie\u00b7der\u00b7lich", "ver\u00b7schert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "ADJA", "NN", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Wenn die Erkl\u00e4rung nun einmahl geschehn,", "tokens": ["Wenn", "die", "Er\u00b7kl\u00e4\u00b7rung", "nun", "ein\u00b7mahl", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Dann haben beyde sich wohl vorzusehn,", "tokens": ["Dann", "ha\u00b7ben", "bey\u00b7de", "sich", "wohl", "vor\u00b7zu\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "PRF", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Da\u00df andre nicht die neue Glut erkennen.", "tokens": ["Da\u00df", "and\u00b7re", "nicht", "die", "neu\u00b7e", "Glut", "er\u00b7ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PTKNEG", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Wo man verborgen liebt und ohne grossen Schein,", "tokens": ["Wo", "man", "ver\u00b7bor\u00b7gen", "liebt", "und", "oh\u00b7ne", "gros\u00b7sen", "Schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VVPP", "VVFIN", "KON", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Da findet sich die rechte Wollust ein,", "tokens": ["Da", "fin\u00b7det", "sich", "die", "rech\u00b7te", "Wol\u00b7lust", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ART", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und nichts, wenn zwey verliebte Hertzen brennen,", "tokens": ["Und", "nichts", ",", "wenn", "zwey", "ver\u00b7lieb\u00b7te", "Hert\u00b7zen", "bren\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "$,", "KOUS", "CARD", "VVFIN", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Ist s\u00fcsser, als verschwiegen seyn.", "tokens": ["Ist", "s\u00fcs\u00b7ser", ",", "als", "ver\u00b7schwie\u00b7gen", "seyn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "$,", "KOUS", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Wenn jedes nun dem andern fest verhei\u00dft,", "tokens": ["Wenn", "je\u00b7des", "nun", "dem", "an\u00b7dern", "fest", "ver\u00b7hei\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "ADV", "ART", "ADJA", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Was ein verliebter Mund und ein entz\u00fcckter Geist", "tokens": ["Was", "ein", "ver\u00b7lieb\u00b7ter", "Mund", "und", "ein", "ent\u00b7z\u00fcck\u00b7ter", "Geist"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "ART", "ADJA", "NN", "KON", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nur je geschickt zu reden und zu dencken,", "tokens": ["Nur", "je", "ge\u00b7schickt", "zu", "re\u00b7den", "und", "zu", "den\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Soll sie ein s\u00fcsses Band der Einigkeit verschrencken;", "tokens": ["Soll", "sie", "ein", "s\u00fcs\u00b7ses", "Band", "der", "Ei\u00b7nig\u00b7keit", "ver\u00b7schren\u00b7cken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ART", "ADJA", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und wann das Schicksal sie gleich von einander rei\u00dft,", "tokens": ["Und", "wann", "das", "Schick\u00b7sal", "sie", "gleich", "von", "ein\u00b7an\u00b7der", "rei\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ART", "NN", "PPER", "ADV", "APPR", "PRF", "VVFIN", "$,"], "meter": "-+-+-++--+-+", "measure": "iambic.hexa.relaxed"}, "line.6": {"text": "Mu\u00df die Best\u00e4ndigkeit de\u00dfwegen doch nicht wancken;", "tokens": ["Mu\u00df", "die", "Be\u00b7st\u00e4n\u00b7dig\u00b7keit", "de\u00df\u00b7we\u00b7gen", "doch", "nicht", "wan\u00b7cken", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "PAV", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Was nicht zugegen ist, das liebt man in Gedancken.", "tokens": ["Was", "nicht", "zu\u00b7ge\u00b7gen", "ist", ",", "das", "liebt", "man", "in", "Ge\u00b7dan\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKNEG", "ADJD", "VAFIN", "$,", "PDS", "VVFIN", "PIS", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Doch kan man auch wohl \u00fcberhoben seyn,", "tokens": ["Doch", "kan", "man", "auch", "wohl", "\u00fc\u00b7berh\u00b7o\u00b7ben", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIS", "ADV", "ADV", "VVPP", "VAINF", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "In steter Sterbens-Angst und \u00fcberh\u00e4uffter Pein,", "tokens": ["In", "ste\u00b7ter", "Ster\u00b7bens\u00b7Angst", "und", "\u00fc\u00b7berh\u00b7\u00e4uff\u00b7ter", "Pein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als wie ein Schatten, zu vergehen,", "tokens": ["Als", "wie", "ein", "Schat\u00b7ten", ",", "zu", "ver\u00b7ge\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "KOKOM", "ART", "NN", "$,", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Aus blosser Ungedult, sein liebstes Kind zu sehen.", "tokens": ["Aus", "blos\u00b7ser", "Un\u00b7ge\u00b7dult", ",", "sein", "liebs\u00b7tes", "Kind", "zu", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,", "PPOSAT", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So liebte zwar die alte Welt;", "tokens": ["So", "lieb\u00b7te", "zwar", "die", "al\u00b7te", "Welt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Doch, da sich alles umgekehret,", "tokens": ["Doch", ",", "da", "sich", "al\u00b7les", "um\u00b7ge\u00b7keh\u00b7ret", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "KOUS", "PRF", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Und uns die neue nun gelindre S\u00e4tze lehret,", "tokens": ["Und", "uns", "die", "neu\u00b7e", "nun", "ge\u00b7lind\u00b7re", "S\u00e4t\u00b7ze", "leh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ART", "ADJA", "ADV", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ist keiner, dem di\u00df Lieben mehr gef\u00e4llt.", "tokens": ["Ist", "kei\u00b7ner", ",", "dem", "di\u00df", "Lie\u00b7ben", "mehr", "ge\u00b7f\u00e4llt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "$,", "PRELS", "PDS", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Sagt, wendet man nicht auch sein Seuffzen \u00fcbel an,", "tokens": ["Sagt", ",", "wen\u00b7det", "man", "nicht", "auch", "sein", "Seuff\u00b7zen", "\u00fc\u00b7bel", "an", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "PIS", "PTKNEG", "ADV", "PPOSAT", "NN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wann es die Sch\u00f6ne nicht verstehn noch h\u00f6ren kan?", "tokens": ["Wann", "es", "die", "Sch\u00f6\u00b7ne", "nicht", "ver\u00b7stehn", "noch", "h\u00f6\u00b7ren", "kan", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "PTKNEG", "VVFIN", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Wann uns die Liebe sprechen hei\u00dft,", "tokens": ["Wann", "uns", "die", "Lie\u00b7be", "spre\u00b7chen", "hei\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ists besser, da\u00df man sich der Lustigkeit beflei\u00dft,", "tokens": ["Ists", "bes\u00b7ser", ",", "da\u00df", "man", "sich", "der", "Lus\u00b7tig\u00b7keit", "be\u00b7flei\u00dft", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "PIS", "PRF", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Als der betr\u00fcbten Redens-Arten,", "tokens": ["Als", "der", "be\u00b7tr\u00fcb\u00b7ten", "Re\u00b7den\u00b7sAr\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die man im Trauer-Spiel und Liebes-B\u00fcchern findt,", "tokens": ["Die", "man", "im", "Trau\u00b7e\u00b7rSpiel", "und", "Lie\u00b7bes\u00b7B\u00fc\u00b7chern", "findt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "APPRART", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ein angenehmer Schertz hat offt mehr zu gewarten,", "tokens": ["Ein", "an\u00b7ge\u00b7neh\u00b7mer", "Schertz", "hat", "offt", "mehr", "zu", "ge\u00b7war\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Als solch ein Jammer-Thon verha\u00dfter Traurigkeit.", "tokens": ["Als", "solch", "ein", "Jam\u00b7mer\u00b7Thon", "ver\u00b7ha\u00df\u00b7ter", "Trau\u00b7rig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Die Liebe, wie bekannt, ist ja ein kleines Kind,", "tokens": ["Die", "Lie\u00b7be", ",", "wie", "be\u00b7kannt", ",", "ist", "ja", "ein", "klei\u00b7nes", "Kind", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PWAV", "ADJD", "$,", "VAFIN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Das man um sein Geschw\u00e4tz und Spielen lieb gewinnt;", "tokens": ["Das", "man", "um", "sein", "Ge\u00b7schw\u00e4tz", "und", "Spie\u00b7len", "lieb", "ge\u00b7winnt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "APPR", "PPOSAT", "NN", "KON", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Doch, wann es \u00fcbel thut und schreyt,", "tokens": ["Doch", ",", "wann", "es", "\u00fc\u00b7bel", "thut", "und", "schreyt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "$,", "PWAV", "PPER", "ADJD", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Und nicht mehr, wie vorhin, sich artig will erzeigen,", "tokens": ["Und", "nicht", "mehr", ",", "wie", "vor\u00b7hin", ",", "sich", "ar\u00b7tig", "will", "er\u00b7zei\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ADV", "$,", "PWAV", "ADV", "$,", "PRF", "ADJD", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "So heisset man es stille schweigen.", "tokens": ["So", "heis\u00b7set", "man", "es", "stil\u00b7le", "schwei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PPER", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wir wollen, wie gesagt, uns dergestalt verbinden,", "tokens": ["Wir", "wol\u00b7len", ",", "wie", "ge\u00b7sagt", ",", "uns", "der\u00b7ge\u00b7stalt", "ver\u00b7bin\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "$,", "PWAV", "VVPP", "$,", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da\u00df unser Thun sonst niemand wissend sey.", "tokens": ["Da\u00df", "un\u00b7ser", "Thun", "sonst", "nie\u00b7mand", "wis\u00b7send", "sey", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "PIS", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Nichts ist beschwerlicher auf dieser Welt zu finden,", "tokens": ["Nichts", "ist", "be\u00b7schwer\u00b7li\u00b7cher", "auf", "die\u00b7ser", "Welt", "zu", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADJD", "APPR", "PDAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Als wann ein Buhler erst so arg schon im Geschrey,", "tokens": ["Als", "wann", "ein", "Buh\u00b7ler", "erst", "so", "arg", "schon", "im", "Ge\u00b7schrey", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PWAV", "ART", "NN", "ADV", "ADV", "ADJD", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Da\u00df ihn die gantze Stadt mit Fingern weisen kan,", "tokens": ["Da\u00df", "ihn", "die", "gant\u00b7ze", "Stadt", "mit", "Fin\u00b7gern", "wei\u00b7sen", "kan", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPR", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und sagen: Seht doch den Verliebten an!", "tokens": ["Und", "sa\u00b7gen", ":", "Seht", "doch", "den", "Ver\u00b7lieb\u00b7ten", "an", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "$.", "VVFIN", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Wer kan ihn ohne Lachen schauen?", "tokens": ["Wer", "kan", "ihn", "oh\u00b7ne", "La\u00b7chen", "schau\u00b7en", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Wann er, mehr kranck und matt,", "tokens": ["Wann", "er", ",", "mehr", "kranck", "und", "matt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "$,", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.9": {"text": "Als mancher, der ein hitzig Fieber hat,", "tokens": ["Als", "man\u00b7cher", ",", "der", "ein", "hit\u00b7zig", "Fie\u00b7ber", "hat", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "$,", "PRELS", "ART", "ADJD", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Zu seiner Liebsten schleicht, ihr heimlich zu vertrauen,", "tokens": ["Zu", "sei\u00b7ner", "Liebs\u00b7ten", "schleicht", ",", "ihr", "heim\u00b7lich", "zu", "ver\u00b7trau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$,", "PPER", "ADJD", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Was man ihm ohnedem schon aus den Augen list.", "tokens": ["Was", "man", "ihm", "oh\u00b7ne\u00b7dem", "schon", "aus", "den", "Au\u00b7gen", "list", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "PPER", "PAV", "ADV", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Glaubt, da\u00df jetzund die kl\u00fcgste Regel ist:", "tokens": ["Glaubt", ",", "da\u00df", "je\u00b7tzund", "die", "kl\u00fcgs\u00b7te", "Re\u00b7gel", "ist", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "ADV", "ART", "ADJA", "NN", "VAFIN", "$."], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.13": {"text": "Verliebt seyn, und es doch nicht scheinen.", "tokens": ["Ver\u00b7liebt", "seyn", ",", "und", "es", "doch", "nicht", "schei\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "VAINF", "$,", "KON", "PPER", "ADV", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Genug, da\u00df eine wei\u00df, wie wir es mit ihr meynen.", "tokens": ["Ge\u00b7nug", ",", "da\u00df", "ei\u00b7ne", "wei\u00df", ",", "wie", "wir", "es", "mit", "ihr", "mey\u00b7nen", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUS", "PIS", "VVFIN", "$,", "PWAV", "PPER", "PPER", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Man sp\u00fchret aus dem Augenlichte", "tokens": ["Man", "sp\u00fch\u00b7ret", "aus", "dem", "Au\u00b7gen\u00b7lich\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Offt der Gedancken tieffsten Grund;", "tokens": ["Offt", "der", "Ge\u00b7dan\u00b7cken", "tieffs\u00b7ten", "Grund", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "ADJA", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Drum sehe man sich vor, sonst wird aus dem Gesichte", "tokens": ["Drum", "se\u00b7he", "man", "sich", "vor", ",", "sonst", "wird", "aus", "dem", "Ge\u00b7sich\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PIS", "PRF", "PTKVZ", "$,", "ADV", "VAFIN", "APPR", "ART", "NN"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Dem Neben-Buhler selbst leicht das Geheimni\u00df kund.", "tokens": ["Dem", "Ne\u00b7ben\u00b7Buh\u00b7ler", "selbst", "leicht", "das", "Ge\u00b7heim\u00b7ni\u00df", "kund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+--+--+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Vor Alters zwar, da muste man aus Noth,", "tokens": ["Vor", "Al\u00b7ters", "zwar", ",", "da", "mus\u00b7te", "man", "aus", "Noth", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "$,", "KOUS", "VMFIN", "PIS", "APPR", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Wann man die Gegenwart der Iris wahrgenommen,", "tokens": ["Wann", "man", "die", "Ge\u00b7gen\u00b7wart", "der", "I\u00b7ris", "wahr\u00b7ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ART", "NN", "ART", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Bald bla\u00df seyn und bald wieder roth,", "tokens": ["Bald", "bla\u00df", "seyn", "und", "bald", "wie\u00b7der", "roth", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAINF", "KON", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Sonst w\u00e4re man in den Verdacht", "tokens": ["Sonst", "w\u00e4\u00b7re", "man", "in", "den", "Ver\u00b7dacht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PIS", "APPR", "ART", "NN"], "meter": "-+--++-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "Der Unbest\u00e4ndigkeit sehr leicht gekommen.", "tokens": ["Der", "Un\u00b7be\u00b7st\u00e4n\u00b7dig\u00b7keit", "sehr", "leicht", "ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Doch die Gewohnheit hat es nun schon abgebracht;", "tokens": ["Doch", "die", "Ge\u00b7wohn\u00b7heit", "hat", "es", "nun", "schon", "ab\u00b7ge\u00b7bracht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Die Liebe zeige sich, bey Schmertzen oder Schertzen,", "tokens": ["Die", "Lie\u00b7be", "zei\u00b7ge", "sich", ",", "bey", "Schmert\u00b7zen", "o\u00b7der", "Schert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "$,", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Niemahlen im Gesicht, wohl aber in dem Hertzen.", "tokens": ["Nie\u00b7mah\u00b7len", "im", "Ge\u00b7sicht", ",", "wohl", "a\u00b7ber", "in", "dem", "Hert\u00b7zen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "$,", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.8": {"line.1": {"text": "Wann uns die Sch\u00f6ne nicht zu freundlich angesehn,", "tokens": ["Wann", "uns", "die", "Sch\u00f6\u00b7ne", "nicht", "zu", "freund\u00b7lich", "an\u00b7ge\u00b7sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ART", "NN", "PTKNEG", "PTKA", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "So w\u00fcnschen wir nicht mehr, vor Kummer, zu erkalten,", "tokens": ["So", "w\u00fcn\u00b7schen", "wir", "nicht", "mehr", ",", "vor", "Kum\u00b7mer", ",", "zu", "er\u00b7kal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "ADV", "$,", "APPR", "NN", "$,", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Noch vor der Zeit ins Grab zu gehn.", "tokens": ["Noch", "vor", "der", "Zeit", "ins", "Grab", "zu", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Man pflegt von Selbst-Mord ietzt nichts mehr zu halten.", "tokens": ["Man", "pflegt", "von", "Selbst\u00b7Mord", "ietzt", "nichts", "mehr", "zu", "hal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NE", "ADV", "PIS", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Was sonst aus Liebes-Trieb die Menschen weggerafft,", "tokens": ["Was", "sonst", "aus", "Lie\u00b7bes\u00b7Trieb", "die", "Men\u00b7schen", "weg\u00b7ge\u00b7rafft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "APPR", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Gifft, Raserey und Dolch, ist alles abgeschafft.", "tokens": ["Gifft", ",", "Ra\u00b7se\u00b7rey", "und", "Dolch", ",", "ist", "al\u00b7les", "ab\u00b7ge\u00b7schafft", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "NN", "$,", "VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Dergleichen Grausamkeit", "tokens": ["Derg\u00b7lei\u00b7chen", "Grau\u00b7sam\u00b7keit"], "token_info": ["word", "word"], "pos": ["PIS", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Wird selten von uns angef\u00fchret,", "tokens": ["Wird", "sel\u00b7ten", "von", "uns", "an\u00b7ge\u00b7f\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Und zwar nur bey Gelegenheit,", "tokens": ["Und", "zwar", "nur", "bey", "Ge\u00b7le\u00b7gen\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "APPR", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.10": {"text": "Weil sie noch manchen Reim in unsern Liedern zieret.", "tokens": ["Weil", "sie", "noch", "man\u00b7chen", "Reim", "in", "un\u00b7sern", "Lie\u00b7dern", "zie\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PIAT", "NN", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Tr\u00e4gt sichs bi\u00dfweilen zu,", "tokens": ["Tr\u00e4gt", "sichs", "bi\u00df\u00b7wei\u00b7len", "zu", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PTKVZ", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.2": {"text": "Da\u00df sie von ihm, und er von ihr, was arges dencket;", "tokens": ["Da\u00df", "sie", "von", "ihm", ",", "und", "er", "von", "ihr", ",", "was", "ar\u00b7ges", "den\u00b7cket", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPER", "$,", "KON", "PPER", "APPR", "PPER", "$,", "PWS", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wohl dem, der alles gleich zu besten lencket.", "tokens": ["Wohl", "dem", ",", "der", "al\u00b7les", "gleich", "zu", "bes\u00b7ten", "len\u00b7cket", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "$,", "PRELS", "PIS", "ADV", "PTKA", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Sonst st\u00f6hret er sich selber seine Ruh.", "tokens": ["Sonst", "st\u00f6h\u00b7ret", "er", "sich", "sel\u00b7ber", "sei\u00b7ne", "Ruh", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Was hilffts, da\u00df wir uns unterwinden,", "tokens": ["Was", "hilffts", ",", "da\u00df", "wir", "uns", "un\u00b7ter\u00b7win\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$,", "KOUS", "PPER", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Durch zu genaue Spur der Sachen Grund zu finden?", "tokens": ["Durch", "zu", "ge\u00b7nau\u00b7e", "Spur", "der", "Sa\u00b7chen", "Grund", "zu", "fin\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ADJA", "NN", "ART", "NN", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ich will euch glauben, glaubt mir auch;", "tokens": ["Ich", "will", "euch", "glau\u00b7ben", ",", "glaubt", "mir", "auch", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVINF", "$,", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Das ist f\u00fcrwahr der l\u00f6blichste Gebrauch.", "tokens": ["Das", "ist", "f\u00fcr\u00b7wahr", "der", "l\u00f6b\u00b7lichs\u00b7te", "Ge\u00b7brauch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Der F\u00fcrwitz tauget nicht,", "tokens": ["Der", "F\u00fcr\u00b7witz", "tau\u00b7get", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "Und qv\u00e4lt uns offt durch wiedrigen Bericht.", "tokens": ["Und", "qv\u00e4lt", "uns", "offt", "durch", "wied\u00b7ri\u00b7gen", "Be\u00b7richt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.11": {"text": "Wie mancher w\u00e4re froh, viel Dinge nicht zu wissen,", "tokens": ["Wie", "man\u00b7cher", "w\u00e4\u00b7re", "froh", ",", "viel", "Din\u00b7ge", "nicht", "zu", "wis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "VAFIN", "ADJD", "$,", "PIAT", "NN", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Nach deren Wissenschafft er sich zuvor beflissen?", "tokens": ["Nach", "de\u00b7ren", "Wis\u00b7sen\u00b7schafft", "er", "sich", "zu\u00b7vor", "be\u00b7flis\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELAT", "NN", "PPER", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Auch mu\u00df die Eiffersucht weit weggebannet werden.", "tokens": ["Auch", "mu\u00df", "die", "Eif\u00b7fer\u00b7sucht", "weit", "weg\u00b7ge\u00b7ban\u00b7net", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ART", "NN", "ADJD", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist wohl was sch\u00f6ners auf der Erden,", "tokens": ["Ist", "wohl", "was", "sch\u00f6\u00b7ners", "auf", "der", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PWS", "ADV", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als wann man glauben kan, da\u00df Demant-feste Treu", "tokens": ["Als", "wann", "man", "glau\u00b7ben", "kan", ",", "da\u00df", "De\u00b7mant\u00b7fes\u00b7te", "Treu"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "PWAV", "PIS", "VVINF", "VMFIN", "$,", "KOUS", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der Grundstein unsrer Liebe sey?", "tokens": ["Der", "Grund\u00b7stein", "uns\u00b7rer", "Lie\u00b7be", "sey", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Und wer es anders macht, der macht sich selbst Beschwehrden.", "tokens": ["Und", "wer", "es", "an\u00b7ders", "macht", ",", "der", "macht", "sich", "selbst", "Be\u00b7schwehr\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "VVFIN", "$,", "PRELS", "VVFIN", "PRF", "ADV", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die Schwachheit ist f\u00fcrwahr bey dem nicht klein,", "tokens": ["Die", "Schwach\u00b7heit", "ist", "f\u00fcr\u00b7wahr", "bey", "dem", "nicht", "klein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "APPR", "PRELS", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Der, ob gleich die, die ihm ihr Hertze giebet,", "tokens": ["Der", ",", "ob", "gleich", "die", ",", "die", "ihm", "ihr", "Hert\u00b7ze", "gie\u00b7bet", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "KOUS", "ADV", "ART", "$,", "PRELS", "PPER", "PPER", "VVFIN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Es noch so sehr betheurt, und endlich zugeflucht,", "tokens": ["Es", "noch", "so", "sehr", "be\u00b7theurt", ",", "und", "end\u00b7lich", "zu\u00b7ge\u00b7flucht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "ADV", "ADJD", "$,", "KON", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Sich selber doch zu \u00fcberzeugen sucht,", "tokens": ["Sich", "sel\u00b7ber", "doch", "zu", "\u00fc\u00b7berz\u00b7eu\u00b7gen", "sucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "ADV", "PTKZU", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.10": {"text": "Er sey noch nicht genug geliebet.", "tokens": ["Er", "sey", "noch", "nicht", "ge\u00b7nug", "ge\u00b7lie\u00b7bet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}