{"textgrid.poem.53869": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Des deutschen Volkes Liederschatz", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.57", "sw:0.42"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": " liebe Katharina,", "tokens": ["lie\u00b7be", "Ka\u00b7tha\u00b7ri\u00b7na", ","], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "NE", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.2": {"text": " komm zu mir nach China!", "tokens": ["komm", "zu", "mir", "nach", "Chi\u00b7na", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPER", "APPR", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "ist hier zu nennen, sowie:", "tokens": ["ist", "hier", "zu", "nen\u00b7nen", ",", "so\u00b7wie", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "ADV", "PTKZU", "VVINF", "$,", "KON", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.4": {"text": " luise \u2013 Luise \u2013 warum bist du denn so bla\u00df?", "tokens": ["lu\u00b7i\u00b7se", "\u2013", "Lu\u00b7i\u00b7se", "\u2013", "wa\u00b7rum", "bist", "du", "denn", "so", "bla\u00df", "?"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "NE", "$(", "PWAV", "VAFIN", "PPER", "ADV", "ADV", "ADJD", "$."], "meter": "+---+-+-+-+-+", "measure": "dactylic.init"}}, "stanza.2": {"line.1": {"text": " wo sind deine Haare,", "tokens": ["wo", "sind", "dei\u00b7ne", "Haa\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.2": {"text": " august \u2013 August?", "tokens": ["au\u00b7gust", "\u2013", "Au\u00b7gust", "?"], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADJD", "$(", "NN", "$."], "meter": "+-+-", "measure": "trochaic.di"}}, "stanza.3": {"line.1": {"text": " in der Hafenbar von Rio bei Laternenlicht", "tokens": ["in", "der", "Ha\u00b7fen\u00b7bar", "von", "Rio", "bei", "La\u00b7ter\u00b7nen\u00b7licht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "NE", "APPR", "NN"], "meter": "--+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": " hatte Jim zum ersten Mal gesehen ihr Gesicht,", "tokens": ["hat\u00b7te", "Jim", "zum", "ers\u00b7ten", "Mal", "ge\u00b7se\u00b7hen", "ihr", "Ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "APPRART", "ADJA", "NN", "VVPP", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}}, "stanza.4": {"line.1": {"text": " hoch zu Ro\u00df mit seinem stolzen Tro\u00df", "tokens": ["hoch", "zu", "Ro\u00df", "mit", "sei\u00b7nem", "stol\u00b7zen", "Tro\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "APPR", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": " der gro\u00dfe Picador,", "tokens": ["der", "gro\u00b7\u00dfe", "Pi\u00b7ca\u00b7dor", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "wobei denn noch festzustellen w\u00e4re, wer bei diesem get\u00e4tigten Gesch\u00e4ft der Ochse gewesen ist.", "tokens": ["wo\u00b7bei", "denn", "noch", "fest\u00b7zu\u00b7stel\u00b7len", "w\u00e4\u00b7re", ",", "wer", "bei", "die\u00b7sem", "ge\u00b7t\u00e4\u00b7tig\u00b7ten", "Ge\u00b7sch\u00e4ft", "der", "O\u00b7chse", "ge\u00b7we\u00b7sen", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "ADV", "VVINF", "VAFIN", "$,", "PWS", "APPR", "PDAT", "ADJA", "NN", "ART", "NN", "VAPP", "VAFIN", "$."], "meter": "+-+-+-+-+-+-+--+-+-+--+-+-+", "measure": "trochaic.octa.plus.relaxed"}}, "stanza.5": {"line.1": {"text": " am R\u00fcdesheimer Schlo\u00df steht eine Linde!", "tokens": ["am", "R\u00fc\u00b7des\u00b7hei\u00b7mer", "Schlo\u00df", "steht", "ei\u00b7ne", "Lin\u00b7de", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "VVFIN", "ART", "NE", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": " der Fr\u00fchlingswind zieht durch der Bl\u00e4tter Gr\u00fcn,", "tokens": ["der", "Fr\u00fch\u00b7lings\u00b7wind", "zieht", "durch", "der", "Bl\u00e4t\u00b7ter", "Gr\u00fcn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": " ein Herz ist eingeschnitzt in ihre Rinde,", "tokens": ["ein", "Herz", "ist", "ein\u00b7ge\u00b7schnitzt", "in", "ih\u00b7re", "Rin\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": " und in dem Herzen steht ein Name dr\u00fcn.", "tokens": ["und", "in", "dem", "Her\u00b7zen", "steht", "ein", "Na\u00b7me", "dr\u00fcn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": " am H\u00fcgel, wo der Flieder bl\u00fcht,", "tokens": ["am", "H\u00fc\u00b7gel", ",", "wo", "der", "Flie\u00b7der", "bl\u00fcht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "PWAV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": " und eine Rosenhecke gl\u00fcht", "tokens": ["und", "ei\u00b7ne", "Ro\u00b7sen\u00b7he\u00b7cke", "gl\u00fcht"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "und:", "tokens": ["und", ":"], "token_info": ["word", "punct"], "pos": ["KON", "$."], "meter": "-", "measure": "single.down"}, "line.4": {"text": " wi\u00dft, dort im Bergrevier,", "tokens": ["wi\u00dft", ",", "dort", "im", "Berg\u00b7re\u00b7vier", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": " da ist die Heimat mein,", "tokens": ["da", "ist", "die", "Hei\u00b7mat", "mein", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "PPOSAT", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": " th\u00fcringer Waldeszier,", "tokens": ["th\u00fc\u00b7rin\u00b7ger", "Wal\u00b7des\u00b7zier", ","], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.7": {"text": " treu denk ich dein!", "tokens": ["treu", "denk", "ich", "dein", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "PPOSAT", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "sowie:", "tokens": ["so\u00b7wie", ":"], "token_info": ["word", "punct"], "pos": ["KON", "$."], "meter": "-+", "measure": "iambic.single"}, "line.9": {"text": " am Rhein, da hab ich das Licht erblickt,", "tokens": ["am", "Rhein", ",", "da", "hab", "ich", "das", "Licht", "er\u00b7blickt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NE", "$,", "ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.10": {"text": " am Rhein, da wuchs ich heran,", "tokens": ["am", "Rhein", ",", "da", "wuchs", "ich", "he\u00b7ran", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NE", "$,", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.11": {"text": " am Rhein, da ist mir manch Streich gegl\u00fcckt \u2013", "tokens": ["am", "Rhein", ",", "da", "ist", "mir", "manch", "Streich", "ge\u00b7gl\u00fcckt", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NE", "$,", "ADV", "VAFIN", "PPER", "PIAT", "NN", "VVPP", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.7": {"line.1": {"text": " ich hab mein Herz in Heidelberg verloren,", "tokens": ["ich", "hab", "mein", "Herz", "in", "Hei\u00b7del\u00b7berg", "ver\u00b7lo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "APPR", "NE", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": " in einer lauen Sommernacht \u2013?", "tokens": ["in", "ei\u00b7ner", "lau\u00b7en", "Som\u00b7mer\u00b7nacht", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": " wer hat die liebe Gro\u00dfmama", "tokens": ["wer", "hat", "die", "lie\u00b7be", "Gro\u00df\u00b7ma\u00b7ma"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": " verkehrt rum aufs Klosett gesetzt?", "tokens": ["ver\u00b7kehrt", "rum", "aufs", "Klo\u00b7sett", "ge\u00b7setzt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "und:", "tokens": ["und", ":"], "token_info": ["word", "punct"], "pos": ["KON", "$."], "meter": "-", "measure": "single.down"}, "line.4": {"text": " das war bei Tante Trullala", "tokens": ["das", "war", "bei", "Tan\u00b7te", "Trul\u00b7la\u00b7la"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "APPR", "NN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": " in D\u00fcsseldorf am Rhein,", "tokens": ["in", "D\u00fcs\u00b7sel\u00b7dorf", "am", "Rhein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "APPRART", "NE", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": " da haben wir die Nacht verbracht", "tokens": ["da", "ha\u00b7ben", "wir", "die", "Nacht", "ver\u00b7bracht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": " voll Seligkeit beim Wein \u2013", "tokens": ["voll", "Se\u00b7lig\u00b7keit", "beim", "Wein", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "APPRART", "NN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": " sone ganze kleine Frau,", "tokens": ["so\u00b7ne", "gan\u00b7ze", "klei\u00b7ne", "Frau", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": " sone ganze kleine Frau \u2013", "tokens": ["so\u00b7ne", "gan\u00b7ze", "klei\u00b7ne", "Frau", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": " sone ganze, ganze, ganze, ganze", "tokens": ["so\u00b7ne", "gan\u00b7ze", ",", "gan\u00b7ze", ",", "gan\u00b7ze", ",", "gan\u00b7ze"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["KON", "ADJA", "$,", "ADJA", "$,", "ADJA", "$,", "ADJA"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": " ganze kleine Frau!", "tokens": ["gan\u00b7ze", "klei\u00b7ne", "Frau", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "ADJA", "NN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.5": {"text": "und:", "tokens": ["und", ":"], "token_info": ["word", "punct"], "pos": ["KON", "$."], "meter": "-", "measure": "single.down"}, "line.6": {"text": " wei\u00dft du, Mutterl, was mir tr\u00e4umt hat?", "tokens": ["wei\u00dft", "du", ",", "Mut\u00b7terl", ",", "was", "mir", "tr\u00e4umt", "hat", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "NN", "$,", "PWS", "PPER", "VVPP", "VAFIN", "$."], "meter": "+-++--+-", "measure": "trochaic.tetra.relaxed"}, "line.7": {"text": " i hab im Himmel die Engerln g'sehn . . .", "tokens": ["i", "hab", "im", "Him\u00b7mel", "die", "En\u00b7gerln", "g'\u00b7sehn", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["NE", "VAFIN", "APPRART", "NN", "ART", "NN", "VVINF", "$.", "$.", "$."], "meter": "+--+--++-+", "measure": "dactylic.di.plus"}, "line.8": {"text": "so singen wir mit nicht minder herber Kraft:", "tokens": ["so", "sin\u00b7gen", "wir", "mit", "nicht", "min\u00b7der", "her\u00b7ber", "Kraft", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "PTKNEG", "ADV", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": " schatz, was ich von dir getr\u00e4umt hab,", "tokens": ["schatz", ",", "was", "ich", "von", "dir", "ge\u00b7tr\u00e4umt", "hab", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PWS", "PPER", "APPR", "PPER", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.10": {"text": " h\u00e4tt ich dir so gern erz\u00e4hlt", "tokens": ["h\u00e4tt", "ich", "dir", "so", "gern", "er\u00b7z\u00e4hlt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PPER", "ADV", "ADV", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.11": {"text": "sowie:", "tokens": ["so\u00b7wie", ":"], "token_info": ["word", "punct"], "pos": ["KON", "$."], "meter": "-+", "measure": "iambic.single"}, "line.12": {"text": " valencia \u2013", "tokens": ["va\u00b7len\u00b7cia", "\u2013"], "token_info": ["word", "punct"], "pos": ["NE", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.13": {"text": " sieben, achte, neune, zehne,", "tokens": ["sie\u00b7ben", ",", "ach\u00b7te", ",", "neu\u00b7ne", ",", "zeh\u00b7ne", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["CARD", "$,", "ADJA", "$,", "ADJA", "$,", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.14": {"text": " bube, Dame, K\u00f6nig, As \u2013", "tokens": ["bu\u00b7be", ",", "Da\u00b7me", ",", "K\u00f6\u00b7nig", ",", "As", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "NN", "$,", "NN", "$,", "NE", "$("], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}