{"textgrid.poem.57298": {"metadata": {"author": {"name": "Klopstock, Friedrich Gottlieb", "birth": "N.A.", "death": "N.A."}, "title": "1L: Hoher Genuss der Sch\u00f6pfung, wenn wir, von des Denkens", "genre": "verse", "period": "N.A.", "pub_year": 1771, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Hoher Genuss der Sch\u00f6pfung, wenn wir, von des Denkens", "tokens": ["Ho\u00b7her", "Ge\u00b7nuss", "der", "Sch\u00f6p\u00b7fung", ",", "wenn", "wir", ",", "von", "des", "Den\u00b7kens"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADJA", "NN", "ART", "NN", "$,", "KOUS", "PPER", "$,", "APPR", "ART", "NN"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Feuer entflamt, sie empfinden, sie erblicken,", "tokens": ["Feu\u00b7er", "ent\u00b7flamt", ",", "sie", "emp\u00b7fin\u00b7den", ",", "sie", "er\u00b7bli\u00b7cken", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVPP", "$,", "PPER", "VVFIN", "$,", "PPER", "VVINF", "$,"], "meter": "+--+--+-+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "H\u00f6ren, staunen vor ihr, vor ihren", "tokens": ["H\u00f6\u00b7ren", ",", "stau\u00b7nen", "vor", "ihr", ",", "vor", "ih\u00b7ren"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "$,", "VVFIN", "APPR", "PPER", "$,", "APPR", "PPOSAT"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Bl\u00fcmchen, und Strassen des Lichts!", "tokens": ["Bl\u00fcm\u00b7chen", ",", "und", "Stras\u00b7sen", "des", "Lichts", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "NN", "ART", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.2": {"line.1": {"text": "Diesem Genuss' erhebt uns beynah, wer uns darstellt,", "tokens": ["Die\u00b7sem", "Ge\u00b7nuss'", "er\u00b7hebt", "uns", "bey\u00b7nah", ",", "wer", "uns", "dar\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "VVFIN", "PPER", "ADV", "$,", "PWS", "PPER", "VVFIN", "$,"], "meter": "+--+-+--+-+-+", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Sch\u00f6pfung, wie du dich dem Sinne, dich dem Geiste", "tokens": ["Sch\u00f6p\u00b7fung", ",", "wie", "du", "dich", "dem", "Sin\u00b7ne", ",", "dich", "dem", "Geis\u00b7te"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "PWAV", "PPER", "PRF", "ART", "NN", "$,", "PRF", "ART", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Offenbarest! wie du aus bittern", "tokens": ["Of\u00b7fen\u00b7ba\u00b7rest", "!", "wie", "du", "aus", "bit\u00b7tern"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "$.", "PWAV", "PPER", "APPR", "ADJA"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Quellen, aus s\u00fcssen uns str\u00f6mst!", "tokens": ["Quel\u00b7len", ",", "aus", "s\u00fcs\u00b7sen", "uns", "str\u00f6mst", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "APPR", "ADJA", "PPER", "VVFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.3": {"line.1": {"text": "Stellt ihr euch selbst Abwesendes dar: so geniesset", "tokens": ["Stellt", "ihr", "euch", "selbst", "Ab\u00b7we\u00b7sen\u00b7des", "dar", ":", "so", "ge\u00b7nies\u00b7set"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "PPER", "ADV", "NN", "PTKVZ", "$.", "ADV", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ihr es durch euch, wie's der Dichter zum Genuss' euch", "tokens": ["Ihr", "es", "durch", "euch", ",", "wie's", "der", "Dich\u00b7ter", "zum", "Ge\u00b7nuss'", "euch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "PPER", "APPR", "PPER", "$,", "KOUS", "ART", "NN", "APPRART", "NN", "PPER"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Gegenw\u00e4rtiget; doch so schnell l\u00e4sst", "tokens": ["Ge\u00b7gen\u00b7w\u00e4r\u00b7ti\u00b7get", ";", "doch", "so", "schnell", "l\u00e4sst"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "$.", "ADV", "ADV", "ADJD", "VVFIN"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Er nicht erscheinen, als ihr,", "tokens": ["Er", "nicht", "er\u00b7schei\u00b7nen", ",", "als", "ihr", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "PTKNEG", "VVINF", "$,", "KOUS", "PPER", "$,"], "meter": "-+-+-++", "measure": "unknown.measure.tetra"}}, "stanza.4": {"line.1": {"text": "Schweigende. O ihr wandelt nicht, fliegt! Doch wie strebet", "tokens": ["Schwei\u00b7gen\u00b7de", ".", "O", "ihr", "wan\u00b7delt", "nicht", ",", "fliegt", "!", "Doch", "wie", "stre\u00b7bet"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$.", "NE", "PPER", "VVFIN", "PTKNEG", "$,", "VVFIN", "$.", "KON", "PWAV", "VVFIN"], "meter": "+--+-+-+++-+-", "measure": "iambic.septa.invert"}, "line.2": {"text": "Er, euch zu nahn! denn er weiss es, dass der Lorber", "tokens": ["Er", ",", "euch", "zu", "nahn", "!", "denn", "er", "weiss", "es", ",", "dass", "der", "Lor\u00b7ber"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "$,", "PPER", "APPR", "ADJA", "$.", "KON", "PPER", "VVFIN", "PPER", "$,", "KOUS", "ART", "NN"], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "F\u00fcr den nahen allein fortgr\u00fcnet,", "tokens": ["F\u00fcr", "den", "na\u00b7hen", "al\u00b7lein", "fort\u00b7gr\u00fc\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "ADV", "VVFIN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Aber dem fernen verwelkt.", "tokens": ["A\u00b7ber", "dem", "fer\u00b7nen", "ver\u00b7welkt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "VVPP", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.5": {"line.1": {"text": "Denken wir recht; so lieben wir auch der Bemerker", "tokens": ["Den\u00b7ken", "wir", "recht", ";", "so", "lie\u00b7ben", "wir", "auch", "der", "Be\u00b7mer\u00b7ker"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "PPER", "ADJD", "$.", "ADV", "VVFIN", "PPER", "ADV", "ART", "NN"], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Wissenschaft, sie, die den Grundbau des Geschafnen", "tokens": ["Wis\u00b7sen\u00b7schaft", ",", "sie", ",", "die", "den", "Grund\u00b7bau", "des", "Ge\u00b7schaf\u00b7nen"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "PPER", "$,", "PRELS", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+---+-", "measure": "unknown.measure.penta"}, "line.3": {"text": "Gern ergr\u00fcbe; die Kraft, die Arten,", "tokens": ["Gern", "er\u00b7gr\u00fc\u00b7be", ";", "die", "Kraft", ",", "die", "Ar\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$.", "ART", "NN", "$,", "ART", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Jede Ver\u00e4nderung forscht.", "tokens": ["Je\u00b7de", "Ver\u00b7\u00e4n\u00b7de\u00b7rung", "forscht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVPP", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.6": {"line.1": {"text": "Selten nicht floss mir froher das Blut, wenn ich sahe,", "tokens": ["Sel\u00b7ten", "nicht", "floss", "mir", "fro\u00b7her", "das", "Blut", ",", "wenn", "ich", "sa\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "VVFIN", "PPER", "ADJD", "ART", "NN", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Stutzte, wie sie von dem Wesen des Geforschten", "tokens": ["Stutz\u00b7te", ",", "wie", "sie", "von", "dem", "We\u00b7sen", "des", "Ge\u00b7forschten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "PWAV", "PPER", "APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.3": {"text": "Dachten. Flogen sie irr; so hub doch", "tokens": ["Dach\u00b7ten", ".", "Flo\u00b7gen", "sie", "irr", ";", "so", "hub", "doch"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "$.", "NN", "PPER", "ADJD", "$.", "ADV", "VVFIN", "ADV"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Seele den k\u00fchneren Flug.", "tokens": ["See\u00b7le", "den", "k\u00fch\u00b7ne\u00b7ren", "Flug", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.7": {"line.1": {"text": "Vieles wird sonst durch Lehre bestimt, ist noch manche", "tokens": ["Vie\u00b7les", "wird", "sonst", "durch", "Leh\u00b7re", "be\u00b7stimt", ",", "ist", "noch", "man\u00b7che"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PIS", "VAFIN", "ADV", "APPR", "NN", "VVFIN", "$,", "VAFIN", "ADV", "PIAT"], "meter": "+-+--+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Wissenschaft, die das Gemeine des Erkanten", "tokens": ["Wis\u00b7sen\u00b7schaft", ",", "die", "das", "Ge\u00b7mei\u00b7ne", "des", "Er\u00b7kan\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$,", "PRELS", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Zeiget; hier sich verzeigt, dort gute", "tokens": ["Zei\u00b7get", ";", "hier", "sich", "ver\u00b7zeigt", ",", "dort", "gu\u00b7te"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "$.", "ADV", "PRF", "VVPP", "$,", "ADV", "ADJA"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Leiterin Suchenden ist.", "tokens": ["Lei\u00b7te\u00b7rin", "Su\u00b7chen\u00b7den", "ist", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.8": {"line.1": {"text": "Andres ist ganz dess Wissen und Thun, der erfindet:", "tokens": ["And\u00b7res", "ist", "ganz", "dess", "Wis\u00b7sen", "und", "Thun", ",", "der", "er\u00b7fin\u00b7det", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ADV", "ART", "NN", "KON", "NN", "$,", "PRELS", "VVFIN", "$."], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Was wir nicht sehn, durch das Wort so in des Lebens", "tokens": ["Was", "wir", "nicht", "sehn", ",", "durch", "das", "Wort", "so", "in", "des", "Le\u00b7bens"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "PTKNEG", "VVINF", "$,", "APPR", "ART", "NN", "ADV", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Glut, so wahr die Gestalt zu bilden,", "tokens": ["Glut", ",", "so", "wahr", "die", "Ge\u00b7stalt", "zu", "bil\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADV", "ADJD", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Dass es, als web' es vor uns!", "tokens": ["Dass", "es", ",", "als", "web'", "es", "vor", "uns", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "KOUS", "VVFIN", "PPER", "APPR", "PPER", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.9": {"line.1": {"text": "Wandelt der Schein. Noch dauret der Kampf um den Vorzug.", "tokens": ["Wan\u00b7delt", "der", "Schein", ".", "Noch", "dau\u00b7ret", "der", "Kampf", "um", "den", "Vor\u00b7zug", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$.", "ADV", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Hat ihn das Werk des Erfinders? des Bemerkers?", "tokens": ["Hat", "ihn", "das", "Werk", "des", "Er\u00b7fin\u00b7ders", "?", "des", "Be\u00b7mer\u00b7kers", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ART", "NN", "$.", "ART", "NN", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Ruh der Hand! auf den Weiser festen", "tokens": ["Ruh", "der", "Hand", "!", "auf", "den", "Wei\u00b7ser", "fes\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "$.", "APPR", "ART", "NN", "VVINF"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Blick; und es w\u00e4ge, wer will!", "tokens": ["Blick", ";", "und", "es", "w\u00e4\u00b7ge", ",", "wer", "will", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$.", "KON", "PPER", "VVFIN", "$,", "PWS", "VMFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.10": {"line.1": {"text": "Weich du von hier, der selbst nicht bemerkt, und nur nachspricht,", "tokens": ["Weich", "du", "von", "hier", ",", "der", "selbst", "nicht", "be\u00b7merkt", ",", "und", "nur", "nach\u00b7spricht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ADV", "$,", "PRELS", "ADV", "PTKNEG", "VVPP", "$,", "KON", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.2": {"text": "Eben darum, weil du diess nur, und nichts mehr thust,", "tokens": ["E\u00b7ben", "da\u00b7rum", ",", "weil", "du", "diess", "nur", ",", "und", "nichts", "mehr", "thust", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PAV", "$,", "KOUS", "PPER", "PDS", "ADV", "$,", "KON", "PIS", "ADV", "VVFIN", "$,"], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Aufschwillst, weich, du entweihst, und schwatzest", "tokens": ["Auf\u00b7schwillst", ",", "weich", ",", "du", "ent\u00b7weihst", ",", "und", "schwat\u00b7zest"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["NN", "$,", "ADJD", "$,", "PPER", "VVFIN", "$,", "KON", "VVFIN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "All dein Geschw\u00e4tz in den Wind!", "tokens": ["All", "dein", "Ge\u00b7schw\u00e4tz", "in", "den", "Wind", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "PPOSAT", "NN", "APPR", "ART", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.11": {"line.1": {"text": "Auch dein Geschw\u00e4tz von dem, was du nennest der Seele", "tokens": ["Auch", "dein", "Ge\u00b7schw\u00e4tz", "von", "dem", ",", "was", "du", "nen\u00b7nest", "der", "See\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "NN", "APPR", "ART", "$,", "PWS", "PPER", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Obere Kraft, was die untre, von erhabnern", "tokens": ["O\u00b7be\u00b7re", "Kraft", ",", "was", "die", "un\u00b7tre", ",", "von", "er\u00b7hab\u00b7nern"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "$,", "PRELS", "ART", "ADJA", "$,", "APPR", "VVINF"], "meter": "+--+--+-+-+-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Wissenschaften im Sand' auff\u00fchrend", "tokens": ["Wis\u00b7sen\u00b7schaf\u00b7ten", "im", "Sand'", "auf\u00b7f\u00fch\u00b7rend"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPRART", "NN", "VVPP"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Deiner Belehrung Geb\u00e4u.", "tokens": ["Dei\u00b7ner", "Be\u00b7leh\u00b7rung", "Ge\u00b7b\u00e4u", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.12": {"line.1": {"text": "Wirket vielleicht die Seele nicht ganz, wenn Gestalt sie", "tokens": ["Wir\u00b7ket", "viel\u00b7leicht", "die", "See\u00b7le", "nicht", "ganz", ",", "wenn", "Ge\u00b7stalt", "sie"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "ADV", "ART", "NN", "PTKNEG", "ADV", "$,", "KOUS", "NN", "PPER"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Schaffet, dass wir in dem Leben die Natur sehn?", "tokens": ["Schaf\u00b7fet", ",", "dass", "wir", "in", "dem", "Le\u00b7ben", "die", "Na\u00b7tur", "sehn", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "APPR", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Ganz nicht, wenn die Natur durchwandelnd,", "tokens": ["Ganz", "nicht", ",", "wenn", "die", "Na\u00b7tur", "durch\u00b7wan\u00b7delnd", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "$,", "KOUS", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bis in ihr Leben sie sieht?", "tokens": ["Bis", "in", "ihr", "Le\u00b7ben", "sie", "sieht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "PPOSAT", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.13": {"line.1": {"text": "\u00bbschweben wohl gar die Schalen dir gleich?\u00ab So verschiednes", "tokens": ["\u00bb", "schwe\u00b7ben", "wohl", "gar", "die", "Scha\u00b7len", "dir", "gleich", "?", "\u00ab", "So", "ver\u00b7schied\u00b7nes"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["$(", "VVFIN", "ADV", "ADV", "ART", "NN", "PPER", "ADV", "$.", "$(", "ADV", "ADJA"], "meter": "+--+-+-++--+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Schwebet nicht gleich. \u00bbUnd dich ahndet bey dem Hinschaun", "tokens": ["Schwe\u00b7bet", "nicht", "gleich", ".", "\u00bb", "Und", "dich", "ahn\u00b7det", "bey", "dem", "Hin\u00b7schaun"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PTKNEG", "ADV", "$.", "$(", "KON", "PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Nicht von Blendung?\u00ab Mich ahndet! denn ich", "tokens": ["Nicht", "von", "Blen\u00b7dung", "?", "\u00ab", "Mich", "ahn\u00b7det", "!", "denn", "ich"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word"], "pos": ["PTKNEG", "APPR", "NN", "$.", "$(", "PPER", "VVFIN", "$.", "KON", "PPER"], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Sagte ja: Schwebet nicht gleich.", "tokens": ["Sag\u00b7te", "ja", ":", "Schwe\u00b7bet", "nicht", "gleich", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "$.", "VVFIN", "PTKNEG", "ADV", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.14": {"line.1": {"text": "Aber es gilt, ich seh' es, es gilt, wie um's Leben!", "tokens": ["A\u00b7ber", "es", "gilt", ",", "ich", "seh'", "es", ",", "es", "gilt", ",", "wie", "um's", "Le\u00b7ben", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "$,", "PWAV", "NE", "NN", "$."], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "Seyd ihr gerecht? \u00bbO du bliebest wohl es selbst nicht,", "tokens": ["Seyd", "ihr", "ge\u00b7recht", "?", "\u00bb", "O", "du", "blie\u00b7best", "wohl", "es", "selbst", "nicht", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PPER", "ADJD", "$.", "$(", "NE", "NE", "VVFIN", "ADV", "PPER", "ADV", "PTKNEG", "$,"], "meter": "+--++-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Wenn, stets heisser im Streit, wir Sandkorn", "tokens": ["Wenn", ",", "stets", "heis\u00b7ser", "im", "Streit", ",", "wir", "Sand\u00b7korn"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "$,", "ADV", "ADJD", "APPRART", "NN", "$,", "PPER", "NN"], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Endlich auch w\u00f6gen, und Haar!\u00ab", "tokens": ["End\u00b7lich", "auch", "w\u00f6\u00b7gen", ",", "und", "Haar", "!", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ADV", "ADV", "VVINF", "$,", "KON", "NN", "$.", "$("], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.15": {"line.1": {"text": "Meint ihr? Da liegt noch eine vor euch von den ernsten", "tokens": ["Meint", "ihr", "?", "Da", "liegt", "noch", "ei\u00b7ne", "vor", "euch", "von", "den", "erns\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$.", "ADV", "VVFIN", "ADV", "ART", "APPR", "PPER", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Wagen! \u00bbUnd die?\u00ab Auch der Nutzen wird gewogen!", "tokens": ["Wa\u00b7gen", "!", "\u00bb", "Und", "die", "?", "\u00ab", "Auch", "der", "Nut\u00b7zen", "wird", "ge\u00b7wo\u00b7gen", "!"], "token_info": ["word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "$(", "KON", "ART", "$.", "$(", "ADV", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "An sich selbst, und zugleich: Oh l\u00e4ngrer", "tokens": ["An", "sich", "selbst", ",", "und", "zu\u00b7gleich", ":", "Oh", "l\u00e4ng\u00b7rer"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PRF", "ADV", "$,", "KON", "ADV", "$.", "XY", "XY"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Etwan auch gr\u00f6sserer sey?", "tokens": ["Et\u00b7wan", "auch", "gr\u00f6s\u00b7se\u00b7rer", "sey", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "VAFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}}}}