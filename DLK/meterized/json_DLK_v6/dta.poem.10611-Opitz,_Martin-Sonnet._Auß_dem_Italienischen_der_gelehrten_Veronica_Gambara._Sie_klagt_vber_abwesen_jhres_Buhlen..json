{"dta.poem.10611": {"metadata": {"author": {"name": "Opitz, Martin", "birth": "N.A.", "death": "N.A."}, "title": "Sonnet.  \n  Au\u00df dem Italienischen der gelehrten Veronica Gambara.  \n Sie klagt vber abwesen jhres Buhlen.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1624", "urn": "urn:nbn:de:kobv:b4-200905197859", "language": ["de:0.99"], "booktitle": "Opitz, Martin: Teutsche P\u00f6emata und: Aristarchvs Wieder die verachtung Teutscher Sprach. Stra\u00dfburg, 1624."}, "poem": {"stanza.1": {"line.1": {"text": "Wann die zwey Augen nicht sich eylend sehen liessen/", "tokens": ["Wann", "die", "zwey", "Au\u00b7gen", "nicht", "sich", "ey\u00b7lend", "se\u00b7hen", "lies\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "CARD", "NN", "PTKNEG", "PRF", "ADJD", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die mein Gem\u00fcth allein erquicken thun in Leidt/", "tokens": ["Die", "mein", "Ge\u00b7m\u00fcth", "al\u00b7lein", "er\u00b7qui\u00b7cken", "thun", "in", "Leidt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "ADV", "VVINF", "VVINF", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die mir in Angst vnd Noth verleyhen sicherheit/", "tokens": ["Die", "mir", "in", "Angst", "vnd", "Noth", "ver\u00b7ley\u00b7hen", "si\u00b7cher\u00b7heit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "KON", "NN", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "So w\u00fcrde/ f\u00f6rcht ich/ mir mein Leben weggerissen:", "tokens": ["So", "w\u00fcr\u00b7de", "/", "f\u00f6rcht", "ich", "/", "mir", "mein", "Le\u00b7ben", "weg\u00b7ge\u00b7ris\u00b7sen", ":"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$(", "VVFIN", "PPER", "$(", "PPER", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Eh werden alle B\u00e4ch ohn einen Tropffen fliessen/", "tokens": ["Eh", "wer\u00b7den", "al\u00b7le", "B\u00e4ch", "ohn", "ei\u00b7nen", "Tropf\u00b7fen", "flies\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PIAT", "NN", "APPR", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Eh wirdt die gantze Welt zu fallen sein bereit/", "tokens": ["Eh", "wirdt", "die", "gant\u00b7ze", "Welt", "zu", "fal\u00b7len", "sein", "be\u00b7reit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "PTKZU", "VVINF", "PPOSAT", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Eh wirdt des Himmels Lauff/ der Meister aller Zeit/", "tokens": ["Eh", "wirdt", "des", "Him\u00b7mels", "Lauff", "/", "der", "Meis\u00b7ter", "al\u00b7ler", "Zeit", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ART", "NN", "NN", "$(", "ART", "NN", "PIAT", "NN", "$("], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.8": {"text": "Wie Nebel/ Wind vnd Dampff im Rauch verschwinden m\u00fcssen.", "tokens": ["Wie", "Ne\u00b7bel", "/", "Wind", "vnd", "Dampff", "im", "Rauch", "ver\u00b7schwin\u00b7den", "m\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "$(", "NN", "KON", "NN", "APPRART", "NN", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Denn da\u00df ich ohne sie k\u00fcnt allzeit fr\u00f6lich leben/", "tokens": ["Denn", "da\u00df", "ich", "oh\u00b7ne", "sie", "k\u00fcnt", "all\u00b7zeit", "fr\u00f6\u00b7lich", "le\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "APPR", "PPER", "VVFIN", "ADV", "ADJD", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Sie seindt mein vffenthalt/ in jhnen lern ich eben", "tokens": ["Sie", "seindt", "mein", "vf\u00b7fent\u00b7halt", "/", "in", "jh\u00b7nen", "lern", "ich", "e\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPOSAT", "NN", "$(", "APPR", "PPER", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Des Himmels lauff vnd art/ als eine weise Fraw.", "tokens": ["Des", "Him\u00b7mels", "lauff", "vnd", "art", "/", "als", "ei\u00b7ne", "wei\u00b7se", "Fraw", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NN", "$(", "KOUS", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Ihr Sternen/ die jhr m\u00fcst vff vnser Leben sehen/", "tokens": ["Ihr", "Ster\u00b7nen", "/", "die", "jhr", "m\u00fcst", "vff", "vn\u00b7ser", "Le\u00b7ben", "se\u00b7hen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PRELS", "PPER", "VMFIN", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Wirdt es/ eh\u2019 ich zu euch verrei\u00df/ auch je geschehen/", "tokens": ["Wirdt", "es", "/", "eh'", "ich", "zu", "euch", "ver\u00b7rei\u00df", "/", "auch", "je", "ge\u00b7sche\u00b7hen", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "$(", "KOUS", "PPER", "APPR", "PPER", "VVFIN", "$(", "ADV", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Da\u00df ich jhn/ oder ja den letzten Todt anschaw?", "tokens": ["Da\u00df", "ich", "jhn", "/", "o\u00b7der", "ja", "den", "letz\u00b7ten", "Todt", "an\u00b7schaw", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "$(", "KON", "ADV", "ART", "ADJA", "NN", "NE", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}