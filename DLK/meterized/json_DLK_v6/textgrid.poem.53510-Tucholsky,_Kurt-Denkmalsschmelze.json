{"textgrid.poem.53510": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Denkmalsschmelze", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Da steht nun Gustav der Verstopfte,", "tokens": ["Da", "steht", "nun", "Gus\u00b7tav", "der", "Ver\u00b7stopf\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "NE", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "aus Eisengu\u00df, die Hand am Knauf.", "tokens": ["aus", "Ei\u00b7sen\u00b7gu\u00df", ",", "die", "Hand", "am", "Knauf", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "ART", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Jedwedes brave Herze klopfte", "tokens": ["Jed\u00b7we\u00b7des", "bra\u00b7ve", "Her\u00b7ze", "klopf\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["PIAT", "ADJA", "NN", "VVFIN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "und schlug zu jenem Standbild auf.", "tokens": ["und", "schlug", "zu", "je\u00b7nem", "Stand\u00b7bild", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PDAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Und da \u2013? Er wackelt auf dem Sockel,", "tokens": ["Und", "da", "\u2013", "?", "Er", "wa\u00b7ckelt", "auf", "dem", "So\u00b7ckel", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "$(", "$.", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "man gab ihm einen kr\u00e4ftigen Schub.", "tokens": ["man", "gab", "ihm", "ei\u00b7nen", "kr\u00e4f\u00b7ti\u00b7gen", "Schub", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Die Adler, seine Ruhmesgockel,", "tokens": ["Die", "Ad\u00b7ler", ",", "sei\u00b7ne", "Ruh\u00b7mes\u00b7go\u00b7ckel", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "das kommt nun alles hin zu Krupp.", "tokens": ["das", "kommt", "nun", "al\u00b7les", "hin", "zu", "Krupp", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "PIS", "ADV", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ein kleiner Hund ist der Entennte", "tokens": ["Ein", "klei\u00b7ner", "Hund", "ist", "der", "En\u00b7tenn\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ART", "NN"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "vermutlich br\u00fcderlich gesinnt.", "tokens": ["ver\u00b7mut\u00b7lich", "br\u00fc\u00b7der\u00b7lich", "ge\u00b7sinnt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er schnuppert an dem Postamente", "tokens": ["Er", "schnup\u00b7pert", "an", "dem", "Pos\u00b7ta\u00b7men\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "und hebt das Bein. Die Tr\u00e4ne rinnt.", "tokens": ["und", "hebt", "das", "Bein", ".", "Die", "Tr\u00e4\u00b7ne", "rinnt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "$.", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Doch pl\u00f6tzlich sieht sein Aug nach oben.", "tokens": ["Doch", "pl\u00f6tz\u00b7lich", "sieht", "sein", "Aug", "nach", "o\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVFIN", "PPOSAT", "NN", "APPR", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der F\u00fcrst ist weg! Wer wei\u00df da Rat?", "tokens": ["Der", "F\u00fcrst", "ist", "weg", "!", "Wer", "wei\u00df", "da", "Rat", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "$.", "PWS", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sein Hinterbein bleibt zwar erhoben,", "tokens": ["Sein", "Hin\u00b7ter\u00b7bein", "bleibt", "zwar", "er\u00b7ho\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "doch tut er nicht mehr, was er tat.", "tokens": ["doch", "tut", "er", "nicht", "mehr", ",", "was", "er", "tat", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "ADV", "$,", "PWS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Du kleiner Hund, sei nicht verwundert.", "tokens": ["Du", "klei\u00b7ner", "Hund", ",", "sei", "nicht", "ver\u00b7wun\u00b7dert", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$,", "VAFIN", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Man kanns verstehn. Du bist verdutzt.", "tokens": ["Man", "kanns", "ver\u00b7stehn", ".", "Du", "bist", "ver\u00b7dutzt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "VVINF", "$.", "PPER", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Denn seit dem Jahre Siebzehnhundert", "tokens": ["Denn", "seit", "dem", "Jah\u00b7re", "Sieb\u00b7zehn\u00b7hun\u00b7dert"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "NE"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "hat ER zum ersten Mal genutzt.", "tokens": ["hat", "Er", "zum", "ers\u00b7ten", "Mal", "ge\u00b7nutzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPRART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}