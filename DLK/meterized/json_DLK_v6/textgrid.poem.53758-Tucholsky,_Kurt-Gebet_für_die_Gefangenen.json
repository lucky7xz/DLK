{"textgrid.poem.53758": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Gebet f\u00fcr die Gefangenen", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Herrgott!", "tokens": ["Herr\u00b7gott", "!"], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.2": {"text": "Wenn du zuf\u00e4llig Zeit hast, dich zwischen zwei B\u00f6rsenbaissen", "tokens": ["Wenn", "du", "zu\u00b7f\u00e4l\u00b7lig", "Zeit", "hast", ",", "dich", "zwi\u00b7schen", "zwei", "B\u00f6r\u00b7sen\u00b7bais\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "NN", "VAFIN", "$,", "PRF", "APPR", "CARD", "NN"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.3": {"text": "und einer d\u00e4mlichen Feldschlacht in Marokko auch einmal um", "tokens": ["und", "ei\u00b7ner", "d\u00e4m\u00b7li\u00b7chen", "Feld\u00b7schlacht", "in", "Ma\u00b7rok\u00b7ko", "auch", "ein\u00b7mal", "um"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "APPR", "NE", "ADV", "ADV", "APPR"], "meter": "-+-+--+-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "die Armen zu k\u00fcmmern:", "tokens": ["die", "Ar\u00b7men", "zu", "k\u00fcm\u00b7mern", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "H\u00f6rst du siebentausend Kommunisten in deutschen Gef\u00e4ngnissen", "tokens": ["H\u00f6rst", "du", "sie\u00b7ben\u00b7tau\u00b7send", "Kom\u00b7mu\u00b7nis\u00b7ten", "in", "deut\u00b7schen", "Ge\u00b7f\u00e4ng\u00b7nis\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "CARD", "NN", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-+--+--+--", "measure": "trochaic.septa.relaxed"}, "line.6": {"text": "wimmern?", "tokens": ["wim\u00b7mern", "?"], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "+-", "measure": "trochaic.single"}, "line.7": {"text": "Kyrie eleison \u2013!", "tokens": ["Ky\u00b7rie", "e\u00b7lei\u00b7son", "\u2013", "!"], "token_info": ["word", "word", "punct", "punct"], "pos": ["NE", "NE", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "Da sind arme Jungen darunter, die sind so mitgelaufen,", "tokens": ["Da", "sind", "ar\u00b7me", "Jun\u00b7gen", "da\u00b7run\u00b7ter", ",", "die", "sind", "so", "mit\u00b7ge\u00b7lau\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADJA", "NN", "PAV", "$,", "PRELS", "VAFIN", "ADV", "VVPP", "$,"], "meter": "--+-+--+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "und nun sind sie den Richtern in die Finger gefallen;", "tokens": ["und", "nun", "sind", "sie", "den", "Rich\u00b7tern", "in", "die", "Fin\u00b7ger", "ge\u00b7fal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "auf sie ist der Polizeikn\u00fcppel niedergesaust,", "tokens": ["auf", "sie", "ist", "der", "Po\u00b7li\u00b7zei\u00b7kn\u00fcp\u00b7pel", "nie\u00b7der\u00b7ge\u00b7saust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+--+--+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "der da ewiglich h\u00e4ngt \u00fcber uns allen . . .", "tokens": ["der", "da", "e\u00b7wig\u00b7lich", "h\u00e4ngt", "\u00fc\u00b7ber", "uns", "al\u00b7len", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "ADV", "ADV", "VVFIN", "APPR", "PPER", "PIAT", "$.", "$.", "$."], "meter": "--+--++--+-", "measure": "anapaest.di.plus"}, "line.5": {"text": "Kyrie eleison \u2013!", "tokens": ["Ky\u00b7rie", "e\u00b7lei\u00b7son", "\u2013", "!"], "token_info": ["word", "word", "punct", "punct"], "pos": ["NE", "NE", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Da sind aber auch alte Kerls dabei, die hatten \u00dcberzeugung,", "tokens": ["Da", "sind", "a\u00b7ber", "auch", "al\u00b7te", "Kerls", "da\u00b7bei", ",", "die", "hat\u00b7ten", "\u00dc\u00b7berz\u00b7eu\u00b7gung", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "ADJA", "NN", "PAV", "$,", "PRELS", "VAFIN", "NN", "$,"], "meter": "--+--+-+-+-+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Herz und Mut \u2013", "tokens": ["Herz", "und", "Mut", "\u2013"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.3": {"text": "das ist aber vor diesen Richtern nicht beliebt,", "tokens": ["das", "ist", "a\u00b7ber", "vor", "die\u00b7sen", "Rich\u00b7tern", "nicht", "be\u00b7liebt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "APPR", "PDAT", "NN", "PTKNEG", "ADJD", "$,"], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "und das bekam ihnen nicht gut . . .", "tokens": ["und", "das", "be\u00b7kam", "ih\u00b7nen", "nicht", "gut", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "PDS", "VVFIN", "PPER", "PTKNEG", "ADJD", "$.", "$.", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Kyrie eleison \u2013!", "tokens": ["Ky\u00b7rie", "e\u00b7lei\u00b7son", "\u2013", "!"], "token_info": ["word", "word", "punct", "punct"], "pos": ["NE", "NE", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.4": {"line.1": {"text": "Da haben auch manche geglaubt, eine Republik zu sch\u00fctzen \u2013", "tokens": ["Da", "ha\u00b7ben", "auch", "man\u00b7che", "ge\u00b7glaubt", ",", "ei\u00b7ne", "Re\u00b7pub\u00b7lik", "zu", "sch\u00fct\u00b7zen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "PIS", "VVPP", "$,", "ART", "NN", "PTKZU", "VVINF", "$("], "meter": "-+--+--+--+-+-+-", "measure": "amphibrach.tetra.plus"}, "line.2": {"text": "aber die hat das gar nicht gewollt.", "tokens": ["a\u00b7ber", "die", "hat", "das", "gar", "nicht", "ge\u00b7wollt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "VAFIN", "PDS", "ADV", "PTKNEG", "VMPP", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.3": {"text": "Fritz Ebert hatte vor seinen Freunden viel mehr Angst", "tokens": ["Fritz", "E\u00b7bert", "hat\u00b7te", "vor", "sei\u00b7nen", "Freun\u00b7den", "viel", "mehr", "Angst"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "VAFIN", "APPR", "PPOSAT", "NN", "ADV", "PIAT", "NN"], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "als vor seinen Feinden \u2013 in diesem Sinne: Schwarz-Rot-Gold!", "tokens": ["als", "vor", "sei\u00b7nen", "Fein\u00b7den", "\u2013", "in", "die\u00b7sem", "Sin\u00b7ne", ":", "Schwa\u00b7rz\u00b7Rot\u00b7Gold", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KOUS", "APPR", "PPOSAT", "NN", "$(", "APPR", "PDAT", "NN", "$.", "NE", "$."], "meter": "+-+-+--+-+-+-++", "measure": "trochaic.octa.plus.relaxed"}, "line.5": {"text": "Kyrie eleison \u2013!", "tokens": ["Ky\u00b7rie", "e\u00b7lei\u00b7son", "\u2013", "!"], "token_info": ["word", "word", "punct", "punct"], "pos": ["NE", "NE", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.5": {"line.1": {"text": "Herrgott! Sie sitzen seit Jahren in kleinen Stuben", "tokens": ["Herr\u00b7gott", "!", "Sie", "sit\u00b7zen", "seit", "Jah\u00b7ren", "in", "klei\u00b7nen", "Stu\u00b7ben"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "PPER", "VVFIN", "APPR", "NN", "APPR", "ADJA", "NN"], "meter": "+--+--+--+-+-", "measure": "dactylic.tri.plus"}, "line.2": {"text": "und sind krank, bla\u00df und ohne Fraun;", "tokens": ["und", "sind", "krank", ",", "bla\u00df", "und", "oh\u00b7ne", "Fraun", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "$,", "ADJD", "KON", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "sie werden von Herrn Aufseher Maschke schikaniert und", "tokens": ["sie", "wer\u00b7den", "von", "Herrn", "Auf\u00b7se\u00b7her", "Maschke", "schi\u00b7ka\u00b7niert", "und"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "NN", "ADJA", "NN", "VVFIN", "KON"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "angebr\u00fcllt,", "tokens": ["an\u00b7ge\u00b7br\u00fcllt", ","], "token_info": ["word", "punct"], "pos": ["VVPP", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "in den Keller geschickt und mitunter verhaun . . .", "tokens": ["in", "den", "Kel\u00b7ler", "ge\u00b7schickt", "und", "mi\u00b7tun\u00b7ter", "ver\u00b7haun", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ART", "NN", "VVPP", "KON", "ADV", "VVINF", "$.", "$.", "$."], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}, "line.6": {"text": "Kyrie eleison \u2013!", "tokens": ["Ky\u00b7rie", "e\u00b7lei\u00b7son", "\u2013", "!"], "token_info": ["word", "word", "punct", "punct"], "pos": ["NE", "NE", "$(", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}}, "stanza.6": {"line.1": {"text": "Manche haben eine Spinne, die ist ihr Freund;", "tokens": ["Man\u00b7che", "ha\u00b7ben", "ei\u00b7ne", "Spin\u00b7ne", ",", "die", "ist", "ihr", "Freund", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "ART", "NN", "$,", "PRELS", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+-+-+-+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "viele sind verzankt, alle verzweifelt und sehnsuchtskrank \u2013", "tokens": ["vie\u00b7le", "sind", "ver\u00b7zankt", ",", "al\u00b7le", "ver\u00b7zwei\u00b7felt", "und", "sehn\u00b7suchts\u00b7krank", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VAFIN", "VVPP", "$,", "PIS", "VVPP", "KON", "VVFIN", "$("], "meter": "+-+-+---+--+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Ein Tag, du G\u00fctiger, ist mitunter tausend Jahr lang!", "tokens": ["Ein", "Tag", ",", "du", "G\u00fc\u00b7ti\u00b7ger", ",", "ist", "mi\u00b7tun\u00b7ter", "tau\u00b7send", "Jahr", "lang", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PPER", "NN", "$,", "VAFIN", "ADV", "CARD", "NN", "ADJD", "$."], "meter": "-+-+--+-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Kyrie . . .", "tokens": ["Ky\u00b7rie", ".", ".", "."], "token_info": ["word", "punct", "punct", "punct"], "pos": ["NE", "$.", "$.", "$."], "meter": "+-", "measure": "trochaic.single"}}, "stanza.7": {"line.1": {"text": "Vielleicht hast du die Freundlichkeit und guckst einmal", "tokens": ["Viel\u00b7leicht", "hast", "du", "die", "Freund\u00b7lich\u00b7keit", "und", "guckst", "ein\u00b7mal"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "KON", "VVFIN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "ins Neue Testament?", "tokens": ["ins", "Neu\u00b7e", "Tes\u00b7ta\u00b7ment", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Bei uns lesen das die Pastoren, aber nur sonntags \u2013,", "tokens": ["Bei", "uns", "le\u00b7sen", "das", "die", "Pas\u00b7to\u00b7ren", ",", "a\u00b7ber", "nur", "sonn\u00b7tags", "\u2013", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "VVFIN", "ART", "ART", "NN", "$,", "ADV", "ADV", "ADV", "$(", "$,"], "meter": "--+--+-+-+-++-", "measure": "anapaest.di.plus"}, "line.4": {"text": "in der Woche regiert das Strafgesetzbuch und der Landgerichts-", "tokens": ["in", "der", "Wo\u00b7che", "re\u00b7giert", "das", "Straf\u00b7ge\u00b7setz\u00b7buch", "und", "der", "Land\u00b7ge\u00b7richts"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVFIN", "ART", "NN", "KON", "ART", "TRUNC"], "meter": "+-+--+-+-+-+-+-+", "measure": "trochaic.octa.plus.relaxed"}, "line.5": {"text": "pr\u00e4sident.", "tokens": ["pr\u00e4\u00b7si\u00b7dent", "."], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "+--", "measure": "dactylic.init"}, "line.6": {"text": " . . . . eleison \u2013!", "tokens": [".", ".", ".", ".", "e\u00b7lei\u00b7son", "\u2013", "!"], "token_info": ["punct", "punct", "punct", "punct", "word", "punct", "punct"], "pos": ["$.", "$.", "$.", "$.", "XY", "$(", "$."], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.8": {"line.1": {"text": "Wei\u00dft du vielleicht, lieber Gott, warum diese Siebentausend", "tokens": ["Wei\u00dft", "du", "viel\u00b7leicht", ",", "lie\u00b7ber", "Gott", ",", "wa\u00b7rum", "die\u00b7se", "Sie\u00b7ben\u00b7tau\u00b7send"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "$,", "ADV", "NN", "$,", "PWAV", "PDAT", "NN"], "meter": "+-+-+-+----+-+-", "measure": "unknown.measure.hexa"}, "line.2": {"text": "in deutsche Gef\u00e4ngnisse kamen?", "tokens": ["in", "deut\u00b7sche", "Ge\u00b7f\u00e4ng\u00b7nis\u00b7se", "ka\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Ich wei\u00df es. Aber ich sags nicht. Du kannst dirs ja denken.", "tokens": ["Ich", "wei\u00df", "es", ".", "A\u00b7ber", "ich", "sags", "nicht", ".", "Du", "kannst", "dirs", "ja", "den\u00b7ken", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$.", "KON", "PPER", "ADV", "PTKNEG", "$.", "PPER", "VMFIN", "PIS", "ADV", "VVINF", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Amen.", "tokens": ["A\u00b7men", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-", "measure": "trochaic.single"}}}}}