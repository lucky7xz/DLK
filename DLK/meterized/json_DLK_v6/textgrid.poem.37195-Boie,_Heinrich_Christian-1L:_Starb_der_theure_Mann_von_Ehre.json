{"textgrid.poem.37195": {"metadata": {"author": {"name": "Boie, Heinrich Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Starb der theure Mann von Ehre", "genre": "verse", "period": "N.A.", "pub_year": 1775, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Starb der theure Mann von Ehre", "tokens": ["Starb", "der", "theu\u00b7re", "Mann", "von", "Eh\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "ADJA", "NN", "APPR", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Starb der Herr von Schafskopf doch!", "tokens": ["Starb", "der", "Herr", "von", "Schafs\u00b7kopf", "doch", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "APPR", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Traun er lebt und schriebe noch,", "tokens": ["Traun", "er", "lebt", "und", "schrie\u00b7be", "noch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "KON", "VVFIN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wenn er nicht gestorben w\u00e4re!", "tokens": ["Wenn", "er", "nicht", "ge\u00b7stor\u00b7ben", "w\u00e4\u00b7re", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Staunen mu\u00dften selbst Minister,", "tokens": ["Stau\u00b7nen", "mu\u00df\u00b7ten", "selbst", "Mi\u00b7nis\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ADV", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gab er ihnen Lehr und Rath.", "tokens": ["Gab", "er", "ih\u00b7nen", "Lehr", "und", "Rath", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wo er falsch gewei\u00dfagt hat,", "tokens": ["Wo", "er", "falsch", "ge\u00b7wei\u00b7\u00dfagt", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADJD", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "War die Zukunft ihm zu d\u00fcster.", "tokens": ["War", "die", "Zu\u00b7kunft", "ihm", "zu", "d\u00fcs\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PPER", "PTKA", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Lob der Gro\u00dfen war der Angel", "tokens": ["Lob", "der", "Gro\u00b7\u00dfen", "war", "der", "An\u00b7gel"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Den er aus nach Gelde warf.", "tokens": ["Den", "er", "aus", "nach", "Gel\u00b7de", "warf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "APPR", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Hatt' er, was ein Mensch bedarf,", "tokens": ["Hatt'", "er", ",", "was", "ein", "Mensch", "be\u00b7darf", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Keines Dings dann sp\u00fcrt' er Mangel.", "tokens": ["Kei\u00b7nes", "Dings", "dann", "sp\u00fcrt'", "er", "Man\u00b7gel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "ADV", "VVFIN", "PPER", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "That er von den Gr\u00e4ueln schreiben,", "tokens": ["That", "er", "von", "den", "Gr\u00e4u\u00b7eln", "schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aufruhr, Propagand' und Mord,", "tokens": ["Auf\u00b7ruhr", ",", "Pro\u00b7pa\u00b7gand'", "und", "Mord", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Immer war sein kluges Wort:", "tokens": ["Im\u00b7mer", "war", "sein", "klu\u00b7ges", "Wort", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wahrlich das kann so nicht bleiben!", "tokens": ["Wahr\u00b7lich", "das", "kann", "so", "nicht", "blei\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VMFIN", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}}, "stanza.5": {"line.1": {"text": "Tr\u00fcbt ihm Frankreichs Sieg die Stunden,", "tokens": ["Tr\u00fcbt", "ihm", "Fran\u00b7kreichs", "Sieg", "die", "Stun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJA", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Gerne fuhr er aus aufs Land.", "tokens": ["Ger\u00b7ne", "fuhr", "er", "aus", "aufs", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "APPRART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Niemals wenn er dort sich fand,", "tokens": ["Nie\u00b7mals", "wenn", "er", "dort", "sich", "fand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "PPER", "ADV", "PRF", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ward er in der Stadt gefunden.", "tokens": ["Ward", "er", "in", "der", "Stadt", "ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Auf dem Land auch nimmer m\u00fc\u00dfig", "tokens": ["Auf", "dem", "Land", "auch", "nim\u00b7mer", "m\u00fc\u00b7\u00dfig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ADV", "ADV", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schrieb er selbst sich manchen Brief.", "tokens": ["Schrieb", "er", "selbst", "sich", "man\u00b7chen", "Brief", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PRF", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "War das Schreiben apokryph,", "tokens": ["War", "das", "Schrei\u00b7ben", "a\u00b7pok\u00b7ry\u00b7ph", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "NE", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "So war Antwort \u00fcberfl\u00fc\u00dfig.", "tokens": ["So", "war", "Ant\u00b7wort", "\u00fc\u00b7berf\u00b7l\u00fc\u00b7\u00dfig", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "ADJD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "F\u00fcrsten die ihm hold gewesen,", "tokens": ["F\u00fcrs\u00b7ten", "die", "ihm", "hold", "ge\u00b7we\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "PPER", "ADJD", "VAPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schrieb er noch, eh er entschlief.", "tokens": ["Schrieb", "er", "noch", ",", "eh", "er", "ent\u00b7schlief", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "$,", "KOUS", "PPER", "VVFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "Schrieb' er mehr im n\u00e4chsten Brief,", "tokens": ["Schrieb'", "er", "mehr", "im", "n\u00e4chs\u00b7ten", "Brief", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "APPRART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Mehr auch h\u00e4tten sie gelesen.", "tokens": ["Mehr", "auch", "h\u00e4t\u00b7ten", "sie", "ge\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "PPER", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Dienerhaft und unerth\u00e4nig", "tokens": ["Die\u00b7ner\u00b7haft", "und", "un\u00b7er\u00b7th\u00e4\u00b7nig"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Trieb er mit der Wahrheit Spiel.", "tokens": ["Trieb", "er", "mit", "der", "Wahr\u00b7heit", "Spiel", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Sezt er eine Null zu viel,", "tokens": ["Sezt", "er", "ei\u00b7ne", "Null", "zu", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "PIS", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "So war keine Null zu wenig.", "tokens": ["So", "war", "kei\u00b7ne", "Null", "zu", "we\u00b7nig", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIAT", "NN", "APPR", "PIS", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Sonder Grund ist man verwundert,", "tokens": ["Son\u00b7der", "Grund", "ist", "man", "ver\u00b7wun\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "PIS", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df sich Rum erschrieb der Tropf.", "tokens": ["Da\u00df", "sich", "Rum", "er\u00b7schrieb", "der", "Tropf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "NE", "VVFIN", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Hatt' er Ruhm, so hatt' er Kopf:", "tokens": ["Hatt'", "er", "Ruhm", ",", "so", "hatt'", "er", "Kopf", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "NN", "$,", "ADV", "VAFIN", "PPER", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wett' ich sieben gegen hundert.", "tokens": ["Wett'", "ich", "sie\u00b7ben", "ge\u00b7gen", "hun\u00b7dert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PRF", "APPR", "CARD", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Was er druckte, Text und Noten,", "tokens": ["Was", "er", "druck\u00b7te", ",", "Text", "und", "No\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Machte manchem Schafskopf Spass.", "tokens": ["Mach\u00b7te", "man\u00b7chem", "Schafs\u00b7kopf", "Spass", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenns der Censor gerne las,", "tokens": ["Wenns", "der", "Cen\u00b7sor", "ger\u00b7ne", "las", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wards vom Censor nie verboten.", "tokens": ["Wards", "vom", "Cen\u00b7sor", "nie", "ver\u00b7bo\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPRART", "NN", "ADV", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Seines Kopfes Widersacher", "tokens": ["Sei\u00b7nes", "Kop\u00b7fes", "Wi\u00b7der\u00b7sa\u00b7cher"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Tauft' er zu Rebellen um.", "tokens": ["Tauft'", "er", "zu", "Re\u00b7bel\u00b7len", "um", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "NN", "PTKVZ", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.3": {"text": "Ward wer lachen konnte stumm,", "tokens": ["Ward", "wer", "la\u00b7chen", "konn\u00b7te", "stumm", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PWS", "VVINF", "VMFIN", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Minder wurden dann die Lacher.", "tokens": ["Min\u00b7der", "wur\u00b7den", "dann", "die", "La\u00b7cher", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "ART", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.12": {"line.1": {"text": "Guten Wein und gutes E\u00dfen", "tokens": ["Gu\u00b7ten", "Wein", "und", "gu\u00b7tes", "E\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "KON", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Fodert' er f\u00fcr seinen Tisch;", "tokens": ["Fo\u00b7dert'", "er", "f\u00fcr", "sei\u00b7nen", "Tisch", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Bei Pasteten Austern Fisch", "tokens": ["Bei", "Pas\u00b7te\u00b7ten", "Aus\u00b7tern", "Fisch"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "NN", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Kont er Gr\u00fctz und Wurst verge\u00dfen.", "tokens": ["Kont", "er", "Gr\u00fctz", "und", "Wurst", "ver\u00b7ge\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "NN", "KON", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Welche Speise bl\u00e4h und stopfe,", "tokens": ["Wel\u00b7che", "Spei\u00b7se", "bl\u00e4h", "und", "stop\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAT", "NN", "ADJD", "KON", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wu\u00dft er auf ein H\u00e4rchen auch.", "tokens": ["Wu\u00dft", "er", "auf", "ein", "H\u00e4r\u00b7chen", "auch", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "ART", "NN", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wars verstopft in seinem Bauch,", "tokens": ["Wars", "ver\u00b7stopft", "in", "sei\u00b7nem", "Bauch", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Dann wars nicht allein im Kopfe.", "tokens": ["Dann", "wars", "nicht", "al\u00b7lein", "im", "Kop\u00b7fe", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "ADV", "APPRART", "NN", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.14": {"line.1": {"text": "Arznei in kleinen Prisen", "tokens": ["Arz\u00b7nei", "in", "klei\u00b7nen", "Pri\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Nahm er selten nur und kurz.", "tokens": ["Nahm", "er", "sel\u00b7ten", "nur", "und", "kurz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADJD", "ADV", "KON", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Schnupft er \u00f6fter Niesewurz,", "tokens": ["Schnupft", "er", "\u00f6f\u00b7ter", "Nie\u00b7se\u00b7wurz", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "NE", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Oefter kam er dann zum niesen.", "tokens": ["Oef\u00b7ter", "kam", "er", "dann", "zum", "nie\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPRART", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Weint und klagt, obgleich vergebens!", "tokens": ["Weint", "und", "klagt", ",", "ob\u00b7gleich", "ver\u00b7ge\u00b7bens", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "$,", "KOUS", "ADV", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Klagt und weinet, wer es mag!", "tokens": ["Klagt", "und", "wei\u00b7net", ",", "wer", "es", "mag", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "KON", "VVFIN", "$,", "PWS", "PPER", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Seines Todes erster Tag", "tokens": ["Sei\u00b7nes", "To\u00b7des", "ers\u00b7ter", "Tag"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "War der lezte seines Lebens.", "tokens": ["War", "der", "lez\u00b7te", "sei\u00b7nes", "Le\u00b7bens", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "PPOSAT", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}