{"textgrid.poem.53061": {"metadata": {"author": {"name": "Dach, Simon", "birth": "N.A.", "death": "N.A."}, "title": "Martin Reggius und Anna Waldeck", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Freund des Himmels vnd des allen", "tokens": ["Freund", "des", "Him\u00b7mels", "vnd", "des", "al\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "KON", "ART", "PIAT"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was dem Himmel mag gefallen,", "tokens": ["Was", "dem", "Him\u00b7mel", "mag", "ge\u00b7fal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "VMFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Hertzlich sind wir zwar betr\u00fcbt,", "tokens": ["Hertz\u00b7lich", "sind", "wir", "zwar", "be\u00b7tr\u00fcbt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Da\u00df du von vns weg bist kommen,", "tokens": ["Da\u00df", "du", "von", "vns", "weg", "bist", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "PPER", "ADV", "VAFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Vnd hast eine dir genommen,", "tokens": ["Vnd", "hast", "ei\u00b7ne", "dir", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "PPER", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Der du vnd die dir beliebt.", "tokens": ["Der", "du", "vnd", "die", "dir", "be\u00b7liebt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "KON", "ART", "PPER", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Nicht da\u00df wir dich solten neiden,", "tokens": ["Nicht", "da\u00df", "wir", "dich", "sol\u00b7ten", "nei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "KOUS", "PPER", "PRF", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df der Liebe s\u00fcsses Leiden", "tokens": ["Da\u00df", "der", "Lie\u00b7be", "s\u00fcs\u00b7ses", "Lei\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Numehr durch ein zartes Bildt,", "tokens": ["Nu\u00b7mehr", "durch", "ein", "zar\u00b7tes", "Bildt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Welches du zu diesen Dingen", "tokens": ["Wel\u00b7ches", "du", "zu", "die\u00b7sen", "Din\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "PDAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Dir lesst an die Seiten bringen,", "tokens": ["Dir", "lesst", "an", "die", "Sei\u00b7ten", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Wird vermehrt vnd bald gestillt.", "tokens": ["Wird", "ver\u00b7mehrt", "vnd", "bald", "ge\u00b7stillt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "KON", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Nein, wir w\u00fcnschen noch von Hertzen,", "tokens": ["Nein", ",", "wir", "w\u00fcn\u00b7schen", "noch", "von", "Hert\u00b7zen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "ADV", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df Ihr also m\u00f6get schertzen,", "tokens": ["Da\u00df", "Ihr", "al\u00b7so", "m\u00f6\u00b7get", "schert\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df von euch, Ihr frisches Paar,", "tokens": ["Da\u00df", "von", "euch", ",", "Ihr", "fri\u00b7sches", "Paar", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "PPER", "$,", "PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Was in diesem newen Leben", "tokens": ["Was", "in", "die\u00b7sem", "ne\u00b7wen", "Le\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "APPR", "PDAT", "ADJA", "NN"], "meter": "---+--+-", "measure": "iambic.di.relaxed"}, "line.5": {"text": "Ihr euch beyde werdet geben,", "tokens": ["Ihr", "euch", "bey\u00b7de", "wer\u00b7det", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PPER", "PIS", "VAFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Zeugen m\u00f6g ein jedes Jahr.", "tokens": ["Zeu\u00b7gen", "m\u00f6g", "ein", "je\u00b7des", "Jahr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "ART", "PIAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Sondern warumm wir vns kr\u00e4ncken", "tokens": ["Son\u00b7dern", "wa\u00b7rumm", "wir", "vns", "kr\u00e4n\u00b7cken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "PPER", "PRF", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist, da\u00df wir daran gedencken,", "tokens": ["Ist", ",", "da\u00df", "wir", "da\u00b7ran", "ge\u00b7den\u00b7cken", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "PAV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie wir nun so lange Zeit", "tokens": ["Wie", "wir", "nun", "so", "lan\u00b7ge", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "ADV", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Immer sind gefunden worden", "tokens": ["Im\u00b7mer", "sind", "ge\u00b7fun\u00b7den", "wor\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "VVPP", "VAPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "In der trewen Freundschafft Orden,", "tokens": ["In", "der", "tre\u00b7wen", "Freund\u00b7schafft", "Or\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Vnd in rechter Trawligkeit.", "tokens": ["Vnd", "in", "rech\u00b7ter", "Traw\u00b7lig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Wie da\u00df doch der Menschen Sinnen", "tokens": ["Wie", "da\u00df", "doch", "der", "Men\u00b7schen", "Sin\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOKOM", "KOUS", "ADV", "ART", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sich so k\u00f6nnen lieb gewinnen,", "tokens": ["Sich", "so", "k\u00f6n\u00b7nen", "lieb", "ge\u00b7win\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "ADV", "VMFIN", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Da\u00df sie sind wie einverleibt,", "tokens": ["Da\u00df", "sie", "sind", "wie", "ein\u00b7ver\u00b7leibt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VAFIN", "KOKOM", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Sich so gantz genaw vmbfassen", "tokens": ["Sich", "so", "gantz", "ge\u00b7naw", "vmb\u00b7fas\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PRF", "ADV", "ADV", "ADJD", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Vnd nicht gerne sich verlassen,", "tokens": ["Vnd", "nicht", "ger\u00b7ne", "sich", "ver\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ADV", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Keins nicht von dem andern bleibt?", "tokens": ["Keins", "nicht", "von", "dem", "an\u00b7dern", "bleibt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKNEG", "APPR", "ART", "ADJA", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Die\u00df hat Theseus gezwungen,", "tokens": ["Die\u00df", "hat", "The\u00b7seus", "ge\u00b7zwun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NE", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Da\u00df er vngeschewt gedrungen", "tokens": ["Da\u00df", "er", "vn\u00b7ge\u00b7schewt", "ge\u00b7drun\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "VVPP"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Durch der finstern H\u00f6llen Pfort,", "tokens": ["Durch", "der", "fins\u00b7tern", "H\u00f6l\u00b7len", "Pfort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Dies hat Diomedt gemachet,", "tokens": ["Dies", "hat", "Dio\u00b7medt", "ge\u00b7ma\u00b7chet", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NE", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.5": {"text": "Da\u00df er die Gefahr verlachet,", "tokens": ["Da\u00df", "er", "die", "Ge\u00b7fahr", "ver\u00b7la\u00b7chet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Vnd den Feind bey Nacht ermordt.", "tokens": ["Vnd", "den", "Feind", "bey", "Nacht", "er\u00b7mordt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "APPR", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Freundschafft, die was weiter gehet", "tokens": ["Freund\u00b7schafft", ",", "die", "was", "wei\u00b7ter", "ge\u00b7het"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["NN", "$,", "PRELS", "PWS", "ADV", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Vnd nur nicht in Worten stehet,", "tokens": ["Vnd", "nur", "nicht", "in", "Wor\u00b7ten", "ste\u00b7het", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "APPR", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "W\u00fcnscht ein stets beysammensein,", "tokens": ["W\u00fcnscht", "ein", "stets", "bey\u00b7sam\u00b7men\u00b7sein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADV", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Ist beharrlich in den N\u00f6hten,", "tokens": ["Ist", "be\u00b7harr\u00b7lich", "in", "den", "N\u00f6h\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Lesset sich viel lieber t\u00f6dten,", "tokens": ["Les\u00b7set", "sich", "viel", "lie\u00b7ber", "t\u00f6d\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADV", "ADV", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Al\u00df sich trennen Noth vnd Pein.", "tokens": ["Al\u00df", "sich", "tren\u00b7nen", "Noth", "vnd", "Pein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ADJA", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Dieses ist, worumb wir eben", "tokens": ["Die\u00b7ses", "ist", ",", "wo\u00b7rumb", "wir", "e\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VAFIN", "$,", "PWAV", "PPER", "ADV"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "W\u00fcnschen noch mit dir zu leben,", "tokens": ["W\u00fcn\u00b7schen", "noch", "mit", "dir", "zu", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Aber weil es Gott gefellt,", "tokens": ["A\u00b7ber", "weil", "es", "Gott", "ge\u00b7fellt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "NN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Der dich auff den Staub der Schulen", "tokens": ["Der", "dich", "auff", "den", "Staub", "der", "Schu\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Anderweit nechst keuschem Buhlen", "tokens": ["An\u00b7der\u00b7weit", "nechst", "keu\u00b7schem", "Buh\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Auch zu seinem Dienst bestellt,", "tokens": ["Auch", "zu", "sei\u00b7nem", "Dienst", "be\u00b7stellt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "M\u00fcssen wir hie sein vergn\u00fcget,", "tokens": ["M\u00fcs\u00b7sen", "wir", "hie", "sein", "ver\u00b7gn\u00fc\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "VAINF", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie der H\u00f6chst' es hat gef\u00fcget,", "tokens": ["Wie", "der", "H\u00f6chst'", "es", "hat", "ge\u00b7f\u00fc\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PPER", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "W\u00fcnschen dir vnd deiner Braut", "tokens": ["W\u00fcn\u00b7schen", "dir", "vnd", "dei\u00b7ner", "Braut"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "KON", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Wahre Liebe, Heil vnd St\u00e4rcke,", "tokens": ["Wah\u00b7re", "Lie\u00b7be", ",", "Heil", "vnd", "St\u00e4r\u00b7cke", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Gottes Geist auch zu dem Wercke,", "tokens": ["Got\u00b7tes", "Geist", "auch", "zu", "dem", "Wer\u00b7cke", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "ADV", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Da\u00df er selbst dir anvertrawt.", "tokens": ["Da\u00df", "er", "selbst", "dir", "an\u00b7ver\u00b7trawt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "PPER", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "Wollen aber vnterdessen", "tokens": ["Wol\u00b7len", "a\u00b7ber", "vn\u00b7ter\u00b7des\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "ADV", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deiner nimmer nicht vergessen,", "tokens": ["Dei\u00b7ner", "nim\u00b7mer", "nicht", "ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADV", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Von dir reden alles best',", "tokens": ["Von", "dir", "re\u00b7den", "al\u00b7les", "best'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVFIN", "PIS", "VVFIN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Vnd sind auch auff dein begehren", "tokens": ["Vnd", "sind", "auch", "auff", "dein", "be\u00b7geh\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ADV", "APPR", "PPOSAT", "VVINF"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.5": {"text": "Kommen, bey dir einzukehren", "tokens": ["Kom\u00b7men", ",", "bey", "dir", "ein\u00b7zu\u00b7keh\u00b7ren"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "APPR", "PPER", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Deine liebe Hochzeit G\u00e4st.", "tokens": ["Dei\u00b7ne", "lie\u00b7be", "Hoch\u00b7zeit", "G\u00e4st", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}