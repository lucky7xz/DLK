{"textgrid.poem.34652": {"metadata": {"author": {"name": "Neukirch, Benjamin", "birth": "N.A.", "death": "N.A."}, "title": "1L: Verschm\u00e4he/ kluger mann/ die schlechten zeilen nicht/", "genre": "verse", "period": "N.A.", "pub_year": 1697, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Verschm\u00e4he/ kluger mann/ die schlechten zeilen nicht/", "tokens": ["Ver\u00b7schm\u00e4\u00b7he", "/", "klu\u00b7ger", "mann", "/", "die", "schlech\u00b7ten", "zei\u00b7len", "nicht", "/"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$(", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "PTKNEG", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die freund und diener dir anitzt zum opffer bringen.", "tokens": ["Die", "freund", "und", "die\u00b7ner", "dir", "a\u00b7nitzt", "zum", "opf\u00b7fer", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "PPER", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wir suchen deinen ruhm in keinen reim zu zwingen:", "tokens": ["Wir", "su\u00b7chen", "dei\u00b7nen", "ruhm", "in", "kei\u00b7nen", "reim", "zu", "zwin\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "APPR", "PIAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wir singen nicht allhier/ was Fama von dir spricht:", "tokens": ["Wir", "sin\u00b7gen", "nicht", "all\u00b7hier", "/", "was", "Fa\u00b7ma", "von", "dir", "spricht", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "$(", "PWS", "NN", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Nicht/ wie der Oder-strand; nicht/ wie der Elbe grentzen/", "tokens": ["Nicht", "/", "wie", "der", "O\u00b7der\u00b7strand", ";", "nicht", "/", "wie", "der", "El\u00b7be", "grent\u00b7zen", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "$(", "KOKOM", "ART", "NN", "$.", "PTKNEG", "$(", "KOKOM", "ART", "NE", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "O welt gepriesnes licht/ von deinen strahlen gl\u00e4ntzen.", "tokens": ["O", "welt", "ge\u00b7pri\u00b7es\u00b7nes", "licht", "/", "von", "dei\u00b7nen", "strah\u00b7len", "gl\u00e4nt\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "ADJA", "NN", "$(", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}}, "stanza.2": {"line.1": {"text": "Es ist was altes schon/ da\u00df/ wie ein jeder sagt/", "tokens": ["Es", "ist", "was", "al\u00b7tes", "schon", "/", "da\u00df", "/", "wie", "ein", "je\u00b7der", "sagt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "ADJA", "ADV", "$(", "KOUS", "$(", "KOKOM", "ART", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Drey hohe schulen sich durch deinen flei\u00df erhoben:", "tokens": ["Drey", "ho\u00b7he", "schu\u00b7len", "sich", "durch", "dei\u00b7nen", "flei\u00df", "er\u00b7ho\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "VVFIN", "PRF", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da\u00df f\u00fcrst und hoff dich liebt/ da\u00df dich die frembden loben/", "tokens": ["Da\u00df", "f\u00fcrst", "und", "hoff", "dich", "liebt", "/", "da\u00df", "dich", "die", "fremb\u00b7den", "lo\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "KON", "VVFIN", "PPER", "VVFIN", "$(", "KOUS", "PPER", "ART", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und man dich weit und breit um recht und urthel plagt.", "tokens": ["Und", "man", "dich", "weit", "und", "breit", "um", "recht", "und", "ur\u00b7thel", "plagt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "PRF", "ADJD", "KON", "ADJD", "APPR", "ADJD", "KON", "NE", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ein ander schreibe viel von deinen ehren-kertzen:", "tokens": ["Ein", "an\u00b7der", "schrei\u00b7be", "viel", "von", "dei\u00b7nen", "eh\u00b7ren\u00b7kert\u00b7zen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Wir schencken dir allhier nichts/ als ergebne hertzen.", "tokens": ["Wir", "schen\u00b7cken", "dir", "all\u00b7hier", "nichts", "/", "als", "er\u00b7geb\u00b7ne", "hert\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PIS", "$(", "KOUS", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Ach auserle\u00dfner Stryck/ schau unser opffer an!", "tokens": ["Ach", "au\u00b7ser\u00b7le\u00df\u00b7ner", "Stryck", "/", "schau", "un\u00b7ser", "opf\u00b7fer", "an", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "ADJA", "NN", "$(", "ADJD", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Was k\u00f6nten wir doch wohl geschickters f\u00fcr dich finden?", "tokens": ["Was", "k\u00f6n\u00b7ten", "wir", "doch", "wohl", "ge\u00b7schick\u00b7ters", "f\u00fcr", "dich", "fin\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "ADV", "ADV", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wir kommen/ wie du weist/ dich s\u00e4mtlich anzubinden/", "tokens": ["Wir", "kom\u00b7men", "/", "wie", "du", "weist", "/", "dich", "s\u00e4mt\u00b7lich", "an\u00b7zu\u00b7bin\u00b7den", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PWAV", "PPER", "VVFIN", "$(", "PPER", "ADJD", "VVIZU", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und binden/ wie du siehst/ mit lauter hertzen an.", "tokens": ["Und", "bin\u00b7den", "/", "wie", "du", "siehst", "/", "mit", "lau\u00b7ter", "hert\u00b7zen", "an", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "$(", "PWAV", "PPER", "VVFIN", "$(", "APPR", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Jedoch was binden wir? Die freyheit ist verschwunden:", "tokens": ["Je\u00b7doch", "was", "bin\u00b7den", "wir", "?", "Die", "frey\u00b7heit", "ist", "ver\u00b7schwun\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "VAFIN", "PPER", "$.", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Weil du sie schon vorl\u00e4ngst mit ketten selbst gebunden.", "tokens": ["Weil", "du", "sie", "schon", "vor\u00b7l\u00e4ngst", "mit", "ket\u00b7ten", "selbst", "ge\u00b7bun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADV", "APPR", "VVFIN", "ADV", "VVPP", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.4": {"line.1": {"text": "Dein angenehmer mund und deine freundlichkeit", "tokens": ["Dein", "an\u00b7ge\u00b7neh\u00b7mer", "mund", "und", "dei\u00b7ne", "freund\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "KON", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Hat auch in diesem uns das k\u00f6nnen schon benommen.", "tokens": ["Hat", "auch", "in", "die\u00b7sem", "uns", "das", "k\u00f6n\u00b7nen", "schon", "be\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "PDAT", "PPER", "PDS", "VMFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wir sind itzt nicht mehr so/ wie wir nach Halle kommen;", "tokens": ["Wir", "sind", "itzt", "nicht", "mehr", "so", "/", "wie", "wir", "nach", "Hal\u00b7le", "kom\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PTKNEG", "ADV", "ADV", "$(", "PWAV", "PPER", "APPR", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Verstand und hertze seyn ver\u00e4ndert und verneu't.", "tokens": ["Ver\u00b7stand", "und", "hert\u00b7ze", "seyn", "ver\u00b7\u00e4n\u00b7dert", "und", "ver\u00b7neu'", "t."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "abbreviation"], "pos": ["NN", "KON", "VVFIN", "PPOSAT", "VVPP", "KON", "XY", "XY"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Denn deiner lehren krafft erhebt uns von der erden;", "tokens": ["Denn", "dei\u00b7ner", "leh\u00b7ren", "krafft", "er\u00b7hebt", "uns", "von", "der", "er\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Dein anblick aber macht/ da\u00df wir zu sclaven werden.", "tokens": ["Dein", "an\u00b7blick", "a\u00b7ber", "macht", "/", "da\u00df", "wir", "zu", "scla\u00b7ven", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "VVFIN", "$(", "KOUS", "PPER", "PTKZU", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Was man dem Orpheus vorzeiten beygelegt/", "tokens": ["Was", "man", "dem", "Or\u00b7pheus", "vor\u00b7zei\u00b7ten", "bey\u00b7ge\u00b7legt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "ART", "NE", "ADV", "VVPP", "$("], "meter": "+--+--+-+-+", "measure": "dactylic.di.plus"}, "line.2": {"text": "Das thustu w\u00fcrcklich noch. Du r\u00fchrst die kalten sinnen/", "tokens": ["Das", "thu\u00b7stu", "w\u00fcrck\u00b7lich", "noch", ".", "Du", "r\u00fchrst", "die", "kal\u00b7ten", "sin\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADJD", "ADV", "$.", "PPER", "VVFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die offtmahls h\u00e4rter sind als steine zu gewinnen.", "tokens": ["Die", "offt\u00b7mahls", "h\u00e4r\u00b7ter", "sind", "als", "stei\u00b7ne", "zu", "ge\u00b7win\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "VAFIN", "KOKOM", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wer dich nur einmahl h\u00f6rt/ der wird auch schon bewegt.", "tokens": ["Wer", "dich", "nur", "ein\u00b7mahl", "h\u00f6rt", "/", "der", "wird", "auch", "schon", "be\u00b7wegt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "ADV", "VVFIN", "$(", "ART", "VAFIN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Wer dich zum freunde hat und doch nicht will entbrennen/", "tokens": ["Wer", "dich", "zum", "freun\u00b7de", "hat", "und", "doch", "nicht", "will", "ent\u00b7bren\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPRART", "NN", "VAFIN", "KON", "ADV", "PTKNEG", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Der mu\u00df ein unmensch seyn und keine regung kennen.", "tokens": ["Der", "mu\u00df", "ein", "un\u00b7mensch", "seyn", "und", "kei\u00b7ne", "re\u00b7gung", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "ADJD", "VAINF", "KON", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "So nimm denn/ grosser Stryk/ das kleinod unsrer brust.", "tokens": ["So", "nimm", "denn", "/", "gros\u00b7ser", "Stryk", "/", "das", "klei\u00b7nod", "uns\u00b7rer", "brust", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVIMP", "ADV", "$(", "ADJA", "NN", "$(", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Vor hastu es geraubt/ itzt wollen wir es schencken.", "tokens": ["Vor", "has\u00b7tu", "es", "ge\u00b7raubt", "/", "itzt", "wol\u00b7len", "wir", "es", "schen\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "PPER", "VVPP", "$(", "ADV", "VMFIN", "PPER", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Du darffst nicht/ wie man pflegt/ auff eine l\u00f6sung dencken:", "tokens": ["Du", "darffst", "nicht", "/", "wie", "man", "pflegt", "/", "auff", "ei\u00b7ne", "l\u00f6\u00b7sung", "den\u00b7cken", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$(", "PWAV", "PIS", "VVFIN", "$(", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Denn die gefangenschafft ersetzet keine lust.", "tokens": ["Denn", "die", "ge\u00b7fan\u00b7gen\u00b7schafft", "er\u00b7set\u00b7zet", "kei\u00b7ne", "lust", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Gib aber/ wo du wilst/ f\u00fcr unsre treue lieder", "tokens": ["Gib", "a\u00b7ber", "/", "wo", "du", "wilst", "/", "f\u00fcr", "uns\u00b7re", "treu\u00b7e", "lie\u00b7der"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVIMP", "ADV", "$(", "PWAV", "PPER", "VMFIN", "$(", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und den verlohrnen schatz/ uns nur dein hertze wieder.", "tokens": ["Und", "den", "ver\u00b7lohr\u00b7nen", "schatz", "/", "uns", "nur", "dein", "hert\u00b7ze", "wie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$(", "PPER", "ADV", "PPOSAT", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}