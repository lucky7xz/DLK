{"textgrid.poem.54171": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Der Zerstreute", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein Blinddarm, der ruht in Palmnicken;", "tokens": ["Mein", "Blind\u00b7darm", ",", "der", "ruht", "in", "Palm\u00b7ni\u00b7cken", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VVFIN", "APPR", "NN", "$."], "meter": "-+--+-++-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "ein Backenzahn und \u00fcberdies", "tokens": ["ein", "Ba\u00b7cken\u00b7zahn", "und", "\u00fc\u00b7ber\u00b7dies"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "ein Milchzahn liegen in Saarbr\u00fccken.", "tokens": ["ein", "Milch\u00b7zahn", "lie\u00b7gen", "in", "Saar\u00b7br\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Mandeln ruhen in Paris.", "tokens": ["Die", "Man\u00b7deln", "ru\u00b7hen", "in", "Pa\u00b7ris", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "So streu ich mich trotz hohen Z\u00f6llen", "tokens": ["So", "streu", "ich", "mich", "trotz", "ho\u00b7hen", "Z\u00f6l\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "weit durch Europa hin durchs Land.", "tokens": ["weit", "durch", "Eu\u00b7ro\u00b7pa", "hin", "durchs", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "NE", "ADV", "APPRART", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Auch hat die Klinik in Neuk\u00f6lln", "tokens": ["Auch", "hat", "die", "Kli\u00b7nik", "in", "Neu\u00b7k\u00f6lln"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "NE"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "noch etwas Nasenscheidewand.", "tokens": ["noch", "et\u00b7was", "Na\u00b7sen\u00b7schei\u00b7de\u00b7wand", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Ein guter Arzt will operieren.", "tokens": ["Ein", "gu\u00b7ter", "Arzt", "will", "o\u00b7pe\u00b7rie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es freut ihn, und es bringt auch Geld.", "tokens": ["Es", "freut", "ihn", ",", "und", "es", "bringt", "auch", "Geld", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "KON", "PPER", "VVFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Viel ist nicht mehr zu amputieren.", "tokens": ["Viel", "ist", "nicht", "mehr", "zu", "am\u00b7pu\u00b7tie\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich bin zu gut f\u00fcr diese Welt.", "tokens": ["Ich", "bin", "zu", "gut", "f\u00fcr", "die\u00b7se", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKA", "ADJD", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Was soll ich armes Luder machen,", "tokens": ["Was", "soll", "ich", "ar\u00b7mes", "Lu\u00b7der", "ma\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "wenn die Posaune blasen mag?", "tokens": ["wenn", "die", "Po\u00b7sau\u00b7ne", "bla\u00b7sen", "mag", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie tret ich an mit meinen sieben Sachen", "tokens": ["Wie", "tret", "ich", "an", "mit", "mei\u00b7nen", "sie\u00b7ben", "Sa\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "APPR", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "am heiligen Auferstehungstag?", "tokens": ["am", "hei\u00b7li\u00b7gen", "Auf\u00b7er\u00b7ste\u00b7hungs\u00b7tag", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "Der liebe Gott macht nicht viel Federlesen.", "tokens": ["Der", "lie\u00b7be", "Gott", "macht", "nicht", "viel", "Fe\u00b7der\u00b7le\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PTKNEG", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "\u00bbherr Tiger!\u00ab ruft er. \u00bbKomm hervor!", "tokens": ["\u00bb", "herr", "Ti\u00b7ger", "!", "\u00ab", "ruft", "er", ".", "\u00bb", "Komm", "her\u00b7vor", "!"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["$(", "NN", "NN", "$.", "$(", "VVFIN", "PPER", "$.", "$(", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie siehst du aus, l\u00e4diertes Wesen?", "tokens": ["Wie", "siehst", "du", "aus", ",", "l\u00e4\u00b7dier\u00b7tes", "We\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPER", "PTKVZ", "$,", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und wo \u2013 wo hast du den Humor?\u00ab", "tokens": ["Und", "wo", "\u2013", "wo", "hast", "du", "den", "Hu\u00b7mor", "?", "\u00ab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PWAV", "$(", "PWAV", "VAFIN", "PPER", "ART", "NN", "$.", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.6": {"line.1": {"text": "\u00bbich las\u00ab \u2013 sag ich dann ohne Bangen \u2013", "tokens": ["\u00bb", "ich", "las", "\u00ab", "\u2013", "sag", "ich", "dann", "oh\u00b7ne", "Ban\u00b7gen", "\u2013"], "token_info": ["punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "$(", "$(", "VVIMP", "PPER", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "\u00bbeinst den Etat der deutschen Generalit\u00e4t.", "tokens": ["\u00bb", "einst", "den", "E\u00b7tat", "der", "deut\u00b7schen", "Ge\u00b7ne\u00b7ra\u00b7li\u00b7t\u00e4t", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Da ist mir der Humor vergangen.\u00ab", "tokens": ["Da", "ist", "mir", "der", "Hu\u00b7mor", "ver\u00b7gan\u00b7gen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Und Gott versteht.", "tokens": ["Und", "Gott", "ver\u00b7steht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Und Gott versteht.", "tokens": ["Und", "Gott", "ver\u00b7steht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}