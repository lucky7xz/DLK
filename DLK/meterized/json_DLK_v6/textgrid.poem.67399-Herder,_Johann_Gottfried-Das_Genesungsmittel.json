{"textgrid.poem.67399": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "Das Genesungsmittel", "genre": "verse", "period": "N.A.", "pub_year": 1789, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Um von der Achtung zu genesen,", "tokens": ["Um", "von", "der", "Ach\u00b7tung", "zu", "ge\u00b7ne\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "APPR", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Die ich unw\u00fcrdig oft Autoren zugewandt,", "tokens": ["Die", "ich", "un\u00b7w\u00fcr\u00b7dig", "oft", "Au\u00b7to\u00b7ren", "zu\u00b7ge\u00b7wandt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADJD", "ADV", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Befahl der Arzt es mir, ein Tagebuch zu lesen:", "tokens": ["Be\u00b7fahl", "der", "Arzt", "es", "mir", ",", "ein", "Ta\u00b7ge\u00b7buch", "zu", "le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "PPER", "PPER", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "\u00bbes ist gelehrt gedruckt und hei\u00dft: ", "tokens": ["\u00bb", "es", "ist", "ge\u00b7lehrt", "ge\u00b7druckt", "und", "hei\u00dft", ":"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "VVPP", "VVPP", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Da tummeln sich die Herrn f\u00fcr das gemeine Wesen", "tokens": ["Da", "tum\u00b7meln", "sich", "die", "Herrn", "f\u00fcr", "das", "ge\u00b7mei\u00b7ne", "We\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PRF", "ART", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Oft ritterlich, doch eben nicht galant.", "tokens": ["Oft", "rit\u00b7ter\u00b7lich", ",", "doch", "e\u00b7ben", "nicht", "ga\u00b7lant", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADV", "ADV", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Was Akritik f\u00fcr Sold im Buche falsch gelesen,", "tokens": ["Was", "Ak\u00b7ri\u00b7tik", "f\u00fcr", "Sold", "im", "Bu\u00b7che", "falsch", "ge\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "APPR", "NE", "APPRART", "NN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Wird antikritisch dann f\u00fcr Zahlung aberkannt.", "tokens": ["Wird", "an\u00b7ti\u00b7kri\u00b7tisch", "dann", "f\u00fcr", "Zah\u00b7lung", "ab\u00b7er\u00b7kannt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "ADV", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Es dr\u00e4ngen sich die Herrn; zu aber- abermalen", "tokens": ["Es", "dr\u00e4n\u00b7gen", "sich", "die", "Herrn", ";", "zu", "a\u00b7ber", "a\u00b7ber\u00b7ma\u00b7len"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ART", "NN", "$.", "APPR", "TRUNC", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wird Akritik bezahlt, Antikritik mu\u00df zahlen.\u00ab", "tokens": ["Wird", "Ak\u00b7ri\u00b7tik", "be\u00b7zahlt", ",", "An\u00b7ti\u00b7kri\u00b7tik", "mu\u00df", "zah\u00b7len", ".", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "NN", "VVPP", "$,", "NN", "VMFIN", "VVINF", "$.", "$("], "meter": "-+-+--+--+-+-", "measure": "iambic.penta.relaxed"}, "line.11": {"text": "Ich folgte meinem Arzt. Mit lustigem Erstaunen", "tokens": ["Ich", "folg\u00b7te", "mei\u00b7nem", "Arzt", ".", "Mit", "lus\u00b7ti\u00b7gem", "Er\u00b7stau\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "$.", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Sah ich den Waffenplatz im deutschen Publicum,", "tokens": ["Sah", "ich", "den", "Waf\u00b7fen\u00b7platz", "im", "deut\u00b7schen", "Pub\u00b7li\u00b7cum", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Die Ritter und ihr Gl\u00fcck und Beider tolle Launen;", "tokens": ["Die", "Rit\u00b7ter", "und", "ihr", "Gl\u00fcck", "und", "Bei\u00b7der", "tol\u00b7le", "Lau\u00b7nen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "PPOSAT", "NN", "KON", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Das Krumme ward mir recht, und das Gerade krumm,", "tokens": ["Das", "Krum\u00b7me", "ward", "mir", "recht", ",", "und", "das", "Ge\u00b7ra\u00b7de", "krumm", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADJD", "$,", "KON", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Und jeder Buchstab schien mir in das Ohr zu raunen:", "tokens": ["Und", "je\u00b7der", "Buch\u00b7stab", "schien", "mir", "in", "das", "Ohr", "zu", "rau\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "PPER", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "\u00bbhinweg von hier! hier ist ein b\u00f6ses S\u00e4culum.\u00ab", "tokens": ["\u00bb", "hin\u00b7weg", "von", "hier", "!", "hier", "ist", "ein", "b\u00f6\u00b7ses", "S\u00e4\u00b7cu\u00b7lum", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "APPR", "ADV", "$.", "ADV", "VAFIN", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Das literarische gemeine deutsche Wesen,", "tokens": ["Das", "li\u00b7te\u00b7ra\u00b7ri\u00b7sche", "ge\u00b7mei\u00b7ne", "deut\u00b7sche", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Nach Pfennigen verkauft, sah ich und war genesen.", "tokens": ["Nach", "Pfen\u00b7ni\u00b7gen", "ver\u00b7kauft", ",", "sah", "ich", "und", "war", "ge\u00b7ne\u00b7sen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "$,", "VVFIN", "PPER", "KON", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}}}}