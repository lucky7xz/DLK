{"textgrid.poem.24628": {"metadata": {"author": {"name": "Hunold, Christian Friedrich", "birth": "N.A.", "death": "N.A."}, "title": "Bey dem Gleditsch- und B\u00f6tticherischen Hochzeit-Feste entschuldigte sich/ da\u00df er nicht zur Hochzeit kommen k\u00f6nte/", "genre": "verse", "period": "N.A.", "pub_year": 1701, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Mein Freund/ sein Hochzeit-Brief war eben angekommen/", "tokens": ["Mein", "Freund", "/", "sein", "Hoch\u00b7zeit\u00b7Brief", "war", "e\u00b7ben", "an\u00b7ge\u00b7kom\u00b7men", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PPOSAT", "NN", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Als ich aus Schlesien von einer Hochzeit kam.", "tokens": ["Als", "ich", "aus", "Schle\u00b7si\u00b7en", "von", "ei\u00b7ner", "Hoch\u00b7zeit", "kam", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "NN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Im Schreiben hat er wohl von Liebe gantz geglommen/", "tokens": ["Im", "Schrei\u00b7ben", "hat", "er", "wohl", "von", "Lie\u00b7be", "gantz", "ge\u00b7glom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VAFIN", "PPER", "ADV", "APPR", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Drum war der Brief noch hei\u00df/ als ich denselben nahm.", "tokens": ["Drum", "war", "der", "Brief", "noch", "hei\u00df", "/", "als", "ich", "den\u00b7sel\u00b7ben", "nahm", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "ART", "NN", "ADV", "ADJD", "$(", "KOUS", "PPER", "PDS", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Er bittet/ er befiehlt/ ich soll nicht aussen bleiben.", "tokens": ["Er", "bit\u00b7tet", "/", "er", "be\u00b7fiehlt", "/", "ich", "soll", "nicht", "aus\u00b7sen", "blei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PPER", "VVFIN", "$(", "PPER", "VMFIN", "PTKNEG", "VVINF", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Die Worte sind wohl sch\u00f6n/ ich wei\u00df auch meine Pflicht:", "tokens": ["Die", "Wor\u00b7te", "sind", "wohl", "sch\u00f6n", "/", "ich", "wei\u00df", "auch", "mei\u00b7ne", "Pflicht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADJD", "$(", "PPER", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Doch aber den Termin vor di\u00dfmahl abzuschreiben/", "tokens": ["Doch", "a\u00b7ber", "den", "Ter\u00b7min", "vor", "di\u00df\u00b7mahl", "ab\u00b7zu\u00b7schrei\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "NN", "APPR", "ADV", "VVIZU", "$("], "meter": "-+--++-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.8": {"text": "Mu\u00df ich gezwungen thun/ und \u00e4ndern kan ichs nicht.", "tokens": ["Mu\u00df", "ich", "ge\u00b7zwun\u00b7gen", "thun", "/", "und", "\u00e4n\u00b7dern", "kan", "ichs", "nicht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "VVPP", "VVINF", "$(", "KON", "VVINF", "VMFIN", "PIS", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Es hat der weite Weg die Lust mir nicht verbohten.", "tokens": ["Es", "hat", "der", "wei\u00b7te", "Weg", "die", "Lust", "mir", "nicht", "ver\u00b7boh\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "ART", "NN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Ich k\u00e4me mit der Post noch gleich zu rechte hin.", "tokens": ["Ich", "k\u00e4\u00b7me", "mit", "der", "Post", "noch", "gleich", "zu", "rech\u00b7te", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ADV", "ADV", "APPR", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Mein Aemtgen machet nur die allermeisten Knoten/", "tokens": ["Mein", "A\u00b7emt\u00b7gen", "ma\u00b7chet", "nur", "die", "al\u00b7ler\u00b7meis\u00b7ten", "Kno\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ADV", "ART", "ADJA", "NN", "$("], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "In welchem ich zwar frey/ doch auch gebunden bin.", "tokens": ["In", "wel\u00b7chem", "ich", "zwar", "frey", "/", "doch", "auch", "ge\u00b7bun\u00b7den", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "ADV", "ADJD", "$(", "ADV", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Zu dem will mich die Furcht an meiner Reise hindern/", "tokens": ["Zu", "dem", "will", "mich", "die", "Furcht", "an", "mei\u00b7ner", "Rei\u00b7se", "hin\u00b7dern", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "VMFIN", "PRF", "ART", "NN", "APPR", "PPOSAT", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Die mir wie jederman/ die schwarze Rotte macht.", "tokens": ["Die", "mir", "wie", "je\u00b7der\u00b7man", "/", "die", "schwar\u00b7ze", "Rot\u00b7te", "macht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "KOKOM", "PIS", "$(", "ART", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Denn k\u00f6nte diese nicht mich unterwegen pl\u00fcndern?", "tokens": ["Denn", "k\u00f6n\u00b7te", "die\u00b7se", "nicht", "mich", "un\u00b7ter\u00b7we\u00b7gen", "pl\u00fcn\u00b7dern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PDS", "PTKNEG", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Gewi\u00df ich zittre gantz/ da ich nur dran gedacht.", "tokens": ["Ge\u00b7wi\u00df", "ich", "zitt\u00b7re", "gantz", "/", "da", "ich", "nur", "dran", "ge\u00b7dacht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "PPER", "VVFIN", "ADV", "$(", "KOUS", "PPER", "ADV", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "O was vor schrecklich Ding will man von ihnen sprechen:", "tokens": ["O", "was", "vor", "schreck\u00b7lich", "Ding", "will", "man", "von", "ih\u00b7nen", "spre\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "PWS", "APPR", "ADJD", "NN", "VMFIN", "PIS", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Sie fr\u00e4ssen Menschen-Fleisch/ und s\u00f6ffen Kinder-Blut.", "tokens": ["Sie", "fr\u00e4s\u00b7sen", "Men\u00b7schen\u00b7Fleisch", "/", "und", "s\u00f6f\u00b7fen", "Kin\u00b7der\u00b7Blut", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$(", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "Das Eisen k\u00f6nten sie wie faules Holtz zerbrechen.", "tokens": ["Das", "Ei\u00b7sen", "k\u00f6n\u00b7ten", "sie", "wie", "fau\u00b7les", "Holtz", "zer\u00b7bre\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "KOKOM", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Dem/ der sie nur erblickt/ entfiele stracks der Muht.", "tokens": ["Dem", "/", "der", "sie", "nur", "er\u00b7blickt", "/", "ent\u00b7fie\u00b7le", "stracks", "der", "Muht", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "$(", "PRELS", "PPER", "ADV", "VVPP", "$(", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Sie lie\u00dfens gar vorher den Leuten selber sagen/", "tokens": ["Sie", "lie\u00b7\u00dfens", "gar", "vor\u00b7her", "den", "Leu\u00b7ten", "sel\u00b7ber", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADV", "ADV", "ART", "NN", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Wenn und zu welcher Zeit die Ankunfft solte seyn:", "tokens": ["Wenn", "und", "zu", "wel\u00b7cher", "Zeit", "die", "An\u00b7kunfft", "sol\u00b7te", "seyn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KON", "APPR", "PWAT", "NN", "ART", "NN", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Da w\u00fcrden Thier und Thor und Kasten aufgeschlagen/", "tokens": ["Da", "w\u00fcr\u00b7den", "Thier", "und", "Thor", "und", "Kas\u00b7ten", "auf\u00b7ge\u00b7schla\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "KON", "NN", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "Da packten sie das Geld vor aller Augen ein.", "tokens": ["Da", "pack\u00b7ten", "sie", "das", "Geld", "vor", "al\u00b7ler", "Au\u00b7gen", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "APPR", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Denn niemand k\u00f6nte sich an H\u00e4nd und F\u00fcssen r\u00fchren.", "tokens": ["Denn", "nie\u00b7mand", "k\u00f6n\u00b7te", "sich", "an", "H\u00e4nd", "und", "F\u00fcs\u00b7sen", "r\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VMFIN", "PRF", "APPR", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Man st\u00fcnde gantz erstarrt/ als wie von Krampff und Gicht.", "tokens": ["Man", "st\u00fcn\u00b7de", "gantz", "er\u00b7starrt", "/", "als", "wie", "von", "Krampff", "und", "Gicht", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "VVPP", "$(", "KOUS", "KOKOM", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Und was man mehr erzehlt von diesen b\u00f6sen Thieren/", "tokens": ["Und", "was", "man", "mehr", "er\u00b7zehlt", "von", "die\u00b7sen", "b\u00f6\u00b7sen", "Thie\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PIS", "ADV", "VVFIN", "APPR", "PDAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Drum sagt mir Furcht und Angst: bey leibe reise nicht.", "tokens": ["Drum", "sagt", "mir", "Furcht", "und", "Angst", ":", "bey", "lei\u00b7be", "rei\u00b7se", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "NN", "KON", "NN", "$.", "APPR", "VVFIN", "ADV", "PTKNEG", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Wie wohl es geht mir nah/ da\u00df ich nicht soll erscheinen.", "tokens": ["Wie", "wohl", "es", "geht", "mir", "nah", "/", "da\u00df", "ich", "nicht", "soll", "er\u00b7schei\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPER", "VVFIN", "PPER", "ADJD", "$(", "KOUS", "PPER", "PTKNEG", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich tr\u00e4fe da den Kern von lieben Freunden an.", "tokens": ["Ich", "tr\u00e4\u00b7fe", "da", "den", "Kern", "von", "lie\u00b7ben", "Freun\u00b7den", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN", "APPR", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich d\u00e4chte nicht/ da\u00df wir zusammen w\u00fcrden weinen/", "tokens": ["Ich", "d\u00e4ch\u00b7te", "nicht", "/", "da\u00df", "wir", "zu\u00b7sam\u00b7men", "w\u00fcr\u00b7den", "wei\u00b7nen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$(", "KOUS", "PPER", "ADV", "VAFIN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wenn die Vertraulichkeit die Hertzen aufgethan.", "tokens": ["Wenn", "die", "Ver\u00b7trau\u00b7lich\u00b7keit", "die", "Hert\u00b7zen", "auf\u00b7ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Mich solte manch Gespr\u00e4ch am wehrten Vater laben.", "tokens": ["Mich", "sol\u00b7te", "manch", "Ge\u00b7spr\u00e4ch", "am", "wehr\u00b7ten", "Va\u00b7ter", "la\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIAT", "NN", "APPRART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ich sehe schon/ wie er die Braut zum Tantze f\u00fchrt.", "tokens": ["Ich", "se\u00b7he", "schon", "/", "wie", "er", "die", "Braut", "zum", "Tant\u00b7ze", "f\u00fchrt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$(", "PWAV", "PPER", "ART", "NN", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ich will das Podagra hiermit beschworen haben/", "tokens": ["Ich", "will", "das", "Po\u00b7da\u00b7gra", "hier\u00b7mit", "be\u00b7schwo\u00b7ren", "ha\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "PAV", "VVINF", "VAFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Da\u00df ihn der b\u00f6se Schalck vor dismahl nicht verirt.", "tokens": ["Da\u00df", "ihn", "der", "b\u00f6\u00b7se", "Schalck", "vor", "dis\u00b7mahl", "nicht", "ver\u00b7irt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "APPR", "NN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Er ", "tokens": ["Er"], "token_info": ["word"], "pos": ["PPER"], "meter": "-", "measure": "single.down"}, "line.10": {"text": "Noch thue/ wie manchmahl/ auf seinem Throne stoltz.", "tokens": ["Noch", "thue", "/", "wie", "manch\u00b7mahl", "/", "auf", "sei\u00b7nem", "Thro\u00b7ne", "stoltz", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "KOKOM", "ADV", "$(", "APPR", "PPOSAT", "NN", "ADJD", "$."], "meter": "-+----+-+-+", "measure": "dactylic.init"}, "line.11": {"text": "Er breche sie auf ietzt und immerfort in St\u00fccken/", "tokens": ["Er", "bre\u00b7che", "sie", "auf", "ietzt", "und", "im\u00b7mer\u00b7fort", "in", "St\u00fc\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "ADV", "KON", "ADV", "APPR", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Man brauchet ohne dem zum Hochzeit-Braten Holtz.", "tokens": ["Man", "brau\u00b7chet", "oh\u00b7ne", "dem", "zum", "Hoch\u00b7zeit\u00b7Bra\u00b7ten", "Holtz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "ART", "APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Philandern f\u00e4nd ich da/ Philandern/ mein vergn\u00fcgen/", "tokens": ["Phi\u00b7lan\u00b7dern", "f\u00e4nd", "ich", "da", "/", "Phi\u00b7lan\u00b7dern", "/", "mein", "ver\u00b7gn\u00fc\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "$(", "NN", "$(", "PPOSAT", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Und k\u00f6ndten wir auch gleich nicht auf das Th\u00fcrmgen gehn:", "tokens": ["Und", "k\u00f6nd\u00b7ten", "wir", "auch", "gleich", "nicht", "auf", "das", "Th\u00fcrm\u00b7gen", "gehn", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADV", "ADV", "PTKNEG", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "So w\u00fcrde sich doch sonst viel angenehmes f\u00fcgen/", "tokens": ["So", "w\u00fcr\u00b7de", "sich", "doch", "sonst", "viel", "an\u00b7ge\u00b7neh\u00b7mes", "f\u00fc\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PRF", "ADV", "ADV", "PIAT", "ADJA", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Da uns der Zeit-vertreib zu Dienste m\u00fcste stehn.", "tokens": ["Da", "uns", "der", "Zeit\u00b7ver\u00b7treib", "zu", "Diens\u00b7te", "m\u00fcs\u00b7te", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Mich d\u00fcnckt ein wehrter Mann in einer runden Krause/", "tokens": ["Mich", "d\u00fcnckt", "ein", "wehr\u00b7ter", "Mann", "in", "ei\u00b7ner", "run\u00b7den", "Krau\u00b7se", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.18": {"text": "Der/ wo mir anders recht/ Magister \u2013 \u2013 heist/", "tokens": ["Der", "/", "wo", "mir", "an\u00b7ders", "recht", "/", "Ma\u00b7gis\u00b7ter", "\u2013", "\u2013", "heist", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "punct", "punct", "word", "punct"], "pos": ["ART", "$(", "PWAV", "PPER", "ADV", "ADJD", "$(", "NN", "$(", "$(", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.19": {"text": "Mein Hochgesch\u00e4tzter Freund/ ist auch im Hochzeit-Hause/", "tokens": ["Mein", "Hoch\u00b7ge\u00b7sch\u00e4tz\u00b7ter", "Freund", "/", "ist", "auch", "im", "Hoch\u00b7zeit\u00b7Hau\u00b7se", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$(", "VAFIN", "ADV", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Wo er gelehrt ", "tokens": ["Wo", "er", "ge\u00b7lehrt"], "token_info": ["word", "word", "word"], "pos": ["PWAV", "PPER", "VVPP"], "meter": "-+-+", "measure": "iambic.di"}, "line.21": {"text": "Das Gl\u00fccke w\u00fcrde mir vieleicht mehr G\u00f6nner schencken.", "tokens": ["Das", "Gl\u00fc\u00b7cke", "w\u00fcr\u00b7de", "mir", "vie\u00b7leicht", "mehr", "G\u00f6n\u00b7ner", "schen\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Di\u00df w\u00e4re schon genug: Ich k\u00f6nte Leipzig sehn.", "tokens": ["Di\u00df", "w\u00e4\u00b7re", "schon", "ge\u00b7nug", ":", "Ich", "k\u00f6n\u00b7te", "Leip\u00b7zig", "sehn", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "$.", "PPER", "VMFIN", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.23": {"text": "Versichert/ wenn ich nur ein solches darf gedencken/", "tokens": ["Ver\u00b7si\u00b7chert", "/", "wenn", "ich", "nur", "ein", "sol\u00b7ches", "darf", "ge\u00b7den\u00b7cken", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$(", "KOUS", "PPER", "ADV", "ART", "PIS", "VMFIN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.24": {"text": "So kan es anderst nicht als h\u00f6chst vergn\u00fcgt geschehn.", "tokens": ["So", "kan", "es", "an\u00b7derst", "nicht", "als", "h\u00f6chst", "ver\u00b7gn\u00fcgt", "ge\u00b7schehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PTKNEG", "KOKOM", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Hier fehlt mir di\u00df und das. Doch drau\u00dfen wirds gefunden.", "tokens": ["Hier", "fehlt", "mir", "di\u00df", "und", "das", ".", "Doch", "drau\u00b7\u00dfen", "wirds", "ge\u00b7fun\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PDS", "KON", "PDS", "$.", "KON", "ADV", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Dort treff ich lauter Marck/ hier nur die Knochen an.", "tokens": ["Dort", "treff", "ich", "lau\u00b7ter", "Marck", "/", "hier", "nur", "die", "Kno\u00b7chen", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PIAT", "NN", "$(", "ADV", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Hier ist die Uhr verstellt. Sie schl\u00e4get wenig Stunden/", "tokens": ["Hier", "ist", "die", "Uhr", "ver\u00b7stellt", ".", "Sie", "schl\u00e4\u00b7get", "we\u00b7nig", "Stun\u00b7den", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVPP", "$.", "PPER", "VVFIN", "PIAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Wo ich Zufriedenheit des Lebens zehlen kan.", "tokens": ["Wo", "ich", "Zu\u00b7frie\u00b7den\u00b7heit", "des", "Le\u00b7bens", "zeh\u00b7len", "kan", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "NN", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Ach ja/ ich mu\u00df allhier im Lande Cabul wohnen.", "tokens": ["Ach", "ja", "/", "ich", "mu\u00df", "all\u00b7hier", "im", "Lan\u00b7de", "Ca\u00b7bul", "woh\u00b7nen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "ADV", "$(", "PPER", "VMFIN", "ADV", "APPRART", "NN", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Zwar machet Sand und Stein mit Gr\u00fctz und Piltzen reich/", "tokens": ["Zwar", "ma\u00b7chet", "Sand", "und", "Stein", "mit", "Gr\u00fctz", "und", "Pilt\u00b7zen", "reich", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "NN", "KON", "NN", "APPR", "NN", "KON", "NN", "ADJD", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "Und Tannen-Zapfen sind so gut/ als wie Citronen:", "tokens": ["Und", "Tan\u00b7nen\u00b7Zap\u00b7fen", "sind", "so", "gut", "/", "als", "wie", "Cit\u00b7ro\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "VAFIN", "ADV", "ADJD", "$(", "KOUS", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.32": {"text": "So ist Italien auch nicht demselben gleich.", "tokens": ["So", "ist", "I\u00b7ta\u00b7li\u00b7en", "auch", "nicht", "dem\u00b7sel\u00b7ben", "gleich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NE", "ADV", "PTKNEG", "PDAT", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Wie gerne m\u00f6cht ich nun das liebe Leipzig sprechen!", "tokens": ["Wie", "ger\u00b7ne", "m\u00f6cht", "ich", "nun", "das", "lie\u00b7be", "Leip\u00b7zig", "spre\u00b7chen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "VMFIN", "PPER", "ADV", "ART", "ADJA", "NE", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Wie hertzlich w\u00fcnscht ich mir ein Hochzeit-Gast zu seyn.", "tokens": ["Wie", "hertz\u00b7lich", "w\u00fcnscht", "ich", "mir", "ein", "Hoch\u00b7zeit\u00b7Gast", "zu", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VVFIN", "PPER", "PRF", "ART", "NN", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "Doch will manch Hinderni\u00df das Wagen-Rad zubrechen:", "tokens": ["Doch", "will", "manch", "Hin\u00b7der\u00b7ni\u00df", "das", "Wa\u00b7gen\u00b7Rad", "zu\u00b7bre\u00b7chen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIAT", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Wohlan/ so sind ich mich doch in Gedancken ein.", "tokens": ["Wo\u00b7hlan", "/", "so", "sind", "ich", "mich", "doch", "in", "Ge\u00b7dan\u00b7cken", "ein", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "ADV", "VAFIN", "PPER", "PRF", "ADV", "APPR", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Die Ehre hab' ich nicht/ die edle Braut zu kennen/", "tokens": ["Die", "Eh\u00b7re", "hab'", "ich", "nicht", "/", "die", "ed\u00b7le", "Braut", "zu", "ken\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "PTKNEG", "$(", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bi\u00df mich ein k\u00fcnftger Tag darinnen gl\u00fccklich macht.", "tokens": ["Bi\u00df", "mich", "ein", "k\u00fcnft\u00b7ger", "Tag", "da\u00b7rin\u00b7nen", "gl\u00fcck\u00b7lich", "macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NN", "ADV", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Indessen ist sie wohl ein sch\u00f6nes Buch zu nennen/", "tokens": ["In\u00b7des\u00b7sen", "ist", "sie", "wohl", "ein", "sch\u00f6\u00b7nes", "Buch", "zu", "nen\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ART", "ADJA", "NN", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wo von er den Verlag/ mein Freund/ an sich gebracht.", "tokens": ["Wo", "von", "er", "den", "Ver\u00b7lag", "/", "mein", "Freund", "/", "an", "sich", "ge\u00b7bracht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPER", "ART", "NN", "$(", "PPOSAT", "NN", "$(", "APPR", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ich wei\u00df schon/ das er sich nichts schlimmes zu geleget.", "tokens": ["Ich", "wei\u00df", "schon", "/", "das", "er", "sich", "nichts", "schlim\u00b7mes", "zu", "ge\u00b7le\u00b7get", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "$(", "PRELS", "PPER", "PRF", "PIS", "ADJA", "PTKZU", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "So/ da\u00df die Heyrath wird der Handlung gleich gesp\u00fchrt.", "tokens": ["So", "/", "da\u00df", "die", "Hey\u00b7rath", "wird", "der", "Hand\u00b7lung", "gleich", "ge\u00b7sp\u00fchrt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "KOUS", "ART", "NN", "VAFIN", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Das ist der wahre Ruhm/ den sein Herr Vater tr\u00e4get/", "tokens": ["Das", "ist", "der", "wah\u00b7re", "Ruhm", "/", "den", "sein", "Herr", "Va\u00b7ter", "tr\u00e4\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$(", "ART", "PPOSAT", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Da\u00df er/ was gut und rar in seinem laden f\u00fchrt.", "tokens": ["Da\u00df", "er", "/", "was", "gut", "und", "rar", "in", "sei\u00b7nem", "la\u00b7den", "f\u00fchrt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$(", "PWS", "ADJD", "KON", "ADJD", "APPR", "PPOSAT", "ADJA", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Ein andrer sucht Gewinst in sch\u00e4ndlichen Scarteqven.", "tokens": ["Ein", "an\u00b7drer", "sucht", "Ge\u00b7winst", "in", "sch\u00e4nd\u00b7li\u00b7chen", "Scar\u00b7teq\u00b7ven", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVFIN", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wie sein Gewissen ist/ so ist auch das Papier.", "tokens": ["Wie", "sein", "Ge\u00b7wis\u00b7sen", "ist", "/", "so", "ist", "auch", "das", "Pa\u00b7pier", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "VAFIN", "$(", "ADV", "VAFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Was Schw\u00e4rmer ausgeheckt/ was Atheisten k\u00f6cken/", "tokens": ["Was", "Schw\u00e4r\u00b7mer", "aus\u00b7ge\u00b7heckt", "/", "was", "A\u00b7theis\u00b7ten", "k\u00f6\u00b7cken", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVPP", "$(", "PWS", "ADJA", "NN", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.12": {"text": "Tr\u00e4gt er zum Aergerni\u00df in offnen Drucke f\u00fcr.", "tokens": ["Tr\u00e4gt", "er", "zum", "A\u00b7er\u00b7ger\u00b7ni\u00df", "in", "off\u00b7nen", "Dru\u00b7cke", "f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPRART", "NN", "APPR", "ADJA", "NN", "APPR", "$."], "meter": "+--+--+-+-+-+", "measure": "dactylic.di.plus"}, "line.13": {"text": "Was Stanck und Unflaht heist/ das bringet er zur Pre\u00dfe.", "tokens": ["Was", "Stanck", "und", "Un\u00b7flaht", "heist", "/", "das", "brin\u00b7get", "er", "zur", "Pre\u00b7\u00dfe", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "KON", "NN", "VVFIN", "$(", "PDS", "VVFIN", "PPER", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Wer Pasqvinaden schmiert/ der hat ihm recht gethan.", "tokens": ["Wer", "Pas\u00b7qvi\u00b7na\u00b7den", "schmiert", "/", "der", "hat", "ihm", "recht", "ge\u00b7than", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VVFIN", "$(", "ART", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Und diente solches nur zu seinem Intere\u00dfe/", "tokens": ["Und", "dien\u00b7te", "sol\u00b7ches", "nur", "zu", "sei\u00b7nem", "In\u00b7te\u00b7re\u00b7\u00dfe", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PIS", "ADV", "APPR", "PPOSAT", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Er n\u00e4hm ein ", "tokens": ["Er", "n\u00e4hm", "ein"], "token_info": ["word", "word", "word"], "pos": ["PPER", "VVFIN", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.17": {"text": "Mein Gleditsch ist ein Feind von dem verw\u00fcnschten Schrifften.", "tokens": ["Mein", "Gle\u00b7ditsch", "ist", "ein", "Feind", "von", "dem", "ver\u00b7w\u00fcnschten", "Schriff\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.18": {"text": "Deswegen sieht man auch den Seegen-Gottes bl\u00fchn.", "tokens": ["Des\u00b7we\u00b7gen", "sieht", "man", "auch", "den", "See\u00b7gen\u00b7Got\u00b7tes", "bl\u00fchn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIS", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.19": {"text": "In solchem wird er sich noch manches denckmahl stifften/", "tokens": ["In", "sol\u00b7chem", "wird", "er", "sich", "noch", "man\u00b7ches", "denck\u00b7mahl", "stiff\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "VAFIN", "PPER", "PRF", "ADV", "PIS", "ADV", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.20": {"text": "Das die gelehrte Welt wird in Verwundrung ziehn.", "tokens": ["Das", "die", "ge\u00b7lehr\u00b7te", "Welt", "wird", "in", "Ver\u00b7wund\u00b7rung", "ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ART", "ADJA", "NN", "VAFIN", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.21": {"text": "Lebt ", "tokens": ["Lebt"], "token_info": ["word"], "pos": ["VVFIN"], "meter": "+", "measure": "single.up"}, "line.22": {"text": "Steht noch ", "tokens": ["Steht", "noch"], "token_info": ["word", "word"], "pos": ["VVFIN", "ADV"], "meter": "--", "measure": "unknown.measure.zero"}, "line.23": {"text": "Hat sich ein Elzevier Unsterblichkeit erworben?", "tokens": ["Hat", "sich", "ein", "El\u00b7ze\u00b7vier", "U\u00b7nsterb\u00b7lich\u00b7keit", "er\u00b7wor\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PRF", "ART", "NN", "NN", "VVPP", "$."], "meter": "-+-+-+++-+-+-", "measure": "unknown.measure.septa"}, "line.24": {"text": "Ey so mu\u00df Gleditsch auch in gleichem Paare gehn.", "tokens": ["Ey", "so", "mu\u00df", "Gle\u00b7ditsch", "auch", "in", "glei\u00b7chem", "Paa\u00b7re", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "VMFIN", "NN", "ADV", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.25": {"text": "Dem Vater ist der Sohn h\u00f6chstr\u00fchmlich nach geschlagen.", "tokens": ["Dem", "Va\u00b7ter", "ist", "der", "Sohn", "h\u00f6chs\u00b7tr\u00fchm\u00b7lich", "nach", "ge\u00b7schla\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "ADV", "APPR", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.26": {"text": "Nur t\u00fcchtiger Verlag steht seinem Handel an.", "tokens": ["Nur", "t\u00fcch\u00b7ti\u00b7ger", "Ver\u00b7lag", "steht", "sei\u00b7nem", "Han\u00b7del", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "VVFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "Und also hab ich erst nicht lange nach zu fragen/", "tokens": ["Und", "al\u00b7so", "hab", "ich", "erst", "nicht", "lan\u00b7ge", "nach", "zu", "fra\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "ADV", "PTKNEG", "ADV", "APPR", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.28": {"text": "Nach was vor einer Braut sein Hertz sich umgethan.", "tokens": ["Nach", "was", "vor", "ei\u00b7ner", "Braut", "sein", "Hertz", "sich", "um\u00b7ge\u00b7than", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "APPR", "ART", "NN", "PPOSAT", "NN", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "Es wird von Ver\u00dfen wohl am Hochzeit-Tage schneihen/", "tokens": ["Es", "wird", "von", "Ver\u00b7\u00dfen", "wohl", "am", "Hoch\u00b7zeit\u00b7Ta\u00b7ge", "schnei\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "ADV", "APPRART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.30": {"text": "Und manch ", "tokens": ["Und", "manch"], "token_info": ["word", "word"], "pos": ["KON", "PIAT"], "meter": "-+", "measure": "iambic.single"}, "line.31": {"text": "Wenn ein Poete mir den Kasten wolte leihen/", "tokens": ["Wenn", "ein", "Poe\u00b7te", "mir", "den", "Kas\u00b7ten", "wol\u00b7te", "lei\u00b7hen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "ART", "NN", "VMFIN", "VVINF", "$("], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.32": {"text": "So schickt ich ebenfals davon ein Carmen ein.", "tokens": ["So", "schickt", "ich", "e\u00b7ben\u00b7fals", "da\u00b7von", "ein", "Car\u00b7men", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "PAV", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.33": {"text": "Jedoch/ was n\u00e4hm ich da? nicht eine Haus-Postille/", "tokens": ["Je\u00b7doch", "/", "was", "n\u00e4hm", "ich", "da", "?", "nicht", "ei\u00b7ne", "Haus\u00b7Po\u00b7stil\u00b7le", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PWS", "VVFIN", "PPER", "ADV", "$.", "PTKNEG", "ART", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.34": {"text": "Mich deucht/ die k\u00f6mt zu alt und sehr best\u00e4ubet raus.", "tokens": ["Mich", "deucht", "/", "die", "k\u00f6mt", "zu", "alt", "und", "sehr", "be\u00b7st\u00e4u\u00b7bet", "raus", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "ART", "VVFIN", "PTKA", "ADJD", "KON", "ADV", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.35": {"text": "Von Wa\u00dfer-Quellen w\u00e4rs nur eine kahle Grille.", "tokens": ["Von", "Wa\u00b7\u00dfe\u00b7rQuel\u00b7len", "w\u00e4rs", "nur", "ei\u00b7ne", "kah\u00b7le", "Gril\u00b7le", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.36": {"text": "Auch sieht mir ein JOURNAL, wie was gemeines aus.", "tokens": ["Auch", "sieht", "mir", "ein", "JoUR\u00b7NAL", ",", "wie", "was", "ge\u00b7mei\u00b7nes", "aus", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$,", "PWAV", "PIS", "ADJA", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.37": {"text": "Es d\u00fcrft ein LEXICON wohl schlechte Reime bringen.", "tokens": ["Es", "d\u00fcrft", "ein", "Le\u00b7XI\u00b7CON", "wohl", "schlech\u00b7te", "Rei\u00b7me", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ADV", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.38": {"text": "W\u00e4r etwas abgeschmackt/ so w\u00e4r es ein ROMAN.", "tokens": ["W\u00e4r", "et\u00b7was", "ab\u00b7ge\u00b7schmackt", "/", "so", "w\u00e4r", "es", "ein", "Ro\u00b7MAN", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VVPP", "$(", "ADV", "VAFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.39": {"text": "Man h\u00f6rt nicht gar zu gern das Lied der Weiber singen/", "tokens": ["Man", "h\u00f6rt", "nicht", "gar", "zu", "gern", "das", "Lied", "der", "Wei\u00b7ber", "sin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PTKNEG", "ADV", "PTKA", "ADV", "ART", "NN", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.40": {"text": "Drum k\u00e4m es ungeschickt auf ein Gesangbuch an.", "tokens": ["Drum", "k\u00e4m", "es", "un\u00b7ge\u00b7schickt", "auf", "ein", "Ge\u00b7sang\u00b7buch", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "ADJD", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.41": {"text": "Was will ich hin und her mit den Gedancken wandern?", "tokens": ["Was", "will", "ich", "hin", "und", "her", "mit", "den", "Ge\u00b7dan\u00b7cken", "wan\u00b7dern", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "PPER", "PTKVZ", "KON", "ADV", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.42": {"text": "Es sey ihm seine Braut in Paradie\u00df-G\u00e4rtlein!", "tokens": ["Es", "sey", "ihm", "sei\u00b7ne", "Braut", "in", "Pa\u00b7ra\u00b7die\u00df\u00b7G\u00e4rt\u00b7lein", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PPOSAT", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.43": {"text": "Hinckt etwan dieser Ver\u00df? Er schick zu Philandern.", "tokens": ["Hinckt", "et\u00b7wan", "die\u00b7ser", "Ver\u00df", "?", "Er", "schick", "zu", "Phi\u00b7lan\u00b7dern", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PDAT", "NN", "$.", "PPER", "VVFIN", "APPR", "NN", "$."], "meter": "-----+-+--+-", "measure": "iambic.tri.relaxed"}, "line.44": {"text": "Derselbe richt't ihm schon die Beine besser ein.", "tokens": ["Der\u00b7sel\u00b7be", "richt't", "ihm", "schon", "die", "Bei\u00b7ne", "bes\u00b7ser", "ein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "ART", "NN", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Jedoch ein Wort in Ernst/ worzu das Hertz geleget:", "tokens": ["Je\u00b7doch", "ein", "Wort", "in", "Ernst", "/", "wor\u00b7zu", "das", "Hertz", "ge\u00b7le\u00b7get", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "APPR", "NE", "$(", "PWAV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es sey sein neuer Stand dem Paradiese gleich!", "tokens": ["Es", "sey", "sein", "neu\u00b7er", "Stand", "dem", "Pa\u00b7ra\u00b7die\u00b7se", "gleich", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Er sey ein Lebens-Baum/ der tausend Fr\u00fcchte tr\u00e4get!", "tokens": ["Er", "sey", "ein", "Le\u00b7bens\u00b7Baum", "/", "der", "tau\u00b7send", "Fr\u00fcch\u00b7te", "tr\u00e4\u00b7get", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$(", "ART", "CARD", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Er sey/ ich sage mehr ein irrdisch Himmelreich.", "tokens": ["Er", "sey", "/", "ich", "sa\u00b7ge", "mehr", "ein", "irr\u00b7disch", "Him\u00b7mel\u00b7reich", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$(", "PPER", "VVFIN", "ADV", "ART", "ADJD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}