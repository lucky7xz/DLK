{"textgrid.poem.55627": {"metadata": {"author": {"name": "Goethe, Johann Wolfgang", "birth": "N.A.", "death": "N.A."}, "title": "9.", "genre": "verse", "period": "N.A.", "pub_year": 1790, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbsag, was enth\u00e4lt die Kirchengeschichte?", "tokens": ["\u00bb", "sag", ",", "was", "ent\u00b7h\u00e4lt", "die", "Kir\u00b7chen\u00b7ge\u00b7schich\u00b7te", "?"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "PWS", "VVFIN", "ART", "NN", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sie wird mir in Gedanken zunichte;", "tokens": ["Sie", "wird", "mir", "in", "Ge\u00b7dan\u00b7ken", "zu\u00b7nich\u00b7te", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Es gibt unendlich viel zu lesen,", "tokens": ["Es", "gibt", "un\u00b7end\u00b7lich", "viel", "zu", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PIS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Was ist denn aber das alles gewesen?\u00ab", "tokens": ["Was", "ist", "denn", "a\u00b7ber", "das", "al\u00b7les", "ge\u00b7we\u00b7sen", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VAFIN", "ADV", "ADV", "ART", "PIS", "VAPP", "$.", "$("], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Zwei Gegner sind es, die sich boxen,", "tokens": ["Zwei", "Geg\u00b7ner", "sind", "es", ",", "die", "sich", "bo\u00b7xen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "VAFIN", "PPER", "$,", "PRELS", "PRF", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Arianer und Orthodoxen.", "tokens": ["Die", "A\u00b7ri\u00b7a\u00b7ner", "und", "Or\u00b7tho\u00b7do\u00b7xen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Durch viele S\u00e4kla dasselbe geschicht,", "tokens": ["Durch", "vie\u00b7le", "S\u00e4k\u00b7la", "das\u00b7sel\u00b7be", "ge\u00b7schicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "PDAT", "VVPP", "$,"], "meter": "+--+--+--+", "measure": "dactylic.tetra"}, "line.4": {"text": "Es dauert bis an das J\u00fcngste Gericht.", "tokens": ["Es", "dau\u00b7ert", "bis", "an", "das", "J\u00fcngs\u00b7te", "Ge\u00b7richt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.3": {"line.1": {"text": "Der Vater ewig in Ruhe bleibt,", "tokens": ["Der", "Va\u00b7ter", "e\u00b7wig", "in", "Ru\u00b7he", "bleibt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Er hat der Welt sich einverleibt.", "tokens": ["Er", "hat", "der", "Welt", "sich", "ein\u00b7ver\u00b7leibt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "PRF", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der Sohn hat Gro\u00dfes unternommen:", "tokens": ["Der", "Sohn", "hat", "Gro\u00b7\u00dfes", "un\u00b7ter\u00b7nom\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Welt zu erl\u00f6sen, ist angekommen;", "tokens": ["Die", "Welt", "zu", "er\u00b7l\u00f6\u00b7sen", ",", "ist", "an\u00b7ge\u00b7kom\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVINF", "$,", "VAFIN", "VVPP", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Hat gut gelehrt und viel ertragen,", "tokens": ["Hat", "gut", "ge\u00b7lehrt", "und", "viel", "er\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "KON", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie das [?] noch heut in unsern Tagen.", "tokens": ["Wie", "das", "?", "noch", "heut", "in", "un\u00b7sern", "Ta\u00b7gen", "."], "token_info": ["word", "word", "punct", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "$(", "$.", "$(", "ADV", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Nun aber kommt der Heilig Geist,", "tokens": ["Nun", "a\u00b7ber", "kommt", "der", "Hei\u00b7lig", "Geist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Er wirkt am Pfingsten allermeist.", "tokens": ["Er", "wirkt", "am", "Pfings\u00b7ten", "al\u00b7ler\u00b7meist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Woher er kommt, wohin er weht,", "tokens": ["Wo\u00b7her", "er", "kommt", ",", "wo\u00b7hin", "er", "weht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das hat noch niemand ausgesp\u00e4ht.", "tokens": ["Das", "hat", "noch", "nie\u00b7mand", "aus\u00b7ge\u00b7sp\u00e4ht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "PIS", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Sie geben ihm nur eine kurze Frist,", "tokens": ["Sie", "ge\u00b7ben", "ihm", "nur", "ei\u00b7ne", "kur\u00b7ze", "Frist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Da er doch Erst' und Letzter ist.", "tokens": ["Da", "er", "doch", "Er\u00b7st'", "und", "Letz\u00b7ter", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NN", "KON", "NN", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.6": {"line.1": {"text": "Deswegen wir treulich, unverstohlen", "tokens": ["Des\u00b7we\u00b7gen", "wir", "treu\u00b7lich", ",", "un\u00b7ver\u00b7stoh\u00b7len"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PAV", "PPER", "ADJD", "$,", "ADJA"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Das alte Credo wiederholen:", "tokens": ["Das", "al\u00b7te", "Cre\u00b7do", "wie\u00b7der\u00b7ho\u00b7len", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Anbetend sind wir all' bereit", "tokens": ["An\u00b7be\u00b7tend", "sind", "wir", "all'", "be\u00b7reit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVPP", "VAFIN", "PPER", "PIAT", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Die ewige Dreifaltigkeit.", "tokens": ["Die", "e\u00b7wi\u00b7ge", "Drei\u00b7fal\u00b7tig\u00b7keit", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Mit Kirchengeschichte was hab ich zu schaffen?", "tokens": ["Mit", "Kir\u00b7chen\u00b7ge\u00b7schich\u00b7te", "was", "hab", "ich", "zu", "schaf\u00b7fen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PWS", "VAFIN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+--+--+--+-", "measure": "amphibrach.tetra"}, "line.2": {"text": "Ich sehe weiter nichts als Pfaffen;", "tokens": ["Ich", "se\u00b7he", "wei\u00b7ter", "nichts", "als", "Pfaf\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PIS", "KOKOM", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie's um die Christen steht, die Gemeinen,", "tokens": ["Wie's", "um", "die", "Chris\u00b7ten", "steht", ",", "die", "Ge\u00b7mei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVFIN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Davon will mir gar nichts erscheinen.", "tokens": ["Da\u00b7von", "will", "mir", "gar", "nichts", "er\u00b7schei\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VMFIN", "PPER", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ich h\u00e4tt auch k\u00f6nnen Gemeinde sagen,", "tokens": ["Ich", "h\u00e4tt", "auch", "k\u00f6n\u00b7nen", "Ge\u00b7mein\u00b7de", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VMFIN", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Ebensowenig w\u00e4re zu erfragen.", "tokens": ["E\u00b7ben\u00b7so\u00b7we\u00b7nig", "w\u00e4\u00b7re", "zu", "er\u00b7fra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}}, "stanza.9": {"line.1": {"text": "Glaubt nicht, da\u00df ich fasele, da\u00df ich dichte;", "tokens": ["Glaubt", "nicht", ",", "da\u00df", "ich", "fa\u00b7se\u00b7le", ",", "da\u00df", "ich", "dich\u00b7te", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKNEG", "$,", "KOUS", "PPER", "VVFIN", "$,", "KOUS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Seht hin und findet mir andre Gestalt!", "tokens": ["Seht", "hin", "und", "fin\u00b7det", "mir", "and\u00b7re", "Ge\u00b7stalt", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "KON", "VVFIN", "PPER", "ADJA", "NN", "$."], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Es ist die ganze Kirchengeschichte", "tokens": ["Es", "ist", "die", "gan\u00b7ze", "Kir\u00b7chen\u00b7ge\u00b7schich\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Mischmasch von Irrtum und von Gewalt.", "tokens": ["Mischmasch", "von", "Irr\u00b7tum", "und", "von", "Ge\u00b7walt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NE", "KON", "APPR", "NN", "$."], "meter": "+-+-++-+", "measure": "unknown.measure.penta"}}, "stanza.10": {"line.1": {"text": "Ihr Gl\u00e4ubigen! r\u00fchmt nur nicht euren Glauben", "tokens": ["Ihr", "Gl\u00e4u\u00b7bi\u00b7gen", "!", "r\u00fchmt", "nur", "nicht", "eu\u00b7ren", "Glau\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$.", "VVFIN", "ADV", "PTKNEG", "PPOSAT", "NN"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Als einzigen! Wir glauben auch wie ihr.", "tokens": ["Als", "ein\u00b7zi\u00b7gen", "!", "Wir", "glau\u00b7ben", "auch", "wie", "ihr", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "$.", "PPER", "VVFIN", "ADV", "KOKOM", "PPER", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Der Forscher l\u00e4\u00dft sich keineswegs berauben", "tokens": ["Der", "For\u00b7scher", "l\u00e4\u00dft", "sich", "kei\u00b7nes\u00b7wegs", "be\u00b7rau\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADV", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Des Erbteils, aller Welt geg\u00f6nnt \u2013 und mir.", "tokens": ["Des", "Erb\u00b7teils", ",", "al\u00b7ler", "Welt", "ge\u00b7g\u00f6nnt", "\u2013", "und", "mir", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PIAT", "NN", "VVPP", "$(", "KON", "PPER", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.11": {"line.1": {"text": "Ein Sadduz\u00e4er will ich bleiben! \u2013", "tokens": ["Ein", "Sad\u00b7du\u00b7z\u00e4\u00b7er", "will", "ich", "blei\u00b7ben", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VMFIN", "PPER", "VVINF", "$.", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das k\u00f6nnte mich zur Verzweiflung treiben,", "tokens": ["Das", "k\u00f6nn\u00b7te", "mich", "zur", "Ver\u00b7zwei\u00b7flung", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "PRF", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Wenn von dem Volk, das hier mich bedr\u00e4ngt,", "tokens": ["Wenn", "von", "dem", "Volk", ",", "das", "hier", "mich", "be\u00b7dr\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "$,", "PRELS", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Auch w\u00fcrde die Ewigkeit eingeengt;", "tokens": ["Auch", "w\u00fcr\u00b7de", "die", "E\u00b7wig\u00b7keit", "ein\u00b7ge\u00b7engt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Das w\u00e4re doch nur der alte Platsch,", "tokens": ["Das", "w\u00e4\u00b7re", "doch", "nur", "der", "al\u00b7te", "Platsch", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Droben g\u00e4b's nur verkl\u00e4rten Klatsch.", "tokens": ["Dro\u00b7ben", "g\u00e4b's", "nur", "ver\u00b7kl\u00e4r\u00b7ten", "Klatsch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ADJA", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.7": {"text": "\u00bbsei nicht so heftig, sei nicht so dumm!", "tokens": ["\u00bb", "sei", "nicht", "so", "hef\u00b7tig", ",", "sei", "nicht", "so", "dumm", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VAFIN", "PTKNEG", "ADV", "ADJD", "$,", "VAFIN", "PTKNEG", "ADV", "ADJD", "$."], "meter": "+--+-+--+", "measure": "iambic.tetra.invert"}, "line.8": {"text": "Da dr\u00fcben bildet sich alles um.\u00ab", "tokens": ["Da", "dr\u00fc\u00b7ben", "bil\u00b7det", "sich", "al\u00b7les", "um", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PRF", "PIS", "PTKVZ", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.12": {"line.1": {"text": "Ich habe nichts gegen die Fr\u00f6mmigkeit,", "tokens": ["Ich", "ha\u00b7be", "nichts", "ge\u00b7gen", "die", "Fr\u00f6m\u00b7mig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Sie ist zugleich Bequemlichkeit;", "tokens": ["Sie", "ist", "zu\u00b7gleich", "Be\u00b7quem\u00b7lich\u00b7keit", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wer ohne Fr\u00f6mmigkeit will leben,", "tokens": ["Wer", "oh\u00b7ne", "Fr\u00f6m\u00b7mig\u00b7keit", "will", "le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "NN", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Mu\u00df gro\u00dfer M\u00fche sich ergeben:", "tokens": ["Mu\u00df", "gro\u00b7\u00dfer", "M\u00fc\u00b7he", "sich", "er\u00b7ge\u00b7ben", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADJA", "NN", "PRF", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Auf seine eigne Hand zu wandern,", "tokens": ["Auf", "sei\u00b7ne", "eig\u00b7ne", "Hand", "zu", "wan\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "PTKZU", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Sich selbst gen\u00fcgen und den andern", "tokens": ["Sich", "selbst", "ge\u00b7n\u00fc\u00b7gen", "und", "den", "an\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PRF", "ADV", "VVINF", "KON", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Und freilich auch dabei vertraun,", "tokens": ["Und", "frei\u00b7lich", "auch", "da\u00b7bei", "ver\u00b7traun", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADV", "PAV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Gott werde wohl auf ihn niederschaun.", "tokens": ["Gott", "wer\u00b7de", "wohl", "auf", "ihn", "nie\u00b7der\u00b7schaun", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.13": {"line.1": {"text": "Wer Wissenschaft und Kunst besitzt,", "tokens": ["Wer", "Wis\u00b7sen\u00b7schaft", "und", "Kunst", "be\u00b7sitzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat auch Religion;", "tokens": ["Hat", "auch", "Re\u00b7li\u00b7gi\u00b7on", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Wer jene beiden nicht besitzt,", "tokens": ["Wer", "je\u00b7ne", "bei\u00b7den", "nicht", "be\u00b7sitzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "PIAT", "PTKNEG", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der habe Religion.", "tokens": ["Der", "ha\u00b7be", "Re\u00b7li\u00b7gi\u00b7on", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Niemand soll ins Kloster gehn,", "tokens": ["Nie\u00b7mand", "soll", "ins", "Klos\u00b7ter", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "APPRART", "NN", "VVINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Als er sei denn wohl versehn", "tokens": ["Als", "er", "sei", "denn", "wohl", "ver\u00b7sehn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "VAFIN", "ADV", "ADV", "VVINF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Mit geh\u00f6rigem S\u00fcndenvorrat,", "tokens": ["Mit", "ge\u00b7h\u00f6\u00b7ri\u00b7gem", "S\u00fcn\u00b7den\u00b7vor\u00b7rat", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Damit es ihm so fr\u00fch als spat", "tokens": ["Da\u00b7mit", "es", "ihm", "so", "fr\u00fch", "als", "spat"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADJD", "KOKOM", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nicht m\u00f6g am Vergn\u00fcgen fehlen,", "tokens": ["Nicht", "m\u00f6g", "am", "Ver\u00b7gn\u00fc\u00b7gen", "feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VMFIN", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Sich mit Reue durchzuqu\u00e4len.", "tokens": ["Sich", "mit", "Reu\u00b7e", "durch\u00b7zu\u00b7qu\u00e4\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "APPR", "NN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "La\u00dft euch nur von Pfaffen sagen,", "tokens": ["La\u00dft", "euch", "nur", "von", "Pfaf\u00b7fen", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "APPR", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was die Kreuzigung eingetragen!", "tokens": ["Was", "die", "Kreu\u00b7zi\u00b7gung", "ein\u00b7ge\u00b7tra\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "VVPP", "$."], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Niemand kommt zum h\u00f6chsten Flor", "tokens": ["Nie\u00b7mand", "kommt", "zum", "h\u00f6chs\u00b7ten", "Flor"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPRART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Von Kranz und Orden,", "tokens": ["Von", "Kranz", "und", "Or\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Wenn einer nicht zuvor", "tokens": ["Wenn", "ei\u00b7ner", "nicht", "zu\u00b7vor"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PIS", "PTKNEG", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "Derb gedroschen worden.", "tokens": ["Derb", "ge\u00b7dro\u00b7schen", "wor\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "VAPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.16": {"line.1": {"text": "Den deutschen Mannen gereicht's zum Ruhm,", "tokens": ["Den", "deut\u00b7schen", "Man\u00b7nen", "ge\u00b7reicht's", "zum", "Ruhm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da\u00df sie geha\u00dft das Christentum,", "tokens": ["Da\u00df", "sie", "ge\u00b7ha\u00dft", "das", "Chris\u00b7ten\u00b7tum", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Bis Herrn Carolus' leidigem Degen", "tokens": ["Bis", "Herrn", "Ca\u00b7ro\u00b7lus'", "lei\u00b7di\u00b7gem", "De\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "NE", "ADJA", "NN"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Die edlen Sachsen unterlegen.", "tokens": ["Die", "ed\u00b7len", "Sach\u00b7sen", "un\u00b7ter\u00b7le\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Doch haben sie lange genug gerungen,", "tokens": ["Doch", "ha\u00b7ben", "sie", "lan\u00b7ge", "ge\u00b7nug", "ge\u00b7run\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Bis endlich die Pfaffen sie bezwungen", "tokens": ["Bis", "end\u00b7lich", "die", "Pfaf\u00b7fen", "sie", "be\u00b7zwun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "ART", "NN", "PPER", "VVPP"], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und sie sich unters Joch geduckt;", "tokens": ["Und", "sie", "sich", "un\u00b7ters", "Joch", "ge\u00b7duckt", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "PRF", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Doch haben sie immer einmal gemuckt.", "tokens": ["Doch", "ha\u00b7ben", "sie", "im\u00b7mer", "ein\u00b7mal", "ge\u00b7muckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Sie lagen nur im halben Schlaf,", "tokens": ["Sie", "la\u00b7gen", "nur", "im", "hal\u00b7ben", "Schlaf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Als Luther die Bibel verdeutscht so brav.", "tokens": ["Als", "Lu\u00b7ther", "die", "Bi\u00b7bel", "ver\u00b7deutscht", "so", "brav", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ART", "NN", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.7": {"text": "Sankt Paulus, wie ein Ritter derb,", "tokens": ["Sankt", "Pau\u00b7lus", ",", "wie", "ein", "Rit\u00b7ter", "derb", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "$,", "PWAV", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Erschien den Rittern minder herb.", "tokens": ["Er\u00b7schien", "den", "Rit\u00b7tern", "min\u00b7der", "herb", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Freiheit erwacht in jeder Brust,", "tokens": ["Frei\u00b7heit", "er\u00b7wacht", "in", "je\u00b7der", "Brust", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Wir protestieren all mit Lust.", "tokens": ["Wir", "pro\u00b7tes\u00b7tie\u00b7ren", "all", "mit", "Lust", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIAT", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "\u00bbist Konkordat und Kirchenplan", "tokens": ["\u00bb", "ist", "Kon\u00b7kor\u00b7dat", "und", "Kir\u00b7chen\u00b7plan"], "token_info": ["punct", "word", "word", "word", "word"], "pos": ["$(", "VAFIN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht gl\u00fccklich durchgef\u00fchrt?\u00ab \u2013", "tokens": ["Nicht", "gl\u00fcck\u00b7lich", "durch\u00b7ge\u00b7f\u00fchrt", "?", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["PTKNEG", "ADJD", "VVPP", "$.", "$(", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ja fangt einmal mit Rom nur an,", "tokens": ["Ja", "fangt", "ein\u00b7mal", "mit", "Rom", "nur", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "VVFIN", "ADV", "APPR", "NE", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da seid ihr angef\u00fchrt.", "tokens": ["Da", "seid", "ihr", "an\u00b7ge\u00b7f\u00fchrt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}