{"textgrid.poem.31285": {"metadata": {"author": {"name": "Ramler, Karl Wilhelm", "birth": "N.A.", "death": "N.A."}, "title": "1L: Soll wieder eine ganze Welt vergehen?", "genre": "verse", "period": "N.A.", "pub_year": 1761, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Soll wieder eine ganze Welt vergehen?", "tokens": ["Soll", "wie\u00b7der", "ei\u00b7ne", "gan\u00b7ze", "Welt", "ver\u00b7ge\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Bricht wieder eine S\u00fcndfluth ein?", "tokens": ["Bricht", "wie\u00b7der", "ei\u00b7ne", "S\u00fcnd\u00b7fluth", "ein", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und sollen wieder alle Tempel und Troph\u00e4en", "tokens": ["Und", "sol\u00b7len", "wie\u00b7der", "al\u00b7le", "Tem\u00b7pel", "und", "Trop\u00b7h\u00e4\u00b7en"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VMFIN", "ADV", "PIAT", "NN", "KON", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Ber\u00fchmte Tr\u00fcmmer seyn?", "tokens": ["Be\u00b7r\u00fchm\u00b7te", "Tr\u00fcm\u00b7mer", "seyn", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Und alle K\u00fcnste sp\u00e4t aus Asch' und Moder", "tokens": ["Und", "al\u00b7le", "K\u00fcns\u00b7te", "sp\u00e4t", "aus", "Asch'", "und", "Mo\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "NN", "ADJD", "APPR", "NE", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Und Todtengr\u00fcften aufersteh'n,", "tokens": ["Und", "Tod\u00b7ten\u00b7gr\u00fcf\u00b7ten", "auf\u00b7er\u00b7steh'n", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und aus der Nacht des regellosen Zufalls oder", "tokens": ["Und", "aus", "der", "Nacht", "des", "re\u00b7gel\u00b7lo\u00b7sen", "Zu\u00b7falls", "o\u00b7der"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ART", "ADJA", "NN", "KON"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Auf ewig untergeh'n?", "tokens": ["Auf", "e\u00b7wig", "un\u00b7ter\u00b7geh'n", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Wenn nun die weise Vorwelt ausgestorben,", "tokens": ["Wenn", "nun", "die", "wei\u00b7se", "Vor\u00b7welt", "aus\u00b7ge\u00b7stor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Das unerzogne Kindeskind", "tokens": ["Das", "un\u00b7er\u00b7zog\u00b7ne", "Kin\u00b7des\u00b7kind"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein R\u00e4uber ist; die nicht zu R\u00e4ubern angeworben,", "tokens": ["Ein", "R\u00e4u\u00b7ber", "ist", ";", "die", "nicht", "zu", "R\u00e4u\u00b7bern", "an\u00b7ge\u00b7wor\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$.", "ART", "PTKNEG", "APPR", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Armsel'ge Pfl\u00fcger sind? \u2013 \u2013", "tokens": ["Arm\u00b7sel'\u00b7ge", "Pfl\u00fc\u00b7ger", "sind", "?", "\u2013", "\u2013"], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["ADJA", "NN", "VAFIN", "$.", "$(", "$("], "meter": "+--+-+", "measure": "iambic.tri.invert"}}, "stanza.4": {"line.1": {"text": "O ihr, verderblicher, als der entbrannte", "tokens": ["O", "ihr", ",", "ver\u00b7derb\u00b7li\u00b7cher", ",", "als", "der", "ent\u00b7brann\u00b7te"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NE", "PPOSAT", "$,", "ADJA", "$,", "KOUS", "ART", "ADJA"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "Vesuv, als unterirdische", "tokens": ["Ve\u00b7suv", ",", "als", "un\u00b7ter\u00b7ir\u00b7di\u00b7sche"], "token_info": ["word", "punct", "word", "word"], "pos": ["NE", "$,", "KOUS", "ADJA"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Gewitter! ihr des magern Hungers Bundsverwandte,", "tokens": ["Ge\u00b7wit\u00b7ter", "!", "ihr", "des", "ma\u00b7gern", "Hun\u00b7gers", "Bunds\u00b7ver\u00b7wand\u00b7te", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$.", "PPER", "ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der Pest Verschworene!", "tokens": ["Der", "Pest", "Ver\u00b7schwo\u00b7re\u00b7ne", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$."], "meter": "-+-+--", "measure": "unknown.measure.di"}}, "stanza.5": {"line.1": {"text": "Die ihr den schnellen Tod in alle Meere", "tokens": ["Die", "ihr", "den", "schnel\u00b7len", "Tod", "in", "al\u00b7le", "Mee\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "ART", "ADJA", "NN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Auf Donnergaleonen bringt,", "tokens": ["Auf", "Don\u00b7ner\u00b7ga\u00b7le\u00b7o\u00b7nen", "bringt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und von Lisboa bis zum kalten Oby Heere", "tokens": ["Und", "von", "Lis\u00b7boa", "bis", "zum", "kal\u00b7ten", "O\u00b7by", "Hee\u00b7re"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "NE", "APPR", "APPRART", "ADJA", "NN", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.4": {"text": "Zum Wechselmorde dingt!", "tokens": ["Zum", "Wech\u00b7sel\u00b7mor\u00b7de", "dingt", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Und ach! mit Deutschlands B\u00fcrgern Deutschlands B\u00fcrger", "tokens": ["Und", "ach", "!", "mit", "Deutschlands", "B\u00fcr\u00b7gern", "Deutschlands", "B\u00fcr\u00b7ger"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "XY", "$.", "APPR", "NE", "NN", "NE", "NN"], "meter": "-+-+---+-", "measure": "unknown.measure.tri"}, "line.2": {"text": "Zerfleischet, einen bessern Held,", "tokens": ["Zer\u00b7flei\u00b7schet", ",", "ei\u00b7nen", "bes\u00b7sern", "Held", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Brennen weisen K\u00f6nig zu betr\u00fcben, W\u00fcrger", "tokens": ["Der", "Bren\u00b7nen", "wei\u00b7sen", "K\u00f6\u00b7nig", "zu", "be\u00b7tr\u00fc\u00b7ben", ",", "W\u00fcr\u00b7ger"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "ADJA", "NN", "PTKZU", "VVINF", "$,", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der Welt und Afterwelt!", "tokens": ["Der", "Welt", "und", "Af\u00b7ter\u00b7welt", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Wenn eurer Mordsucht einst ein Friede wehret,", "tokens": ["Wenn", "eu\u00b7rer", "Mord\u00b7sucht", "einst", "ein", "Frie\u00b7de", "weh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Der jedem das geraubte Land", "tokens": ["Der", "je\u00b7dem", "das", "ge\u00b7raub\u00b7te", "Land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und seine bangen Feste wieder gibt, \u2013 verheeret,", "tokens": ["Und", "sei\u00b7ne", "ban\u00b7gen", "Fes\u00b7te", "wie\u00b7der", "gibt", ",", "\u2013", "ver\u00b7hee\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct"], "pos": ["KON", "PPOSAT", "ADJA", "NN", "ADV", "VVFIN", "$,", "$(", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Entv\u00f6lkert, abgebrannt:", "tokens": ["Ent\u00b7v\u00f6l\u00b7kert", ",", "ab\u00b7ge\u00b7brannt", ":"], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVFIN", "$,", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Ihr K\u00f6nige, wie wird es euch nicht reuen,", "tokens": ["Ihr", "K\u00f6\u00b7ni\u00b7ge", ",", "wie", "wird", "es", "euch", "nicht", "reu\u00b7en", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWAV", "VAFIN", "PPER", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "(wo nicht die fromme Reue fleucht,", "tokens": ["(", "wo", "nicht", "die", "from\u00b7me", "Reu\u00b7e", "fleucht", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "PTKNEG", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Durch Wollust, falsche Weisheit, lauter Schmeicheleien", "tokens": ["Durch", "Wol\u00b7lust", ",", "fal\u00b7sche", "Weis\u00b7heit", ",", "lau\u00b7ter", "Schmei\u00b7che\u00b7lei\u00b7en"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "$,", "ADJA", "NN", "$,", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Des H\u00f6flings weggescheucht)", "tokens": ["Des", "H\u00f6f\u00b7lings", "weg\u00b7ge\u00b7scheucht", ")"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Da\u00df euer Stahl unmenschlich Millionen", "tokens": ["Da\u00df", "eu\u00b7er", "Stahl", "un\u00b7menschlich", "Mil\u00b7lion\u00b7en"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "ADJD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Urenkels\u00f6hne niederstie\u00df:", "tokens": ["Ur\u00b7en\u00b7kel\u00b7s\u00f6h\u00b7ne", "nie\u00b7ders\u00b7tie\u00df", ":"], "token_info": ["word", "word", "punct"], "pos": ["NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df keiner, satt des Ungl\u00fccks, seine Legionen", "tokens": ["Da\u00df", "kei\u00b7ner", ",", "satt", "des", "Un\u00b7gl\u00fccks", ",", "sei\u00b7ne", "Le\u00b7gi\u00b7o\u00b7nen"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PIS", "$,", "KOUI", "ART", "NN", "$,", "PPOSAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Das Blutfeld r\u00e4umen hie\u00df,", "tokens": ["Das", "Blut\u00b7feld", "r\u00e4u\u00b7men", "hie\u00df", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Und lieber, schuldlos, tapfer, durch die Wogen", "tokens": ["Und", "lie\u00b7ber", ",", "schuld\u00b7los", ",", "tap\u00b7fer", ",", "durch", "die", "Wo\u00b7gen"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["KON", "ADV", "$,", "ADJD", "$,", "ADJD", "$,", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Des stillen Oceans den Pfad", "tokens": ["Des", "stil\u00b7len", "O\u00b7ceans", "den", "Pfad"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "ART", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Gesuchet, eine Welt entdeckt, ein Volk erzogen,", "tokens": ["Ge\u00b7su\u00b7chet", ",", "ei\u00b7ne", "Welt", "ent\u00b7deckt", ",", "ein", "Volk", "er\u00b7zo\u00b7gen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "ART", "NN", "VVPP", "$,", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Wie Manko Kapak that,", "tokens": ["Wie", "Man\u00b7ko", "Ka\u00b7pak", "that", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "NN", "VVFIN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Der neue Sch\u00f6pfer seiner Vatererde:", "tokens": ["Der", "neu\u00b7e", "Sch\u00f6p\u00b7fer", "sei\u00b7ner", "Va\u00b7te\u00b7rer\u00b7de", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Er theilte Feld und Binsenhaus", "tokens": ["Er", "theil\u00b7te", "Feld", "und", "Bin\u00b7sen\u00b7haus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und Weib und Kleid und Zucht und G\u00f6tter einer Heerde", "tokens": ["Und", "Weib", "und", "Kleid", "und", "Zucht", "und", "G\u00f6t\u00b7ter", "ei\u00b7ner", "Heer\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NN", "KON", "NN", "KON", "NN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Zerstreuter Wilden aus;", "tokens": ["Zer\u00b7streu\u00b7ter", "Wil\u00b7den", "aus", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Und hie\u00df dem frommen Volk ein Sohn der Sonne,", "tokens": ["Und", "hie\u00df", "dem", "from\u00b7men", "Volk", "ein", "Sohn", "der", "Son\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "ADJA", "NN", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Gleich milde, wachsam so wie sie,", "tokens": ["Gleich", "mil\u00b7de", ",", "wach\u00b7sam", "so", "wie", "sie", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "ADJD", "ADV", "KOKOM", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und so wie sie des neugebornen Landes Wonne,", "tokens": ["Und", "so", "wie", "sie", "des", "neu\u00b7ge\u00b7bor\u00b7nen", "Lan\u00b7des", "Won\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "KOKOM", "PPER", "ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und ewig jung wie sie.", "tokens": ["Und", "e\u00b7wig", "jung", "wie", "sie", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ADJD", "KOKOM", "PPER", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}