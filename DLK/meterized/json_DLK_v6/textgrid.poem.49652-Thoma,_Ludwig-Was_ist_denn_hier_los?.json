{"textgrid.poem.49652": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Was ist denn hier los?", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Von Nord nach S\u00fcd, von S\u00fcd nach Nord", "tokens": ["Von", "Nord", "nach", "S\u00fcd", ",", "von", "S\u00fcd", "nach", "Nord"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NE", "APPR", "NN", "$,", "APPR", "NN", "APPR", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ert\u00f6net mildernd manches Wort:", "tokens": ["Er\u00b7t\u00f6\u00b7net", "mil\u00b7dernd", "man\u00b7ches", "Wort", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbihr seid ja auch ganz annehmbar,", "tokens": ["\u00bb", "ihr", "seid", "ja", "auch", "ganz", "an\u00b7nehm\u00b7bar", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ADV", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und alle sind wir deutsch \u2013 nich wahr?", "tokens": ["Und", "al\u00b7le", "sind", "wir", "deutsch", "\u2013", "nich", "wahr", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PIS", "VAFIN", "PPER", "ADJD", "$(", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Wir sind doch Br\u00fcder sozusagen,", "tokens": ["Wir", "sind", "doch", "Br\u00fc\u00b7der", "so\u00b7zu\u00b7sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und haben es vereint geschlagen,", "tokens": ["Und", "ha\u00b7ben", "es", "ver\u00b7eint", "ge\u00b7schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "VVPP", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das hehre Anno siebenzich", "tokens": ["Das", "heh\u00b7re", "An\u00b7no", "sie\u00b7ben\u00b7zich"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In Treue fest auf Hieb und Stich!\u00ab", "tokens": ["In", "Treu\u00b7e", "fest", "auf", "Hieb", "und", "Stich", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "NN", "ADJD", "APPR", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Was ist denn das f\u00fcr ein Getute", "tokens": ["Was", "ist", "denn", "das", "f\u00fcr", "ein", "Ge\u00b7tu\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ADV", "ART", "APPR", "ART", "NN"], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Von festem Kitt und Bruderblute,", "tokens": ["Von", "fes\u00b7tem", "Kitt", "und", "Bru\u00b7der\u00b7blu\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als ging's zu End' mit einemmal \u2013?", "tokens": ["Als", "ging's", "zu", "End'", "mit", "ei\u00b7nem\u00b7mal", "\u2013", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "VVFIN", "APPR", "NN", "APPR", "ADV", "$(", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und alles \u2013 weil ein Jeneral \u2013 \u2013", "tokens": ["Und", "al\u00b7les", "\u2013", "weil", "ein", "Je\u00b7ne\u00b7ral", "\u2013", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PIS", "$(", "KOUS", "ART", "NN", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Weil sich ein kleiner Herr verfahren?", "tokens": ["Weil", "sich", "ein", "klei\u00b7ner", "Herr", "ver\u00b7fah\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das macht nach dreiundvierzig Jahren,", "tokens": ["Das", "macht", "nach", "drei\u00b7und\u00b7vier\u00b7zig", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df man die bange Frage stellt,", "tokens": ["Da\u00df", "man", "die", "ban\u00b7ge", "Fra\u00b7ge", "stellt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ob unser Deutschland l\u00e4nger h\u00e4lt!", "tokens": ["Ob", "un\u00b7ser", "Deutschland", "l\u00e4n\u00b7ger", "h\u00e4lt", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ADJD", "VVFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "Nerv\u00f6se Zeit nerv\u00f6ser Zappler,", "tokens": ["Ner\u00b7v\u00f6\u00b7se", "Zeit", "ner\u00b7v\u00f6\u00b7ser", "Zapp\u00b7ler", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADJA", "NN", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.2": {"text": "Luftangef\u00fcllter Phrasenpappler!", "tokens": ["Luf\u00b7tan\u00b7ge\u00b7f\u00fcll\u00b7ter", "Phra\u00b7sen\u00b7papp\u00b7ler", "!"], "token_info": ["word", "word", "punct"], "pos": ["NN", "NE", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Begraben liegt blo\u00df hier der Hund,", "tokens": ["Be\u00b7gra\u00b7ben", "liegt", "blo\u00df", "hier", "der", "Hund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df man das Maul nicht halten kunnt'.", "tokens": ["Da\u00df", "man", "das", "Maul", "nicht", "hal\u00b7ten", "kunnt'", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ART", "NN", "PTKNEG", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}