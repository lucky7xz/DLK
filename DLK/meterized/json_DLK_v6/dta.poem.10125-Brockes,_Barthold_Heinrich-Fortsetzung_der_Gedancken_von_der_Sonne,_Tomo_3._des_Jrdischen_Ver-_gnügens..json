{"dta.poem.10125": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Fortsetzung der Gedancken von der  \n Sonne,  Tomo  3. des Jrdischen Ver-  \n gn\u00fcgens.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1735", "urn": "urn:nbn:de:kobv:b4-20086-0", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Da\u00df es der Wahrheit \u00e4hnlich scheint,", "tokens": ["Da\u00df", "es", "der", "Wahr\u00b7heit", "\u00e4hn\u00b7lich", "scheint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn man von allen Sonnen meint,", "tokens": ["Wenn", "man", "von", "al\u00b7len", "Son\u00b7nen", "meint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "APPR", "PIAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df sie nur Oeffnungen am Firmament,", "tokens": ["Da\u00df", "sie", "nur", "Oeff\u00b7nun\u00b7gen", "am", "Fir\u00b7ma\u00b7ment", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wodurch ein Theil vom Licht, so GOttes Thron \u00fcmschr\u00e4nckt,", "tokens": ["Wo\u00b7durch", "ein", "Theil", "vom", "Licht", ",", "so", "Got\u00b7tes", "Thron", "\u00fcm\u00b7schr\u00e4nckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPRART", "NN", "$,", "ADV", "NN", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Zum Nutz der Creatur, begrentzt sich abw\u00e4rts senckt;", "tokens": ["Zum", "Nutz", "der", "Crea\u00b7tur", ",", "be\u00b7grentzt", "sich", "ab\u00b7w\u00e4rts", "senckt", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,", "VVFIN", "PRF", "ADV", "VVFIN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.6": {"text": "Darin hat mich noch j\u00fcngst ein heller Morgen", "tokens": ["Da\u00b7rin", "hat", "mich", "noch", "j\u00fcngst", "ein", "hel\u00b7ler", "Mor\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PAV", "VAFIN", "PPER", "ADV", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Noch mehr befestigt und best\u00e4rckt.", "tokens": ["Noch", "mehr", "be\u00b7fes\u00b7tigt", "und", "be\u00b7st\u00e4rckt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "KON", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Mein gr\u00fcner Vorhang hielte mich", "tokens": ["Mein", "gr\u00fc\u00b7ner", "Vor\u00b7hang", "hiel\u00b7te", "mich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Jm Bette, vor dem Strahl der Sonnen, noch verborgen,", "tokens": ["Jm", "Bet\u00b7te", ",", "vor", "dem", "Strahl", "der", "Son\u00b7nen", ",", "noch", "ver\u00b7bor\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "APPR", "ART", "NN", "ART", "NN", "$,", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Da ich in Schatten eigentlich", "tokens": ["Da", "ich", "in", "Schat\u00b7ten", "ei\u00b7gent\u00b7lich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "APPR", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Den Durchbruch des so offt getheilten Lichts bemerckt.", "tokens": ["Den", "Durch\u00b7bruch", "des", "so", "offt", "ge\u00b7theil\u00b7ten", "Lichts", "be\u00b7merckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADV", "ADV", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ich sah\u2019 an tausend tausend Orten,", "tokens": ["Ich", "sah'", "an", "tau\u00b7send", "tau\u00b7send", "Or\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "CARD", "CARD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Als so viel kleine runde Pforten,", "tokens": ["Als", "so", "viel", "klei\u00b7ne", "run\u00b7de", "Pfor\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Viel zarte Lichter, recht als helle Stern\u2019, und zwar,", "tokens": ["Viel", "zar\u00b7te", "Lich\u00b7ter", ",", "recht", "als", "hel\u00b7le", "Stern'", ",", "und", "zwar", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$,", "ADJD", "KOKOM", "ADJA", "NN", "$,", "KON", "ADV", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Nach mehr erweiterten, und mehr verengten Grentzen,", "tokens": ["Nach", "mehr", "er\u00b7wei\u00b7ter\u00b7ten", ",", "und", "mehr", "ver\u00b7eng\u00b7ten", "Grent\u00b7zen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "$,", "KON", "PIAT", "ADJA", "NN", "$,"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.9": {"text": "Von erster, anderer, und dritter Gr\u00f6sse, gl\u00e4ntzen.", "tokens": ["Von", "ers\u00b7ter", ",", "an\u00b7de\u00b7rer", ",", "und", "drit\u00b7ter", "Gr\u00f6s\u00b7se", ",", "gl\u00e4nt\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "ADJA", "$,", "ADJA", "$,", "KON", "ADJA", "NN", "$,", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Jhr unterschiedlicher, nie gleicher Abstand war", "tokens": ["Ihr", "un\u00b7ter\u00b7schied\u00b7li\u00b7cher", ",", "nie", "glei\u00b7cher", "Ab\u00b7stand", "war"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "$,", "ADV", "ADJA", "NN", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ein dentlich Ebenbild der mancherley Figuren,", "tokens": ["Ein", "dent\u00b7lich", "E\u00b7ben\u00b7bild", "der", "man\u00b7cher\u00b7ley", "Fi\u00b7gu\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "NN", "ART", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "So die Astronomie uns in den Sternen zeigen.", "tokens": ["So", "die", "Ast\u00b7ro\u00b7no\u00b7mie", "uns", "in", "den", "Ster\u00b7nen", "zei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+++-+-+-+-+-", "measure": "unknown.measure.septa"}, "line.4": {"text": "Ich fand mit leichter M\u00fch dieselben Creaturen;", "tokens": ["Ich", "fand", "mit", "leich\u00b7ter", "M\u00fch", "die\u00b7sel\u00b7ben", "Crea\u00b7tu\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN", "PDAT", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "Weil ihre Menge mir Gelegenheit genug", "tokens": ["Weil", "ih\u00b7re", "Men\u00b7ge", "mir", "Ge\u00b7le\u00b7gen\u00b7heit", "ge\u00b7nug"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPOSAT", "NN", "PPER", "NN", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Zu mehren, zu zerstreun, zu theilen, zu vereinen,", "tokens": ["Zu", "meh\u00b7ren", ",", "zu", "zer\u00b7streun", ",", "zu", "thei\u00b7len", ",", "zu", "ver\u00b7ei\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PTKZU", "VVINF", "$,", "PTKZU", "VVINF", "$,", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Und vorzustellen gab: da\u00df ich mit Fug", "tokens": ["Und", "vor\u00b7zu\u00b7stel\u00b7len", "gab", ":", "da\u00df", "ich", "mit", "Fug"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVIZU", "VVFIN", "$.", "KOUS", "PPER", "APPR", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Den grossen B\u00e4ren, und den kleinen,", "tokens": ["Den", "gros\u00b7sen", "B\u00e4\u00b7ren", ",", "und", "den", "klei\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "KON", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Orion, Pleiades, den Fuhrmann, nebst den Ziegen,", "tokens": ["O\u00b7rion", ",", "Plei\u00b7a\u00b7des", ",", "den", "Fuhr\u00b7mann", ",", "nebst", "den", "Zie\u00b7gen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "ART", "NN", "$,", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.10": {"text": "Den Schwan, Delphin, den Stier, durch ein gewisses f\u00fcgen,", "tokens": ["Den", "Schwan", ",", "Del\u00b7phin", ",", "den", "Stier", ",", "durch", "ein", "ge\u00b7wis\u00b7ses", "f\u00fc\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NE", "$,", "ART", "NN", "$,", "APPR", "ART", "ADJA", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Durch ein willk\u00fchrliches verbinden und zertrennen,", "tokens": ["Durch", "ein", "will\u00b7k\u00fchr\u00b7li\u00b7ches", "ver\u00b7bin\u00b7den", "und", "zer\u00b7tren\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "VVINF", "KON", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Dir h\u00e4tte deutlich zeigen k\u00f6nnen.", "tokens": ["Dir", "h\u00e4t\u00b7te", "deut\u00b7lich", "zei\u00b7gen", "k\u00f6n\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "VVINF", "VMINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Hier\u00fcber fielen mir aufs neu", "tokens": ["Hier\u00b7\u00fc\u00b7ber", "fie\u00b7len", "mir", "aufs", "neu"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "PPER", "APPRART", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die vorigen Gedancken bey:", "tokens": ["Die", "vo\u00b7ri\u00b7gen", "Ge\u00b7dan\u00b7cken", "bey", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie sehr w\u00fcrd\u2019 einer, dacht ich, irren,", "tokens": ["Wie", "sehr", "w\u00fcrd'", "ei\u00b7ner", ",", "dacht", "ich", ",", "ir\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "ADV", "VAFIN", "PIS", "$,", "VVFIN", "PPER", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie l\u00e4cherlich w\u00fcrd\u2019 er, was wahr und falsch, verwirren,", "tokens": ["Wie", "l\u00e4\u00b7cher\u00b7lich", "w\u00fcrd'", "er", ",", "was", "wahr", "und", "falsch", ",", "ver\u00b7wir\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "PPER", "$,", "PWS", "ADJD", "KON", "ADJD", "$,", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der diese Lichterchen f\u00fcr eigne Lichter nehmen,", "tokens": ["Der", "die\u00b7se", "Lich\u00b7ter\u00b7chen", "f\u00fcr", "eig\u00b7ne", "Lich\u00b7ter", "neh\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PDAT", "NN", "APPR", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und ihren Ursprung nicht,", "tokens": ["Und", "ih\u00b7ren", "Ur\u00b7sprung", "nicht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "PTKNEG", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.7": {"text": "Bey einem allgemeinen Licht,", "tokens": ["Bey", "ei\u00b7nem", "all\u00b7ge\u00b7mei\u00b7nen", "Licht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Beym Strahl der Sonnen, suchen wollte!", "tokens": ["Beym", "Strahl", "der", "Son\u00b7nen", ",", "su\u00b7chen", "woll\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$,", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Er m\u00fcste sich gewi\u00df des groben Jrrthums sch\u00e4men.", "tokens": ["Er", "m\u00fcs\u00b7te", "sich", "ge\u00b7wi\u00df", "des", "gro\u00b7ben", "Jrr\u00b7thums", "sch\u00e4\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PRF", "ADV", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "So kann (und ist es gantz vermuthlich) in der That", "tokens": ["So", "kann", "(", "und", "ist", "es", "gantz", "ver\u00b7muth\u00b7lich", ")", "in", "der", "That"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VMFIN", "$(", "KON", "VAFIN", "PPER", "ADV", "ADJD", "$(", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Das pr\u00e4chtige Sapphirne Firmament", "tokens": ["Das", "pr\u00e4ch\u00b7ti\u00b7ge", "Sap\u00b7phir\u00b7ne", "Fir\u00b7ma\u00b7ment"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "NN"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.12": {"text": "Allein der Vorhang seyn,", "tokens": ["Al\u00b7lein", "der", "Vor\u00b7hang", "seyn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VAINF", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "Wodurch der hinter ihm vorhandne Schein", "tokens": ["Wo\u00b7durch", "der", "hin\u00b7ter", "ihm", "vor\u00b7hand\u00b7ne", "Schein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "APPR", "PPER", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Von GOttes heilgem Thron, zu unserm Nutzen brennt.", "tokens": ["Von", "Got\u00b7tes", "heil\u00b7gem", "Thron", ",", "zu", "un\u00b7serm", "Nut\u00b7zen", "brennt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADJA", "NN", "$,", "APPR", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Es kommt aufs mindste dieses mir", "tokens": ["Es", "kommt", "aufs", "minds\u00b7te", "die\u00b7ses", "mir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "ADJA", "NN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.16": {"text": "Als ein Gedancke f\u00fcr,", "tokens": ["Als", "ein", "Ge\u00b7dan\u00b7cke", "f\u00fcr", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "APPR", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.17": {"text": "Der, zu des Sch\u00f6pfers Ruhm, was Grosses in sich hat.", "tokens": ["Der", ",", "zu", "des", "Sch\u00f6p\u00b7fers", "Ruhm", ",", "was", "Gros\u00b7ses", "in", "sich", "hat", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "$,", "APPR", "ART", "NN", "NN", "$,", "PWS", "NN", "APPR", "PRF", "VAFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}