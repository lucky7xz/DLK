{"textgrid.poem.49641": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Bei Dressel", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.71", "af:0.28"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbwie geht's in Afrika?\u00ab \u2013 \u00bbJe nun, wir m\u00fcssen,", "tokens": ["\u00bb", "wie", "geht's", "in", "Af\u00b7ri\u00b7ka", "?", "\u00ab", "\u2013", "\u00bb", "Je", "nun", ",", "wir", "m\u00fcs\u00b7sen", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct", "punct", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["$(", "PWAV", "VVFIN", "APPR", "NE", "$.", "$(", "$(", "$(", "ADV", "ADV", "$,", "PPER", "VMFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Was wir uns eingebrockt, geduldig fressen.", "tokens": ["Was", "wir", "uns", "ein\u00b7ge\u00b7brockt", ",", "ge\u00b7dul\u00b7dig", "fres\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "PPER", "PPER", "VVPP", "$,", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Es bleibt nichts andres, und von allen Schl\u00fcssen", "tokens": ["Es", "bleibt", "nichts", "and\u00b7res", ",", "und", "von", "al\u00b7len", "Schl\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PIS", "PIS", "$,", "KON", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Ist dieser Schlu\u00df der Lage angemessen.\u00ab", "tokens": ["Ist", "die\u00b7ser", "Schlu\u00df", "der", "La\u00b7ge", "an\u00b7ge\u00b7mes\u00b7sen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ART", "NN", "VVPP", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "\u00bbes kostet uns wohl ziemlich viele Leute?\u00ab", "tokens": ["\u00bb", "es", "kos\u00b7tet", "uns", "wohl", "ziem\u00b7lich", "vie\u00b7le", "Leu\u00b7te", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PPER", "VVFIN", "PPER", "ADV", "ADV", "PIAT", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "\u00bbja. Ziemlich viele; gestern etwa hundert,", "tokens": ["\u00bb", "ja", ".", "Ziem\u00b7lich", "vie\u00b7le", ";", "ge\u00b7stern", "et\u00b7wa", "hun\u00b7dert", ","], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "$.", "ADV", "PIS", "$.", "ADV", "ADV", "CARD", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und gegen hundertf\u00fcnfzig wieder heute.", "tokens": ["Und", "ge\u00b7gen", "hun\u00b7dert\u00b7f\u00fcnf\u00b7zig", "wie\u00b7der", "heu\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "CARD", "ADV", "ADV", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Es hat jedoch die Leitung nie gewundert.", "tokens": ["Es", "hat", "je\u00b7doch", "die", "Lei\u00b7tung", "nie", "ge\u00b7wun\u00b7dert", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ART", "NN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Denn, sehen Sie, der Krieg in Kolonien", "tokens": ["Denn", ",", "se\u00b7hen", "Sie", ",", "der", "Krieg", "in", "Ko\u00b7lo\u00b7ni\u00b7en"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "$,", "VVFIN", "PPER", "$,", "ART", "NN", "APPR", "NE"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Ist uns noch neu. Wir m\u00fcssen Lehrgeld geben", "tokens": ["Ist", "uns", "noch", "neu", ".", "Wir", "m\u00fcs\u00b7sen", "Lehr\u00b7geld", "ge\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "$.", "PPER", "VMFIN", "NN", "VVINF"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und aus dem Schaden die Erfahrung ziehen.", "tokens": ["Und", "aus", "dem", "Scha\u00b7den", "die", "Er\u00b7fah\u00b7rung", "zie\u00b7hen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Das kostet eben Geld, und kostet Leben.", "tokens": ["Das", "kos\u00b7tet", "e\u00b7ben", "Geld", ",", "und", "kos\u00b7tet", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "NN", "$,", "KON", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Und dann \u2013 pardong, darf ich um Feuer bitten?", "tokens": ["Und", "dann", "\u2013", "par\u00b7dong", ",", "darf", "ich", "um", "Feu\u00b7er", "bit\u00b7ten", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$(", "NE", "$,", "VMFIN", "PPER", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wie schlecht die heurigen Importen brennen! \u2013", "tokens": ["Wie", "schlecht", "die", "heu\u00b7ri\u00b7gen", "Im\u00b7por\u00b7ten", "bren\u00b7nen", "!", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "ADJD", "ART", "ADJA", "NN", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und dann, wir haben auch sehr viel gelitten,", "tokens": ["Und", "dann", ",", "wir", "ha\u00b7ben", "auch", "sehr", "viel", "ge\u00b7lit\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "PPER", "VAFIN", "ADV", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Weil wir das Klima noch zu wenig kennen.", "tokens": ["Weil", "wir", "das", "Kli\u00b7ma", "noch", "zu", "we\u00b7nig", "ken\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "ADV", "PTKA", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Im \u00fcbrigen, was soll das laute Klagen,", "tokens": ["Im", "\u00fcb\u00b7ri\u00b7gen", ",", "was", "soll", "das", "lau\u00b7te", "Kla\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$,", "PWS", "VMFIN", "PDS", "VVFIN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Wenn wir dreitausend oder mehr verlieren?", "tokens": ["Wenn", "wir", "drei\u00b7tau\u00b7send", "o\u00b7der", "mehr", "ver\u00b7lie\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "CARD", "KON", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Wir k\u00f6nnen den Verlust doch wirklich tragen!", "tokens": ["Wir", "k\u00f6n\u00b7nen", "den", "Ver\u00b7lust", "doch", "wirk\u00b7lich", "tra\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Der Sekt ist warm. He, Kellner, gut frappieren!\u00ab", "tokens": ["Der", "Sekt", "ist", "warm", ".", "He", ",", "Kell\u00b7ner", ",", "gut", "frap\u00b7pie\u00b7ren", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$.", "ITJ", "$,", "NN", "$,", "ADJD", "VVINF", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}