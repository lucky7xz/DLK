{"textgrid.poem.57930": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Vor\u00f6rtlersorgen", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Einst waren wir mant Pysen", "tokens": ["Einst", "wa\u00b7ren", "wir", "mant", "Py\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und unser Ort ein Dorf;", "tokens": ["Und", "un\u00b7ser", "Ort", "ein", "Dorf", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Gro\u00dfst\u00e4dter sind wir nun", "tokens": ["Gro\u00df\u00b7st\u00e4d\u00b7ter", "sind", "wir", "nun"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Und k\u00f6nnen uns dicketun,", "tokens": ["Und", "k\u00f6n\u00b7nen", "uns", "di\u00b7cke\u00b7tun", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "VVINF", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Das ist so klar wie Torf.", "tokens": ["Das", "ist", "so", "klar", "wie", "Torf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "ADJD", "KOKOM", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Zwar werden wir verwaltet", "tokens": ["Zwar", "wer\u00b7den", "wir", "ver\u00b7wal\u00b7tet"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "So'n bi\u00dfchen nebenher;", "tokens": ["So'n", "bi\u00df\u00b7chen", "ne\u00b7ben\u00b7her", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "ADV", "$."], "meter": "-+-+--", "measure": "unknown.measure.di"}, "line.3": {"text": "Es hei\u00dft: \u00bbMacht euren Knix,", "tokens": ["Es", "hei\u00dft", ":", "\u00bb", "Macht", "eu\u00b7ren", "Knix", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "NN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Zu sagen habt'r nix!", "tokens": ["Zu", "sa\u00b7gen", "habt'r", "nix", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "VAFIN", "NE", "$."], "meter": "-+--+", "measure": "iambic.di.chol"}, "line.5": {"text": "Was wollt'r denn noch mehr?\u00ab", "tokens": ["Was", "wollt'r", "denn", "noch", "mehr", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "ADV", "ADV", "ADV", "$.", "$("], "meter": "-+--+", "measure": "iambic.di.chol"}}, "stanza.3": {"line.1": {"text": "Wir dachten an Wasserleitung", "tokens": ["Wir", "dach\u00b7ten", "an", "Was\u00b7ser\u00b7lei\u00b7tung"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Und Kanalisation;", "tokens": ["Und", "Ka\u00b7na\u00b7li\u00b7sa\u00b7ti\u00b7on", ";"], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Gedanken, die sind frei,", "tokens": ["Ge\u00b7dan\u00b7ken", ",", "die", "sind", "frei", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PRELS", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Die Eingemeinderei", "tokens": ["Die", "Ein\u00b7ge\u00b7mein\u00b7de\u00b7rei"], "token_info": ["word", "word"], "pos": ["ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Ist nix als Dekoration.", "tokens": ["Ist", "nix", "als", "De\u00b7ko\u00b7ra\u00b7ti\u00b7on", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "KOUS", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Wir riechen auf'n Proppen.", "tokens": ["Wir", "rie\u00b7chen", "auf'n", "Prop\u00b7pen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Der Proppen schmeckt nach mehr;", "tokens": ["Der", "Prop\u00b7pen", "schmeckt", "nach", "mehr", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Mehr gibt es aber nicht", "tokens": ["Mehr", "gibt", "es", "a\u00b7ber", "nicht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "PPER", "ADV", "PTKNEG"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "An Wasser und an Licht", "tokens": ["An", "Was\u00b7ser", "und", "an", "Licht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "APPR", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und sonstigem Zubeh\u00f6r.", "tokens": ["Und", "sons\u00b7ti\u00b7gem", "Zu\u00b7be\u00b7h\u00f6r", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.5": {"line.1": {"text": "Die Sache, die ist bitter,", "tokens": ["Die", "Sa\u00b7che", ",", "die", "ist", "bit\u00b7ter", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und bitter ist nicht s\u00fc\u00df;", "tokens": ["Und", "bit\u00b7ter", "ist", "nicht", "s\u00fc\u00df", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PTKNEG", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wir fielen sch\u00f6n hinein,", "tokens": ["Wir", "fie\u00b7len", "sch\u00f6n", "hin\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Adj\u00f6, du holder Schein,", "tokens": ["Ad\u00b7j\u00f6", ",", "du", "hol\u00b7der", "Schein", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Die Sache, die ist mies.", "tokens": ["Die", "Sa\u00b7che", ",", "die", "ist", "mies", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VAFIN", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Wir gr\u00fcnden B\u00fcrgervereine,", "tokens": ["Wir", "gr\u00fcn\u00b7den", "B\u00fcr\u00b7ger\u00b7ver\u00b7ei\u00b7ne", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Vielleichtens hilft uns das;", "tokens": ["Viel\u00b7leich\u00b7tens", "hilft", "uns", "das", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PDS", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und n\u00fctzt es auch nicht viel,", "tokens": ["Und", "n\u00fctzt", "es", "auch", "nicht", "viel", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "PTKNEG", "ADV", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.4": {"text": "Es ist ein sch\u00f6nes Spiel", "tokens": ["Es", "ist", "ein", "sch\u00f6\u00b7nes", "Spiel"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Und macht uns bannig'n Spa\u00df.", "tokens": ["Und", "macht", "uns", "ban\u00b7nig'n", "Spa\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADJD", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}