{"textgrid.poem.67951": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "19. F\u00fcr die Priesterehe", "genre": "verse", "period": "N.A.", "pub_year": 1773, "urn": "N.A.", "language": ["de:0.85", "en:0.14"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Auch der gute Priscian wird nicht respektiret!", "tokens": ["Auch", "der", "gu\u00b7te", "Pri\u00b7sci\u00b7an", "wird", "nicht", "res\u00b7pek\u00b7ti\u00b7ret", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "VAFIN", "PTKNEG", "VVFIN", "$."], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.2": {"text": "Gar das Wort Sacerdos", "tokens": ["Gar", "das", "Wort", "Sa\u00b7cer\u00b7dos"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "NE"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.3": {"text": "Voraus hie\u00df es hic", "tokens": ["Vo\u00b7raus", "hie\u00df", "es", "hic"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NE"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Jetzo hei\u00dft es: armer hic! haec ist exsuliret.", "tokens": ["Jet\u00b7zo", "hei\u00dft", "es", ":", "ar\u00b7mer", "hic", "!", "haec", "ist", "ex\u00b7su\u00b7li\u00b7ret", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "ADJA", "NE", "$.", "NE", "VAFIN", "VVFIN", "$."], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}}, "stanza.2": {"line.1": {"text": "Leider! so mu\u00df immer ja Gottes Kirche leiden,", "tokens": ["Lei\u00b7der", "!", "so", "mu\u00df", "im\u00b7mer", "ja", "Got\u00b7tes", "Kir\u00b7che", "lei\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "ADV", "VMFIN", "ADV", "ADV", "NN", "NN", "VVINF", "$,"], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.2": {"text": "Was er selbst zusammen gab, soll der Mensch nicht scheiden,", "tokens": ["Was", "er", "selbst", "zu\u00b7sam\u00b7men", "gab", ",", "soll", "der", "Mensch", "nicht", "schei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "ADV", "VVFIN", "$,", "VMFIN", "ART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Was Gott bei der Sch\u00f6pfung sprach, sprach er ja zu Beiden:", "tokens": ["Was", "Gott", "bei", "der", "Sch\u00f6p\u00b7fung", "sprach", ",", "sprach", "er", "ja", "zu", "Bei\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "APPR", "ART", "NN", "VVFIN", "$,", "VVFIN", "PPER", "ADV", "PTKA", "PIS", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "\u00bbwachset und vermehret euch, mehrt die Welt mit Freuden.\u00ab", "tokens": ["\u00bb", "wach\u00b7set", "und", "ver\u00b7meh\u00b7ret", "euch", ",", "mehrt", "die", "Welt", "mit", "Freu\u00b7den", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVFIN", "KON", "VVFIN", "PPER", "$,", "VVFIN", "ART", "NN", "APPR", "NN", "$.", "$("], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.3": {"line.1": {"text": "Aber Jammer jetzt und Weh, die verlassen m\u00fcssen,", "tokens": ["A\u00b7ber", "Jam\u00b7mer", "jetzt", "und", "Weh", ",", "die", "ver\u00b7las\u00b7sen", "m\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "KON", "NN", "$,", "PRELS", "VVINF", "VMINF", "$,"], "meter": "+-+-+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Die so sanft sich zu uns that, scheiden von der S\u00fcssen!", "tokens": ["Die", "so", "sanft", "sich", "zu", "uns", "that", ",", "schei\u00b7den", "von", "der", "S\u00fcs\u00b7sen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJD", "PRF", "APPR", "PPER", "VVFIN", "$,", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "\u00bbo Pabst Innocentius, du wirst b\u00fcssen m\u00fcssen,", "tokens": ["\u00bb", "o", "Pabst", "In\u00b7no\u00b7cen\u00b7ti\u00b7us", ",", "du", "wirst", "b\u00fcs\u00b7sen", "m\u00fcs\u00b7sen", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM", "FM", "FM", "$,", "PPER", "VAFIN", "VVINF", "VMINF", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Da\u00df du unser Leben uns halb hinweg gerissen.", "tokens": ["Da\u00df", "du", "un\u00b7ser", "Le\u00b7ben", "uns", "halb", "hin\u00b7weg", "ge\u00b7ris\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "PPER", "ADJD", "ADV", "VVPP", "$."], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.4": {"line.1": {"text": "Bist du Innocentius, der die Unschuld liebet?", "tokens": ["Bist", "du", "In\u00b7no\u00b7cen\u00b7ti\u00b7us", ",", "der", "die", "Un\u00b7schuld", "lie\u00b7bet", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "NE", "$,", "PRELS", "ART", "NN", "VVFIN", "$."], "meter": "+-+-+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Und was jung er selbst geno\u00df, andern nicht mehr giebet.\u00ab", "tokens": ["Und", "was", "jung", "er", "selbst", "ge\u00b7no\u00df", ",", "an\u00b7dern", "nicht", "mehr", "gie\u00b7bet", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PWS", "ADJD", "PPER", "ADV", "VVFIN", "$,", "PIS", "PTKNEG", "ADV", "VVFIN", "$.", "$("], "meter": "--+-+-++-+-+-", "measure": "anapaest.init"}, "line.3": {"text": "Andern nicht verg\u00f6nnt als Greis, was er jung ge\u00fcbet \u2013", "tokens": ["An\u00b7dern", "nicht", "ver\u00b7g\u00f6nnt", "als", "Greis", ",", "was", "er", "jung", "ge\u00b7\u00fc\u00b7bet", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "PTKNEG", "VVPP", "KOKOM", "NN", "$,", "PWS", "PPER", "ADJD", "VVPP", "$("], "meter": "+-+-+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Bitte Gott, Pabst Innocenz, da\u00df ers dir vergiebet.", "tokens": ["Bit\u00b7te", "Gott", ",", "Pabst", "In\u00b7no\u00b7cenz", ",", "da\u00df", "ers", "dir", "ver\u00b7gie\u00b7bet", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "$,", "NN", "NE", "$,", "KOUS", "PIS", "PPER", "VVFIN", "$."], "meter": "+--+-++-+--+-", "measure": "iambic.hexa.invert"}}, "stanza.5": {"line.1": {"text": "Was war Adams Lebenslauf? S\u00f6hn' und T\u00f6chter zeugen!", "tokens": ["Was", "war", "A\u00b7dams", "Le\u00b7bens\u00b7lauf", "?", "S\u00f6hn'", "und", "T\u00f6ch\u00b7ter", "zeu\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "NE", "NN", "$.", "NN", "KON", "NN", "VVFIN", "$."], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Und das alte Testament macht sich dies zu eigen,", "tokens": ["Und", "das", "al\u00b7te", "Tes\u00b7ta\u00b7ment", "macht", "sich", "dies", "zu", "ei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "VVFIN", "PRF", "PDS", "PTKA", "ADJD", "$,"], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Und den alten Bund will ja nicht der Neue beugen,", "tokens": ["Und", "den", "al\u00b7ten", "Bund", "will", "ja", "nicht", "der", "Neu\u00b7e", "beu\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "VMFIN", "ADV", "PTKNEG", "ART", "ADJA", "VVINF", "$,"], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Patriarchen, K\u00f6nige und Propheten zeugen.", "tokens": ["Pat\u00b7ri\u00b7ar\u00b7chen", ",", "K\u00f6\u00b7ni\u00b7ge", "und", "Pro\u00b7phe\u00b7ten", "zeu\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "NN", "VVFIN", "$."], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.6": {"line.1": {"text": "Paulus, der Apostel, ward hoch hinauf entz\u00fccket,", "tokens": ["Pau\u00b7lus", ",", "der", "A\u00b7pos\u00b7tel", ",", "ward", "hoch", "hin\u00b7auf", "ent\u00b7z\u00fc\u00b7cket", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "NN", "$,", "VAFIN", "ADJD", "ADV", "VVFIN", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Was er in drei Himmeln sah, wer hat das erblicket?", "tokens": ["Was", "er", "in", "drei", "Him\u00b7meln", "sah", ",", "wer", "hat", "das", "er\u00b7bli\u00b7cket", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "CARD", "NN", "VVFIN", "$,", "PWS", "VAFIN", "PDS", "VVFIN", "$."], "meter": "-+--+-+----+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und was spricht er, wenn er uns wieder n\u00e4her r\u00fccket?", "tokens": ["Und", "was", "spricht", "er", ",", "wenn", "er", "uns", "wie\u00b7der", "n\u00e4\u00b7her", "r\u00fc\u00b7cket", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "VVFIN", "PPER", "$,", "KOUS", "PPER", "PRF", "ADV", "ADJD", "VVFIN", "$."], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "\u00bbjeder, spricht er, hab sein Weib, hab es unzerst\u00fccket.\u00ab", "tokens": ["\u00bb", "je\u00b7der", ",", "spricht", "er", ",", "hab", "sein", "Weib", ",", "hab", "es", "un\u00b7zer\u00b7st\u00fc\u00b7cket", ".", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PIS", "$,", "VVFIN", "PPER", "$,", "VAFIN", "PPOSAT", "NN", "$,", "VAFIN", "PPER", "ADJD", "$.", "$("], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}}, "stanza.7": {"line.1": {"text": "Ich bleib auch bei Paulus Wort, bei der guten Gabe:", "tokens": ["Ich", "bleib", "auch", "bei", "Pau\u00b7lus", "Wort", ",", "bei", "der", "gu\u00b7ten", "Ga\u00b7be", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NE", "NN", "$,", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "\u00bblieben Br\u00fcder, es ist gut, da\u00df ein Weib man habe,", "tokens": ["\u00bb", "lie\u00b7ben", "Br\u00fc\u00b7der", ",", "es", "ist", "gut", ",", "da\u00df", "ein", "Weib", "man", "ha\u00b7be", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "$,", "PPER", "VAFIN", "ADJD", "$,", "KOUS", "ART", "NN", "PIS", "VAFIN", "$,"], "meter": "+-+--+-+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Jedermann sein eigen Weib und sich an ihr labe,", "tokens": ["Je\u00b7der\u00b7mann", "sein", "ei\u00b7gen", "Weib", "und", "sich", "an", "ihr", "la\u00b7be", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PPOSAT", "ADJA", "NN", "KON", "PRF", "APPR", "PPER", "VVFIN", "$,"], "meter": "+---+-+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.4": {"text": "Und da\u00df jeder Priester auch seine eigne habe.\u00ab", "tokens": ["Und", "da\u00df", "je\u00b7der", "Pries\u00b7ter", "auch", "sei\u00b7ne", "eig\u00b7ne", "ha\u00b7be", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "KOUS", "PIAT", "NN", "ADV", "PPOSAT", "ADJA", "VAFIN", "$.", "$("], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.8": {"line.1": {"text": "Denn mich d\u00fcnket, es ist hart und nicht feine Sitte,", "tokens": ["Denn", "mich", "d\u00fcn\u00b7ket", ",", "es", "ist", "hart", "und", "nicht", "fei\u00b7ne", "Sit\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "ADJD", "KON", "PTKNEG", "ADJA", "NN", "$,"], "meter": "--+---+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Da\u00df ein armer Priester sich erst zu Gaste bitte,", "tokens": ["Da\u00df", "ein", "ar\u00b7mer", "Pries\u00b7ter", "sich", "erst", "zu", "Gas\u00b7te", "bit\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "PRF", "ADV", "APPR", "NN", "PTKANT", "$,"], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Bei der Tochter, Nichte, Frau in des Nachbars H\u00fctte,", "tokens": ["Bei", "der", "Toch\u00b7ter", ",", "Nich\u00b7te", ",", "Frau", "in", "des", "Nach\u00b7bars", "H\u00fct\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "NN", "$,", "NN", "APPR", "ART", "NN", "NN", "$,"], "meter": "+-+-+-+--+-+-", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Lieben Herren, das ist hart und nicht feine Sitte.", "tokens": ["Lie\u00b7ben", "Her\u00b7ren", ",", "das", "ist", "hart", "und", "nicht", "fei\u00b7ne", "Sit\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "PDS", "VAFIN", "ADJD", "KON", "PTKNEG", "ADJA", "NN", "$."], "meter": "+-+---+--+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.9": {"line.1": {"text": "Darum, heilger V\u00e4ter, hilf, hilf uns aus den N\u00f6then,", "tokens": ["Da\u00b7rum", ",", "heil\u00b7ger", "V\u00e4\u00b7ter", ",", "hilf", ",", "hilf", "uns", "aus", "den", "N\u00f6\u00b7then", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "ADJA", "NN", "$,", "VVIMP", "$,", "VVIMP", "PPER", "APPR", "ART", "NN", "$,"], "meter": "--+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Da\u00df das Paternoster wir bald selbander beten:", "tokens": ["Da\u00df", "das", "Pa\u00b7ter\u00b7nos\u00b7ter", "wir", "bald", "sel\u00b7ban\u00b7der", "be\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PPER", "ADV", "ADV", "VVINF", "$."], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.3": {"text": "Priester denn und Priesterin werden mich vertreten,", "tokens": ["Pries\u00b7ter", "denn", "und", "Pries\u00b7te\u00b7rin", "wer\u00b7den", "mich", "ver\u00b7tre\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "KON", "NN", "VAFIN", "PPER", "VVPP", "$,"], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Und f\u00fcr meine S\u00fcndenschuld Paternoster beten.", "tokens": ["Und", "f\u00fcr", "mei\u00b7ne", "S\u00fcn\u00b7den\u00b7schuld", "Pa\u00b7ter\u00b7nos\u00b7ter", "be\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPOSAT", "NN", "NN", "VVINF", "$."], "meter": "--+-+--+-+-+-", "measure": "iambic.penta.relaxed"}}}}}