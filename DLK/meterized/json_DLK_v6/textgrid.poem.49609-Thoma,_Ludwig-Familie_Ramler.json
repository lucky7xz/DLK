{"textgrid.poem.49609": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Familie Ramler", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Herr Ramler war in M\u00fcnchen Rentner.", "tokens": ["Herr", "Ram\u00b7ler", "war", "in", "M\u00fcn\u00b7chen", "Rent\u00b7ner", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NE", "VAFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Er wog die zwei bekannten Zentner", "tokens": ["Er", "wog", "die", "zwei", "be\u00b7kann\u00b7ten", "Zent\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "CARD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und wohnte in der Lindwurmstra\u00df',", "tokens": ["Und", "wohn\u00b7te", "in", "der", "Lind\u00b7wurm\u00b7stra\u00df'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wo er dazu ein Haus besa\u00df.", "tokens": ["Wo", "er", "da\u00b7zu", "ein", "Haus", "be\u00b7sa\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PAV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sein Barverm\u00f6gen, wie sie sagen,", "tokens": ["Sein", "Bar\u00b7ver\u00b7m\u00f6\u00b7gen", ",", "wie", "sie", "sa\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PWAV", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hat vierzigtausend Mark betragen,", "tokens": ["Hat", "vier\u00b7zig\u00b7tau\u00b7send", "Mark", "be\u00b7tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "CARD", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das ist verzinst mit vier Prozent", "tokens": ["Das", "ist", "ver\u00b7zinst", "mit", "vier", "Pro\u00b7zent"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ADV", "APPR", "CARD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ganz h\u00fcbsch. Und Ramler war solvent.", "tokens": ["Ganz", "h\u00fcbsch", ".", "Und", "Ram\u00b7ler", "war", "sol\u00b7vent", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$.", "KON", "NE", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Er war nicht t\u00e4tig und gesch\u00e4ftlich", "tokens": ["Er", "war", "nicht", "t\u00e4\u00b7tig", "und", "ge\u00b7sch\u00e4ft\u00b7lich"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und auch nicht arbeitsleidensch\u00e4ftlich,", "tokens": ["Und", "auch", "nicht", "ar\u00b7beits\u00b7lei\u00b7den\u00b7sch\u00e4ft\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er nahm den Mietzins p\u00fcnktlich hin", "tokens": ["Er", "nahm", "den", "Miet\u00b7zins", "p\u00fcnkt\u00b7lich", "hin"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und steigerte auch manchmal ihn.", "tokens": ["Und", "stei\u00b7ger\u00b7te", "auch", "manch\u00b7mal", "ihn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Er lie\u00df sich jeden Tag verlocken", "tokens": ["Er", "lie\u00df", "sich", "je\u00b7den", "Tag", "ver\u00b7lo\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "PIAT", "NN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zu Tertel, Schafkopf und Tarocken,", "tokens": ["Zu", "Ter\u00b7tel", ",", "Schaf\u00b7kopf", "und", "Ta\u00b7ro\u00b7cken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "War bei drei Kegelabend' und", "tokens": ["War", "bei", "drei", "Ke\u00b7ge\u00b7la\u00b7bend'", "und"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "CARD", "NN", "KON"], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}, "line.4": {"text": "Beim Zimmerstutzensch\u00fctzenbund.", "tokens": ["Beim", "Zim\u00b7mer\u00b7stut\u00b7zen\u00b7sch\u00fct\u00b7zen\u00b7bund", "."], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Ich d\u00e4chte, hiemit sei gegeben", "tokens": ["Ich", "d\u00e4ch\u00b7te", ",", "hie\u00b7mit", "sei", "ge\u00b7ge\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "PAV", "VAFIN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Inhalt von Herrn Ramlers Leben.", "tokens": ["Der", "In\u00b7halt", "von", "Herrn", "Ram\u00b7lers", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "NE", "NN", "$."], "meter": "-+--++-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und sie \u2013 was seine Frau betraf \u2013", "tokens": ["Und", "sie", "\u2013", "was", "sei\u00b7ne", "Frau", "be\u00b7traf", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "$(", "PWS", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hie\u00df Zenzi und geborne Graf.", "tokens": ["Hie\u00df", "Zen\u00b7zi", "und", "ge\u00b7bor\u00b7ne", "Graf", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Sie war nat\u00fcrlich neununddrei\u00dfig,", "tokens": ["Sie", "war", "na\u00b7t\u00fcr\u00b7lich", "neun\u00b7und\u00b7drei\u00b7\u00dfig", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In ihrem Fache auch so flei\u00dfig", "tokens": ["In", "ih\u00b7rem", "Fa\u00b7che", "auch", "so", "flei\u00b7\u00dfig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "ADV", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie seinerseits der Herr Gemahl,", "tokens": ["Wie", "sei\u00b7ner\u00b7seits", "der", "Herr", "Ge\u00b7mahl", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Gatte ihrer fr\u00fchen Wahl.", "tokens": ["Der", "Gat\u00b7te", "ih\u00b7rer", "fr\u00fc\u00b7hen", "Wahl", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Fast als der Inhalt von zwei Blusen", "tokens": ["Fast", "als", "der", "In\u00b7halt", "von", "zwei", "Blu\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "NN", "APPR", "CARD", "NN"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Erschien ihr ungeheurer Busen.", "tokens": ["Er\u00b7schien", "ihr", "un\u00b7ge\u00b7heu\u00b7rer", "Bu\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "F\u00fcr jemand, der die F\u00fclle liebt,", "tokens": ["F\u00fcr", "je\u00b7mand", ",", "der", "die", "F\u00fcl\u00b7le", "liebt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der sch\u00f6nste Anblick, den es gibt.", "tokens": ["Der", "sch\u00f6ns\u00b7te", "An\u00b7blick", ",", "den", "es", "gibt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Und dann die Rundung unterm R\u00fccken", "tokens": ["Und", "dann", "die", "Run\u00b7dung", "un\u00b7term", "R\u00fc\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War meterweise ein Entz\u00fccken.", "tokens": ["War", "me\u00b7ter\u00b7wei\u00b7se", "ein", "Ent\u00b7z\u00fc\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Im Geiste legt man seine Hand", "tokens": ["Im", "Geis\u00b7te", "legt", "man", "sei\u00b7ne", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "PIS", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auf dieses sch\u00f6ne Wunderland.", "tokens": ["Auf", "die\u00b7ses", "sch\u00f6\u00b7ne", "Wun\u00b7der\u00b7land", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Man kann sich denken, da\u00df ihr Gatte", "tokens": ["Man", "kann", "sich", "den\u00b7ken", ",", "da\u00df", "ihr", "Gat\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PIS", "VMFIN", "PRF", "VVINF", "$,", "KOUS", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nicht viel Verst\u00e4ndnis f\u00fcr sie hatte.", "tokens": ["Nicht", "viel", "Ver\u00b7st\u00e4nd\u00b7nis", "f\u00fcr", "sie", "hat\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "PIAT", "NN", "APPR", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nach zwanzig Jahr' bleibt nichts zur\u00fcck", "tokens": ["Nach", "zwan\u00b7zig", "Jahr'", "bleibt", "nichts", "zu\u00b7r\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "CARD", "NN", "VVFIN", "PIS", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vom Feuer und vom Ehegl\u00fcck.", "tokens": ["Vom", "Feu\u00b7er", "und", "vom", "E\u00b7he\u00b7gl\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Sie war, wie viele, unverstanden,", "tokens": ["Sie", "war", ",", "wie", "vie\u00b7le", ",", "un\u00b7ver\u00b7stan\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "PWAV", "PIS", "$,", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das hei\u00dft, es kam ihr auch abhanden", "tokens": ["Das", "hei\u00dft", ",", "es", "kam", "ihr", "auch", "ab\u00b7han\u00b7den"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der Honig, der ihr lieblich schien,", "tokens": ["Der", "Ho\u00b7nig", ",", "der", "ihr", "lieb\u00b7lich", "schien", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und sonstiges von ihrem Bien.", "tokens": ["Und", "sons\u00b7ti\u00b7ges", "von", "ih\u00b7rem", "Bi\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Der Ehe waren auch gelungen", "tokens": ["Der", "E\u00b7he", "wa\u00b7ren", "auch", "ge\u00b7lun\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADV", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zwei T\u00f6chter mit Bef\u00e4higungen,", "tokens": ["Zwei", "T\u00f6ch\u00b7ter", "mit", "Be\u00b7f\u00e4\u00b7hi\u00b7gun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Zenzi z\u00e4hlte achtzehn Jahr',", "tokens": ["Die", "Zen\u00b7zi", "z\u00e4hl\u00b7te", "acht\u00b7zehn", "Jahr'", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "CARD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als Fanny kaum noch siebzehn war.", "tokens": ["Als", "Fan\u00b7ny", "kaum", "noch", "sieb\u00b7zehn", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NE", "ADV", "ADV", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Sie waren beide rund entwickelt,", "tokens": ["Sie", "wa\u00b7ren", "bei\u00b7de", "rund", "ent\u00b7wi\u00b7ckelt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nur da\u00df die Fanny stark gepickelt", "tokens": ["Nur", "da\u00df", "die", "Fan\u00b7ny", "stark", "ge\u00b7pi\u00b7ckelt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "NN", "ADJD", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Von saurem Blute schien und wohl", "tokens": ["Von", "sau\u00b7rem", "Blu\u00b7te", "schien", "und", "wohl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "KON", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "War schuld Papa sein Alkohol.", "tokens": ["War", "schuld", "Pa\u00b7pa", "sein", "Al\u00b7ko\u00b7hol", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Der Grundcharakter der Erscheinung", "tokens": ["Der", "Grund\u00b7cha\u00b7rak\u00b7ter", "der", "Er\u00b7schei\u00b7nung"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War nach der allgemeinen Meinung", "tokens": ["War", "nach", "der", "all\u00b7ge\u00b7mei\u00b7nen", "Mei\u00b7nung"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der von Mama, sehr rund und nett,", "tokens": ["Der", "von", "Ma\u00b7ma", ",", "sehr", "rund", "und", "nett", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "NN", "$,", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Entwicklungsf\u00e4hig im Korsett.", "tokens": ["Ent\u00b7wick\u00b7lungs\u00b7f\u00e4\u00b7hig", "im", "Kor\u00b7sett", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "APPRART", "NN", "$."], "meter": "-+-+-++-", "measure": "unknown.measure.tetra"}}, "stanza.14": {"line.1": {"text": "Den dito hinteren Partien", "tokens": ["Den", "di\u00b7to", "hin\u00b7te\u00b7ren", "Par\u00b7ti\u00b7en"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "ADV", "ADJA", "NN"], "meter": "+--+--+--", "measure": "dactylic.tri"}, "line.2": {"text": "War jetzt schon mancher Reiz verliehen,", "tokens": ["War", "jetzt", "schon", "man\u00b7cher", "Reiz", "ver\u00b7lie\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADV", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sie gaukelten im Zukunftsbild", "tokens": ["Sie", "gau\u00b7kel\u00b7ten", "im", "Zu\u00b7kunfts\u00b7bild"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als angenehmstes Lustgefild.", "tokens": ["Als", "an\u00b7ge\u00b7nehms\u00b7tes", "Lust\u00b7ge\u00b7fild", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "So weit nun alles, was pers\u00f6nlich", "tokens": ["So", "weit", "nun", "al\u00b7les", ",", "was", "per\u00b7s\u00f6n\u00b7lich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "ADJD", "ADV", "PIS", "$,", "PRELS", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Den Leser int'ressiert. Gew\u00f6hnlich", "tokens": ["Den", "Le\u00b7ser", "i\u00b7nt'res\u00b7siert", ".", "Ge\u00b7w\u00f6hn\u00b7lich"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "VVFIN", "$.", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Hat die Familie zweckbestrebt", "tokens": ["Hat", "die", "Fa\u00b7mi\u00b7lie", "zweck\u00b7bes\u00b7trebt"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "VVFIN"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "So m\u00fchelos als froh gelebt.", "tokens": ["So", "m\u00fc\u00b7he\u00b7los", "als", "froh", "ge\u00b7lebt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KOKOM", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Am Vormittag beim Franziskaner,", "tokens": ["Am", "Vor\u00b7mit\u00b7tag", "beim", "Fran\u00b7zis\u00b7ka\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Am Nachmittag dann nach getaner", "tokens": ["Am", "Nach\u00b7mit\u00b7tag", "dann", "nach", "ge\u00b7ta\u00b7ner"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "ADV", "APPR", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Verdauung eine Tasse Kaffee.", "tokens": ["Ver\u00b7dau\u00b7ung", "ei\u00b7ne", "Tas\u00b7se", "Kaf\u00b7fee", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So \u00e4hnlich war die Grundidee.", "tokens": ["So", "\u00e4hn\u00b7lich", "war", "die", "Grun\u00b7di\u00b7dee", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Des Abends ging dann ins Theater,", "tokens": ["Des", "A\u00b7bends", "ging", "dann", "ins", "The\u00b7a\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was weiblich war, indes der Vater", "tokens": ["Was", "weib\u00b7lich", "war", ",", "in\u00b7des", "der", "Va\u00b7ter"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "ADJD", "VAFIN", "$,", "ADV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Bettschwer' sich durch Bier verschafft", "tokens": ["Die", "Bett\u00b7schwer'", "sich", "durch", "Bier", "ver\u00b7schafft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "PRF", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und f\u00fcnf, sechs Schoppen Rebensaft.", "tokens": ["Und", "f\u00fcnf", ",", "sechs", "Schop\u00b7pen", "Re\u00b7ben\u00b7saft", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "$,", "CARD", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Des Nachts kam Amor an die Betten.", "tokens": ["Des", "Nachts", "kam", "A\u00b7mor", "an", "die", "Bet\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "VVFIN", "NE", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Vielleicht, da\u00df ihn die T\u00f6chter h\u00e4tten", "tokens": ["Viel\u00b7leicht", ",", "da\u00df", "ihn", "die", "T\u00f6ch\u00b7ter", "h\u00e4t\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "KOUS", "PPER", "ART", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Begr\u00fc\u00dft, doch waren sie noch dumm.", "tokens": ["Be\u00b7gr\u00fc\u00dft", ",", "doch", "wa\u00b7ren", "sie", "noch", "dumm", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "$,", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Vater drehte sich blo\u00df um.", "tokens": ["Der", "Va\u00b7ter", "dreh\u00b7te", "sich", "blo\u00df", "um", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADV", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Mama sah ihn mit Seufzen wandern", "tokens": ["Ma\u00b7ma", "sah", "ihn", "mit", "Seuf\u00b7zen", "wan\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "APPR", "NN", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Vermutlich hin zu einem andern,", "tokens": ["Ver\u00b7mut\u00b7lich", "hin", "zu", "ei\u00b7nem", "an\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADV", "APPR", "ART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der dankbar sich dem Gott erschlo\u00df", "tokens": ["Der", "dank\u00b7bar", "sich", "dem", "Gott", "er\u00b7schlo\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "PRF", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und nicht mit Zimmerstutzen scho\u00df.", "tokens": ["Und", "nicht", "mit", "Zim\u00b7mer\u00b7stut\u00b7zen", "scho\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "In dieser b\u00fcrgerlichen Weise", "tokens": ["In", "die\u00b7ser", "b\u00fcr\u00b7ger\u00b7li\u00b7chen", "Wei\u00b7se"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verbrachte man in Ramlers Kreise", "tokens": ["Ver\u00b7brach\u00b7te", "man", "in", "Ram\u00b7lers", "Krei\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PIS", "APPR", "NE", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Den Tages-, Wochen-, Mondenlauf.", "tokens": ["Den", "Ta\u00b7ges", ",", "Wo\u00b7chen", ",", "Mon\u00b7den\u00b7lauf", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "TRUNC", "$,", "TRUNC", "$,", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In diesem Jahre h\u00f6rt' es auf.", "tokens": ["In", "die\u00b7sem", "Jah\u00b7re", "h\u00f6rt'", "es", "auf", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.21": {"line.1": {"text": "Und zwar, wie stets am Isarstrande,", "tokens": ["Und", "zwar", ",", "wie", "stets", "am", "Is\u00b7ar\u00b7stran\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "$,", "PWAV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Kam das Verderbnis nun zustande", "tokens": ["Kam", "das", "Ver\u00b7derb\u00b7nis", "nun", "zu\u00b7stan\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "ART", "NN", "ADV", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Im Karneval. Es war auch hier", "tokens": ["Im", "Kar\u00b7ne\u00b7val", ".", "Es", "war", "auch", "hier"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "$.", "PPER", "VAFIN", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie immer, doch es waren vier.", "tokens": ["Wie", "im\u00b7mer", ",", "doch", "es", "wa\u00b7ren", "vier", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "$,", "KON", "PPER", "VAFIN", "CARD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Begonnen hat es bei der Mutter.", "tokens": ["Be\u00b7gon\u00b7nen", "hat", "es", "bei", "der", "Mut\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Sie war zu reif, zerging wie Butter", "tokens": ["Sie", "war", "zu", "reif", ",", "zer\u00b7ging", "wie", "But\u00b7ter"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PTKA", "ADJD", "$,", "VVFIN", "KOKOM", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Am Feuer eines Augenblicks.", "tokens": ["Am", "Feu\u00b7er", "ei\u00b7nes", "Au\u00b7gen\u00b7blicks", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Fast ohne Walten des Geschicks.", "tokens": ["Fast", "oh\u00b7ne", "Wal\u00b7ten", "des", "Ge\u00b7schicks", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.23": {"line.1": {"text": "Ihr Mann war wieder beim Tarocken,", "tokens": ["Ihr", "Mann", "war", "wie\u00b7der", "beim", "Ta\u00b7ro\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da konnte sie sofort verlocken", "tokens": ["Da", "konn\u00b7te", "sie", "so\u00b7fort", "ver\u00b7lo\u00b7cken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Mensch von Schmederers Ballett.", "tokens": ["Ein", "Mensch", "von", "Schme\u00b7de\u00b7rers", "Bal\u00b7lett", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie schwamm in Gl\u00fcck und er in Fett.", "tokens": ["Sie", "schwamm", "in", "Gl\u00fcck", "und", "er", "in", "Fett", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "KON", "PPER", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Der S\u00fcndenfall war unabwendlich", "tokens": ["Der", "S\u00fcn\u00b7den\u00b7fall", "war", "un\u00b7ab\u00b7wend\u00b7lich"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "ADJD"], "meter": "-+-+-++--", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Und er geschah so selbstverst\u00e4ndlich,", "tokens": ["Und", "er", "ge\u00b7schah", "so", "selbst\u00b7ver\u00b7st\u00e4nd\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Als wenn es wirklich gar nichts w\u00e4r';", "tokens": ["Als", "wenn", "es", "wirk\u00b7lich", "gar", "nichts", "w\u00e4r'", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "ADJD", "ADV", "PIS", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie dachte ebenso wie er.", "tokens": ["Sie", "dach\u00b7te", "e\u00b7ben\u00b7so", "wie", "er", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "KOKOM", "PPER", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.25": {"line.1": {"text": "Und dachte an den Zimmerstutzen;", "tokens": ["Und", "dach\u00b7te", "an", "den", "Zim\u00b7mer\u00b7stut\u00b7zen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das war ihr hinterher von Nutzen", "tokens": ["Das", "war", "ihr", "hin\u00b7ter\u00b7her", "von", "Nut\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zu ihrer Selbstentschuldigung", "tokens": ["Zu", "ih\u00b7rer", "Selbs\u00b7tent\u00b7schul\u00b7di\u00b7gung"], "token_info": ["word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bei diesem ersten Seitensprung.", "tokens": ["Bei", "die\u00b7sem", "ers\u00b7ten", "Sei\u00b7ten\u00b7sprung", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Merkw\u00fcrdig doch, wie oft wir sehen", "tokens": ["Merk\u00b7w\u00fcr\u00b7dig", "doch", ",", "wie", "oft", "wir", "se\u00b7hen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADJD", "ADV", "$,", "PWAV", "ADV", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das Gleiche gleicherzeit geschehen,", "tokens": ["Das", "Glei\u00b7che", "glei\u00b7cher\u00b7zeit", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Dies hei\u00dft dann wohl Duplizit\u00e4t", "tokens": ["Dies", "hei\u00dft", "dann", "wohl", "Du\u00b7pli\u00b7zi\u00b7t\u00e4t"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "ADV", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der F\u00e4lle, wer so was versteht.", "tokens": ["Der", "F\u00e4l\u00b7le", ",", "wer", "so", "was", "ver\u00b7steht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PWS", "ADV", "PWS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Als Zenzi fiel, am gleichen Tage", "tokens": ["Als", "Zen\u00b7zi", "fiel", ",", "am", "glei\u00b7chen", "Ta\u00b7ge"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUS", "NE", "VVFIN", "$,", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War Ramler in derselben Lage,", "tokens": ["War", "Ram\u00b7ler", "in", "der\u00b7sel\u00b7ben", "La\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "APPR", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und glaubte, da\u00df die Lumperei", "tokens": ["Und", "glaub\u00b7te", ",", "da\u00df", "die", "Lum\u00b7pe\u00b7rei"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "$,", "KOUS", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Allein auf seiner Seite sei.", "tokens": ["Al\u00b7lein", "auf", "sei\u00b7ner", "Sei\u00b7te", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PPOSAT", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Das reizt so manche G\u00e4nseriche", "tokens": ["Das", "reizt", "so", "man\u00b7che", "G\u00e4n\u00b7se\u00b7ri\u00b7che"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ADV", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Fast st\u00e4rker wie das Eigentliche;", "tokens": ["Fast", "st\u00e4r\u00b7ker", "wie", "das", "Ei\u00b7gent\u00b7li\u00b7che", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die S\u00fcnde liegt im Intellekt", "tokens": ["Die", "S\u00fcn\u00b7de", "liegt", "im", "In\u00b7tel\u00b7lekt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und schwelgt wie nichts als wie im Sekt.", "tokens": ["Und", "schwelgt", "wie", "nichts", "als", "wie", "im", "Sekt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "KOKOM", "PIS", "KOKOM", "KOKOM", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.29": {"line.1": {"text": "Es war, vermittelst auch des Sektes,", "tokens": ["Es", "war", ",", "ver\u00b7mit\u00b7telst", "auch", "des", "Sek\u00b7tes", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "$,", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein S\u00fcndenfall des Intellektes,", "tokens": ["Ein", "S\u00fcn\u00b7den\u00b7fall", "des", "In\u00b7tel\u00b7lek\u00b7tes", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und Ramler freute sich am Schein,", "tokens": ["Und", "Ram\u00b7ler", "freu\u00b7te", "sich", "am", "Schein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "VVFIN", "PRF", "APPRART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein l\u00fcderlicher Mensch zu sein.", "tokens": ["Ein", "l\u00fc\u00b7der\u00b7li\u00b7cher", "Mensch", "zu", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.30": {"line.1": {"text": "Ihm diente f\u00f6rmlich zur Reklame", "tokens": ["Ihm", "dien\u00b7te", "f\u00f6rm\u00b7lich", "zur", "Re\u00b7kla\u00b7me"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPRART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Das aufgeputzte Mensch, die Dame,", "tokens": ["Das", "auf\u00b7ge\u00b7putz\u00b7te", "Mensch", ",", "die", "Da\u00b7me", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mit der er so umschlungen sa\u00df", "tokens": ["Mit", "der", "er", "so", "um\u00b7schlun\u00b7gen", "sa\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "ADV", "VVINF", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und irgend etwas Teures fra\u00df.", "tokens": ["Und", "ir\u00b7gend", "et\u00b7was", "Teu\u00b7res", "fra\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.31": {"line.1": {"text": "Den Schlu\u00df des Abends zu erraten,", "tokens": ["Den", "Schlu\u00df", "des", "A\u00b7bends", "zu", "er\u00b7ra\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist unschwer. Da\u00df er noch in Taten", "tokens": ["Ist", "un\u00b7schwer", ".", "Da\u00df", "er", "noch", "in", "Ta\u00b7ten"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADJD", "$.", "KOUS", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der Schlechtigkeit zu Ende ging,", "tokens": ["Der", "Schlech\u00b7tig\u00b7keit", "zu", "En\u00b7de", "ging", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Meinung hief\u00fcr ist gering.", "tokens": ["Die", "Mei\u00b7nung", "hie\u00b7f\u00fcr", "ist", "ge\u00b7ring", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PAV", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.32": {"line.1": {"text": "Jedoch der Wille und Versuche", "tokens": ["Je\u00b7doch", "der", "Wil\u00b7le", "und", "Ver\u00b7su\u00b7che"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gereichen ebenso zum Fluche,", "tokens": ["Ge\u00b7rei\u00b7chen", "e\u00b7ben\u00b7so", "zum", "Flu\u00b7che", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Weil immerhin, sagt der Jurist,", "tokens": ["Weil", "im\u00b7mer\u00b7hin", ",", "sagt", "der", "Ju\u00b7rist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "Die Absicht schon verwerflich ist.", "tokens": ["Die", "Ab\u00b7sicht", "schon", "ver\u00b7werf\u00b7lich", "ist", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.33": {"line.1": {"text": "So war nun Zenzi nebst dem Gatten", "tokens": ["So", "war", "nun", "Zen\u00b7zi", "nebst", "dem", "Gat\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADV", "NE", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auf schiefem Weg', und beide hatten", "tokens": ["Auf", "schie\u00b7fem", "Weg'", ",", "und", "bei\u00b7de", "hat\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$,", "KON", "PIS", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Schuld an dem verbotnen Gift,", "tokens": ["Die", "Schuld", "an", "dem", "ver\u00b7bot\u00b7nen", "Gift", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was ihre T\u00f6chter anbetrifft.", "tokens": ["Was", "ih\u00b7re", "T\u00f6ch\u00b7ter", "an\u00b7be\u00b7tr\u00b7ifft", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.34": {"line.1": {"text": "Er nicht daheim, sie auf dem Balle \u2013", "tokens": ["Er", "nicht", "da\u00b7heim", ",", "sie", "auf", "dem", "Bal\u00b7le", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKNEG", "ADV", "$,", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Du lieber Gott, in diesem Falle", "tokens": ["Du", "lie\u00b7ber", "Gott", ",", "in", "die\u00b7sem", "Fal\u00b7le"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ADV", "NN", "$,", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Denkt sich ein Kind und sagt f\u00fcr sich:", "tokens": ["Denkt", "sich", "ein", "Kind", "und", "sagt", "f\u00fcr", "sich", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ART", "NN", "KON", "VVFIN", "APPR", "PRF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich mach' es nach, und warum nich?", "tokens": ["Ich", "mach'", "es", "nach", ",", "und", "wa\u00b7rum", "nich", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKVZ", "$,", "KON", "PWAV", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.35": {"line.1": {"text": "F\u00fcr Zenzi gab sich ein gelockter", "tokens": ["F\u00fcr", "Zen\u00b7zi", "gab", "sich", "ein", "ge\u00b7lock\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NE", "VVFIN", "PRF", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Student, ein sogenannter Dokter,", "tokens": ["Stu\u00b7dent", ",", "ein", "so\u00b7ge\u00b7nann\u00b7ter", "Dok\u00b7ter", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Mit so viel Dummheit eingefa\u00dft,", "tokens": ["Mit", "so", "viel", "Dumm\u00b7heit", "ein\u00b7ge\u00b7fa\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "PIAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie es f\u00fcr junge M\u00e4dchen pa\u00dft.", "tokens": ["Wie", "es", "f\u00fcr", "jun\u00b7ge", "M\u00e4d\u00b7chen", "pa\u00dft", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.36": {"line.1": {"text": "Im Anfang sch\u00fcchtern, sp\u00e4ter frecher,", "tokens": ["Im", "An\u00b7fang", "sch\u00fcch\u00b7tern", ",", "sp\u00e4\u00b7ter", "fre\u00b7cher", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVINF", "$,", "ADJD", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zuletzt ein Sittlichkeitsverbrecher,", "tokens": ["Zu\u00b7letzt", "ein", "Sitt\u00b7lich\u00b7keits\u00b7ver\u00b7bre\u00b7cher", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zuerst ein froher Jugenddrang,", "tokens": ["Zu\u00b7erst", "ein", "fro\u00b7her", "Ju\u00b7gend\u00b7drang", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dann zielbewu\u00dfter Wachtelfang.", "tokens": ["Dann", "ziel\u00b7be\u00b7wu\u00df\u00b7ter", "Wach\u00b7tel\u00b7fang", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.37": {"line.1": {"text": "Erst str\u00e4ubte sich die arme Trude,", "tokens": ["Erst", "str\u00e4ub\u00b7te", "sich", "die", "ar\u00b7me", "Tru\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PRF", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Dann sa\u00df sie doch in seiner Bude;", "tokens": ["Dann", "sa\u00df", "sie", "doch", "in", "sei\u00b7ner", "Bu\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der Engel, der sie stets besch\u00fctzt,", "tokens": ["Der", "En\u00b7gel", ",", "der", "sie", "stets", "be\u00b7sch\u00fctzt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Entfernte sich, weil er nichts n\u00fctzt.", "tokens": ["Ent\u00b7fern\u00b7te", "sich", ",", "weil", "er", "nichts", "n\u00fctzt", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "$,", "KOUS", "PPER", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.38": {"line.1": {"text": "Sie ging mit einer absoluten", "tokens": ["Sie", "ging", "mit", "ei\u00b7ner", "ab\u00b7so\u00b7lu\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Verwegenheit schon auf Redouten", "tokens": ["Ver\u00b7we\u00b7gen\u00b7heit", "schon", "auf", "Re\u00b7dou\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und sah als flotter Domino", "tokens": ["Und", "sah", "als", "flot\u00b7ter", "Do\u00b7mi\u00b7no"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "KOKOM", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Den Vater einmal irgendwo.", "tokens": ["Den", "Va\u00b7ter", "ein\u00b7mal", "ir\u00b7gend\u00b7wo", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.39": {"line.1": {"text": "Und alles, was sie da bemerkte,", "tokens": ["Und", "al\u00b7les", ",", "was", "sie", "da", "be\u00b7merk\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "$,", "PRELS", "PPER", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "War so, da\u00df es sie noch best\u00e4rkte.", "tokens": ["War", "so", ",", "da\u00df", "es", "sie", "noch", "be\u00b7st\u00e4rk\u00b7te", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "$,", "KOUS", "PPER", "PPER", "ADV", "VVFIN", "$."], "meter": "-+---+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Wie schnell entgleitet aus der Hand", "tokens": ["Wie", "schnell", "ent\u00b7glei\u00b7tet", "aus", "der", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADJD", "VVPP", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das z\u00e4rteste Familienband!", "tokens": ["Das", "z\u00e4r\u00b7tes\u00b7te", "Fa\u00b7mi\u00b7li\u00b7en\u00b7band", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.40": {"line.1": {"text": "So ging 's bei Ramlers im Terzette.", "tokens": ["So", "ging", "'s", "bei", "Ram\u00b7lers", "im", "Ter\u00b7zet\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NE", "APPRART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Was aber frag' ich, ja was h\u00e4tte", "tokens": ["Was", "a\u00b7ber", "frag'", "ich", ",", "ja", "was", "h\u00e4t\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "ADV", "VVFIN", "PPER", "$,", "ADV", "PWS", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nun Fanny noch zur\u00fcckgedr\u00e4ngt,", "tokens": ["Nun", "Fan\u00b7ny", "noch", "zu\u00b7r\u00fcck\u00b7ge\u00b7dr\u00e4ngt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie sie an diesem Abgrund h\u00e4ngt?", "tokens": ["Wie", "sie", "an", "die\u00b7sem", "Ab\u00b7grund", "h\u00e4ngt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "PDAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.41": {"line.1": {"text": "Ein Zahnarzt war es, der die \u00c4rmste", "tokens": ["Ein", "Zahn\u00b7arzt", "war", "es", ",", "der", "die", "\u00c4rms\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PPER", "$,", "PRELS", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Durch G\u00fcte und aufs allerw\u00e4rmste,", "tokens": ["Durch", "G\u00fc\u00b7te", "und", "aufs", "al\u00b7ler\u00b7w\u00e4rms\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPRART", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Fast v\u00e4terlich darum beschwor,", "tokens": ["Fast", "v\u00e4\u00b7ter\u00b7lich", "da\u00b7rum", "be\u00b7schwor", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PAV", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df sie den Tugendpreis verlor.", "tokens": ["Da\u00df", "sie", "den", "Tu\u00b7gend\u00b7preis", "ver\u00b7lor", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.42": {"line.1": {"text": "Der Habicht wird nur desto k\u00fchner,", "tokens": ["Der", "Ha\u00b7bicht", "wird", "nur", "des\u00b7to", "k\u00fch\u00b7ner", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn eins der sanften, guten H\u00fchner", "tokens": ["Wenn", "eins", "der", "sanf\u00b7ten", ",", "gu\u00b7ten", "H\u00fch\u00b7ner"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "PIS", "ART", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "In seinen Krallen \u00e4ngstlich hupft.", "tokens": ["In", "sei\u00b7nen", "Kral\u00b7len", "\u00e4ngst\u00b7lich", "hupft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Federn werden ausgerupft.", "tokens": ["Die", "Fe\u00b7dern", "wer\u00b7den", "aus\u00b7ge\u00b7rupft", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.43": {"line.1": {"text": "Das \u00bbwie\u00ab erla\u00dft mir, euch zu schildern.", "tokens": ["Das", "\u00bb", "wie", "\u00ab", "er\u00b7la\u00dft", "mir", ",", "euch", "zu", "schil\u00b7dern", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "$(", "PWAV", "$(", "VVFIN", "PPER", "$,", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es f\u00fchrte nur zu solchen Bildern,", "tokens": ["Es", "f\u00fchr\u00b7te", "nur", "zu", "sol\u00b7chen", "Bil\u00b7dern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df jemand mit bewegter Hand", "tokens": ["Da\u00df", "je\u00b7mand", "mit", "be\u00b7weg\u00b7ter", "Hand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PIS", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie hinterlegt' als Denunziant.", "tokens": ["Sie", "hin\u00b7ter\u00b7legt'", "als", "De\u00b7nun\u00b7zi\u00b7ant", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "KOUS", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.44": {"line.1": {"text": "Kurz: Fanny ", "tokens": ["Kurz", ":", "Fan\u00b7ny"], "token_info": ["word", "punct", "word"], "pos": ["ADJD", "$.", "NE"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Von duftender Charakterg\u00fcte,", "tokens": ["Von", "duf\u00b7ten\u00b7der", "Cha\u00b7rak\u00b7ter\u00b7g\u00fc\u00b7te", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und war entbl\u00e4ttert und gepfl\u00fcckt,", "tokens": ["Und", "war", "ent\u00b7bl\u00e4t\u00b7tert", "und", "ge\u00b7pfl\u00fcckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "VVPP", "KON", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wie es so manchem Zahnarzt gl\u00fcckt.", "tokens": ["Wie", "es", "so", "man\u00b7chem", "Zahn\u00b7arzt", "gl\u00fcckt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.45": {"line.1": {"text": "Der Maler der Familie Ramler", "tokens": ["Der", "Ma\u00b7ler", "der", "Fa\u00b7mi\u00b7lie", "Ram\u00b7ler"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "NE"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Wird sozusagen Lumpensammler.", "tokens": ["Wird", "so\u00b7zu\u00b7sa\u00b7gen", "Lum\u00b7pen\u00b7samm\u00b7ler", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die beiden T\u00f6chter, sie und er,", "tokens": ["Die", "bei\u00b7den", "T\u00f6ch\u00b7ter", ",", "sie", "und", "er", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "$,", "PPER", "KON", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wer schlechter ist? Die Wahl ist schwer.", "tokens": ["Wer", "schlech\u00b7ter", "ist", "?", "Die", "Wahl", "ist", "schwer", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "$.", "ART", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.46": {"line.1": {"text": "Was dann? So fr\u00e4gt man tief in Sorgen:", "tokens": ["Was", "dann", "?", "So", "fr\u00e4gt", "man", "tief", "in", "Sor\u00b7gen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "$.", "ADV", "VVFIN", "PIS", "ADJD", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie wird die Zukunft, wird das Morgen?", "tokens": ["Wie", "wird", "die", "Zu\u00b7kunft", ",", "wird", "das", "Mor\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "$,", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie kann es gehen? frag' ich mich.", "tokens": ["Wie", "kann", "es", "ge\u00b7hen", "?", "frag'", "ich", "mich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "VVINF", "$.", "VVFIN", "PPER", "PRF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich glaube, ziemlich sengerich.", "tokens": ["Ich", "glau\u00b7be", ",", "ziem\u00b7lich", "sen\u00b7ge\u00b7rich", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.47": {"line.1": {"text": "Die Mutter ist nun schon im Schusse,", "tokens": ["Die", "Mut\u00b7ter", "ist", "nun", "schon", "im", "Schus\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ADV", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So weit von ihrem Geniusse,", "tokens": ["So", "weit", "von", "ih\u00b7rem", "Ge\u00b7ni\u00b7us\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "So weit von ihrem alten Gl\u00fcck.", "tokens": ["So", "weit", "von", "ih\u00b7rem", "al\u00b7ten", "Gl\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Alte findet nicht zur\u00fcck.", "tokens": ["Die", "Al\u00b7te", "fin\u00b7det", "nicht", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKNEG", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.48": {"line.1": {"text": "Der Vater bleibt \u2013 das l\u00e4\u00dft sich denken \u2013", "tokens": ["Der", "Va\u00b7ter", "bleibt", "\u2013", "das", "l\u00e4\u00dft", "sich", "den\u00b7ken", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$(", "PDS", "VVFIN", "PRF", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Lump, bis er in den Gelenken", "tokens": ["Ein", "Lump", ",", "bis", "er", "in", "den", "Ge\u00b7len\u00b7ken"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "KOUS", "PPER", "APPR", "ART", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Die Gicht versp\u00fcrt. Am Marterpfahl", "tokens": ["Die", "Gicht", "ver\u00b7sp\u00fcrt", ".", "Am", "Mar\u00b7ter\u00b7pfahl"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "NN", "VVPP", "$.", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird er wohl fromm und klerikal.", "tokens": ["Wird", "er", "wohl", "fromm", "und", "kle\u00b7ri\u00b7kal", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.49": {"line.1": {"text": "Die T\u00f6chter werden sich entwickeln", "tokens": ["Die", "T\u00f6ch\u00b7ter", "wer\u00b7den", "sich", "ent\u00b7wi\u00b7ckeln"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "PRF", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In wilder Lust. Von ihren Pickeln", "tokens": ["In", "wil\u00b7der", "Lust", ".", "Von", "ih\u00b7ren", "Pi\u00b7ckeln"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "$.", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wird Fanny im Gesichte frei.", "tokens": ["Wird", "Fan\u00b7ny", "im", "Ge\u00b7sich\u00b7te", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Seele? Das ist zweierlei.", "tokens": ["Die", "See\u00b7le", "?", "Das", "ist", "zwei\u00b7er\u00b7lei", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$.", "PDS", "VAFIN", "CARD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.50": {"line.1": {"text": "Hier kann nichts Gutes mehr entsprie\u00dfen.", "tokens": ["Hier", "kann", "nichts", "Gu\u00b7tes", "mehr", "ent\u00b7sprie\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Papa wird zimmerstutzenschie\u00dfen;", "tokens": ["Pa\u00b7pa", "wird", "zim\u00b7mer\u00b7stut\u00b7zen\u00b7schie\u00b7\u00dfen", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die Mutter l\u00e4\u00dft es gern geschehn,", "tokens": ["Die", "Mut\u00b7ter", "l\u00e4\u00dft", "es", "gern", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sie achtet 's nicht und denkt an wen.", "tokens": ["Sie", "ach\u00b7tet", "'s", "nicht", "und", "denkt", "an", "wen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "KON", "VVFIN", "APPR", "PWS", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.51": {"line.1": {"text": "Verlassen wir die \u00f6de St\u00e4tte!", "tokens": ["Ver\u00b7las\u00b7sen", "wir", "die", "\u00f6\u00b7de", "St\u00e4t\u00b7te", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn jeder Mensch die Tugend h\u00e4tte,", "tokens": ["Wenn", "je\u00b7der", "Mensch", "die", "Tu\u00b7gend", "h\u00e4t\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "ART", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die uns von selber innewohnt,", "tokens": ["Die", "uns", "von", "sel\u00b7ber", "in\u00b7ne\u00b7wohnt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dann w\u00fcrde sie nicht so belohnt.", "tokens": ["Dann", "w\u00fcr\u00b7de", "sie", "nicht", "so", "be\u00b7lohnt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PTKNEG", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}