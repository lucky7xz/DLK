{"dta.poem.19003": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "Ode.  \n Drunckenheit.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1641", "urn": "urn:nbn:de:kobv:b4-200905198111", "language": ["de:0.99"], "booktitle": "Weckherlin, Georg Rodolf: Gaistliche und Weltliche Gedichte. Amsterdam, 1641."}, "poem": {"stanza.1": {"line.1": {"text": "K\u00f6nt jhr mich dan sunst gar nichts fragen/", "tokens": ["K\u00f6nt", "jhr", "mich", "dan", "sunst", "gar", "nichts", "fra\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "ADV", "ADV", "ADV", "PIS", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Jhr Herren/ meine gute Freind?", "tokens": ["Ihr", "Her\u00b7ren", "/", "mei\u00b7ne", "gu\u00b7te", "Freind", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$(", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Dan was ich euch k\u00f6nd newes sagen/", "tokens": ["Dan", "was", "ich", "euch", "k\u00f6nd", "ne\u00b7wes", "sa\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "PPER", "PPER", "VMFIN", "PIS", "VVINF", "$("], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.4": {"text": "Wie starck vnd wa jetzund der Feind?", "tokens": ["Wie", "starck", "vnd", "wa", "je\u00b7tzund", "der", "Feind", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "KON", "XY", "ADV", "ART", "NN", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Ich bit (doch wollet mir verzeyhen)", "tokens": ["Ich", "bit", "(", "doch", "wol\u00b7let", "mir", "ver\u00b7zey\u00b7hen", ")"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "ADV", "VMFIN", "PPER", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Mit fragen nicht zu fahren fort/", "tokens": ["Mit", "fra\u00b7gen", "nicht", "zu", "fah\u00b7ren", "fort", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "PTKNEG", "PTKZU", "VVINF", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Dan sunsten will ich euch verleyhen", "tokens": ["Dan", "suns\u00b7ten", "will", "ich", "euch", "ver\u00b7ley\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVINF", "VMFIN", "PPER", "PPER", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Kein einig wort.", "tokens": ["Kein", "ei\u00b7nig", "wort", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJD", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Ich red nicht gern von schm\u00e4hen/ tr\u00f6wen", "tokens": ["Ich", "red", "nicht", "gern", "von", "schm\u00e4\u00b7hen", "/", "tr\u00f6\u00b7wen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PPER", "VVFIN", "PTKNEG", "ADV", "APPR", "ADJA", "$(", "VVINF"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Von raub/ brunst/ krieg/ vnglick vnd noht/", "tokens": ["Von", "raub", "/", "brunst", "/", "krieg", "/", "vnglick", "vnd", "noht", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$(", "VVFIN", "$(", "NN", "$(", "ADJD", "KON", "NN", "$("], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "Sondern allein/ Vns zuerfrewen/", "tokens": ["Son\u00b7dern", "al\u00b7lein", "/", "Vns", "zu\u00b7er\u00b7fre\u00b7wen", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "ADV", "$(", "PPER", "VVINF", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Von gutem wildbret/ wein vnd brot.", "tokens": ["Von", "gu\u00b7tem", "wild\u00b7bret", "/", "wein", "vnd", "brot", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "$(", "PTKVZ", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Den Man der wein mit lieb entz\u00fcndet/", "tokens": ["Den", "Man", "der", "wein", "mit", "lieb", "ent\u00b7z\u00fcn\u00b7det", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ART", "NN", "APPR", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Vnd das brot st\u00e4rcket jhm den leib", "tokens": ["Vnd", "das", "brot", "st\u00e4r\u00b7cket", "jhm", "den", "leib"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN", "PPER", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Da\u00df Er das wildbret besser findet", "tokens": ["Da\u00df", "Er", "das", "wild\u00b7bret", "bes\u00b7ser", "fin\u00b7det"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PDS", "VVFIN", "ADJD", "VVFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Bey seinem Weib.", "tokens": ["Bey", "sei\u00b7nem", "Weib", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "So lang zu reden/ lesen/ h\u00f6ren/", "tokens": ["So", "lang", "zu", "re\u00b7den", "/", "le\u00b7sen", "/", "h\u00f6\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VVINF", "$(", "VVINF", "$(", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Vnd mit dem haupt/ hut/ kn\u00fc/ fu\u00df/ hand", "tokens": ["Vnd", "mit", "dem", "haupt", "/", "hut", "/", "kn\u00fc", "/", "fu\u00df", "/", "hand"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["KON", "APPR", "ART", "NN", "$(", "VVFIN", "$(", "XY", "$(", "PTKVZ", "$(", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Gesanten/ Herren/ K\u00f6nig ehren/", "tokens": ["Ge\u00b7san\u00b7ten", "/", "Her\u00b7ren", "/", "K\u00f6\u00b7nig", "eh\u00b7ren", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$(", "NN", "$(", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So lang zu sprachen an der wand;", "tokens": ["So", "lang", "zu", "spra\u00b7chen", "an", "der", "wand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKZU", "VVFIN", "APPR", "ART", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "So lang zuschreiben vnd zu reden", "tokens": ["So", "lang", "zu\u00b7schrei\u00b7ben", "vnd", "zu", "re\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VVINF", "KON", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Von Gabor/ Tilly/ Wallenstein/", "tokens": ["Von", "Ga\u00b7bor", "/", "Til\u00b7ly", "/", "Wal\u00b7len\u00b7stein", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NE", "$(", "NE", "$(", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Von Franckreich/ Welschland/ Dennmarck/", "tokens": ["Von", "Fran\u00b7ck\u00b7reich", "/", "Wel\u00b7schland", "/", "Denn\u00b7marck", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["APPR", "NE", "$(", "NN", "$(", "NN", "$("], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.8": {"text": "Schweden/", "tokens": ["Schwe\u00b7den", "/"], "token_info": ["word", "punct"], "pos": ["NE", "$("], "meter": "+-", "measure": "trochaic.single"}, "line.9": {"text": "Ist eine pein.", "tokens": ["Ist", "ei\u00b7ne", "pein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.4": {"line.1": {"text": "Darumb fort/ fort mit solchem trawren/", "tokens": ["Da\u00b7rumb", "fort", "/", "fort", "mit", "sol\u00b7chem", "traw\u00b7ren", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PTKVZ", "$(", "PTKVZ", "APPR", "PIAT", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df man al\u00dfbald bed\u00f6ck den tisch/", "tokens": ["Da\u00df", "man", "al\u00df\u00b7bald", "be\u00b7d\u00f6ck", "den", "tisch", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "ADJD", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd keiner la\u00df die m\u00fch sich dawren/", "tokens": ["Vnd", "kei\u00b7ner", "la\u00df", "die", "m\u00fch", "sich", "daw\u00b7ren", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ART", "ADJD", "PRF", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wan wein/ brot/ flaisch vnd alles frisch;", "tokens": ["Wan", "wein", "/", "brot", "/", "flaisch", "vnd", "al\u00b7les", "frisch", ";"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "$(", "VVFIN", "$(", "ADJD", "KON", "PIS", "ADJD", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Der erst bey tisch soll der erst drincken/", "tokens": ["Der", "erst", "bey", "tisch", "soll", "der", "erst", "drin\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJD", "VMFIN", "ART", "ADV", "VVINF", "$("], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "So/ Herren/ wie behend? wolan/", "tokens": ["So", "/", "Her\u00b7ren", "/", "wie", "be\u00b7hend", "?", "wo\u00b7lan", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["ADV", "$(", "NN", "$(", "PWAV", "VVPP", "$.", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Schenck voll/ die Fraw thut dir nicht wincken/", "tokens": ["Schenck", "voll", "/", "die", "Fraw", "thut", "dir", "nicht", "win\u00b7cken", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "$(", "ART", "NN", "VVFIN", "PPER", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Nu fang ich an.", "tokens": ["Nu", "fang", "ich", "an", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.5": {"line.1": {"text": "Ho! Toman/ Lamy/ Sering/ Rumler/", "tokens": ["Ho", "!", "To\u00b7man", "/", "La\u00b7my", "/", "Se\u00b7ring", "/", "Rum\u00b7ler", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$.", "NE", "$(", "NE", "$(", "NE", "$(", "NE", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es gilt euch diser mu\u00df herumb/", "tokens": ["Es", "gilt", "euch", "di\u00b7ser", "mu\u00df", "he\u00b7rumb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PDS", "VMFIN", "ADV", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich waissz/ jhr seit all gute Tumler/", "tokens": ["Ich", "waissz", "/", "jhr", "seit", "all", "gu\u00b7te", "Tum\u00b7ler", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PPER", "APPR", "PIAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd liebet nicht was quad vnd krumb/", "tokens": ["Vnd", "lie\u00b7bet", "nicht", "was", "quad", "vnd", "krumb", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "FM", "FM", "KON", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Dan nur das/ so man kaum kan manglen/", "tokens": ["Dan", "nur", "das", "/", "so", "man", "kaum", "kan", "mang\u00b7len", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "$(", "ADV", "PIS", "ADV", "VMFIN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die weiber wissen auch wol was", "tokens": ["Die", "wei\u00b7ber", "wis\u00b7sen", "auch", "wol", "was"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "PWS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Gedenckend al\u00dfbald an das anglen/", "tokens": ["Ge\u00b7den\u00b7ckend", "al\u00df\u00b7bald", "an", "das", "ang\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Au\u00df ist mein gla\u00df.", "tokens": ["Au\u00df", "ist", "mein", "gla\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.6": {"line.1": {"text": "Nim weg von meinem Ohr die Feder/", "tokens": ["Nim", "weg", "von", "mei\u00b7nem", "Ohr", "die", "Fe\u00b7der", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "APPR", "PPOSAT", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gib mir daf\u00fcr ein Messer her;", "tokens": ["Gib", "mir", "da\u00b7f\u00fcr", "ein", "Mes\u00b7ser", "her", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PAV", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ho/ Schweitzer/ kotz Kreutz/ zeuch von leder/", "tokens": ["Ho", "/", "Schweit\u00b7zer", "/", "kotz", "Kreutz", "/", "zeuch", "von", "le\u00b7der", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "APPR", "NN", "$(", "VVIMP", "APPR", "NE", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd Schwaitzer gleich streb nu nach ehr:", "tokens": ["Vnd", "Schwait\u00b7zer", "gleich", "streb", "nu", "nach", "ehr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "ADV", "VVFIN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wolan/ jhr dapfere soldaten/", "tokens": ["Wo\u00b7lan", "/", "jhr", "dap\u00b7fe\u00b7re", "sol\u00b7da\u00b7ten", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Mit vnverzagtem frischem muht/", "tokens": ["Mit", "vn\u00b7ver\u00b7zag\u00b7tem", "fri\u00b7schem", "muht", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Waget zu newen/ freyen thaten", "tokens": ["Wa\u00b7get", "zu", "ne\u00b7wen", "/", "frey\u00b7en", "tha\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NE", "PTKZU", "VVINF", "$(", "ADJA", "VVFIN"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.8": {"text": "Nu flaisch vnd blut.", "tokens": ["Nu", "flaisch", "vnd", "blut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.7": {"line.1": {"text": "Feind haben wir gnug zu bestreitten", "tokens": ["Feind", "ha\u00b7ben", "wir", "gnug", "zu", "be\u00b7streit\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPER", "ADV", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In dem Vortrab vnd dem Nachtrab/", "tokens": ["In", "dem", "Vor\u00b7trab", "vnd", "dem", "Nach\u00b7trab", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "ART", "NN", "$("], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Nu greiffet an auff allen seitten/", "tokens": ["Nu", "greif\u00b7fet", "an", "auff", "al\u00b7len", "seit\u00b7ten", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PTKVZ", "APPR", "PIS", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd schneidet k\u00f6pff vnd schenckel ab:", "tokens": ["Vnd", "schnei\u00b7det", "k\u00f6pff", "vnd", "schen\u00b7ckel", "ab", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "NE", "KON", "ADJD", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "In dem sich straich/ schnit/ bissz vermischen/", "tokens": ["In", "dem", "sich", "straich", "/", "schnit", "/", "bissz", "ver\u00b7mi\u00b7schen", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PRF", "ADJD", "$(", "VVFIN", "$(", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Vnd der Nachtrab mag hitzig sein/", "tokens": ["Vnd", "der", "Nach\u00b7trab", "mag", "hit\u00b7zig", "sein", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VMFIN", "ADJD", "VAINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "So ruff ich stehts euch zu erfrischen/", "tokens": ["So", "ruff", "ich", "stehts", "euch", "zu", "er\u00b7fri\u00b7schen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "VVFIN", "PPER", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Ho! schenck vns ein.", "tokens": ["Ho", "!", "schenck", "vns", "ein", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.8": {"line.1": {"text": "Sih/ wie mit brechen/ schneiden/ beissen/", "tokens": ["Sih", "/", "wie", "mit", "bre\u00b7chen", "/", "schnei\u00b7den", "/", "beis\u00b7sen", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "KOKOM", "APPR", "VVINF", "$(", "VVINF", "$(", "VVINF", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Dem lieben Feind wir machen grau\u00df!", "tokens": ["Dem", "lie\u00b7ben", "Feind", "wir", "ma\u00b7chen", "grau\u00df", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPER", "VVFIN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "La\u00df mich das Spanf\u00e4hrlein zerreissen/", "tokens": ["La\u00df", "mich", "das", "Span\u00b7f\u00e4hr\u00b7lein", "zer\u00b7reis\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Stich dem Kalbskopff die angen au\u00df:", "tokens": ["Stich", "dem", "Kalbs\u00b7kopff", "die", "an\u00b7gen", "au\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "So/ so/ wirff damit an die Frawen/", "tokens": ["So", "/", "so", "/", "wirff", "da\u00b7mit", "an", "die", "Fra\u00b7wen", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "ADV", "$(", "VVFIN", "PAV", "APPR", "ART", "NN", "$("], "meter": "--+--+-+-", "measure": "anapaest.di.plus"}, "line.6": {"text": "Die wan sie schon so s\u00fc\u00df vnd milt", "tokens": ["Die", "wan", "sie", "schon", "so", "s\u00fc\u00df", "vnd", "milt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PWAV", "PPER", "ADV", "ADV", "ADJD", "KON", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Doch k\u00f6nden hawen vnd auch klawen;", "tokens": ["Doch", "k\u00f6n\u00b7den", "ha\u00b7wen", "vnd", "auch", "kla\u00b7wen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "VVINF", "KON", "ADV", "VVINF", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "Es gilt/ es gilt.", "tokens": ["Es", "gilt", "/", "es", "gilt", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "PPER", "VVFIN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.9": {"line.1": {"text": "Wan die soldaten vor Roschellen/", "tokens": ["Wan", "die", "sol\u00b7da\u00b7ten", "vor", "Ro\u00b7schel\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "NN", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Wan die soldaten vor Stralsun d/", "tokens": ["Wan", "die", "sol\u00b7da\u00b7ten", "vor", "Stral\u00b7sun", "d", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "APPR", "NE", "XY", "$("], "meter": "+--+--++-", "measure": "dactylic.di.plus"}, "line.3": {"text": "Die Mawren k\u00f6nten so wol f\u00e4llen/", "tokens": ["Die", "Maw\u00b7ren", "k\u00f6n\u00b7ten", "so", "wol", "f\u00e4l\u00b7len", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ADV", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie hertzhafft wir in diser stund", "tokens": ["Wie", "hertz\u00b7hafft", "wir", "in", "di\u00b7ser", "stund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Nu st\u00fcrmen w\u00e4llen die Pasteyen/", "tokens": ["Nu", "st\u00fcr\u00b7men", "w\u00e4l\u00b7len", "die", "Pas\u00b7te\u00b7yen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "VVFIN", "ART", "NN", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Ich sag die starck wildbret pastet/", "tokens": ["Ich", "sag", "die", "starck", "wild\u00b7bret", "pas\u00b7tet", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "VVFIN", "VVFIN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "So w\u00fcrden sich nicht lang mehr freyhen", "tokens": ["So", "w\u00fcr\u00b7den", "sich", "nicht", "lang", "mehr", "frey\u00b7hen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PRF", "PTKNEG", "ADJD", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Die beede St\u00e4t.", "tokens": ["Die", "bee\u00b7de", "St\u00e4t", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.10": {"line.1": {"text": "Frisch auff/ wer ist der beste treffer?", "tokens": ["Frisch", "auff", "/", "wer", "ist", "der", "bes\u00b7te", "tref\u00b7fer", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "$(", "PWS", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ha/ ha/ frisch her! ho/ ich bin wund/", "tokens": ["Ha", "/", "ha", "/", "frisch", "her", "!", "ho", "/", "ich", "bin", "wund", "/"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "ADJD", "PTKVZ", "$.", "XY", "$(", "PPER", "VAFIN", "ADJD", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Das pulver ist von saltz vnd pfeffer/", "tokens": ["Das", "pul\u00b7ver", "ist", "von", "saltz", "vnd", "pfef\u00b7fer", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VAFIN", "APPR", "NE", "KON", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ho! die brunst ist in meinem mund:", "tokens": ["Ho", "!", "die", "brunst", "ist", "in", "mei\u00b7nem", "mund", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "ART", "NN", "VAFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.5": {"text": "Doch sih/ es hat euch auch getroffen;", "tokens": ["Doch", "sih", "/", "es", "hat", "euch", "auch", "ge\u00b7trof\u00b7fen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVIMP", "$(", "PPER", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Zu l\u00f6schen mu\u00df es nicht mehr sein", "tokens": ["Zu", "l\u00f6\u00b7schen", "mu\u00df", "es", "nicht", "mehr", "sein"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PTKZU", "VVINF", "VMFIN", "PPER", "PTKNEG", "ADV", "VAINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Gedruncken/ sondern starck gesoffen/", "tokens": ["Ge\u00b7drun\u00b7cken", "/", "son\u00b7dern", "starck", "ge\u00b7sof\u00b7fen", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "KON", "ADJD", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "So schenck nur ein.", "tokens": ["So", "schenck", "nur", "ein", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.11": {"line.1": {"text": "Durch disen becher seind wir Siger;", "tokens": ["Durch", "di\u00b7sen", "be\u00b7cher", "seind", "wir", "Si\u00b7ger", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "So sauff herumb knap/ munder/ doll/", "tokens": ["So", "sauff", "he\u00b7rumb", "knap", "/", "mun\u00b7der", "/", "doll", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADV", "ADJD", "ADV", "ADJD", "$(", "ADJD", "$(", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Drinck aus/ es gilt der alten Schwiger/", "tokens": ["Drinck", "aus", "/", "es", "gilt", "der", "al\u00b7ten", "Schwi\u00b7ger", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKVZ", "$(", "PPER", "VVFIN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich bin schon mehr dan halb/ gar/ voll:", "tokens": ["Ich", "bin", "schon", "mehr", "dan", "halb", "/", "gar", "/", "voll", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "ADV", "ADJD", "$(", "ADV", "$(", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Darumb so lassz den K\u00e4\u00df herbringen;", "tokens": ["Da\u00b7rumb", "so", "lassz", "den", "K\u00e4\u00df", "her\u00b7brin\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Kom k\u00fcssz/ so k\u00fc\u00df mich artlich/ so;", "tokens": ["Kom", "k\u00fcssz", "/", "so", "k\u00fc\u00df", "mich", "art\u00b7lich", "/", "so", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VVFIN", "NE", "$(", "ADV", "VVFIN", "PPER", "ADJD", "$(", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "La\u00df vns ein lied zusamen singen/", "tokens": ["La\u00df", "vns", "ein", "lied", "zu\u00b7sa\u00b7men", "sin\u00b7gen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ART", "NN", "VVINF", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Hem hoscha ho!", "tokens": ["Hem", "hosc\u00b7ha", "ho", "!"], "token_info": ["word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.12": {"line.1": {"text": "Die Schw\u00e4blein/ die so gar gern schw\u00e4tzen", "tokens": ["Die", "Schw\u00e4b\u00b7lein", "/", "die", "so", "gar", "gern", "schw\u00e4t\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$(", "ART", "ADV", "ADV", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "In Th\u00fcringen dem dollen land/", "tokens": ["In", "Th\u00fc\u00b7rin\u00b7gen", "dem", "dol\u00b7len", "land", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Frassen ein Rad f\u00fcr eine bretzen", "tokens": ["Fras\u00b7sen", "ein", "Rad", "f\u00fcr", "ei\u00b7ne", "bret\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "NN", "APPR", "ART", "VVINF"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Mit einem K\u00e4\u00df au\u00df Schweitzerland:", "tokens": ["Mit", "ei\u00b7nem", "K\u00e4\u00df", "au\u00df", "Schweit\u00b7zer\u00b7land", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "In vnsrer hipschen Frawen namen/", "tokens": ["In", "vns\u00b7rer", "hip\u00b7schen", "Fra\u00b7wen", "na\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Schwab/ Schweitzer/ Th\u00fcringer/ Frantzo\u00df/", "tokens": ["Schwab", "/", "Schweit\u00b7zer", "/", "Th\u00fc\u00b7rin\u00b7ger", "/", "Frant\u00b7zo\u00df", "/"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "NN", "$(", "NN", "$(", "NE", "$("], "meter": "-+-+--++", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "So singet fr\u00f6lich nu zu samen/", "tokens": ["So", "sin\u00b7get", "fr\u00f6\u00b7lich", "nu", "zu", "sa\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJD", "ADV", "PTKZU", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Kom k\u00fc\u00df mich Ro\u00df.", "tokens": ["Kom", "k\u00fc\u00df", "mich", "Ro\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "NN", "$."], "meter": "---+", "measure": "unknown.measure.single"}}, "stanza.13": {"line.1": {"text": "O da\u00df die Schweitzer mit den l\u00e4tzen/", "tokens": ["O", "da\u00df", "die", "Schweit\u00b7zer", "mit", "den", "l\u00e4t\u00b7zen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KOUS", "ART", "NN", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Schwaben mit dem Leberlein", "tokens": ["Die", "Schwa\u00b7ben", "mit", "dem", "Le\u00b7berl\u00b7ein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Welschen mit den frischen Metzen", "tokens": ["Die", "Wel\u00b7schen", "mit", "den", "fri\u00b7schen", "Met\u00b7zen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die Th\u00fcringer mit bier vnd wein", "tokens": ["Die", "Th\u00fc\u00b7rin\u00b7ger", "mit", "bier", "vnd", "wein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NE", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "In jhrer hipschen Frawen namen", "tokens": ["In", "jhrer", "hip\u00b7schen", "Fra\u00b7wen", "na\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Ein jeder fr\u00f6lich/ frisch herumb", "tokens": ["Ein", "je\u00b7der", "fr\u00f6\u00b7lich", "/", "frisch", "he\u00b7rumb"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ART", "PIS", "ADJD", "$(", "ADJD", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Sing/ spring vnd drinck: vnd allzusamen/", "tokens": ["Sing", "/", "spring", "vnd", "drinck", ":", "vnd", "all\u00b7zu\u00b7sa\u00b7men", "/"], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NE", "$(", "VVFIN", "KON", "PTKVZ", "$.", "KON", "ADJD", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "K\u00fcssz mich widrumb.", "tokens": ["K\u00fcssz", "mich", "wid\u00b7rumb", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PPER", "PTKVZ", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.14": {"line.1": {"text": "Nu schenck vns ein den grossen becher/", "tokens": ["Nu", "schenck", "vns", "ein", "den", "gros\u00b7sen", "be\u00b7cher", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Schenck voll/ So/ ho! Jhr liebe freind/", "tokens": ["Schenck", "voll", "/", "So", "/", "ho", "!", "Ihr", "lie\u00b7be", "freind", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "$(", "ADV", "$(", "ITJ", "$.", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein jeder guter Zecher/ Stecher/", "tokens": ["Ein", "je\u00b7der", "gu\u00b7ter", "Ze\u00b7cher", "/", "Ste\u00b7cher", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "NN", "$(", "NN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So offt als vil Buchstaben seind", "tokens": ["So", "offt", "als", "vil", "Buch\u00b7sta\u00b7ben", "seind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "KOKOM", "PIAT", "NN", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "In seines lieben Stechblats namen", "tokens": ["In", "sei\u00b7nes", "lie\u00b7ben", "Stech\u00b7blats", "na\u00b7men"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Hie disen gantz abdrincken soll/", "tokens": ["Hie", "di\u00b7sen", "gantz", "ab\u00b7drin\u00b7cken", "soll", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "ADV", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Ich neunmahl/ rechnet jhr zusamen/", "tokens": ["Ich", "neun\u00b7mahl", "/", "rech\u00b7net", "jhr", "zu\u00b7sa\u00b7men", "/"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "$(", "VVFIN", "PPER", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Es gilt gantz voll.", "tokens": ["Es", "gilt", "gantz", "voll", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.15": {"line.1": {"text": "Wol/ hat ein jeder abgedruncken/", "tokens": ["Wol", "/", "hat", "ein", "je\u00b7der", "ab\u00b7ge\u00b7drun\u00b7cken", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "VAFIN", "ART", "PIAT", "NN", "$("], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Drey/ f\u00fcnff/ sechs/ sieben/ zehen mahl?", "tokens": ["Drey", "/", "f\u00fcnff", "/", "sechs", "/", "sie\u00b7ben", "/", "ze\u00b7hen", "mahl", "?"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["CARD", "$(", "CARD", "$(", "CARD", "$(", "VVINF", "$(", "CARD", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ist dises k\u00e4\u00df/ fisch oder schuncken?", "tokens": ["Ist", "di\u00b7ses", "k\u00e4\u00df", "/", "fisch", "o\u00b7der", "schun\u00b7cken", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "VVFIN", "$(", "ADJD", "KON", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Ist dises pferd graw oder fahl/", "tokens": ["Ist", "di\u00b7ses", "pferd", "graw", "o\u00b7der", "fahl", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "ADJD", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Darauff ich schwitz? gib her die flaschen/", "tokens": ["Dar\u00b7auff", "ich", "schwitz", "?", "gib", "her", "die", "fla\u00b7schen", "/"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "PPER", "ADJD", "$.", "VVIMP", "ADV", "ART", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Es gilt Herr Grey/ Herr Gro/ Gro/ groll/", "tokens": ["Es", "gilt", "Herr", "Grey", "/", "Herr", "Gro", "/", "Gro", "/", "groll", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "NE", "$(", "NN", "NE", "$(", "NE", "$(", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "So. dise w\u00e4sch wirt wol gewaschen/", "tokens": ["So", ".", "di\u00b7se", "w\u00e4sch", "wirt", "wol", "ge\u00b7wa\u00b7schen", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$.", "PDAT", "ADJD", "VAFIN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Seit jhr all doll?", "tokens": ["Seit", "jhr", "all", "doll", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.16": {"line.1": {"text": "Ho/ seind das Reutter oder M\u00fccken?", "tokens": ["Ho", "/", "seind", "das", "Reut\u00b7ter", "o\u00b7der", "M\u00fc\u00b7cken", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "VAFIN", "ART", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Buff/ buff/ es ist ein hafenk\u00e4\u00df:", "tokens": ["Buff", "/", "buff", "/", "es", "ist", "ein", "ha\u00b7fen\u00b7k\u00e4\u00df", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "NE", "$(", "PPER", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Zu zucken/ schmucken/ schlucken/ drucken/", "tokens": ["Zu", "zu\u00b7cken", "/", "schmu\u00b7cken", "/", "schlu\u00b7cken", "/", "dru\u00b7cken", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PTKZU", "VVINF", "$(", "VVINF", "$(", "VVINF", "$(", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Warumb ist doch der A. das gs\u00e4\u00df?", "tokens": ["Wa\u00b7rumb", "ist", "doch", "der", "A.", "das", "gs\u00e4\u00df", "?"], "token_info": ["word", "word", "word", "word", "abbreviation", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ADV", "ART", "APPRART", "PDS", "VVFIN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Pfuy dich/ ki\u00df mich/ thust du da schm\u00f6cken?", "tokens": ["Pfuy", "dich", "/", "ki\u00df", "mich", "/", "thust", "du", "da", "schm\u00f6\u00b7cken", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "$(", "VVIMP", "PPER", "$(", "VVFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Wer zornig ist der ist ein Lump/", "tokens": ["Wer", "zor\u00b7nig", "ist", "der", "ist", "ein", "Lump", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "VAFIN", "ART", "VAFIN", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Hey ho/ das ding die Z\u00e4hn thut bl\u00f6cken", "tokens": ["Hey", "ho", "/", "das", "ding", "die", "Z\u00e4hn", "thut", "bl\u00f6\u00b7cken"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["XY", "XY", "$(", "ART", "NN", "ART", "NN", "VVFIN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Bumb bidi bump.", "tokens": ["Bumb", "bi\u00b7di", "bump", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["FM.la", "FM.la", "FM.la", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.17": {"line.1": {"text": "Ha/ duck den kopff/ schei\u00df/ bei\u00df/ Meerwunder.", "tokens": ["Ha", "/", "duck", "den", "kopff", "/", "schei\u00df", "/", "bei\u00df", "/", "Meer\u00b7wun\u00b7der", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NE", "$(", "VVIMP", "ART", "NN", "$(", "VVFIN", "$(", "VVIMP", "$(", "NN", "$."], "meter": "-+-+-+++-", "measure": "unknown.measure.penta"}, "line.2": {"text": "Nu brauset/ sauset laut das Meer;", "tokens": ["Nu", "brau\u00b7set", "/", "sau\u00b7set", "laut", "das", "Meer", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$(", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein regen/ hagel/ blitz vnd dunder/", "tokens": ["Ein", "re\u00b7gen", "/", "ha\u00b7gel", "/", "blitz", "vnd", "dun\u00b7der", "/"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$(", "NE", "$(", "VVIMP", "KON", "ADJA", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Hey/ von Hayschrecken ein Kriegsheer;", "tokens": ["Hey", "/", "von", "Haysc\u00b7hre\u00b7cken", "ein", "Kriegs\u00b7heer", ";"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$(", "APPR", "NN", "ART", "NN", "$."], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.5": {"text": "Ho! schlag den Elefanten nider/", "tokens": ["Ho", "!", "schlag", "den", "E\u00b7lef\u00b7an\u00b7ten", "ni\u00b7der", "/"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVFIN", "ART", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Es ist ein storck/ ha nein/ ein lau\u00df/", "tokens": ["Es", "ist", "ein", "storck", "/", "ha", "nein", "/", "ein", "lau\u00df", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$(", "NE", "PTKANT", "$(", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Glick zu/ gut nacht/ kom k\u00fcssz mich wider/", "tokens": ["Glick", "zu", "/", "gut", "nacht", "/", "kom", "k\u00fcssz", "mich", "wi\u00b7der", "/"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "$(", "ADJD", "NN", "$(", "APPRART", "NN", "PPER", "PTKVZ", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Das liecht ist au\u00df.", "tokens": ["Das", "liecht", "ist", "au\u00df", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}