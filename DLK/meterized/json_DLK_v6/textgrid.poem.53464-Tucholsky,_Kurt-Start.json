{"textgrid.poem.53464": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Start", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Das Auge hinterw\u00e4rts gedreht: so sitzt der Weise", "tokens": ["Das", "Au\u00b7ge", "hin\u00b7ter\u00b7w\u00e4rts", "ge\u00b7dreht", ":", "so", "sitzt", "der", "Wei\u00b7se"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VVFIN", "$.", "ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "und \u00fcberdenkt sich still-bewegt die Jahreskreise,", "tokens": ["und", "\u00fc\u00b7ber\u00b7denkt", "sich", "still\u00b7be\u00b7wegt", "die", "Jah\u00b7res\u00b7krei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "und wie sie so, und da\u00df sie ohne Schlu\u00df . . .", "tokens": ["und", "wie", "sie", "so", ",", "und", "da\u00df", "sie", "oh\u00b7ne", "Schlu\u00df", ".", ".", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "$,", "KON", "KOUS", "PPER", "APPR", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "wo unsereins bestimmt mal abgehn mu\u00df.", "tokens": ["wo", "un\u00b7ser\u00b7eins", "be\u00b7stimmt", "mal", "ab\u00b7gehn", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "ADV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Hier \u00fcberkommen ihn die tr\u00fcben Sentimenter:", "tokens": ["Hier", "\u00fc\u00b7ber\u00b7kom\u00b7men", "ihn", "die", "tr\u00fc\u00b7ben", "Sen\u00b7ti\u00b7men\u00b7ter", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "er greift zum gr\u00fcnen Cura\u00e7ao (denn den kennt er)", "tokens": ["er", "greift", "zum", "gr\u00fc\u00b7nen", "Cu\u00b7ra\u00e7ao", "(", "denn", "den", "kennt", "er", ")"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "ADJA", "NN", "$(", "KON", "ART", "VVFIN", "PPER", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "und schl\u00fcrft das Gift und sieht das alte Jahr,", "tokens": ["und", "schl\u00fcrft", "das", "Gift", "und", "sieht", "das", "al\u00b7te", "Jahr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "KON", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "und wie es gar nicht allzu fr\u00f6hlich war.", "tokens": ["und", "wie", "es", "gar", "nicht", "all\u00b7zu", "fr\u00f6h\u00b7lich", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPER", "ADV", "PTKNEG", "PTKA", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Da ist zum ersten immerhin die Balkanmesse,", "tokens": ["Da", "ist", "zum", "ers\u00b7ten", "im\u00b7mer\u00b7hin", "die", "Bal\u00b7kan\u00b7mes\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPRART", "ADJA", "ADV", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "zum zweiten \u2013 heu nos miseros! \u2013 die B\u00f6rsenbaisse,", "tokens": ["zum", "zwei\u00b7ten", "\u2013", "heu", "nos", "mi\u00b7se\u00b7ros", "!", "\u2013", "die", "B\u00f6r\u00b7sen\u00b7bais\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "$(", "FM.la", "FM.la", "FM.la", "$.", "$(", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "zum dritten, vierten . . . Doch stets trostbereit", "tokens": ["zum", "drit\u00b7ten", ",", "vier\u00b7ten", ".", ".", ".", "Doch", "stets", "trost\u00b7be\u00b7reit"], "token_info": ["word", "word", "punct", "word", "punct", "punct", "punct", "word", "word", "word"], "pos": ["APPRART", "ADJA", "$,", "ADJA", "$.", "$.", "$.", "KON", "ADV", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "in aller Tr\u00fcbsal blieb der Gattin Z\u00e4rtlichkeit.", "tokens": ["in", "al\u00b7ler", "Tr\u00fcb\u00b7sal", "blieb", "der", "Gat\u00b7tin", "Z\u00e4rt\u00b7lich\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVFIN", "ART", "NN", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Und du, mein Blatt, j\u00e4hrst dich zum zehnten Mal auf Erden!", "tokens": ["Und", "du", ",", "mein", "Blatt", ",", "j\u00e4hrst", "dich", "zum", "zehn\u00b7ten", "Mal", "auf", "Er\u00b7den", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "$,", "PPOSAT", "NN", "$,", "VVIMP", "PPER", "APPRART", "ADJA", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Du brauchst nicht (auf dem Umschlag) dunkelrot zu werden!", "tokens": ["Du", "brauchst", "nicht", "(", "auf", "dem", "Um\u00b7schlag", ")", "dun\u00b7kel\u00b7rot", "zu", "wer\u00b7den", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "$(", "APPR", "ART", "NN", "$(", "ADJD", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Wir alle altern \u2013 du allein bleibst jung!", "tokens": ["Wir", "al\u00b7le", "al\u00b7tern", "\u2013", "du", "al\u00b7lein", "bleibst", "jung", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PIAT", "ADJA", "$(", "PPER", "ADV", "VVFIN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Begleite uns auf unsrer Wanderung!", "tokens": ["Be\u00b7glei\u00b7te", "uns", "auf", "uns\u00b7rer", "Wan\u00b7de\u00b7rung", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+--", "measure": "unknown.measure.tetra"}}, "stanza.5": {"line.1": {"text": "Prost Neu . . . ja, ja! Der Cura\u00e7ao und Silvester", "tokens": ["Prost", "Neu", ".", ".", ".", "ja", ",", "ja", "!", "Der", "Cu\u00b7ra\u00e7ao", "und", "Sil\u00b7ves\u00b7ter"], "token_info": ["word", "word", "punct", "punct", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "NE", "$.", "$.", "$.", "PTKANT", "$,", "ADV", "$.", "ART", "NN", "KON", "NN"], "meter": "-+-+-+--++-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "bedr\u00fccken dich, mein Sohn \u2013 zieh dir den Leibgurt fester!", "tokens": ["be\u00b7dr\u00fc\u00b7cken", "dich", ",", "mein", "Sohn", "\u2013", "zieh", "dir", "den", "Leib\u00b7gurt", "fes\u00b7ter", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "PPOSAT", "NN", "$(", "VVFIN", "PPER", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Verla\u00df Mama Philosophias Scho\u00df:", "tokens": ["Ver\u00b7la\u00df", "Ma\u00b7ma", "Phi\u00b7lo\u00b7so\u00b7phi\u00b7as", "Scho\u00df", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "NE", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Eins, zwei \u2013 und los!", "tokens": ["Eins", ",", "zwei", "\u2013", "und", "los", "!"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "CARD", "$(", "KON", "PTKVZ", "$."], "meter": "-+-+", "measure": "iambic.di"}}}}}