{"textgrid.poem.55891": {"metadata": {"author": {"name": "Rilke, Rainer Maria", "birth": "N.A.", "death": "N.A."}, "title": "Des Lied der Waise", "genre": "verse", "period": "N.A.", "pub_year": 1900, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich bin Niemand und werde auch Niemand sein.", "tokens": ["Ich", "bin", "Nie\u00b7mand", "und", "wer\u00b7de", "auch", "Nie\u00b7mand", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PIS", "KON", "VAFIN", "ADV", "PIS", "VAINF", "$."], "meter": "--+--+--+-+", "measure": "anapaest.tri.plus"}, "line.2": {"text": "Jetzt bin ich ja zum Sein noch zu klein;", "tokens": ["Jetzt", "bin", "ich", "ja", "zum", "Sein", "noch", "zu", "klein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "APPRART", "PPOSAT", "ADV", "PTKA", "ADJD", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "aber auch sp\u00e4ter.", "tokens": ["a\u00b7ber", "auch", "sp\u00e4\u00b7ter", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "$."], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.2": {"line.1": {"text": "M\u00fctter und V\u00e4ter,", "tokens": ["M\u00fct\u00b7ter", "und", "V\u00e4\u00b7ter", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "erbarmt euch mein.", "tokens": ["er\u00b7barmt", "euch", "mein", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.3": {"line.1": {"text": "Zwar es lohnt nicht des Pflegens M\u00fch:", "tokens": ["Zwar", "es", "lohnt", "nicht", "des", "Pfle\u00b7gens", "M\u00fch", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PTKNEG", "ART", "NN", "NE", "$."], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "ich werde doch gem\u00e4ht.", "tokens": ["ich", "wer\u00b7de", "doch", "ge\u00b7m\u00e4ht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Mich kann keiner brauchen: jetzt ist es zu fr\u00fch", "tokens": ["Mich", "kann", "kei\u00b7ner", "brau\u00b7chen", ":", "jetzt", "ist", "es", "zu", "fr\u00fch"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PIS", "VVFIN", "$.", "ADV", "VAFIN", "PPER", "PTKA", "ADJD"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "und morgen ist es zu sp\u00e4t.", "tokens": ["und", "mor\u00b7gen", "ist", "es", "zu", "sp\u00e4t", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "PPER", "PTKA", "ADJD", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.4": {"line.1": {"text": "Ich habe nur dieses eine Kleid,", "tokens": ["Ich", "ha\u00b7be", "nur", "die\u00b7ses", "ei\u00b7ne", "Kleid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "PDAT", "ART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "es wird d\u00fcnn und es verbleicht,", "tokens": ["es", "wird", "d\u00fcnn", "und", "es", "ver\u00b7bleicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "KON", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "aber es h\u00e4lt eine Ewigkeit", "tokens": ["a\u00b7ber", "es", "h\u00e4lt", "ei\u00b7ne", "E\u00b7wig\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ART", "NN"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "auch noch vor Gott vielleicht.", "tokens": ["auch", "noch", "vor", "Gott", "viel\u00b7leicht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "APPR", "NN", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Ich habe nur dieses bi\u00dfchen Haar", "tokens": ["Ich", "ha\u00b7be", "nur", "die\u00b7ses", "bi\u00df\u00b7chen", "Haar"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "PDAT", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "(immer dasselbe blieb),", "tokens": ["(", "im\u00b7mer", "das\u00b7sel\u00b7be", "blieb", ")", ","], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "PDAT", "VVFIN", "$(", "$,"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "das einmal Eines Liebstes war.", "tokens": ["das", "ein\u00b7mal", "Ei\u00b7nes", "Liebs\u00b7tes", "war", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "ART", "NN", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Nun hat er nichts mehr lieb.", "tokens": ["Nun", "hat", "er", "nichts", "mehr", "lieb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PIS", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}