{"textgrid.poem.67939": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "7. Die Chevy-Jagd", "genre": "verse", "period": "N.A.", "pub_year": 1773, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Percy aus Northumberland", "tokens": ["Der", "Per\u00b7cy", "aus", "Nor\u00b7thum\u00b7ber\u00b7land"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NE"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Einen Schwur zu Gott th\u00e4t er,", "tokens": ["Ei\u00b7nen", "Schwur", "zu", "Gott", "th\u00e4t", "er", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "PPER", "$,"], "meter": "+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Zu jagen auf Chyviaths Bergen,", "tokens": ["Zu", "ja\u00b7gen", "auf", "Chy\u00b7vi\u00b7aths", "Ber\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPR", "NE", "NN", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Drei Tag' lang rings umher,", "tokens": ["Drei", "Tag'", "lang", "rings", "um\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADJD", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Zum Trutz dem Ritter Duglas,", "tokens": ["Zum", "Trutz", "dem", "Rit\u00b7ter", "Du\u00b7glas", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NN", "NE", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.6": {"text": "Und wer je mit ihm w\u00e4r.", "tokens": ["Und", "wer", "je", "mit", "ihm", "w\u00e4r", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "APPR", "PPER", "VAFIN", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}}, "stanza.2": {"line.1": {"text": "Die fettsten Hirsch' in ganz Chiviat", "tokens": ["Die", "fetts\u00b7ten", "Hirsch'", "in", "ganz", "Chi\u00b7vi\u00b7at"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ADV", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sprach, wollt er schiessen und f\u00fchren ihm weg: \u2013", "tokens": ["Sprach", ",", "wollt", "er", "schies\u00b7sen", "und", "f\u00fch\u00b7ren", "ihm", "weg", ":", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "$,", "VMFIN", "PPER", "VVINF", "KON", "VVFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mein' Treu! sprach Ritter Duglas,", "tokens": ["Mein'", "Treu", "!", "sprach", "Rit\u00b7ter", "Du\u00b7glas", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Ich will ihm weisen den Weg.", "tokens": ["Ich", "will", "ihm", "wei\u00b7sen", "den", "Weg", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.3": {"line.1": {"text": "Der Percy dann aus Banbrow kam,", "tokens": ["Der", "Per\u00b7cy", "dann", "aus", "Ban\u00b7brow", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit ihm eine m\u00e4chtge Schaar:", "tokens": ["Mit", "ihm", "ei\u00b7ne", "m\u00e4cht\u00b7ge", "Schaar", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wohl funfzehnhundert Sch\u00fctzen k\u00fchn", "tokens": ["Wohl", "funf\u00b7zehn\u00b7hun\u00b7dert", "Sch\u00fct\u00b7zen", "k\u00fchn"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "CARD", "NN", "VVINF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Aus drei Bezirken dar.", "tokens": ["Aus", "drei", "Be\u00b7zir\u00b7ken", "dar", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "CARD", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Es begann am Montag' Morgen,", "tokens": ["Es", "be\u00b7gann", "am", "Mon\u00b7tag'", "Mor\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Auf Chiviats H\u00fcgeln hoch:", "tokens": ["Auf", "Chi\u00b7vi\u00b7ats", "H\u00fc\u00b7geln", "hoch", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Das Kind wehklags, noch ungebohrn!", "tokens": ["Das", "Kind", "weh\u00b7klags", ",", "noch", "un\u00b7ge\u00b7bohrn", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es ward sehr jammrig noch.", "tokens": ["Es", "ward", "sehr", "jamm\u00b7rig", "noch", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Die Treiber trieben durch den Wald,", "tokens": ["Die", "Trei\u00b7ber", "trie\u00b7ben", "durch", "den", "Wald", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu regen auf das Thier:", "tokens": ["Zu", "re\u00b7gen", "auf", "das", "Thier", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "APPR", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Die Sch\u00fctzen bogen nieder sich", "tokens": ["Die", "Sch\u00fct\u00b7zen", "bo\u00b7gen", "nie\u00b7der", "sich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit breiten Bogen Klirr.", "tokens": ["Mit", "brei\u00b7ten", "Bo\u00b7gen", "Klirr", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Dann das Wild strich durch den Wald", "tokens": ["Dann", "das", "Wild", "strich", "durch", "den", "Wald"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "ADJD", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Dorther und da und hier:", "tokens": ["Dor\u00b7ther", "und", "da", "und", "hier", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ADV", "KON", "ADV", "$."], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.3": {"text": "Grauhunde sp\u00fcrten in Busch und Baum,", "tokens": ["Grau\u00b7hun\u00b7de", "sp\u00fcr\u00b7ten", "in", "Busch", "und", "Baum", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "APPR", "NN", "KON", "NE", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Zu springen an das Thier.", "tokens": ["Zu", "sprin\u00b7gen", "an", "das", "Thier", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "APPR", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Es began[n] auf Chiviats Bergen,", "tokens": ["Es", "be\u00b7gan", "n", "auf", "Chi\u00b7vi\u00b7ats", "Ber\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$(", "XY", "$(", "APPR", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Am Montag Morgens fr\u00fch:", "tokens": ["Am", "Mon\u00b7tag", "Mor\u00b7gens", "fr\u00fch", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da's Eine Stund' Nachmittag war,", "tokens": ["Da's", "Ei\u00b7ne", "Stund'", "Nach\u00b7mit\u00b7tag", "war", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ART", "NN", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hatten hundert Hirsche sie.", "tokens": ["Hat\u00b7ten", "hun\u00b7dert", "Hir\u00b7sche", "sie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "CARD", "NN", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Sie bliesen Tod aufm Feld umher,", "tokens": ["Sie", "blie\u00b7sen", "Tod", "aufm", "Feld", "um\u00b7her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sie trugen zusammen schier:", "tokens": ["Sie", "tru\u00b7gen", "zu\u00b7sam\u00b7men", "schier", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Zur Niederlag' der Percy kam,", "tokens": ["Zur", "Nie\u00b7der\u00b7lag'", "der", "Per\u00b7cy", "kam", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "NE", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sah das erlegte Thier.", "tokens": ["Sah", "das", "er\u00b7leg\u00b7te", "Thier", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Er sprach: \u00bbes war des Duglas Wort,", "tokens": ["Er", "sprach", ":", "\u00bb", "es", "war", "des", "Du\u00b7glas", "Wort", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "$(", "PPER", "VAFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mich heut zu sprechen hier;", "tokens": ["Mich", "heut", "zu", "spre\u00b7chen", "hier", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PTKZU", "VVINF", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch wust ich wohl (und schwur zu Gott)", "tokens": ["Doch", "wust", "ich", "wohl", "(", "und", "schwur", "zu", "Gott", ")"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ADV", "$(", "KON", "VVFIN", "APPR", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er w\u00fcrd' nicht kommen mir.\u00ab", "tokens": ["Er", "w\u00fcrd'", "nicht", "kom\u00b7men", "mir", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "VVFIN", "PPER", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Ein Squire dann aus Northumberland", "tokens": ["Ein", "Squi\u00b7re", "dann", "aus", "Nor\u00b7thum\u00b7ber\u00b7land"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "APPR", "NE"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Zuletzt er ward gewahr,", "tokens": ["Zu\u00b7letzt", "er", "ward", "ge\u00b7wahr", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Der Ritter Duglas zog heran,", "tokens": ["Der", "Rit\u00b7ter", "Du\u00b7glas", "zog", "he\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+++-+", "measure": "unknown.measure.penta"}, "line.4": {"text": "Mit ihm ein' grosse Schaar.", "tokens": ["Mit", "ihm", "ein'", "gros\u00b7se", "Schaar", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ART", "ADJA", "NN", "$."], "meter": "--+--+", "measure": "anapaest.di.plus"}}, "stanza.11": {"line.1": {"text": "Mit Hellepart und Speer und Schwerd:", "tokens": ["Mit", "Hel\u00b7le\u00b7part", "und", "Speer", "und", "Schwerd", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zu schauen weit und breit;", "tokens": ["Zu", "schau\u00b7en", "weit", "und", "breit", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Wohl k\u00fchnre Leut' von Herz und Hand", "tokens": ["Wohl", "k\u00fchn\u00b7re", "Leut'", "von", "Herz", "und", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJA", "NN", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hat nicht die Christenheit.", "tokens": ["Hat", "nicht", "die", "Chris\u00b7ten\u00b7heit", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Wohl zwanzighundert Speeresleut',", "tokens": ["Wohl", "zwan\u00b7zig\u00b7hun\u00b7dert", "Spee\u00b7res\u00b7leut'", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "CARD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ohn eingen Fleck und Fehl;", "tokens": ["Ohn", "ein\u00b7gen", "Fleck", "und", "Fehl", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie waren gebohren l\u00e4ngs der Twid',", "tokens": ["Sie", "wa\u00b7ren", "ge\u00b7boh\u00b7ren", "l\u00e4ngs", "der", "Twid'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "APPR", "ART", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Im Zirk von Tiwid\u00e4hl.", "tokens": ["Im", "Zirk", "von", "Ti\u00b7wi\u00b7d\u00e4hl", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPR", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "\u00bbla\u00dft ab vom Thier, der Percy sprach,", "tokens": ["\u00bb", "la\u00dft", "ab", "vom", "Thier", ",", "der", "Per\u00b7cy", "sprach", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "VVIMP", "PTKVZ", "APPRART", "NN", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nehmt eurer Bogen wahr:", "tokens": ["Nehmt", "eu\u00b7rer", "Bo\u00b7gen", "wahr", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Nie hattet ihr, wie jetzt, sie noth;", "tokens": ["Nie", "hat\u00b7tet", "ihr", ",", "wie", "jetzt", ",", "sie", "noth", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$,", "PWAV", "ADV", "$,", "PPER", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Seit euch die Mutter gebahr.\u00ab", "tokens": ["Seit", "euch", "die", "Mut\u00b7ter", "ge\u00b7bahr", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPER", "ART", "NN", "VVFIN", "$.", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.14": {"line.1": {"text": "Der feste Duglas auf dem Ro\u00df,", "tokens": ["Der", "fes\u00b7te", "Du\u00b7glas", "auf", "dem", "Ro\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Ritt seinem Heer voran:", "tokens": ["Ritt", "sei\u00b7nem", "Heer", "vo\u00b7ran", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Seine R\u00fcstung gl\u00e4nzt, wie gl\u00fchend Erz,", "tokens": ["Sei\u00b7ne", "R\u00fcs\u00b7tung", "gl\u00e4nzt", ",", "wie", "gl\u00fc\u00b7hend", "Erz", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "$,", "PWAV", "ADJD", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Nie gabs einen bravern Mann.", "tokens": ["Nie", "gabs", "ei\u00b7nen", "bra\u00b7vern", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "\u00bbsagt, sprach er, was f\u00fcr Leut' ihr seid?", "tokens": ["\u00bb", "sagt", ",", "sprach", "er", ",", "was", "f\u00fcr", "Leut'", "ihr", "seid", "?"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "$,", "VVFIN", "PPER", "$,", "PRELS", "APPR", "NN", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Oder wessen Leut' seid ihr?", "tokens": ["O\u00b7der", "wes\u00b7sen", "Leut'", "seid", "ihr", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VAFIN", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wer gab euch Macht, zu jagen,", "tokens": ["Wer", "gab", "euch", "Macht", ",", "zu", "ja\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "NN", "$,", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "In meinem Revier allhier?\u00ab", "tokens": ["In", "mei\u00b7nem", "Re\u00b7vier", "all\u00b7hier", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "$.", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.16": {"line.1": {"text": "Der Erste Mann, der Antwort gab,", "tokens": ["Der", "Ers\u00b7te", "Mann", ",", "der", "Ant\u00b7wort", "gab", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "War Percy hastig schier:", "tokens": ["War", "Per\u00b7cy", "has\u00b7tig", "schier", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NE", "ADJD", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbwir wollen nicht sagen, wer wir sind?", "tokens": ["\u00bb", "wir", "wol\u00b7len", "nicht", "sa\u00b7gen", ",", "wer", "wir", "sind", "?"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VMFIN", "PTKNEG", "VVINF", "$,", "PWS", "PPER", "VAFIN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Oder wessen Leute wir?", "tokens": ["O\u00b7der", "wes\u00b7sen", "Leu\u00b7te", "wir", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "PPER", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Aber jagen wollen wir hier im Forst,", "tokens": ["A\u00b7ber", "ja\u00b7gen", "wol\u00b7len", "wir", "hier", "im", "Forst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "VMFIN", "PPER", "ADV", "APPRART", "NN", "$,"], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}, "line.6": {"text": "Zu Trotz den Deinen und dir.", "tokens": ["Zu", "Trotz", "den", "Dei\u00b7nen", "und", "dir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPR", "ART", "NN", "KON", "PPER", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.17": {"line.1": {"text": "Die fettsten Hirsch' in ganz Chiviat", "tokens": ["Die", "fetts\u00b7ten", "Hirsch'", "in", "ganz", "Chi\u00b7vi\u00b7at"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ADV", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Haben wir geschossen und f\u00fchren sie weg!\u00ab", "tokens": ["Ha\u00b7ben", "wir", "ge\u00b7schos\u00b7sen", "und", "f\u00fch\u00b7ren", "sie", "weg", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VAFIN", "PPER", "VVPP", "KON", "VVFIN", "PPER", "PTKVZ", "$.", "$("], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "\u00bbmein' Treu, sprach Ritter Duglas,", "tokens": ["\u00bb", "mein'", "Treu", ",", "sprach", "Rit\u00b7ter", "Du\u00b7glas", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PPOSAT", "NN", "$,", "VVFIN", "NE", "NE", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Ich will euch weisen den Weg.\u00ab", "tokens": ["Ich", "will", "euch", "wei\u00b7sen", "den", "Weg", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "ART", "NN", "$.", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.18": {"line.1": {"text": "Dann sprach der edle Duglas", "tokens": ["Dann", "sprach", "der", "ed\u00b7le", "Du\u00b7glas"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Zum Lord Percy sprach er:", "tokens": ["Zum", "Lord", "Per\u00b7cy", "sprach", "er", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "NE", "VVFIN", "PPER", "$."], "meter": "--+-+-", "measure": "anapaest.init"}, "line.3": {"text": "\u00bbzu t\u00f6dten diese unschuldge Leut',", "tokens": ["\u00bb", "zu", "t\u00f6d\u00b7ten", "die\u00b7se", "un\u00b7schuld\u00b7ge", "Leut'", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKZU", "VVINF", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Das w\u00e4r ja S\u00fcnde schwer.", "tokens": ["Das", "w\u00e4r", "ja", "S\u00fcn\u00b7de", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "NN", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.19": {"line.1": {"text": "Aber Percy, du bist ein Lord von Land,", "tokens": ["A\u00b7ber", "Per\u00b7cy", ",", "du", "bist", "ein", "Lord", "von", "Land", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "$,", "PPER", "VAFIN", "ART", "NN", "APPR", "NN", "$,"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Und ich vom Stande dein:", "tokens": ["Und", "ich", "vom", "Stan\u00b7de", "dein", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "NN", "PPOSAT", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "La\u00df unsre Leut beiseit hier stehn,", "tokens": ["La\u00df", "uns\u00b7re", "Leut", "bei\u00b7seit", "hier", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPOSAT", "NN", "APPR", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und wir zwei fechten allein.\u00ab", "tokens": ["Und", "wir", "zwei", "fech\u00b7ten", "al\u00b7lein", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPER", "CARD", "NN", "ADV", "$.", "$("], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.20": {"line.1": {"text": "\u00bbnun straf mich Gott! der Percy sprach,", "tokens": ["\u00bb", "nun", "straf", "mich", "Gott", "!", "der", "Per\u00b7cy", "sprach", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "NN", "$.", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wer dazu Nein! je sag'!", "tokens": ["Wer", "da\u00b7zu", "Nein", "!", "je", "sag'", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PWS", "PAV", "PTKANT", "$.", "ADV", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Mein Seel', du wackrer Duglas,", "tokens": ["Mein", "Seel'", ",", "du", "wack\u00b7rer", "Du\u00b7glas", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPER", "ADJA", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Sollt nie erleben den Tag.", "tokens": ["Sollt", "nie", "er\u00b7le\u00b7ben", "den", "Tag", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "VVFIN", "ART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.21": {"line.1": {"text": "In England, Schottland, Frankreich", "tokens": ["In", "En\u00b7gland", ",", "Schott\u00b7land", ",", "Fran\u00b7kreich"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "NE", "$,", "NN", "$,", "NE"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Hat keinen ein Weib gebohrn;", "tokens": ["Hat", "kei\u00b7nen", "ein", "Weib", "ge\u00b7bohrn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Dem, helf mir Gott und gutes Gl\u00fcck!", "tokens": ["Dem", ",", "helf", "mir", "Gott", "und", "gu\u00b7tes", "Gl\u00fcck", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "$,", "VVFIN", "PPER", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich nicht gleich trete vorn.\u00ab", "tokens": ["Ich", "nicht", "gleich", "tre\u00b7te", "vorn", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "PTKNEG", "ADV", "VVFIN", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.22": {"line.1": {"text": "Ein Squire dann aus Northumberland,", "tokens": ["Ein", "Squi\u00b7re", "dann", "aus", "Nor\u00b7thum\u00b7ber\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "NE", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Withrington war sein Nam,", "tokens": ["Wit\u00b7hring\u00b7ton", "war", "sein", "Nam", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sprach: \u00bbsoll mans in S\u00fcdengland sag'n", "tokens": ["Sprach", ":", "\u00bb", "soll", "mans", "in", "S\u00fc\u00b7den\u00b7gland", "sag'n"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word", "word"], "pos": ["NN", "$.", "$(", "VMFIN", "PIS", "APPR", "NN", "VVINF"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "K\u00f6nig Heinrich an mit Scham?", "tokens": ["K\u00f6\u00b7nig", "Hein\u00b7rich", "an", "mit", "Scham", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "APPR", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Ihr zwei seid reiche Lords und ich", "tokens": ["Ihr", "zwei", "seid", "rei\u00b7che", "Lords", "und", "ich"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "CARD", "VAFIN", "ADJA", "NN", "KON", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein armer Squire im Land;", "tokens": ["Ein", "ar\u00b7mer", "Squi\u00b7re", "im", "Land", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPRART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Und soll meinen Herrn da fechten sehn,", "tokens": ["Und", "soll", "mei\u00b7nen", "Herrn", "da", "fech\u00b7ten", "sehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPOSAT", "NN", "ADV", "VVINF", "VVINF", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Und stehn voll Scham und Schand?", "tokens": ["Und", "stehn", "voll", "Scham", "und", "Schand", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Nein, traun, so lang ich Waffen trag'", "tokens": ["Nein", ",", "traun", ",", "so", "lang", "ich", "Waf\u00b7fen", "trag'"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "VVINF", "$,", "ADV", "ADJD", "PPER", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Soll fehlen nicht Herz und Hand.\u00ab", "tokens": ["Soll", "feh\u00b7len", "nicht", "Herz", "und", "Hand", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "VVFIN", "PTKNEG", "NN", "KON", "NN", "$.", "$("], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.24": {"line.1": {"text": "Den Tag, den Tag, den grausen Tag,", "tokens": ["Den", "Tag", ",", "den", "Tag", ",", "den", "grau\u00b7sen", "Tag", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Es ward noch blutig sehr;", "tokens": ["Es", "ward", "noch", "blu\u00b7tig", "sehr", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Aus ist mein erster Sang hier,", "tokens": ["Aus", "ist", "mein", "ers\u00b7ter", "Sang", "hier", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "VAFIN", "PPOSAT", "ADJA", "NN", "ADV", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Und bald sing' ich euch mehr.", "tokens": ["Und", "bald", "sing'", "ich", "euch", "mehr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "PPER", "ADV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}