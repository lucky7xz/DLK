{"textgrid.poem.56139": {"metadata": {"author": {"name": "Rilke, Rainer Maria", "birth": "N.A.", "death": "N.A."}, "title": "24.", "genre": "verse", "period": "N.A.", "pub_year": 1900, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sollen wir unsere uralte Freundschaft, die gro\u00dfen", "tokens": ["Sol\u00b7len", "wir", "un\u00b7se\u00b7re", "ur\u00b7al\u00b7te", "Freund\u00b7schaft", ",", "die", "gro\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["VMFIN", "PPER", "PPOSAT", "ADJA", "NN", "$,", "ART", "ADJA"], "meter": "+--+--+--+--+-", "measure": "dactylic.penta"}, "line.2": {"text": "niemals werbenden G\u00f6tter, weil sie der harte", "tokens": ["nie\u00b7mals", "wer\u00b7ben\u00b7den", "G\u00f6t\u00b7ter", ",", "weil", "sie", "der", "har\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "NN", "$,", "KOUS", "PPER", "ART", "ADJA"], "meter": "+-+--+--+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Stahl, den wir streng erzogen, nicht kennt, versto\u00dfen", "tokens": ["Stahl", ",", "den", "wir", "streng", "er\u00b7zo\u00b7gen", ",", "nicht", "kennt", ",", "ver\u00b7sto\u00b7\u00dfen"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["NN", "$,", "PRELS", "PPER", "ADJD", "VVPP", "$,", "PTKNEG", "VVFIN", "$,", "VVFIN"], "meter": "+--+-+--+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "oder sie pl\u00f6tzlich suchen auf einer Karte?", "tokens": ["o\u00b7der", "sie", "pl\u00f6tz\u00b7lich", "su\u00b7chen", "auf", "ei\u00b7ner", "Kar\u00b7te", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADJD", "VVFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-++-+-", "measure": "unknown.measure.hexa"}}, "stanza.2": {"line.1": {"text": "Diese gewaltigen Freunde, die uns die Toten", "tokens": ["Die\u00b7se", "ge\u00b7wal\u00b7ti\u00b7gen", "Freun\u00b7de", ",", "die", "uns", "die", "To\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PDAT", "ADJA", "NN", "$,", "PRELS", "PPER", "ART", "NN"], "meter": "+--+--+-++-+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "nehmen, r\u00fchren nirgends an unsere R\u00e4der.", "tokens": ["neh\u00b7men", ",", "r\u00fch\u00b7ren", "nir\u00b7gends", "an", "un\u00b7se\u00b7re", "R\u00e4\u00b7der", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVINF", "$,", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+--+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Unsere Gastm\u00e4hler haben wir weit \u2013, unsere B\u00e4der,", "tokens": ["Un\u00b7se\u00b7re", "Gast\u00b7m\u00e4h\u00b7ler", "ha\u00b7ben", "wir", "weit", "\u2013", ",", "un\u00b7se\u00b7re", "B\u00e4\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "ADJD", "$(", "$,", "PPOSAT", "NN", "$,"], "meter": "+--++-+--+---+-", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "fortger\u00fcckt, und ihre uns lang schon zu langsamen Boten", "tokens": ["fort\u00b7ge\u00b7r\u00fcckt", ",", "und", "ih\u00b7re", "uns", "lang", "schon", "zu", "lang\u00b7sa\u00b7men", "Bo\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["VVPP", "$,", "KON", "PPOSAT", "PPER", "ADJD", "ADV", "APPR", "ADJA", "NN"], "meter": "+-+-+--+--+--+-", "measure": "hexameter"}}, "stanza.3": {"line.1": {"text": "\u00fcberholen wir immer. Einsamer nun auf einander", "tokens": ["\u00fc\u00b7berh\u00b7o\u00b7len", "wir", "im\u00b7mer", ".", "Ein\u00b7sa\u00b7mer", "nun", "auf", "ein\u00b7an\u00b7der"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "$.", "NN", "ADV", "APPR", "PRF"], "meter": "--+--+-++-+--+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "ganz angewiesen, ohne einander zu kennen,", "tokens": ["ganz", "an\u00b7ge\u00b7wie\u00b7sen", ",", "oh\u00b7ne", "ein\u00b7an\u00b7der", "zu", "ken\u00b7nen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVPP", "$,", "KOUI", "PRF", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "f\u00fchren wir nicht mehr die Pfade als sch\u00f6ne M\u00e4ander,", "tokens": ["f\u00fch\u00b7ren", "wir", "nicht", "mehr", "die", "Pfa\u00b7de", "als", "sch\u00f6\u00b7ne", "M\u00e4\u00b7an\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PTKNEG", "ADV", "ART", "NN", "KOUS", "ADJA", "NN", "$,"], "meter": "+--+--+--+--+-", "measure": "dactylic.penta"}}, "stanza.4": {"line.1": {"text": "sondern als Grade. Nur noch in Dampfkesseln brennen", "tokens": ["son\u00b7dern", "als", "Gra\u00b7de", ".", "Nur", "noch", "in", "Dampf\u00b7kes\u00b7seln", "bren\u00b7nen"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "NN", "$.", "ADV", "ADV", "APPR", "NN", "VVINF"], "meter": "+--+-+--+--+-", "measure": "iambic.penta.invert"}, "line.2": {"text": "die einstigen Feuer und heben die H\u00e4mmer, die immer", "tokens": ["die", "eins\u00b7ti\u00b7gen", "Feu\u00b7er", "und", "he\u00b7ben", "die", "H\u00e4m\u00b7mer", ",", "die", "im\u00b7mer"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "KON", "VVFIN", "ART", "NN", "$,", "PRELS", "ADV"], "meter": "-+--+--+--+-+-+", "measure": "amphibrach.tetra.plus"}, "line.3": {"text": "gr\u00f6\u00dfern. Wir aber nehmen an Kraft ab, wie Schwimmer.", "tokens": ["gr\u00f6\u00b7\u00dfern", ".", "Wir", "a\u00b7ber", "neh\u00b7men", "an", "Kraft", "ab", ",", "wie", "Schwim\u00b7mer", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$.", "PPER", "ADV", "VVFIN", "APPR", "NN", "PTKVZ", "$,", "PWAV", "NN", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}}}}}