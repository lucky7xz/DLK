{"textgrid.poem.53696": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "H\u00e4ndler und Helden", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "\u00bbalte Memoiren? Alte Memoiren?\u00ab", "tokens": ["\u00bb", "al\u00b7te", "Me\u00b7moi\u00b7ren", "?", "Al\u00b7te", "Me\u00b7moi\u00b7ren", "?", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "ADJA", "NN", "$.", "ADJA", "NN", "$.", "$("], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Die reisigen Helden der Wotans-Eiche,", "tokens": ["Die", "rei\u00b7si\u00b7gen", "Hel\u00b7den", "der", "Wo\u00b7tans\u00b7Ei\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "die Erzgepanzerten aus dem Reiche,", "tokens": ["die", "Erz\u00b7ge\u00b7pan\u00b7zer\u00b7ten", "aus", "dem", "Rei\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ART", "NE", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "die deutschen F\u00fchrer der glorreichen Zeit \u2013", "tokens": ["die", "deut\u00b7schen", "F\u00fch\u00b7rer", "der", "glor\u00b7rei\u00b7chen", "Zeit", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "sie z\u00fcckten das Schwert. Nun war es so weit.", "tokens": ["sie", "z\u00fcck\u00b7ten", "das", "Schwert", ".", "Nun", "war", "es", "so", "weit", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$.", "ADV", "VAFIN", "PPER", "ADV", "ADJD", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Zuerst aber z\u00fcckten die Paladine", "tokens": ["Zu\u00b7erst", "a\u00b7ber", "z\u00fcck\u00b7ten", "die", "Pa\u00b7la\u00b7di\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "ART", "NN"], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.7": {"text": "eine amerikanische Schreibmaschine", "tokens": ["ei\u00b7ne", "a\u00b7me\u00b7ri\u00b7ka\u00b7ni\u00b7sche", "Schreib\u00b7ma\u00b7schi\u00b7ne"], "token_info": ["word", "word", "word"], "pos": ["ART", "ADJA", "NN"], "meter": "+-+---+-+-+-", "measure": "unknown.measure.penta"}, "line.8": {"text": "sowie eine freundliche Tippmamsell:", "tokens": ["so\u00b7wie", "ei\u00b7ne", "freund\u00b7li\u00b7che", "Tipp\u00b7mam\u00b7sell", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "--+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.9": {"text": "\u00bbbitte, Fr\u00e4ulein, schreiben Sie schnell!\u00ab", "tokens": ["\u00bb", "bit\u00b7te", ",", "Fr\u00e4u\u00b7lein", ",", "schrei\u00b7ben", "Sie", "schnell", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PTKANT", "$,", "NN", "$,", "VVFIN", "PPER", "ADJD", "$.", "$("], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.10": {"text": "Das Fleisch ist willig. Das Hirn ist weich.", "tokens": ["Das", "Fleisch", "ist", "wil\u00b7lig", ".", "Das", "Hirn", "ist", "weich", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$.", "ART", "NN", "VAFIN", "ADJD", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.11": {"text": "O du geliebtes deutsches Reich! . . .", "tokens": ["O", "du", "ge\u00b7lieb\u00b7tes", "deut\u00b7sches", "Reich", "!", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["NE", "NE", "ADJA", "ADJA", "NN", "$.", "$.", "$.", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Alte Memoiren! Alte Memoiren!", "tokens": ["Al\u00b7te", "Me\u00b7moi\u00b7ren", "!", "Al\u00b7te", "Me\u00b7moi\u00b7ren", "!"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "ADJA", "NN", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}}, "stanza.2": {"line.1": {"text": "Alte Memoiren? Alte Memoiren?", "tokens": ["Al\u00b7te", "Me\u00b7moi\u00b7ren", "?", "Al\u00b7te", "Me\u00b7moi\u00b7ren", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "ADJA", "NN", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Die B\u00e4nde liegen fertig verpackt.", "tokens": ["Die", "B\u00e4n\u00b7de", "lie\u00b7gen", "fer\u00b7tig", "ver\u00b7packt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Darein ist alles hineinversackt:", "tokens": ["Da\u00b7rein", "ist", "al\u00b7les", "hin\u00b7ein\u00b7ver\u00b7sackt", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PIS", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Entschuldigung, Besch\u00f6nigung, Material \u2013", "tokens": ["Ent\u00b7schul\u00b7di\u00b7gung", ",", "Be\u00b7sch\u00f6\u00b7ni\u00b7gung", ",", "Ma\u00b7te\u00b7ri\u00b7al", "\u2013"], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "oh, du geduldige, statistische Zahl!", "tokens": ["oh", ",", "du", "ge\u00b7dul\u00b7di\u00b7ge", ",", "sta\u00b7tis\u00b7ti\u00b7sche", "Zahl", "!"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADJD", "$,", "PPER", "ADJA", "$,", "ADJA", "NN", "$."], "meter": "+--+--++--+", "measure": "dactylic.di.plus"}, "line.6": {"text": "Kartenpl\u00e4ne, ein artig Ragout \u2013", "tokens": ["Kar\u00b7ten\u00b7pl\u00e4\u00b7ne", ",", "ein", "ar\u00b7tig", "Ra\u00b7gout", "\u2013"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "ART", "ADJD", "NN", "$("], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.7": {"text": "und ein bi\u00dfchen Beschimpfung der Heimat dazu.", "tokens": ["und", "ein", "bi\u00df\u00b7chen", "Be\u00b7schimp\u00b7fung", "der", "Hei\u00b7mat", "da\u00b7zu", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "ART", "NN", "PTKVZ", "$."], "meter": "--+--+--+--+", "measure": "anapaest.tetra.plus"}}, "stanza.3": {"line.1": {"text": "Alte Memoiren? Alte Memoiren?", "tokens": ["Al\u00b7te", "Me\u00b7moi\u00b7ren", "?", "Al\u00b7te", "Me\u00b7moi\u00b7ren", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "ADJA", "NN", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Dann ziehen sie die Pistole voll Lust", "tokens": ["Dann", "zie\u00b7hen", "sie", "die", "Pis\u00b7to\u00b7le", "voll", "Lust"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "ADJD", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "und setzen sie auf die Verlegerbrust.", "tokens": ["und", "set\u00b7zen", "sie", "auf", "die", "Ver\u00b7le\u00b7ger\u00b7brust", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Und sprechen, gest\u00fctzt auf das mannhafte Schwert:", "tokens": ["Und", "spre\u00b7chen", ",", "ge\u00b7st\u00fctzt", "auf", "das", "mann\u00b7haf\u00b7te", "Schwert", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "$,", "VVPP", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.5": {"text": "\u00bbsuum cuique. Was ist das wert \u2013?\u00ab", "tokens": ["\u00bb", "su\u00b7um", "cui\u00b7que", ".", "Was", "ist", "das", "wert", "\u2013", "?", "\u00ab"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "FM.la", "FM.la", "$.", "PWS", "VAFIN", "PDS", "VVFIN", "$(", "$.", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.6": {"text": "Und sie handeln mit englischen Insulanern,", "tokens": ["Und", "sie", "han\u00b7deln", "mit", "eng\u00b7li\u00b7schen", "In\u00b7su\u00b7la\u00b7nern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "--+--+--+-+-", "measure": "anapaest.tri.plus"}, "line.7": {"text": "mit dollarbesitzenden Amerikanern . . .", "tokens": ["mit", "dol\u00b7lar\u00b7be\u00b7sit\u00b7zen\u00b7den", "A\u00b7me\u00b7ri\u00b7ka\u00b7nern", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ADJA", "NN", "$.", "$.", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.8": {"text": "Der Feindbund scheint ihnen auf einmal verwandelt.", "tokens": ["Der", "Feind\u00b7bund", "scheint", "ih\u00b7nen", "auf", "ein\u00b7mal", "ver\u00b7wan\u00b7delt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "APPR", "ADV", "VVPP", "$."], "meter": "--+-+-+-+-+-", "measure": "anapaest.init"}, "line.9": {"text": "Und sie handeln, wie man nur in Galizien handelt.", "tokens": ["Und", "sie", "han\u00b7deln", ",", "wie", "man", "nur", "in", "Ga\u00b7li\u00b7zi\u00b7en", "han\u00b7delt", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVINF", "$,", "PWAV", "PIS", "ADV", "APPR", "NE", "VVFIN", "$."], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}, "line.10": {"text": "Alte Memoiren? Alte Memoiren?", "tokens": ["Al\u00b7te", "Me\u00b7moi\u00b7ren", "?", "Al\u00b7te", "Me\u00b7moi\u00b7ren", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$.", "ADJA", "NN", "$."], "meter": "+--+-+--+-", "measure": "iambic.tetra.invert"}}, "stanza.4": {"line.1": {"text": "Einer aber von M\u00fcnchen bis Zossen", "tokens": ["Ei\u00b7ner", "a\u00b7ber", "von", "M\u00fcn\u00b7chen", "bis", "Zos\u00b7sen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "ADV", "APPR", "NE", "APPR", "NN"], "meter": "+-+--+--+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "hat von allen den Vogel abgeschossen.", "tokens": ["hat", "von", "al\u00b7len", "den", "Vo\u00b7gel", "ab\u00b7ge\u00b7schos\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "PIAT", "ART", "NN", "VVPP", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Wem flog wohl am h\u00f6chsten der preu\u00dfische Aar?", "tokens": ["Wem", "flog", "wohl", "am", "h\u00f6chs\u00b7ten", "der", "preu\u00b7\u00dfi\u00b7sche", "Aar", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "ADV", "APPRART", "ADJA", "ART", "ADJA", "NN", "$."], "meter": "-+--+--+--+", "measure": "amphibrach.tetra"}, "line.4": {"text": "Unser Kaiser bekam das Rekordhonorar!", "tokens": ["Un\u00b7ser", "Kai\u00b7ser", "be\u00b7kam", "das", "Re\u00b7kord\u00b7ho\u00b7no\u00b7rar", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "ART", "NN", "$."], "meter": "+-+--+-+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.5": {"text": "Zween Juden haben ihn unterst\u00fctzt;", "tokens": ["Zween", "Ju\u00b7den", "ha\u00b7ben", "ihn", "un\u00b7ter\u00b7st\u00fctzt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "so hat ihm doch Zion einmal gen\u00fctzt . . . !", "tokens": ["so", "hat", "ihm", "doch", "Zi\u00b7on", "ein\u00b7mal", "ge\u00b7n\u00fctzt", ".", ".", ".", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "NE", "ADV", "VVPP", "$.", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Potz Hakenkreuz und Feindesschreck:", "tokens": ["Potz", "Ha\u00b7ken\u00b7kreuz", "und", "Fein\u00b7des\u00b7schreck", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Bis zum letzten Hauch von Verleger und Scheck!", "tokens": ["Bis", "zum", "letz\u00b7ten", "Hauch", "von", "Ver\u00b7le\u00b7ger", "und", "Scheck", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "APPRART", "ADJA", "NN", "APPR", "NN", "KON", "NN", "$."], "meter": "--+-+--++-+", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Und dies ist der Wahlspruch des Hohenzollers:", "tokens": ["Und", "dies", "ist", "der", "Wahl\u00b7spruch", "des", "Ho\u00b7hen\u00b7zol\u00b7lers", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VAFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.10": {"text": "\u00bbkeine Partein mehr. Nur noch Dollars \u2013!\u00ab", "tokens": ["\u00bb", "kei\u00b7ne", "Par\u00b7tein", "mehr", ".", "Nur", "noch", "Dol\u00b7lars", "\u2013", "!", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct", "punct"], "pos": ["$(", "PIAT", "NN", "ADV", "$.", "ADV", "ADV", "NN", "$(", "$.", "$("], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}}}}