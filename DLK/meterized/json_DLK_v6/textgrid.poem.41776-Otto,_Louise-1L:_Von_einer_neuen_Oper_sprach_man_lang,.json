{"textgrid.poem.41776": {"metadata": {"author": {"name": "Otto, Louise", "birth": "N.A.", "death": "N.A."}, "title": "1L: Von einer neuen Oper sprach man lang,", "genre": "verse", "period": "N.A.", "pub_year": 1857, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Von einer neuen Oper sprach man lang,", "tokens": ["Von", "ei\u00b7ner", "neu\u00b7en", "O\u00b7per", "sprach", "man", "lang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "VVFIN", "PIS", "ADJD", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Voll rauschender Musik und holdem Sang,", "tokens": ["Voll", "rau\u00b7schen\u00b7der", "Mu\u00b7sik", "und", "hol\u00b7dem", "Sang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Die Deinen Namen uns verk\u00fcndet;", "tokens": ["Die", "Dei\u00b7nen", "Na\u00b7men", "uns", "ver\u00b7k\u00fcn\u00b7det", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und alles Neue lockte mich herbei", "tokens": ["Und", "al\u00b7les", "Neu\u00b7e", "lock\u00b7te", "mich", "her\u00b7bei"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIS", "ADJA", "VVFIN", "PPER", "PTKVZ"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Wenn eines deutschen Namens Weih'", "tokens": ["Wenn", "ei\u00b7nes", "deut\u00b7schen", "Na\u00b7mens", "Weih'"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sich deutschem Werk verb\u00fcndet", "tokens": ["Sich", "deut\u00b7schem", "Werk", "ver\u00b7b\u00fcn\u00b7det"], "token_info": ["word", "word", "word", "word"], "pos": ["PRF", "ADJA", "NN", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "In Dresdens Opernhause weilt ich nun:", "tokens": ["In", "Dres\u00b7dens", "O\u00b7pern\u00b7hau\u00b7se", "weilt", "ich", "nun", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "NN", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "\u00bbrienzi\u00ab hie\u00df die Oper, \u00bbRoms Tribun\u00ab.", "tokens": ["\u00bb", "rien\u00b7zi", "\u00ab", "hie\u00df", "die", "O\u00b7per", ",", "\u00bb", "Roms", "Tri\u00b7bun", "\u00ab", "."], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "FM.la", "$(", "VVFIN", "ART", "NN", "$,", "$(", "NE", "NE", "$(", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Mit vollen, feierlichen Kl\u00e4ngen", "tokens": ["Mit", "vol\u00b7len", ",", "fei\u00b7er\u00b7li\u00b7chen", "Kl\u00e4n\u00b7gen"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["APPR", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Begann sie, da Dein kleiner Zauberstab", "tokens": ["Be\u00b7gann", "sie", ",", "da", "Dein", "klei\u00b7ner", "Zau\u00b7ber\u00b7stab"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "$,", "KOUS", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Das erste Zeichen dem Orchester gab,", "tokens": ["Das", "ers\u00b7te", "Zei\u00b7chen", "dem", "Or\u00b7ches\u00b7ter", "gab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Da\u00df T\u00f6n' an T\u00f6ne dr\u00e4ngen.", "tokens": ["Da\u00df", "T\u00f6n'", "an", "T\u00f6\u00b7ne", "dr\u00e4n\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Ersch\u00fcttert lauscht das dichtgef\u00fcllte Haus", "tokens": ["Er\u00b7sch\u00fct\u00b7tert", "lauscht", "das", "dicht\u00b7ge\u00b7f\u00fcll\u00b7te", "Haus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Wagt kaum zu atmen in dem Tongebraus,", "tokens": ["Wagt", "kaum", "zu", "at\u00b7men", "in", "dem", "Ton\u00b7ge\u00b7braus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PTKZU", "VVINF", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Ruft beifallst\u00fcrmend in die Scene,", "tokens": ["Ruft", "bei\u00b7fall\u00b7st\u00fcr\u00b7mend", "in", "die", "Sce\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und immer neu bricht sich Begeistrung Bahn,", "tokens": ["Und", "im\u00b7mer", "neu", "bricht", "sich", "Be\u00b7geis\u00b7trung", "Bahn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "VVFIN", "PRF", "NN", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Ruft bald \u00bbRienzi\u00ab und bald \u00bbAdrian\u00ab,", "tokens": ["Ruft", "bald", "\u00bb", "Rien\u00b7zi", "\u00ab", "und", "bald", "\u00bb", "Ad\u00b7ri\u00b7an", "\u00ab", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "punct"], "pos": ["VVFIN", "ADV", "$(", "NE", "$(", "KON", "ADV", "$(", "NE", "$(", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "\u00bbcolonna und Irene\u00ab!", "tokens": ["\u00bb", "co\u00b7lon\u00b7na", "und", "I\u00b7re\u00b7ne", "\u00ab", "!"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "NE", "KON", "NN", "$(", "$."], "meter": "+-+-+--", "measure": "unknown.measure.tri"}}, "stanza.4": {"line.1": {"text": "Todtbleich und bebend fand ich mich am Schlu\u00df \u2013", "tokens": ["Todt\u00b7bleich", "und", "be\u00b7bend", "fand", "ich", "mich", "am", "Schlu\u00df", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "KON", "ADJD", "VVFIN", "PPER", "PRF", "APPRART", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Eins wu\u00dft ich nur: ", "tokens": ["Eins", "wu\u00dft", "ich", "nur", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Der mich mit Gottesmacht bezwungen.", "tokens": ["Der", "mich", "mit", "Got\u00b7tes\u00b7macht", "be\u00b7zwun\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Genius, der mit Titanenkraft", "tokens": ["Ein", "Ge\u00b7nius", ",", "der", "mit", "Ti\u00b7ta\u00b7nen\u00b7kraft"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "PRELS", "APPR", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.5": {"text": "Das Alte st\u00fcrzte und ein Neues schafft,", "tokens": ["Das", "Al\u00b7te", "st\u00fcrz\u00b7te", "und", "ein", "Neu\u00b7es", "schafft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "KON", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Ein neues Reich errungen.", "tokens": ["Ein", "neu\u00b7es", "Reich", "er\u00b7run\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Da kam der Splitterrichter eitle Zunft", "tokens": ["Da", "kam", "der", "Split\u00b7ter\u00b7rich\u00b7ter", "eit\u00b7le", "Zunft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Und m\u00e4kelte mit alter Unvernunft", "tokens": ["Und", "m\u00e4\u00b7kel\u00b7te", "mit", "al\u00b7ter", "Un\u00b7ver\u00b7nunft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "An dem, das ihr zu hoch gegeben.", "tokens": ["An", "dem", ",", "das", "ihr", "zu", "hoch", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "PRELS", "PPER", "PTKA", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ich l\u00e4chelte zu ihrem h\u00e4m'schen Wort \u2013", "tokens": ["Ich", "l\u00e4\u00b7chel\u00b7te", "zu", "ih\u00b7rem", "h\u00e4m'\u00b7schen", "Wort", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Seit jenem Tag warst Du mein Held und Hort", "tokens": ["Seit", "je\u00b7nem", "Tag", "warst", "Du", "mein", "Held", "und", "Hort"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "VAFIN", "PPER", "PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Im kunstgeweihten Leben.", "tokens": ["Im", "kunst\u00b7ge\u00b7weih\u00b7ten", "Le\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Des \u00bbfliegenden Holl\u00e4nders\u00ab Geisterschiff", "tokens": ["Des", "\u00bb", "flie\u00b7gen\u00b7den", "Hol\u00b7l\u00e4n\u00b7ders", "\u00ab", "Geis\u00b7ter\u00b7schiff"], "token_info": ["word", "punct", "word", "word", "punct", "word"], "pos": ["ART", "$(", "ADJA", "NN", "$(", "NN"], "meter": "-+--++-+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "\u00bbtannh\u00e4users\u00ab und des Wolframs Harfengriff", "tokens": ["\u00bb", "tann\u00b7h\u00e4u\u00b7sers", "\u00ab", "und", "des", "Wolf\u00b7rams", "Har\u00b7fen\u00b7griff"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "FM", "$(", "KON", "ART", "NE", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Und \u00bbLohengrins\u00ab erhabnes T\u00f6nen \u2013:", "tokens": ["Und", "\u00bb", "Lo\u00b7hen\u00b7grins", "\u00ab", "er\u00b7hab\u00b7nes", "T\u00f6\u00b7nen", "\u2013", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "punct"], "pos": ["KON", "$(", "NE", "$(", "ADJA", "NN", "$(", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Die folgten nach, wie Stern an Stern sich reiht,", "tokens": ["Die", "folg\u00b7ten", "nach", ",", "wie", "Stern", "an", "Stern", "sich", "reiht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PTKVZ", "$,", "PWAV", "NN", "APPR", "NN", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Durchbrechend hell der Wolken Dunkelheit", "tokens": ["Durch\u00b7bre\u00b7chend", "hell", "der", "Wol\u00b7ken", "Dun\u00b7kel\u00b7heit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJD", "ADJD", "ART", "NN", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Am Himmel alles Sch\u00f6nen,", "tokens": ["Am", "Him\u00b7mel", "al\u00b7les", "Sch\u00f6\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Und immer neu, wie jenes erste mal,", "tokens": ["Und", "im\u00b7mer", "neu", ",", "wie", "je\u00b7nes", "ers\u00b7te", "mal", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ADJD", "$,", "PWAV", "PDS", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Da sich Begeistrung in das Herz mir stahl", "tokens": ["Da", "sich", "Be\u00b7geis\u00b7trung", "in", "das", "Herz", "mir", "stahl"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PRF", "NN", "APPR", "ART", "NN", "PPER", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Hab' ichs entz\u00fcckt bekennen m\u00fcssen \u2013", "tokens": ["Hab'", "ichs", "ent\u00b7z\u00fcckt", "be\u00b7ken\u00b7nen", "m\u00fcs\u00b7sen", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "VVPP", "VVINF", "VMFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und hab' es \u2013 o wie gern \u2013 bekannt!", "tokens": ["Und", "hab'", "es", "\u2013", "o", "wie", "gern", "\u2013", "be\u00b7kannt", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "$(", "FM", "KOKOM", "ADV", "$(", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Du hast entdeckt ein neues Land,", "tokens": ["Du", "hast", "ent\u00b7deckt", "ein", "neu\u00b7es", "Land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Kolumbus! la\u00df Dich gr\u00fc\u00dfen.", "tokens": ["Ko\u00b7lum\u00b7bus", "!", "la\u00df", "Dich", "gr\u00fc\u00b7\u00dfen", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$.", "VVIMP", "PPER", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Und ob wie er vervehmet und verbannt,", "tokens": ["Und", "ob", "wie", "er", "ver\u00b7veh\u00b7met", "und", "ver\u00b7bannt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "KOKOM", "PPER", "VVFIN", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Du einsam weilst im fernen, fremden Land", "tokens": ["Du", "ein\u00b7sam", "weilst", "im", "fer\u00b7nen", ",", "frem\u00b7den", "Land"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "ADJD", "VVFIN", "APPRART", "ADJA", "$,", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Dein Stern kann nicht erbleichen.", "tokens": ["Dein", "Stern", "kann", "nicht", "er\u00b7blei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Mit Donnert\u00f6nen dringt Dein Name weit", "tokens": ["Mit", "Don\u00b7ner\u00b7t\u00f6\u00b7nen", "dringt", "Dein", "Na\u00b7me", "weit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "NN", "VVFIN", "PPOSAT", "NN", "ADJD"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Er gl\u00e4nzt in sieggewohnter Herrlichkeit", "tokens": ["Er", "gl\u00e4nzt", "in", "sieg\u00b7ge\u00b7wohn\u00b7ter", "Herr\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Als unser Bundeszeichen.", "tokens": ["Als", "un\u00b7ser", "Bun\u00b7des\u00b7zei\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Dir winkt der Tempel der Unsterblichkeit,", "tokens": ["Dir", "winkt", "der", "Tem\u00b7pel", "der", "U\u00b7nsterb\u00b7lich\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die jeden Genius der Zukunft weiht,", "tokens": ["Die", "je\u00b7den", "Ge\u00b7nius", "der", "Zu\u00b7kunft", "weiht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Der seinem Volk vorangegangen.", "tokens": ["Der", "sei\u00b7nem", "Volk", "vor\u00b7an\u00b7ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es folgt Dir nach zum Reich, das Du erschaut,", "tokens": ["Es", "folgt", "Dir", "nach", "zum", "Reich", ",", "das", "Du", "er\u00b7schaut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "APPR", "APPRART", "NN", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Der Zukunft Kunstwerk wird einst hoch erbaut", "tokens": ["Der", "Zu\u00b7kunft", "Kunst\u00b7werk", "wird", "einst", "hoch", "er\u00b7baut"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VAFIN", "ADV", "ADJD", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Und Dir geweihet prangen.", "tokens": ["Und", "Dir", "ge\u00b7wei\u00b7het", "pran\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVPP", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}