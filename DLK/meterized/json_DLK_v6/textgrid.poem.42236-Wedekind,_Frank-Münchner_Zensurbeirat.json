{"textgrid.poem.42236": {"metadata": {"author": {"name": "Wedekind, Frank", "birth": "N.A.", "death": "N.A."}, "title": "M\u00fcnchner Zensurbeirat", "genre": "verse", "period": "N.A.", "pub_year": 1891, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Die Zensur w\u00e4hlt einen Beirat,", "tokens": ["Die", "Zen\u00b7sur", "w\u00e4hlt", "ei\u00b7nen", "Bei\u00b7rat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+---", "measure": "iambic.di.relaxed"}, "line.2": {"text": "Und der Beirat r\u00e4t genau,", "tokens": ["Und", "der", "Bei\u00b7rat", "r\u00e4t", "ge\u00b7nau", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VVFIN", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie in einer Musterheirat", "tokens": ["Wie", "in", "ei\u00b7ner", "Mus\u00b7ter\u00b7hei\u00b7rat"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die normale Ehefrau.", "tokens": ["Die", "nor\u00b7ma\u00b7le", "E\u00b7he\u00b7frau", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Dreimal \u00bbJa\u00ab auf alle Fragen,", "tokens": ["Drei\u00b7mal", "\u00bb", "Ja", "\u00ab", "auf", "al\u00b7le", "Fra\u00b7gen", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PTKANT", "$(", "APPR", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie der Zensor sie bespricht.", "tokens": ["Wie", "der", "Zen\u00b7sor", "sie", "be\u00b7spricht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "PPER", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "\u00bbnein\u00ab darf nur der Zensor sagen,", "tokens": ["\u00bb", "nein", "\u00ab", "darf", "nur", "der", "Zen\u00b7sor", "sa\u00b7gen", ","], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PTKANT", "$(", "VMFIN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "F\u00fcr den Beirat gibt's das nicht.", "tokens": ["F\u00fcr", "den", "Bei\u00b7rat", "gibt's", "das", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVFIN", "PDS", "PTKNEG", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Sollte je ein Rat sich lohnen,", "tokens": ["Soll\u00b7te", "je", "ein", "Rat", "sich", "loh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADV", "ART", "NN", "PRF", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Weil ihr Leid die Menschheit klagt,", "tokens": ["Weil", "ihr", "Leid", "die", "Menschheit", "klagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Dann, um sein Gehirn zu schonen,", "tokens": ["Dann", ",", "um", "sein", "Ge\u00b7hirn", "zu", "scho\u00b7nen", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "KOUI", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Wird der Beirat nicht gefragt.", "tokens": ["Wird", "der", "Bei\u00b7rat", "nicht", "ge\u00b7fragt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "PTKNEG", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Und zu solchen Narrenspossen,", "tokens": ["Und", "zu", "sol\u00b7chen", "Nar\u00b7rens\u00b7pos\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aller Menschenw\u00fcrde bar,", "tokens": ["Al\u00b7ler", "Men\u00b7schen\u00b7w\u00fcr\u00b7de", "bar", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Bieten heut sich unverdrossen", "tokens": ["Bie\u00b7ten", "heut", "sich", "un\u00b7ver\u00b7dros\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "PRF", "ADJD"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Lauter Ehrenm\u00e4nner dar.", "tokens": ["Lau\u00b7ter", "Eh\u00b7ren\u00b7m\u00e4n\u00b7ner", "dar", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}}}}