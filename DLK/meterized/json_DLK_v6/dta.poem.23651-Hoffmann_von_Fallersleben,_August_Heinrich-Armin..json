{"dta.poem.23651": {"metadata": {"author": {"name": "Hoffmann von Fallersleben, August Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Armin.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1840", "urn": "urn:nbn:de:kobv:b4-200905192626", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Uns ist in alten Sagen gar wunderviel gesagt,", "tokens": ["Uns", "ist", "in", "al\u00b7ten", "Sa\u00b7gen", "gar", "wun\u00b7der\u00b7viel", "ge\u00b7sagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ADJA", "NN", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Wonach in unsern Tagen das Publicum nicht fragt.", "tokens": ["Wo\u00b7nach", "in", "un\u00b7sern", "Ta\u00b7gen", "das", "Pub\u00b7li\u00b7cum", "nicht", "fragt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "ART", "NN", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Ich aber will berichten was heute nur geschieht,", "tokens": ["Ich", "a\u00b7ber", "will", "be\u00b7rich\u00b7ten", "was", "heu\u00b7te", "nur", "ge\u00b7schieht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VMFIN", "VVINF", "PRELS", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Nur sch\u00f6ne neue Geschichten. Und also hebt sich an das Lied.", "tokens": ["Nur", "sch\u00f6\u00b7ne", "neu\u00b7e", "Ge\u00b7schich\u00b7ten", ".", "Und", "al\u00b7so", "hebt", "sich", "an", "das", "Lied", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "ADJA", "NN", "$.", "KON", "ADV", "VVFIN", "PRF", "APPR", "ART", "NN", "$."], "meter": "-+-+--+--+-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.2": {"line.1": {"text": "Es kam vom Himmel nieder der deutsche Held Armin,", "tokens": ["Es", "kam", "vom", "Him\u00b7mel", "nie\u00b7der", "der", "deut\u00b7sche", "Held", "Ar\u00b7min", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPRART", "NN", "PTKVZ", "ART", "ADJA", "NN", "NE", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Seit grauen Zeiten wieder, er kam, wir sahen ihn;", "tokens": ["Seit", "grau\u00b7en", "Zei\u00b7ten", "wie\u00b7der", ",", "er", "kam", ",", "wir", "sa\u00b7hen", "ihn", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "ADV", "$,", "PPER", "VVFIN", "$,", "PPER", "VVFIN", "PPER", "$."], "meter": "-+-+--+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Er war noch stets derselbe, er ging ganz frank und frei,", "tokens": ["Er", "war", "noch", "stets", "der\u00b7sel\u00b7be", ",", "er", "ging", "ganz", "frank", "und", "frei", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PDAT", "$,", "PPER", "VVFIN", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Er wollte Deutschland sehen, ob's noch dasselbe Deutschland sei.", "tokens": ["Er", "woll\u00b7te", "Deutschland", "se\u00b7hen", ",", "ob's", "noch", "das\u00b7sel\u00b7be", "Deutschland", "sei", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NE", "VVINF", "$,", "KOUS", "ADV", "PDAT", "NN", "VAFIN", "$."], "meter": "-+-+---+-+-+-", "measure": "unknown.measure.penta"}}, "stanza.3": {"line.1": {"text": "Im Teutoburger Walde da lie\u00df er sich herab,", "tokens": ["Im", "Teu\u00b7to\u00b7bur\u00b7ger", "Wal\u00b7de", "da", "lie\u00df", "er", "sich", "her\u00b7ab", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "ADV", "VVFIN", "PPER", "PRF", "ADV", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Er dacht' an Alles wieder was einst sich dort begab.", "tokens": ["Er", "dacht'", "an", "Al\u00b7les", "wie\u00b7der", "was", "einst", "sich", "dort", "be\u00b7gab", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PIS", "ADV", "PWS", "ADV", "PRF", "ADV", "VVFIN", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Da fragt ihn ein Gensd'arme: \u201ewo haben Sie Ihren Pa\u00df?\u201c", "tokens": ["Da", "fragt", "ihn", "ein", "Gens\u00b7d'\u00b7ar\u00b7me", ":", "\u201e", "wo", "ha\u00b7ben", "Sie", "Ih\u00b7ren", "Pa\u00df", "?", "\u201c"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$.", "$(", "PWAV", "VAFIN", "PPER", "PPOSAT", "NN", "$.", "$("], "meter": "-+--+-+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Es erwiedert ihm der Recke: \u201ewas k\u00fcmmert dich denn wunder das?\u201c", "tokens": ["Es", "er\u00b7wie\u00b7dert", "ihm", "der", "Re\u00b7cke", ":", "\u201e", "was", "k\u00fcm\u00b7mert", "dich", "denn", "wun\u00b7der", "das", "?", "\u201c"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "$.", "$(", "PWS", "VVFIN", "PPER", "ADV", "ADJD", "PDS", "$.", "$("], "meter": "--+---+--+-+-+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.4": {"line.1": {"text": "\u201eich bin ein Officiante, ich thue nur meine Pflicht,", "tokens": ["\u201e", "ich", "bin", "ein", "Of\u00b7fi\u00b7ci\u00b7an\u00b7te", ",", "ich", "thue", "nur", "mei\u00b7ne", "Pflicht", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VAFIN", "ART", "NN", "$,", "PPER", "VVFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Und thue gar nichts weiter als was die Vorschrift spricht:", "tokens": ["Und", "thue", "gar", "nichts", "wei\u00b7ter", "als", "was", "die", "Vor\u00b7schrift", "spricht", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PIS", "ADV", "KOUS", "PIS", "ART", "NN", "VVFIN", "$."], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Wer ohne Pa\u00df hier kommet, wer sich nicht legitimirt,", "tokens": ["Wer", "oh\u00b7ne", "Pa\u00df", "hier", "kom\u00b7met", ",", "wer", "sich", "nicht", "le\u00b7gi\u00b7ti\u00b7mirt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "NN", "ADV", "VVFIN", "$,", "PWS", "PRF", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.4": {"text": "Der wird von Polizeiwegen sofort hier arretiert.\u201c", "tokens": ["Der", "wird", "von", "Po\u00b7li\u00b7zei\u00b7we\u00b7gen", "so\u00b7fort", "hier", "ar\u00b7re\u00b7tiert", ".", "\u201c"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VAFIN", "APPR", "NN", "ADV", "ADV", "VVFIN", "$.", "$("], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}}, "stanza.5": {"line.1": {"text": "Zum Gl\u00fccke kam gegangen ein alter Edelmann,", "tokens": ["Zum", "Gl\u00fc\u00b7cke", "kam", "ge\u00b7gan\u00b7gen", "ein", "al\u00b7ter", "E\u00b7del\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVFIN", "VVPP", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Der hatte sich von ferne schon geh\u00f6rt die Sachen an;", "tokens": ["Der", "hat\u00b7te", "sich", "von", "fer\u00b7ne", "schon", "ge\u00b7h\u00f6rt", "die", "Sa\u00b7chen", "an", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PRF", "APPR", "ADV", "ADV", "VVFIN", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.3": {"text": "Es war ihm aus der Kindheit Armins Portr\u00e4t bekannt:", "tokens": ["Es", "war", "ihm", "aus", "der", "Kind\u00b7heit", "Ar\u00b7mins", "Por\u00b7tr\u00e4t", "be\u00b7kannt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "APPR", "ART", "NN", "NE", "NE", "PTKVZ", "$."], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "\u201ef\u00fcr diesen Fremden b\u00fcrg' ich.\u201c Er nahm ihn gleich auch bei", "tokens": ["\u201e", "f\u00fcr", "die\u00b7sen", "Frem\u00b7den", "b\u00fcr\u00b7g'", "ich", ".", "\u201c", "Er", "nahm", "ihn", "gleich", "auch", "bei"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "APPR", "PDAT", "NN", "VVFIN", "PPER", "$.", "$(", "PPER", "VVFIN", "PPER", "ADV", "ADV", "APPR"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.5": {"text": "der Hand.", "tokens": ["der", "Hand", "."], "token_info": ["word", "word", "punct"], "pos": ["ART", "NN", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.6": {"line.1": {"text": "Und f\u00fchrt' ihn durch den Schlo\u00dfhof in den alten Rittersaal;", "tokens": ["Und", "f\u00fchrt'", "ihn", "durch", "den", "Schlo\u00df\u00b7hof", "in", "den", "al\u00b7ten", "Rit\u00b7ter\u00b7saal", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.2": {"text": "Das Gesinde hie\u00df er kommen, es bracht' ihm einen Pokal,", "tokens": ["Das", "Ge\u00b7sin\u00b7de", "hie\u00df", "er", "kom\u00b7men", ",", "es", "bracht'", "ihm", "ei\u00b7nen", "Po\u00b7kal", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "VVINF", "$,", "PPER", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "--+-+-+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Das war ein echter R\u00f6mer, den schenkt er ganz voll Wein,", "tokens": ["Das", "war", "ein", "ech\u00b7ter", "R\u00f6\u00b7mer", ",", "den", "schenkt", "er", "ganz", "voll", "Wein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN", "$,", "ART", "VVFIN", "PPER", "ADV", "ADJD", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und bot ihn auf Deutschlands Freiheit dem viellieben Gaste sein.", "tokens": ["Und", "bot", "ihn", "auf", "Deutschlands", "Frei\u00b7heit", "dem", "viel\u00b7lie\u00b7ben", "Gas\u00b7te", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "NE", "NN", "ART", "ADJA", "NN", "VAINF", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}}, "stanza.7": {"line.1": {"text": "\u201eja, sprach Armin, ich ", "tokens": ["\u201e", "ja", ",", "sprach", "Ar\u00b7min", ",", "ich"], "token_info": ["punct", "word", "punct", "word", "word", "punct", "word"], "pos": ["$(", "PTKANT", "$,", "VVFIN", "NE", "$,", "PPER"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.2": {"text": "Ich bin des Fechtens m\u00fcde, was hat man auch zuletzt?", "tokens": ["Ich", "bin", "des", "Fech\u00b7tens", "m\u00fc\u00b7de", ",", "was", "hat", "man", "auch", "zu\u00b7letzt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "ADJD", "$,", "PWS", "VAFIN", "PIS", "ADV", "ADV", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Doch ewig hass' ich die R\u00f6mer und ewig bei Tag und Nacht,", "tokens": ["Doch", "e\u00b7wig", "hass'", "ich", "die", "R\u00f6\u00b7mer", "und", "e\u00b7wig", "bei", "Tag", "und", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VAFIN", "PPER", "ART", "NN", "KON", "ADJD", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+--+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Sie haben uns stets das Schlechte, und gewi\u00df auch die P\u00e4sse", "tokens": ["Sie", "ha\u00b7ben", "uns", "stets", "das", "Schlech\u00b7te", ",", "und", "ge\u00b7wi\u00df", "auch", "die", "P\u00e4s\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "ART", "NN", "$,", "KON", "ADV", "ADV", "ART", "NN"], "meter": "-+--+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "hergebracht.\u201c", "tokens": ["her\u00b7ge\u00b7bracht", ".", "\u201c"], "token_info": ["word", "punct", "punct"], "pos": ["VVPP", "$.", "$("], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.8": {"line.1": {"text": "Der Edelmann versetzte: \u201eBes\u00e4nftige dich nur!", "tokens": ["Der", "E\u00b7del\u00b7mann", "ver\u00b7setz\u00b7te", ":", "\u201e", "Be\u00b7s\u00e4nf\u00b7ti\u00b7ge", "dich", "nur", "!"], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "$(", "VVFIN", "PPER", "ADV", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Es ist in der Welt von R\u00f6mern jetzt kaum noch eine Spur;", "tokens": ["Es", "ist", "in", "der", "Welt", "von", "R\u00f6\u00b7mern", "jetzt", "kaum", "noch", "ei\u00b7ne", "Spur", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ART", "NN", "APPR", "NN", "ADV", "ADV", "ADV", "ART", "NN", "$."], "meter": "-+--+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Du hast sie ja vertilget, kein Mensch spricht mehr Latein,", "tokens": ["Du", "hast", "sie", "ja", "ver\u00b7til\u00b7get", ",", "kein", "Mensch", "spricht", "mehr", "La\u00b7tein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "VVFIN", "$,", "PIAT", "NN", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+--++-+-", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Du hast ihn ausgel\u00f6schet des R\u00f6merreiches Glanz und Schein.\u201c", "tokens": ["Du", "hast", "ihn", "aus\u00b7ge\u00b7l\u00f6\u00b7schet", "des", "R\u00f6\u00b7mer\u00b7rei\u00b7ches", "Glanz", "und", "Schein", ".", "\u201c"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVFIN", "ART", "ADJA", "NN", "KON", "NN", "$.", "$("], "meter": "-+-+-+--+-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.9": {"line.1": {"text": "\u201ees beten zwar die Christen in Latein noch hie und da,", "tokens": ["\u201e", "es", "be\u00b7ten", "zwar", "die", "Chris\u00b7ten", "in", "La\u00b7tein", "noch", "hie", "und", "da", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "ADV", "ART", "NN", "APPR", "NN", "ADV", "ADV", "KON", "ADV", "$,"], "meter": "-+-+-+-++--+-+", "measure": "iambic.septa.relaxed"}, "line.2": {"text": "Auch lernen die Juristen draus ihre Principia;", "tokens": ["Auch", "ler\u00b7nen", "die", "Ju\u00b7ris\u00b7ten", "draus", "ih\u00b7re", "Prin\u00b7ci\u00b7pia", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PAV", "PPOSAT", "NN", "$."], "meter": "-+--++--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Auch treiben es die Gelehrten und halten noch viel darauf,", "tokens": ["Auch", "trei\u00b7ben", "es", "die", "Ge\u00b7lehr\u00b7ten", "und", "hal\u00b7ten", "noch", "viel", "da\u00b7rauf", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "KON", "VVFIN", "ADV", "ADV", "PAV", "$,"], "meter": "-+-+--+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Doch, glaub' ich, endlich h\u00f6ret der Bettel mal von selber auf.\u201c", "tokens": ["Doch", ",", "glaub'", "ich", ",", "end\u00b7lich", "h\u00f6\u00b7ret", "der", "Bet\u00b7tel", "mal", "von", "sel\u00b7ber", "auf", ".", "\u201c"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "$,", "VVFIN", "PPER", "$,", "ADV", "VVFIN", "ART", "NN", "ADV", "APPR", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+-+--+-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.10": {"line.1": {"text": "\u201eso etwas darf nicht k\u00fcmmern, das ist bei uns der Brauch:", "tokens": ["\u201e", "so", "et\u00b7was", "darf", "nicht", "k\u00fcm\u00b7mern", ",", "das", "ist", "bei", "uns", "der", "Brauch", ":"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "PIS", "VMFIN", "PTKNEG", "VVINF", "$,", "PDS", "VAFIN", "APPR", "PPER", "ART", "NN", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Ein Deutscher ist ein Gelehrter, drum lernt er Alles auch.", "tokens": ["Ein", "Deut\u00b7scher", "ist", "ein", "Ge\u00b7lehr\u00b7ter", ",", "drum", "lernt", "er", "Al\u00b7les", "auch", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "NN", "$,", "PAV", "VVFIN", "PPER", "PIS", "ADV", "$."], "meter": "-+-++-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.3": {"text": "Du hast in deiner Jugend ja auch gelernt Latein,", "tokens": ["Du", "hast", "in", "dei\u00b7ner", "Ju\u00b7gend", "ja", "auch", "ge\u00b7lernt", "La\u00b7tein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPOSAT", "NN", "ADV", "ADV", "VVPP", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und bist kein R\u00f6mer gewesen \u2014 Trink aus! ich schenke wieder ein.\u201c", "tokens": ["Und", "bist", "kein", "R\u00f6\u00b7mer", "ge\u00b7we\u00b7sen", "Trink", "aus", "!", "ich", "schen\u00b7ke", "wie\u00b7der", "ein", ".", "\u201c"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VAFIN", "PIAT", "NN", "VAPP", "$(", "VVIMP", "PTKVZ", "$.", "PPER", "VVFIN", "ADV", "PTKVZ", "$.", "$("], "meter": "-+-+--+-+-+-+--+", "measure": "iambic.septa.relaxed"}}, "stanza.11": {"line.1": {"text": "\u201edoch sei mir gottwillkommen, du hoher Held Armin!", "tokens": ["\u201e", "doch", "sei", "mir", "gott\u00b7will\u00b7kom\u00b7men", ",", "du", "ho\u00b7her", "Held", "Ar\u00b7min", "!"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADV", "VAFIN", "PPER", "VVINF", "$,", "PPER", "ADJA", "NN", "NE", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "O la\u00df mich dich umfangen, o la\u00df mich vor dir knien!", "tokens": ["O", "la\u00df", "mich", "dich", "um\u00b7fan\u00b7gen", ",", "o", "la\u00df", "mich", "vor", "dir", "kni\u00b7en", "!"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PPER", "PRF", "VVINF", "$,", "FM", "VVIMP", "PPER", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Du bist doch stets derselbe, mit deinem blonden Haar,", "tokens": ["Du", "bist", "doch", "stets", "der\u00b7sel\u00b7be", ",", "mit", "dei\u00b7nem", "blon\u00b7den", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "PDAT", "$,", "APPR", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Mit deinem liebevollen, deinem sch\u00f6nen blauen Augenpaar!\u201c", "tokens": ["Mit", "dei\u00b7nem", "lie\u00b7be\u00b7vol\u00b7len", ",", "dei\u00b7nem", "sch\u00f6\u00b7nen", "blau\u00b7en", "Au\u00b7gen\u00b7paar", "!", "\u201c"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPR", "PPOSAT", "VVFIN", "$,", "PPOSAT", "ADJA", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}}, "stanza.12": {"line.1": {"text": "\u201everg\u00f6nne da\u00df ich lese, wie lieb und werth du bist,", "tokens": ["\u201e", "ver\u00b7g\u00f6n\u00b7ne", "da\u00df", "ich", "le\u00b7se", ",", "wie", "lieb", "und", "werth", "du", "bist", ","], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "KOUS", "PPER", "VVFIN", "$,", "PWAV", "ADJD", "KON", "ADJD", "PPER", "VAFIN", "$,"], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Wie jede deiner Thaten uns hoch und heilig ist \u2014 \u201c", "tokens": ["Wie", "je\u00b7de", "dei\u00b7ner", "Tha\u00b7ten", "uns", "hoch", "und", "hei\u00b7lig", "ist", "\u201c"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWAV", "PIAT", "PPOSAT", "NN", "PPER", "ADJD", "KON", "ADJD", "VAFIN", "$(", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Es las darauf der Edelmann ihm aus dem Lohenstein;", "tokens": ["Es", "las", "da\u00b7rauf", "der", "E\u00b7del\u00b7mann", "ihm", "aus", "dem", "Lo\u00b7hen\u00b7stein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PAV", "ART", "NN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.4": {"text": "Bald kam ein s\u00fc\u00dfer Schlummer, Nacht war's, der Held Armin", "tokens": ["Bald", "kam", "ein", "s\u00fc\u00b7\u00dfer", "Schlum\u00b7mer", ",", "Nacht", "wa\u00b7r's", ",", "der", "Held", "Ar\u00b7min"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "$,", "NN", "VAFIN", "$,", "ART", "NN", "NE"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.5": {"text": "schlief ein.", "tokens": ["schlief", "ein", "."], "token_info": ["word", "word", "punct"], "pos": ["VVFIN", "PTKVZ", "$."], "meter": "-+", "measure": "iambic.single"}}, "stanza.13": {"line.1": {"text": "Und als am hellen Tage Armin erwachet war,", "tokens": ["Und", "als", "am", "hel\u00b7len", "Ta\u00b7ge", "Ar\u00b7min", "er\u00b7wa\u00b7chet", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "APPRART", "ADJA", "NN", "NE", "VVFIN", "VAFIN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Da kamen alle und brachten ihm ihren Gl\u00fcckwunsch dar;", "tokens": ["Da", "ka\u00b7men", "al\u00b7le", "und", "brach\u00b7ten", "ihm", "ih\u00b7ren", "Gl\u00fcck\u00b7wunsch", "dar", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "KON", "VVFIN", "PPER", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Es kam die Frau mit den Fr\u00e4uleins, es kam der Edelmann,", "tokens": ["Es", "kam", "die", "Frau", "mit", "den", "Fr\u00e4u\u00b7leins", ",", "es", "kam", "der", "E\u00b7del\u00b7mann", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+--++-+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Und alle sahen den Helden mit Blicken minniglichen an.", "tokens": ["Und", "al\u00b7le", "sa\u00b7hen", "den", "Hel\u00b7den", "mit", "Bli\u00b7cken", "min\u00b7nig\u00b7li\u00b7chen", "an", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "ART", "NN", "APPR", "NN", "VVFIN", "PTKVZ", "$."], "meter": "-+-+--+--+-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.14": {"line.1": {"text": "Und unterdessen eilte die M\u00e4hr' von Mund zu Mund,", "tokens": ["Und", "un\u00b7ter\u00b7des\u00b7sen", "eil\u00b7te", "die", "M\u00e4hr'", "von", "Mund", "zu", "Mund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "ART", "NN", "APPR", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Und durch die Eisenbahnen ward's allen Deutschen kund:", "tokens": ["Und", "durch", "die", "Ei\u00b7sen\u00b7bah\u00b7nen", "ward's", "al\u00b7len", "Deut\u00b7schen", "kund", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "NN", "VAFIN", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Er ist da, ist wiedergekommen Deutschlands Befreier Armin!", "tokens": ["Er", "ist", "da", ",", "ist", "wie\u00b7der\u00b7ge\u00b7kom\u00b7men", "Deutschlands", "Be\u00b7frei\u00b7er", "Ar\u00b7min", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "$,", "VAFIN", "VVPP", "NE", "NE", "NE", "$."], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.4": {"text": "Im Teutoburger Walde, kommt her, kommt her und sehet selber ihn!", "tokens": ["Im", "Teu\u00b7to\u00b7bur\u00b7ger", "Wal\u00b7de", ",", "kommt", "her", ",", "kommt", "her", "und", "se\u00b7het", "sel\u00b7ber", "ihn", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "$,", "VVFIN", "PTKVZ", "$,", "VVFIN", "PTKVZ", "KON", "VVFIN", "ADV", "PPER", "$."], "meter": "-+-+-+--+-+-+-+-+", "measure": "iambic.octa.plus.relaxed"}}, "stanza.15": {"line.1": {"text": "Da schickten die Westphalen als Festcomit", "tokens": ["Da", "schick\u00b7ten", "die", "West\u00b7pha\u00b7len", "als", "Fest\u00b7co\u00b7mit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "KOUS", "NN"], "meter": "-+--++--+-+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Grobk\u00f6rnigen und feisten Pumpernickel ihm zu,", "tokens": ["Grob\u00b7k\u00f6r\u00b7ni\u00b7gen", "und", "feis\u00b7ten", "Pum\u00b7per\u00b7ni\u00b7ckel", "ihm", "zu", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJA", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.3": {"text": "Es schickten die alten Sassen ihm echte Cheruskerwurst,", "tokens": ["Es", "schick\u00b7ten", "die", "al\u00b7ten", "Sas\u00b7sen", "ihm", "ech\u00b7te", "Che\u00b7rus\u00b7ker\u00b7wurst", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "PPER", "ADJA", "NN", "$,"], "meter": "-+--+-+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und andre deutschen St\u00e4mme dachten an des Helden guten Durst.", "tokens": ["Und", "and\u00b7re", "deut\u00b7schen", "St\u00e4m\u00b7me", "dach\u00b7ten", "an", "des", "Hel\u00b7den", "gu\u00b7ten", "Durst", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "ADJA", "NN", "VVFIN", "APPR", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}}, "stanza.16": {"line.1": {"text": "Es sandten ihm die Baiern mit Bock ein Fuderfa\u00df,", "tokens": ["Es", "sand\u00b7ten", "ihm", "die", "Bai\u00b7ern", "mit", "Bock", "ein", "Fu\u00b7der\u00b7fa\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "APPR", "NE", "ART", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Weil das in ihrem Lande noch immer das beste was;", "tokens": ["Weil", "das", "in", "ih\u00b7rem", "Lan\u00b7de", "noch", "im\u00b7mer", "das", "bes\u00b7te", "was", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "APPR", "PPOSAT", "NN", "ADV", "ADV", "ART", "ADJA", "PWS", "$."], "meter": "-+-+-+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Es sandten darauf die Franken Bocksbeutel wohl verpicht", "tokens": ["Es", "sand\u00b7ten", "da\u00b7rauf", "die", "Fran\u00b7ken", "Bocks\u00b7beu\u00b7tel", "wohl", "ver\u00b7picht"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PAV", "ART", "NN", "NE", "ADV", "VVPP"], "meter": "-+--+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und die freien St\u00e4dte Cigarren aus Havanna, sie hatten", "tokens": ["Und", "die", "frei\u00b7en", "St\u00e4d\u00b7te", "Ci\u00b7gar\u00b7ren", "aus", "Ha\u00b7van\u00b7na", ",", "sie", "hat\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN", "NN", "APPR", "NE", "$,", "PPER", "VAFIN"], "meter": "+-+-+-+-+-+-+-+-", "measure": "trochaic.octa.plus"}, "line.5": {"text": "Deutscheres nicht.", "tokens": ["Deut\u00b7sche\u00b7res", "nicht", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "PTKNEG", "$."], "meter": "+--+", "measure": "iambic.di.chol"}}, "stanza.17": {"line.1": {"text": "Und wie ein Schwarm Heuschrecken kamen von Pyrmont herbei", "tokens": ["Und", "wie", "ein", "Schwarm", "Heu\u00b7schre\u00b7cken", "ka\u00b7men", "von", "Pyr\u00b7mont", "her\u00b7bei"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "ART", "NN", "NN", "VVFIN", "APPR", "NE", "PTKVZ"], "meter": "-+-+-+-+--++-+", "measure": "iambic.septa.relaxed"}, "line.2": {"text": "Die Naturforscher und Aerzte f\u00fcnfhundert und f\u00fcnfzigerlei;", "tokens": ["Die", "Na\u00b7tur\u00b7for\u00b7scher", "und", "A\u00b7erz\u00b7te", "f\u00fcnf\u00b7hun\u00b7dert", "und", "f\u00fcnf\u00b7zi\u00b7ger\u00b7lei", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVFIN", "KON", "CARD", "$."], "meter": "--+-+-+--++--+--+", "measure": "iambic.septa.relaxed"}, "line.3": {"text": "Sie hielten die zehnte Spazierfahrt in solcher Gesch\u00e4ftigkeit,", "tokens": ["Sie", "hiel\u00b7ten", "die", "zehn\u00b7te", "Spa\u00b7zier\u00b7fahrt", "in", "sol\u00b7cher", "Ge\u00b7sch\u00e4f\u00b7tig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+--+--+--+--+-+", "measure": "amphibrach.penta.plus"}, "line.4": {"text": "Da\u00df sie des Essens verga\u00dfen und zum Trinken sich nahmen keine Zeit.", "tokens": ["Da\u00df", "sie", "des", "Es\u00b7sens", "ver\u00b7ga\u00b7\u00dfen", "und", "zum", "Trin\u00b7ken", "sich", "nah\u00b7men", "kei\u00b7ne", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVINF", "KON", "APPRART", "NN", "PRF", "VVFIN", "PIAT", "NN", "$."], "meter": "-+-+--+-+-+--+-+-+", "measure": "iambic.octa.plus.relaxed"}}, "stanza.18": {"line.1": {"text": "Sie wollten die deutsche Trinksucht erforschen am Helden Armin,", "tokens": ["Sie", "woll\u00b7ten", "die", "deut\u00b7sche", "Trink\u00b7sucht", "er\u00b7for\u00b7schen", "am", "Hel\u00b7den", "Ar\u00b7min", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "NN", "VVFIN", "APPRART", "NN", "NE", "$,"], "meter": "-+--+-+--+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Ob Gott in so fr\u00fchen Zeiten schon uns dieselbe verliehn,", "tokens": ["Ob", "Gott", "in", "so", "fr\u00fc\u00b7hen", "Zei\u00b7ten", "schon", "uns", "die\u00b7sel\u00b7be", "ver\u00b7liehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "ADV", "ADJA", "NN", "ADV", "PPER", "PDAT", "VVINF", "$,"], "meter": "-+--+-+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Sie wollten nach Pariser Zoller ihm messen seinen Schlund", "tokens": ["Sie", "woll\u00b7ten", "nach", "Pa\u00b7ri\u00b7ser", "Zol\u00b7ler", "ihm", "mes\u00b7sen", "sei\u00b7nen", "Schlund"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "NE", "NE", "PPER", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Und dann in Oken's Isis promulgieren den Sachbefund.", "tokens": ["Und", "dann", "in", "O\u00b7ken's", "I\u00b7sis", "pro\u00b7mul\u00b7gie\u00b7ren", "den", "Sach\u00b7be\u00b7fund", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "NE", "NE", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+--+-+", "measure": "iambic.septa.relaxed"}}, "stanza.19": {"line.1": {"text": "Es befand sich einer drunter, der schien ein Agent zu sein", "tokens": ["Es", "be\u00b7fand", "sich", "ei\u00b7ner", "drun\u00b7ter", ",", "der", "schien", "ein", "A\u00b7gent", "zu", "sein"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PRF", "ART", "ADJA", "$,", "PRELS", "VVFIN", "ART", "NN", "PTKZU", "VAINF"], "meter": "--+-+-+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Von dem Jenaer beliebten Mineralogen-Verein;", "tokens": ["Von", "dem", "Je\u00b7naer", "be\u00b7lieb\u00b7ten", "Mi\u00b7ne\u00b7ra\u00b7lo\u00b7gen\u00b7Ver\u00b7ein", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NE", "ADJA", "NN", "$."], "meter": "+--+-+-+--+--+", "measure": "iambic.hexa.invert"}, "line.3": {"text": "Der zog ein Diplom aus der Tasche: \u201edem deutschen Freiheitsstein!\u201c", "tokens": ["Der", "zog", "ein", "Dip\u00b7lom", "aus", "der", "Ta\u00b7sche", ":", "\u201e", "dem", "deut\u00b7schen", "Frei\u00b7heits\u00b7stein", "!", "\u201c"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$.", "$(", "ART", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+-+--+-+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Da sprach von Lemgo ein Steinmetz: \u201emit Nichten, das ist doch", "tokens": ["Da", "sprach", "von", "Lem\u00b7go", "ein", "Stein\u00b7metz", ":", "\u201e", "mit", "Nich\u00b7ten", ",", "das", "ist", "doch"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "NE", "ART", "NN", "$.", "$(", "APPR", "NN", "$,", "PDS", "VAFIN", "ADV"], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "zu gemein!&#8220;", "tokens": ["zu", "ge\u00b7mein", "!", "&#8220;"], "token_info": ["word", "word", "punct", "XML_entity"], "pos": ["PTKA", "ADJD", "$.", "$("], "meter": "+-+", "measure": "trochaic.di"}}, "stanza.20": {"line.1": {"text": "Auch kamen in selber Stunde von M\u00fcnchen und von Berlin", "tokens": ["Auch", "ka\u00b7men", "in", "sel\u00b7ber", "Stun\u00b7de", "von", "M\u00fcn\u00b7chen", "und", "von", "Ber\u00b7lin"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "ADJA", "NN", "APPR", "NE", "KON", "APPR", "NE"], "meter": "-+--+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Zwei ber\u00fchmte Mitglieder der ber\u00fchmten Akademien:", "tokens": ["Zwei", "be\u00b7r\u00fchm\u00b7te", "Mit\u00b7glie\u00b7der", "der", "be\u00b7r\u00fchm\u00b7ten", "A\u00b7ka\u00b7de\u00b7mi\u00b7en", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "ART", "ADJA", "NN", "$."], "meter": "+-+--+-+-+--+--+", "measure": "trochaic.septa.relaxed"}, "line.3": {"text": "Herr Ze\u00fcne war der eine, (der fehlt bei keinem Fest!)", "tokens": ["Herr", "Ze\u00fc\u00b7ne", "war", "der", "ei\u00b7ne", ",", "(", "der", "fehlt", "bei", "kei\u00b7nem", "Fest", "!", ")"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "NE", "VAFIN", "ART", "ART", "$,", "$(", "ART", "VVFIN", "APPR", "PIAT", "NN", "$.", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Der andere war Herr Ma\u00dfmann, die sollten forschen aufs Allerbest.", "tokens": ["Der", "an\u00b7de\u00b7re", "war", "Herr", "Ma\u00df\u00b7mann", ",", "die", "soll\u00b7ten", "for\u00b7schen", "aufs", "Al\u00b7ler\u00b7best", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VAFIN", "NN", "NN", "$,", "PRELS", "PIAT", "NN", "APPRART", "NN", "$."], "meter": "-+-+-++--+-+-+-+-", "measure": "iambic.octa.plus.relaxed"}}, "stanza.21": {"line.1": {"text": "Der eine nur erdkundlich, wie Germania damals war,", "tokens": ["Der", "ei\u00b7ne", "nur", "erd\u00b7kund\u00b7lich", ",", "wie", "Ger\u00b7ma\u00b7nia", "da\u00b7mals", "war", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADV", "ADJD", "$,", "PWAV", "NE", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.2": {"text": "Ob blaue Augen hatten die Teutonen und blondes Haar?", "tokens": ["Ob", "blau\u00b7e", "Au\u00b7gen", "hat\u00b7ten", "die", "Teu\u00b7to\u00b7nen", "und", "blon\u00b7des", "Haar", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "VAFIN", "ART", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.3": {"text": "Der andere philologisch wie sich selber schrieb' Armin,", "tokens": ["Der", "an\u00b7de\u00b7re", "phi\u00b7lo\u00b7lo\u00b7gisch", "wie", "sich", "sel\u00b7ber", "schrieb'", "Ar\u00b7min", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "ADJD", "KOKOM", "PRF", "ADV", "VVFIN", "NE", "$,"], "meter": "-+--+--+--+-+-+", "measure": "amphibrach.tetra.plus"}, "line.4": {"text": "Ob deutsch, ob teutsch, was richtig und welches vorzuziehn?", "tokens": ["Ob", "deutsch", ",", "ob", "teutsch", ",", "was", "rich\u00b7tig", "und", "wel\u00b7ches", "vor\u00b7zu\u00b7ziehn", "?"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJD", "$,", "KOUS", "ADJD", "$,", "PRELS", "ADJD", "KON", "PWS", "VVINF", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.22": {"line.1": {"text": "Auch stellte sich Herr Albrich, ein kleines M\u00e4nnlein ein, \u2014", "tokens": ["Auch", "stell\u00b7te", "sich", "Herr", "Al\u00b7brich", ",", "ein", "klei\u00b7nes", "M\u00e4nn\u00b7lein", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "VVFIN", "PRF", "NN", "NE", "$,", "ART", "ADJA", "NN", "PTKVZ", "$,", "$("], "meter": "-+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Er war fast au\u00dfer Athem vom Philologenverein,", "tokens": ["Er", "war", "fast", "au\u00b7\u00dfer", "A\u00b7them", "vom", "Phi\u00b7lo\u00b7lo\u00b7gen\u00b7ver\u00b7ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Der sollt' Arminium fragen, wie man spreche das Latein,", "tokens": ["Der", "sollt'", "Ar\u00b7mi\u00b7nium", "fra\u00b7gen", ",", "wie", "man", "spre\u00b7che", "das", "La\u00b7tein", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "VVINF", "$,", "PWAV", "PIS", "VVFIN", "ART", "NN", "$,"], "meter": "--+--+-+-+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "Und ob damals die Schulmeister in Rom nur Sklaven gewesen sei'n?", "tokens": ["Und", "ob", "da\u00b7mals", "die", "Schul\u00b7meis\u00b7ter", "in", "Rom", "nur", "Skla\u00b7ven", "ge\u00b7we\u00b7sen", "sei'n", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "ART", "NN", "APPR", "NE", "ADV", "NN", "VAPP", "VAINF", "$."], "meter": "-----++--+-+--+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.23": {"line.1": {"text": "Es kamen auf Fl\u00fcgeln des Sanges die S\u00e4nger aus Schwabenland,", "tokens": ["Es", "ka\u00b7men", "auf", "Fl\u00fc\u00b7geln", "des", "San\u00b7ges", "die", "S\u00e4n\u00b7ger", "aus", "Schwa\u00b7ben\u00b7land", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "ART", "NN", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+--+--+--+--+-+", "measure": "amphibrach.penta.plus"}, "line.2": {"text": "Weil sonst kein anderer S\u00e4nger in Zunft und Ansehn stand;", "tokens": ["Weil", "sonst", "kein", "an\u00b7de\u00b7rer", "S\u00e4n\u00b7ger", "in", "Zunft", "und", "An\u00b7sehn", "stand", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PIAT", "ADJA", "NN", "APPR", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Sie brachten von der Freiheit gar manchen s\u00fc\u00dfen Bar,", "tokens": ["Sie", "brach\u00b7ten", "von", "der", "Frei\u00b7heit", "gar", "man\u00b7chen", "s\u00fc\u00b7\u00dfen", "Bar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Da von dieser Freiheit zu singen noch keinem bisher verboten war.", "tokens": ["Da", "von", "die\u00b7ser", "Frei\u00b7heit", "zu", "sin\u00b7gen", "noch", "kei\u00b7nem", "bis\u00b7her", "ver\u00b7bo\u00b7ten", "war", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PDAT", "NN", "PTKZU", "VVFIN", "ADV", "PIS", "ADV", "VVPP", "VAFIN", "$."], "meter": "--+-+--+--+--+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.24": {"line.1": {"text": "Sie brachten auch gro\u00dfe Listen zu einem Denkmal herbei,", "tokens": ["Sie", "brach\u00b7ten", "auch", "gro\u00b7\u00dfe", "Lis\u00b7ten", "zu", "ei\u00b7nem", "Denk\u00b7mal", "her\u00b7bei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADJA", "NN", "APPR", "ART", "NN", "PTKVZ", "$,"], "meter": "-+--+-+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Genehmigt von allen F\u00fcrsten und auch von der Polizei;", "tokens": ["Ge\u00b7neh\u00b7migt", "von", "al\u00b7len", "F\u00fcrs\u00b7ten", "und", "auch", "von", "der", "Po\u00b7li\u00b7zei", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "PIAT", "NN", "KON", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+--+-+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Sie luden mit Subscriptionen jeden biderben Deutschen ein,", "tokens": ["Sie", "lu\u00b7den", "mit", "Sub\u00b7scrip\u00b7ti\u00b7o\u00b7nen", "je\u00b7den", "bi\u00b7der\u00b7ben", "Deut\u00b7schen", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "PIAT", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+--+-+-+", "measure": "iambic.octa.plus.relaxed"}, "line.4": {"text": "Es sollte das Armins-Denkmal ein Denkmal aller Deutschen sein.", "tokens": ["Es", "soll\u00b7te", "das", "Ar\u00b7mins\u00b7Denk\u00b7mal", "ein", "Denk\u00b7mal", "al\u00b7ler", "Deut\u00b7schen", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "NN", "ART", "NN", "PIAT", "NN", "VAINF", "$."], "meter": "-+--+-+--+-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.25": {"line.1": {"text": "Es waren von K\u00f6ln am Rheine elftausend Jungfraun geschickt,", "tokens": ["Es", "wa\u00b7ren", "von", "K\u00f6ln", "am", "Rhei\u00b7ne", "elf\u00b7tau\u00b7send", "Jung\u00b7fraun", "ge\u00b7schickt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NE", "APPRART", "NE", "ADJD", "NN", "VVPP", "$,"], "meter": "-+--+-+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Die brachten ein seidenes F\u00e4hnlein, drin mit Gold und Perlen gestickt,", "tokens": ["Die", "brach\u00b7ten", "ein", "sei\u00b7de\u00b7nes", "F\u00e4hn\u00b7lein", ",", "drin", "mit", "Gold", "und", "Per\u00b7len", "ge\u00b7stickt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ART", "PPOSAT", "NN", "$,", "ADV", "APPR", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+--+--+-+-+-+--+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Gar lieblich anzuschauen, ein heiliger Hermann stand,", "tokens": ["Gar", "lieb\u00b7lich", "an\u00b7zu\u00b7schau\u00b7en", ",", "ein", "hei\u00b7li\u00b7ger", "Her\u00b7mann", "stand", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVIZU", "$,", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Weil mit der Heiligen H\u00fclfe Armin befreit das deutsche Land.", "tokens": ["Weil", "mit", "der", "Hei\u00b7li\u00b7gen", "H\u00fcl\u00b7fe", "Ar\u00b7min", "be\u00b7freit", "das", "deut\u00b7sche", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN", "NE", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+--+--+-+--+-+-+", "measure": "dactylic.di.plus"}}, "stanza.26": {"line.1": {"text": "Von D\u00fcsseldorf und M\u00fcnchen kam ein Wagen mit K\u00fcnstlern an,", "tokens": ["Von", "D\u00fcs\u00b7sel\u00b7dorf", "und", "M\u00fcn\u00b7chen", "kam", "ein", "Wa\u00b7gen", "mit", "K\u00fcnst\u00b7lern", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "KON", "NE", "VVFIN", "ART", "NN", "APPR", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.2": {"text": "Ihre Aufwartung zu machen dem gr\u00f6\u00dften deutschen Mann;", "tokens": ["Ih\u00b7re", "Auf\u00b7war\u00b7tung", "zu", "ma\u00b7chen", "dem", "gr\u00f6\u00df\u00b7ten", "deut\u00b7schen", "Mann", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "+--+--+--+-+-+", "measure": "dactylic.tri.plus"}, "line.3": {"text": "Sie wollten ihn zeichnen und malen, radieren und modelliern,", "tokens": ["Sie", "woll\u00b7ten", "ihn", "zeich\u00b7nen", "und", "ma\u00b7len", ",", "ra\u00b7die\u00b7ren", "und", "mo\u00b7del\u00b7li\u00b7ern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "KON", "VVINF", "$,", "VVFIN", "KON", "VVINF", "$,"], "meter": "-+--+--+--++-+-+-", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "In Stein und Marmor hauen, in Erz gie\u00dfen und lithographiern.", "tokens": ["In", "Stein", "und", "Mar\u00b7mor", "hau\u00b7en", ",", "in", "Erz", "gie\u00b7\u00dfen", "und", "li\u00b7tho\u00b7gra\u00b7phiern", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVINF", "$,", "APPR", "NN", "VVINF", "KON", "VVFIN", "$."], "meter": "-+-+-+-+++-+-+-+", "measure": "unknown.measure.octa.plus"}}, "stanza.27": {"line.1": {"text": "Es sa\u00df Armin im Sessel, wusste nicht wohin? woher?", "tokens": ["Es", "sa\u00df", "Ar\u00b7min", "im", "Ses\u00b7sel", ",", "wuss\u00b7te", "nicht", "wo\u00b7hin", "?", "wo\u00b7her", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NE", "APPRART", "NN", "$,", "VVFIN", "PTKNEG", "PWAV", "$.", "PWAV", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.2": {"text": "Von allem Sehen und H\u00f6ren war ihm das Herz so schwer.", "tokens": ["Von", "al\u00b7lem", "Se\u00b7hen", "und", "H\u00f6\u00b7ren", "war", "ihm", "das", "Herz", "so", "schwer", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "NN", "KON", "NN", "VAFIN", "PPER", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+--+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Was andre gerne m\u00f6chten, das f\u00fchlte recht der Held;", "tokens": ["Was", "and\u00b7re", "ger\u00b7ne", "m\u00f6ch\u00b7ten", ",", "das", "f\u00fchl\u00b7te", "recht", "der", "Held", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PIS", "ADV", "VMFIN", "$,", "PDS", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Den Drang nach Ruhme f\u00fchlet nur wer ber\u00fchmt ist in der Welt.", "tokens": ["Den", "Drang", "nach", "Ruh\u00b7me", "f\u00fch\u00b7let", "nur", "wer", "be\u00b7r\u00fchmt", "ist", "in", "der", "Welt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "VVFIN", "ADV", "PWS", "ADJD", "VAFIN", "APPR", "ART", "NN", "$."], "meter": "-+-+--+-+-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.28": {"line.1": {"text": "Armin in heiterem Ernste nahm den R\u00f6mer in die Hand:", "tokens": ["Ar\u00b7min", "in", "hei\u00b7te\u00b7rem", "Erns\u00b7te", "nahm", "den", "R\u00f6\u00b7mer", "in", "die", "Hand", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ADJA", "NN", "VVFIN", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "---+--+-+-+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "\u201ehoch lebe die deutsche Freiheit! hoch lebe das Vaterland!\u201c", "tokens": ["\u201e", "hoch", "le\u00b7be", "die", "deut\u00b7sche", "Frei\u00b7heit", "!", "hoch", "le\u00b7be", "das", "Va\u00b7ter\u00b7land", "!", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "VVFIN", "ART", "ADJA", "NN", "$.", "ADJD", "VVFIN", "ART", "NN", "$.", "$("], "meter": "+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}, "line.3": {"text": "Und alle, alle riefen: \u201esie lebe fr\u00fch und spat!\u201c", "tokens": ["Und", "al\u00b7le", ",", "al\u00b7le", "rie\u00b7fen", ":", "\u201e", "sie", "le\u00b7be", "fr\u00fch", "und", "spat", "!", "\u201c"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PIS", "$,", "PIS", "VVFIN", "$.", "$(", "PPER", "VVFIN", "ADJD", "KON", "VVFIN", "$.", "$("], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Zwar war im Saale zugegen gar mancher geheime Rath.", "tokens": ["Zwar", "war", "im", "Saa\u00b7le", "zu\u00b7ge\u00b7gen", "gar", "man\u00b7cher", "ge\u00b7hei\u00b7me", "Rath", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPRART", "NN", "ADJD", "ADV", "PIAT", "ADJA", "NN", "$."], "meter": "-+-+--+--+--+-+", "measure": "iambic.hexa.relaxed"}}, "stanza.29": {"line.1": {"text": "Armin in heiterem Ernste nahm den Becher wieder jetzund:", "tokens": ["Ar\u00b7min", "in", "hei\u00b7te\u00b7rem", "Erns\u00b7te", "nahm", "den", "Be\u00b7cher", "wie\u00b7der", "je\u00b7tzund", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ADJA", "NN", "VVFIN", "ART", "NN", "ADV", "ADV", "$."], "meter": "+--+--+-+-+-+---", "measure": "dactylic.di.plus"}, "line.2": {"text": "\u201ehoch alle Majest\u00e4ten und hoch der deutsche Bund!\u201c", "tokens": ["\u201e", "hoch", "al\u00b7le", "Ma\u00b7jes\u00b7t\u00e4\u00b7ten", "und", "hoch", "der", "deut\u00b7sche", "Bund", "!", "\u201c"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "PIAT", "NN", "KON", "ADJD", "ART", "ADJA", "NN", "$.", "$("], "meter": "---+-+--+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und alle, alle riefen: \u201erecht lang' in Einigkeit!\u201c", "tokens": ["Und", "al\u00b7le", ",", "al\u00b7le", "rie\u00b7fen", ":", "\u201e", "recht", "lang'", "in", "Ei\u00b7nig\u00b7keit", "!", "\u201c"], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PIS", "$,", "PIS", "VVFIN", "$.", "$(", "ADV", "ADV", "APPR", "NN", "$.", "$("], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Zwar waren im Saale zugegen Cherusker genug zur Zeit.", "tokens": ["Zwar", "wa\u00b7ren", "im", "Saa\u00b7le", "zu\u00b7ge\u00b7gen", "Che\u00b7rus\u00b7ker", "ge\u00b7nug", "zur", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPRART", "NN", "ADJD", "NN", "ADV", "APPRART", "NN", "$."], "meter": "-+--+--+-+-+-+-+", "measure": "amphibrach.tri.plus"}}, "stanza.30": {"line.1": {"text": "Kaum war es ausgesprochen, da kam vom Leinestrom", "tokens": ["Kaum", "war", "es", "aus\u00b7ge\u00b7spro\u00b7chen", ",", "da", "kam", "vom", "Lei\u00b7nes\u00b7trom"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "VVINF", "$,", "ADV", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Ein Zug von Professoren mit einem sch\u00f6nen Diplom.", "tokens": ["Ein", "Zug", "von", "Pro\u00b7fes\u00b7so\u00b7ren", "mit", "ei\u00b7nem", "sch\u00f6\u00b7nen", "Dip\u00b7lom", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Georgia Augusta hatte einstimmig sich resolviert", "tokens": ["Ge\u00b7or\u00b7gia", "Au\u00b7gus\u00b7ta", "hat\u00b7te", "ein\u00b7stim\u00b7mig", "sich", "re\u00b7sol\u00b7viert"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "NE", "VAFIN", "ADJD", "PRF", "VVFIN"], "meter": "-+-+--+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und Armin den hehren Helden zum ", "tokens": ["Und", "Ar\u00b7min", "den", "heh\u00b7ren", "Hel\u00b7den", "zum"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "NE", "ART", "ADJA", "NN", "APPRART"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.31": {"line.1": {"text": "Armin in heiterem Ernste nahm in die Hand das Diplom:", "tokens": ["Ar\u00b7min", "in", "hei\u00b7te\u00b7rem", "Erns\u00b7te", "nahm", "in", "die", "Hand", "das", "Dip\u00b7lom", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ADJA", "NN", "VVFIN", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "+--+--+-+--+-+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "\u201egut da\u00df ich es noch erfahre \u2014 was ich gethan an Rom", "tokens": ["\u201e", "gut", "da\u00df", "ich", "es", "noch", "er\u00b7fah\u00b7re", "was", "ich", "ge\u00b7than", "an", "Rom"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "ADJD", "KOUS", "PPER", "PPER", "ADV", "VVFIN", "$(", "PWS", "PPER", "VVPP", "APPR", "NE"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.3": {"text": "Ist also Recht gewesen ist Recht bis auf diesen Tag!", "tokens": ["Ist", "al\u00b7so", "Recht", "ge\u00b7we\u00b7sen", "ist", "Recht", "bis", "auf", "die\u00b7sen", "Tag", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "NN", "VAPP", "VAFIN", "NN", "APPR", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.4": {"text": "Gott gebe, da\u00df es den Sieben, wie's mir jetzt geht, ergehen mag!\u201c", "tokens": ["Gott", "ge\u00b7be", ",", "da\u00df", "es", "den", "Sie\u00b7ben", ",", "wie's", "mir", "jetzt", "geht", ",", "er\u00b7ge\u00b7hen", "mag", "!", "\u201c"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["NN", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN", "$,", "KOUS", "PPER", "ADV", "VVFIN", "$,", "VVINF", "VMFIN", "$.", "$("], "meter": "+---+-+--+-+-+-+", "measure": "trochaic.septa.relaxed"}}, "stanza.32": {"line.1": {"text": "Schon war es Nacht geworden, der W\u00e4chter blies ins Horn,", "tokens": ["Schon", "war", "es", "Nacht", "ge\u00b7wor\u00b7den", ",", "der", "W\u00e4ch\u00b7ter", "blies", "ins", "Horn", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "VAPP", "$,", "PRELS", "NE", "VVFIN", "APPRART", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Da kam ein Bote geritten mit einem goldenen Sporn", "tokens": ["Da", "kam", "ein", "Bo\u00b7te", "ge\u00b7rit\u00b7ten", "mit", "ei\u00b7nem", "gol\u00b7de\u00b7nen", "Sporn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "VVPP", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+--+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Und einem Pergamentbriefe, \u2014 er kam noch zu rechter Zeit, \u2014", "tokens": ["Und", "ei\u00b7nem", "Per\u00b7ga\u00b7ment\u00b7brie\u00b7fe", ",", "er", "kam", "noch", "zu", "rech\u00b7ter", "Zeit", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "ART", "NN", "$,", "$(", "PPER", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$,", "$("], "meter": "-+-+-++-+--+-+-", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Es war darin eine Bulla von Seiner Heiligkeit.", "tokens": ["Es", "war", "da\u00b7rin", "ei\u00b7ne", "Bul\u00b7la", "von", "Sei\u00b7ner", "Hei\u00b7lig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PAV", "ART", "NN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+--++-+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.33": {"line.1": {"text": "Armin begann zu lesen, er sch\u00fcttelte das Haupt;", "tokens": ["Ar\u00b7min", "be\u00b7gann", "zu", "le\u00b7sen", ",", "er", "sch\u00fct\u00b7tel\u00b7te", "das", "Haupt", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PTKZU", "VVINF", "$,", "PPER", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Da\u00df er sein Latein verlernet, das h\u00e4tt' er nicht geglaubt.", "tokens": ["Da\u00df", "er", "sein", "La\u00b7tein", "ver\u00b7ler\u00b7net", ",", "das", "h\u00e4tt'", "er", "nicht", "ge\u00b7glaubt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVFIN", "$,", "PDS", "VAFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+--+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Er lie\u00df von einem Professor sich die Bulla klassisch vertiern", "tokens": ["Er", "lie\u00df", "von", "ei\u00b7nem", "Pro\u00b7fes\u00b7sor", "sich", "die", "Bul\u00b7la", "klas\u00b7sisch", "ver\u00b7ti\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "PRF", "ART", "NE", "ADJD", "VVINF"], "meter": "-+-+-+-+--+-+--+-", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "Und dann zu besserm Verst\u00e4ndni\u00df im Tacitusstile expliciern.", "tokens": ["Und", "dann", "zu", "bes\u00b7serm", "Ver\u00b7st\u00e4nd\u00b7ni\u00df", "im", "Ta\u00b7ci\u00b7tus\u00b7sti\u00b7le", "ex\u00b7pli\u00b7ci\u00b7ern", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+--+-+-+-+-+-+-", "measure": "iambic.octa.plus.relaxed"}}, "stanza.34": {"line.1": {"text": "Seine Heiligkeit begehret, da\u00df sich der Held Armin", "tokens": ["Sei\u00b7ne", "Hei\u00b7lig\u00b7keit", "be\u00b7ge\u00b7hret", ",", "da\u00df", "sich", "der", "Held", "Ar\u00b7min"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "$,", "KOUS", "PRF", "ART", "NN", "NE"], "meter": "+-+-+--+-+-+-+", "measure": "trochaic.septa.relaxed"}, "line.2": {"text": "Bei seinem gro\u00dfen Einflu\u00df jetzt wolle gern unterziehn,", "tokens": ["Bei", "sei\u00b7nem", "gro\u00b7\u00dfen", "Ein\u00b7flu\u00df", "jetzt", "wol\u00b7le", "gern", "un\u00b7ter\u00b7ziehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "ADV", "VMFIN", "ADV", "VVINF", "$,"], "meter": "-+-+-++-+--+-+", "measure": "iambic.septa.relaxed"}, "line.3": {"text": "Ein Friedenswerk zu stiften von wegen gemischter Eh'n,", "tokens": ["Ein", "Frie\u00b7dens\u00b7werk", "zu", "stif\u00b7ten", "von", "we\u00b7gen", "ge\u00b7mischter", "Eh'n", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PTKZU", "VVFIN", "APPR", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "In Germania k\u00f6nn' und d\u00fcrf' es so uncanonisch nicht mehr gehn.", "tokens": ["In", "Ger\u00b7ma\u00b7nia", "k\u00f6nn'", "und", "d\u00fcr\u00b7f'", "es", "so", "un\u00b7ca\u00b7no\u00b7nisch", "nicht", "mehr", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "VMFIN", "KON", "VMFIN", "PPER", "ADV", "ADJD", "PTKNEG", "ADV", "VVINF", "$."], "meter": "+-+-+-+-+-+-+-+-+", "measure": "trochaic.octa.plus"}}, "stanza.35": {"line.1": {"text": "Um dazu anzuspornen, erfolg' hier ein Symbol;", "tokens": ["Um", "da\u00b7zu", "an\u00b7zu\u00b7spor\u00b7nen", ",", "er\u00b7fol\u00b7g'", "hier", "ein", "Sym\u00b7bol", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PAV", "VVIZU", "$,", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+--+--+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Wer's Wohl der Kirche wolle, erlang' auch so ", "tokens": ["Wer's", "Wohl", "der", "Kir\u00b7che", "wol\u00b7le", ",", "er\u00b7lang'", "auch", "so"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ART", "NN", "VMFIN", "$,", "VVFIN", "ADV", "ADV"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und wen die Kirche begnade, sei begnadet f\u00fcr alle Zeit:", "tokens": ["Und", "wen", "die", "Kir\u00b7che", "be\u00b7gna\u00b7de", ",", "sei", "be\u00b7gna\u00b7det", "f\u00fcr", "al\u00b7le", "Zeit", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ART", "NN", "VVFIN", "$,", "VAFIN", "VVPP", "APPR", "PIAT", "NN", "$."], "meter": "-+-+--+-+-+--+-+", "measure": "iambic.septa.relaxed"}, "line.4": {"text": "So, meinte der Philologe, so schriebe Seine Heiligkeit.", "tokens": ["So", ",", "mein\u00b7te", "der", "Phi\u00b7lo\u00b7lo\u00b7ge", ",", "so", "schrie\u00b7be", "Sei\u00b7ne", "Hei\u00b7lig\u00b7keit", "."], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "VVFIN", "ART", "NN", "$,", "ADV", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-+-+", "measure": "iambic.octa.plus"}}, "stanza.36": {"line.1": {"text": "Ihm war so angst geworden, dem edlen Helden Armin,", "tokens": ["Ihm", "war", "so", "angst", "ge\u00b7wor\u00b7den", ",", "dem", "ed\u00b7len", "Hel\u00b7den", "Ar\u00b7min", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "VVPP", "VAPP", "$,", "ART", "ADJA", "NN", "NE", "$,"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Trotz aller Freud' und Wonne wollt' er nach Walhalla ziehn.", "tokens": ["Trotz", "al\u00b7ler", "Freud'", "und", "Won\u00b7ne", "wollt'", "er", "nach", "Wal\u00b7hal\u00b7la", "ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "KON", "NN", "VMFIN", "PPER", "APPR", "NE", "VVINF", "$."], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Da hielt den gro\u00dfen Deutschen zu unserm hohen Gl\u00fcck", "tokens": ["Da", "hielt", "den", "gro\u00b7\u00dfen", "Deut\u00b7schen", "zu", "un\u00b7serm", "ho\u00b7hen", "Gl\u00fcck"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "NN", "APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Auf einige Minuten ein frohes Ereigni\u00df noch zur\u00fcck.", "tokens": ["Auf", "ei\u00b7ni\u00b7ge", "Mi\u00b7nu\u00b7ten", "ein", "fro\u00b7hes", "Er\u00b7eig\u00b7ni\u00df", "noch", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "ART", "ADJA", "NN", "ADV", "PTKVZ", "$."], "meter": "-+-+-+--+--+-+-+", "measure": "iambic.septa.relaxed"}}, "stanza.37": {"line.1": {"text": "Es kam ein F\u00fcrst geritten, der erhob mit eigener Hand", "tokens": ["Es", "kam", "ein", "F\u00fcrst", "ge\u00b7rit\u00b7ten", ",", "der", "er\u00b7hob", "mit", "ei\u00b7ge\u00b7ner", "Hand"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "NN", "VVPP", "$,", "PRELS", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+--+", "measure": "iambic.septa.chol"}, "line.2": {"text": "Und sportelfrei den Helden in den deutschen Adelstand.", "tokens": ["Und", "spor\u00b7tel\u00b7frei", "den", "Hel\u00b7den", "in", "den", "deut\u00b7schen", "A\u00b7del\u00b7stand", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "ART", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.3": {"text": "Das war zu viel \u2014 da starb er. Nun hei\u00dft es doch fortan:", "tokens": ["Das", "war", "zu", "viel", "da", "starb", "er", ".", "Nun", "hei\u00dft", "es", "doch", "for\u00b7tan", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "APPR", "PIAT", "$(", "ADV", "VVFIN", "PPER", "$.", "ADV", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+--+-++--+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Das Vaterland hat gerettet ein alter deutscher Edelmann.", "tokens": ["Das", "Va\u00b7ter\u00b7land", "hat", "ge\u00b7ret\u00b7tet", "ein", "al\u00b7ter", "deut\u00b7scher", "E\u00b7del\u00b7mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "ART", "ADJA", "ADJA", "NN", "$."], "meter": "-+-+--+--+-+-+-+", "measure": "iambic.septa.relaxed"}}}}}