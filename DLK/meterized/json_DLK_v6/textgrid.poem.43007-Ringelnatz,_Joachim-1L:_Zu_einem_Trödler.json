{"textgrid.poem.43007": {"metadata": {"author": {"name": "Ringelnatz, Joachim", "birth": "N.A.", "death": "N.A."}, "title": "1L: Zu einem Tr\u00f6dler", "genre": "verse", "period": "N.A.", "pub_year": 1908, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Zu einem Tr\u00f6dler", "tokens": ["Zu", "ei\u00b7nem", "Tr\u00f6d\u00b7ler"], "token_info": ["word", "word", "word"], "pos": ["APPR", "ART", "NN"], "meter": "-+-+-", "measure": "iambic.di"}, "line.2": {"text": "Kam ein Greis mit einer sauern", "tokens": ["Kam", "ein", "Greis", "mit", "ei\u00b7ner", "sau\u00b7ern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "ART", "NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Gurke,", "tokens": ["Gur\u00b7ke", ","], "token_info": ["word", "punct"], "pos": ["NE", "$,"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Sprach: \u00bbIch bin ein Gnadenbr\u00f6tler", "tokens": ["Sprach", ":", "\u00bb", "Ich", "bin", "ein", "Gna\u00b7den\u00b7br\u00f6t\u00b7ler"], "token_info": ["word", "punct", "punct", "word", "word", "word", "word"], "pos": ["NN", "$.", "$(", "PPER", "VAFIN", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Bei einem Bauern.", "tokens": ["Bei", "ei\u00b7nem", "Bau\u00b7ern", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Der ist ein Schurke.", "tokens": ["Der", "ist", "ein", "Schur\u00b7ke", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-", "measure": "iambic.di"}}, "stanza.2": {"line.1": {"text": "Diese Gurke bringe ich aus Not.", "tokens": ["Die\u00b7se", "Gur\u00b7ke", "brin\u00b7ge", "ich", "aus", "Not", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDAT", "NN", "VVFIN", "PPER", "APPR", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Kleine Kn\u00f6pfe m\u00f6chte ich daf\u00fcr.", "tokens": ["Klei\u00b7ne", "Kn\u00f6p\u00b7fe", "m\u00f6ch\u00b7te", "ich", "da\u00b7f\u00fcr", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VMFIN", "PPER", "PAV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Denn man kann sich nicht mit Gnadenbrot", "tokens": ["Denn", "man", "kann", "sich", "nicht", "mit", "Gna\u00b7den\u00b7brot"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PIS", "VMFIN", "PRF", "PTKNEG", "APPR", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Kn\u00f6pfe kaufen f\u00fcr die Hosent\u00fcr.\u00ab", "tokens": ["Kn\u00f6p\u00b7fe", "kau\u00b7fen", "f\u00fcr", "die", "Ho\u00b7sen\u00b7t\u00fcr", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "VVINF", "APPR", "ART", "NN", "$.", "$("], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Und der Tr\u00f6dlersmann verschm\u00e4hte", "tokens": ["Und", "der", "Tr\u00f6d\u00b7lers\u00b7mann", "ver\u00b7schm\u00e4h\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nicht die Gurke noch des Greises Wort,", "tokens": ["Nicht", "die", "Gur\u00b7ke", "noch", "des", "Grei\u00b7ses", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ART", "NN", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Denn der kam ihm sehr bed\u00fcrftig vor,", "tokens": ["Denn", "der", "kam", "ihm", "sehr", "be\u00b7d\u00fcrf\u00b7tig", "vor", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "VVFIN", "PPER", "ADV", "ADJD", "PTKVZ", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Sondern b\u00fcckte sich und n\u00e4hte", "tokens": ["Son\u00b7dern", "b\u00fcck\u00b7te", "sich", "und", "n\u00e4h\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PRF", "KON", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Hundert goldne Kn\u00f6pfe ihm sofort", "tokens": ["Hun\u00b7dert", "gold\u00b7ne", "Kn\u00f6p\u00b7fe", "ihm", "so\u00b7fort"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["CARD", "ADJA", "NN", "PPER", "ADV"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.6": {"text": "Eigenh\u00e4ndig an das Hosentor.", "tokens": ["Ei\u00b7gen\u00b7h\u00e4n\u00b7dig", "an", "das", "Ho\u00b7sen\u00b7tor", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Und der Greis sprach: \u00bbDanke\u00ab und verneigte", "tokens": ["Und", "der", "Greis", "sprach", ":", "\u00bb", "Dan\u00b7ke", "\u00ab", "und", "ver\u00b7neig\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word"], "pos": ["KON", "ART", "NN", "VVFIN", "$.", "$(", "PTKANT", "$(", "KON", "VVFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Sich und ging mit offnem Hosenlatz", "tokens": ["Sich", "und", "ging", "mit", "off\u00b7nem", "Ho\u00b7sen\u00b7latz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PRF", "KON", "VVFIN", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Selig durch die Stra\u00dfen, und er zeigte", "tokens": ["Se\u00b7lig", "durch", "die", "Stra\u00b7\u00dfen", ",", "und", "er", "zeig\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "APPR", "ART", "NN", "$,", "KON", "PPER", "VVFIN"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.4": {"text": "Allen Menschen seinen goldnen Schatz.", "tokens": ["Al\u00b7len", "Men\u00b7schen", "sei\u00b7nen", "gold\u00b7nen", "Schatz", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "PPOSAT", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.5": {"line.1": {"text": "Bis ihn schlie\u00dflich ein gewisses", "tokens": ["Bis", "ihn", "schlie\u00df\u00b7lich", "ein", "ge\u00b7wis\u00b7ses"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPER", "ADV", "ART", "ADJA"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Schicksal in ein Irrenhaus berief,", "tokens": ["Schick\u00b7sal", "in", "ein", "Ir\u00b7ren\u00b7haus", "be\u00b7rief", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.3": {"text": "Ob Erregung \u00f6ffentlichen \u00c4rgernisses.", "tokens": ["Ob", "Er\u00b7re\u00b7gung", "\u00f6f\u00b7fent\u00b7li\u00b7chen", "\u00c4r\u00b7ger\u00b7nis\u00b7ses", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADJA", "NN", "$."], "meter": "-+---+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Bis er Kn\u00f6pfe schluckte und entschlief.", "tokens": ["Bis", "er", "Kn\u00f6p\u00b7fe", "schluck\u00b7te", "und", "ent\u00b7schlief", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VVFIN", "KON", "VVFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}}}}