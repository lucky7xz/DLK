{"textgrid.poem.53328": {"metadata": {"author": {"name": "Dach, Simon", "birth": "N.A.", "death": "N.A."}, "title": "Bittere Klage vber des weiland GrosAchtbaren, Hochgelarten vnd Weitber\u00fchmten H. Robert Roberthins, Churfl. Brandenb. Preussischen Ober- vnd Regiments-Secretary Meines, nechst Gott, hertzliebsten vnd getrewesten Freundes vnd hohen Gutth\u00e4ters Vnverhofftem vnd recht hochbetr\u00fcbtem aber seligem Hintritt aus dieser Welt, aus wehm\u00fctigem Hertzen vnd schuldigster Trew gef\u00fchret von mir", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Andern hab ich bis anher", "tokens": ["An\u00b7dern", "hab", "ich", "bis", "an\u00b7her"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "VAFIN", "PPER", "APPR", "ADJA"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Die sich fanden in Beschwer,", "tokens": ["Die", "sich", "fan\u00b7den", "in", "Be\u00b7schwer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "K\u00f6nnen Trost ertheilen,", "tokens": ["K\u00f6n\u00b7nen", "Trost", "er\u00b7thei\u00b7len", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Wer wird mir in dieser Noht,", "tokens": ["Wer", "wird", "mir", "in", "die\u00b7ser", "Noht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "PDAT", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Da mein liebster Freund mir todt,", "tokens": ["Da", "mein", "liebs\u00b7ter", "Freund", "mir", "todt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "PPER", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Meine Wunden heilen?", "tokens": ["Mei\u00b7ne", "Wun\u00b7den", "hei\u00b7len", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.2": {"line.1": {"text": "O der Mann nach meinem Sinn", "tokens": ["O", "der", "Mann", "nach", "mei\u00b7nem", "Sinn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "ART", "NN", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Roberthin mein Trost ist hin,", "tokens": ["Ro\u00b7bert\u00b7hin", "mein", "Trost", "ist", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "NN", "VAFIN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Der, in dessen Leben", "tokens": ["Der", ",", "in", "des\u00b7sen", "Le\u00b7ben"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["ART", "$,", "APPR", "PRELAT", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Meines sich befand, mein Raht,", "tokens": ["Mei\u00b7nes", "sich", "be\u00b7fand", ",", "mein", "Raht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "PRF", "VVFIN", "$,", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Meine Rhue vnd Zuflucht hat", "tokens": ["Mei\u00b7ne", "Rhue", "vnd", "Zu\u00b7flucht", "hat"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "KON", "NN", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Gutte Nacht gegeben.", "tokens": ["Gut\u00b7te", "Nacht", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.3": {"line.1": {"text": "Niemand fodder' itzt von mir", "tokens": ["Nie\u00b7mand", "fod\u00b7der'", "itzt", "von", "mir"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADV", "APPR", "PPER"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Etwas von belebter Zier,", "tokens": ["Et\u00b7was", "von", "be\u00b7leb\u00b7ter", "Zier", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ach ich kan nicht geigen,", "tokens": ["Ach", "ich", "kan", "nicht", "gei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "PPER", "VMFIN", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Der mein Phoebus vormals war", "tokens": ["Der", "mein", "Phoe\u00b7bus", "vor\u00b7mals", "war"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PPOSAT", "NN", "ADV", "VAFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Liegt im Sarg und auff der Bahr,", "tokens": ["Liegt", "im", "Sarg", "und", "auff", "der", "Bahr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPRART", "NN", "KON", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Vnd heisst nun mich schweigen.", "tokens": ["Vnd", "heisst", "nun", "mich", "schwei\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "PPER", "VVFIN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.4": {"line.1": {"text": "Allen Seiten bin ich feind", "tokens": ["Al\u00b7len", "Sei\u00b7ten", "bin", "ich", "feind"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "VAFIN", "PPER", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ohn die etwa mit mir weint,", "tokens": ["Ohn", "die", "et\u00b7wa", "mit", "mir", "weint", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADV", "APPR", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Was ich erst zu k\u00fcssen", "tokens": ["Was", "ich", "erst", "zu", "k\u00fcs\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "ADV", "PTKZU", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Vnd sehr hoch zu halten pflag", "tokens": ["Vnd", "sehr", "hoch", "zu", "hal\u00b7ten", "pflag"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "ADJD", "PTKZU", "VVINF", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Was mir wie im Hertzen lag,", "tokens": ["Was", "mir", "wie", "im", "Hert\u00b7zen", "lag", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "KOKOM", "APPRART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Stohss' ich jetzt mit F\u00fcssen.", "tokens": ["Stohss'", "ich", "jetzt", "mit", "F\u00fcs\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "APPR", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.5": {"line.1": {"text": "Selbs mein gr\u00fcner Helicon", "tokens": ["Selbs", "mein", "gr\u00fc\u00b7ner", "He\u00b7li\u00b7con"], "token_info": ["word", "word", "word", "word"], "pos": ["NE", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ist mir jetzund Gram und Hohn,", "tokens": ["Ist", "mir", "je\u00b7tzund", "Gram", "und", "Hohn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wild von Dorn- vnd Hecken,", "tokens": ["Wild", "von", "Dorn", "vnd", "He\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "TRUNC", "KON", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Wird von Grauen stets bewahrt,", "tokens": ["Wird", "von", "Grau\u00b7en", "stets", "be\u00b7wahrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "NN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ist ein Platz da aller art", "tokens": ["Ist", "ein", "Platz", "da", "al\u00b7ler", "art"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "ADV", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Schlangen sich verstecken;", "tokens": ["Schlan\u00b7gen", "sich", "ver\u00b7ste\u00b7cken", ";"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PRF", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.6": {"line.1": {"text": "Ist ein Ort den ich verflucht,", "tokens": ["Ist", "ein", "Ort", "den", "ich", "ver\u00b7flucht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ART", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wer darinnen Quellen sucht,", "tokens": ["Wer", "da\u00b7rin\u00b7nen", "Quel\u00b7len", "sucht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "NN", "VVFIN", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.3": {"text": "Suchet Milch zu saugen", "tokens": ["Su\u00b7chet", "Milch", "zu", "sau\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "NN", "PTKZU", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Aus dem Felsen und dem Stal,", "tokens": ["Aus", "dem", "Fel\u00b7sen", "und", "dem", "Stal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "KON", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ohn das Wasser, so f\u00fcr Qual", "tokens": ["Ohn", "das", "Was\u00b7ser", ",", "so", "f\u00fcr", "Qual"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "$,", "ADV", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Rinnt aus meinen Augen.", "tokens": ["Rinnt", "aus", "mei\u00b7nen", "Au\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.7": {"line.1": {"text": "Ist wer unter vns betr\u00fcbt", "tokens": ["Ist", "wer", "un\u00b7ter", "vns", "be\u00b7tr\u00fcbt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PWS", "APPR", "PPER", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Vber dem, so er geliebt,", "tokens": ["Vber", "dem", ",", "so", "er", "ge\u00b7liebt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "$,", "ADV", "PPER", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Kommt ich helff euch weinen,", "tokens": ["Kommt", "ich", "helff", "euch", "wei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "VVFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Witwen, Waisen, und was mehr", "tokens": ["Wit\u00b7wen", ",", "Wai\u00b7sen", ",", "und", "was", "mehr"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word"], "pos": ["NN", "$,", "NN", "$,", "KON", "PWS", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Klaget aus der massen sehr", "tokens": ["Kla\u00b7get", "aus", "der", "mas\u00b7sen", "sehr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Den Verlust der Seinen.", "tokens": ["Den", "Ver\u00b7lust", "der", "Sei\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "PPOSS", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.8": {"line.1": {"text": "Sonderlich wo in der Welt", "tokens": ["Son\u00b7der\u00b7lich", "wo", "in", "der", "Welt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "PWAV", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Sich ein Theseus noch enth\u00e4lt", "tokens": ["Sich", "ein", "The\u00b7seus", "noch", "ent\u00b7h\u00e4lt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PRF", "ART", "NE", "ADV", "VVFIN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Der sich zwar verschworen", "tokens": ["Der", "sich", "zwar", "ver\u00b7schwo\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "PRF", "ADV", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Dein zu seyn, Pirithous,", "tokens": ["Dein", "zu", "seyn", ",", "Pi\u00b7rit\u00b7hous", ","], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["PPOSAT", "PTKZU", "VAINF", "$,", "NE", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Aber dein entrahten mus,", "tokens": ["A\u00b7ber", "dein", "ent\u00b7rah\u00b7ten", "mus", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Weil er dich verlohren.", "tokens": ["Weil", "er", "dich", "ver\u00b7loh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.9": {"line.1": {"text": "Komm du Pilades Geschlecht,", "tokens": ["Komm", "du", "Pi\u00b7la\u00b7des", "Ge\u00b7schlecht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "NN", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Sag sind meine Thr\u00e4nen recht", "tokens": ["Sag", "sind", "mei\u00b7ne", "Thr\u00e4\u00b7nen", "recht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "PPOSAT", "NN", "ADJD"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Trew und auserlesen?", "tokens": ["Trew", "und", "au\u00b7ser\u00b7le\u00b7sen", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Giebt der Nachtwelt dan Bescheid,", "tokens": ["Giebt", "der", "Nacht\u00b7welt", "dan", "Be\u00b7scheid", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "ADV", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Da\u00df ein Paar auch dieser Zeit", "tokens": ["Da\u00df", "ein", "Paar", "auch", "die\u00b7ser", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "ADV", "PDAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Sey, was du, gewesen.", "tokens": ["Sey", ",", "was", "du", ",", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["VAFIN", "$,", "PWS", "PPER", "$,", "VAPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.10": {"line.1": {"text": "O was heb ich immer an!", "tokens": ["O", "was", "heb", "ich", "im\u00b7mer", "an", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "PWS", "VVFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Zwar ich sol dem thewren Mann", "tokens": ["Zwar", "ich", "sol", "dem", "thew\u00b7ren", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPER", "VMFIN", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Jetzt ein Denckmal stellen,", "tokens": ["Jetzt", "ein", "Denck\u00b7mal", "stel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Welches wan es vmb mich wer", "tokens": ["Wel\u00b7ches", "wan", "es", "vmb", "mich", "wer"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "PWAV", "PPER", "APPR", "PPER", "PWS"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Hat verdient, so ist es Er,", "tokens": ["Hat", "ver\u00b7dient", ",", "so", "ist", "es", "Er", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "VVPP", "$,", "ADV", "VAFIN", "PPER", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Vnd in tausent F\u00e4llen.", "tokens": ["Vnd", "in", "tau\u00b7sent", "F\u00e4l\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "CARD", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.11": {"line.1": {"text": "Aber meine Krafft ist fort,", "tokens": ["A\u00b7ber", "mei\u00b7ne", "Krafft", "ist", "fort", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Ich vermag schier nicht ein Wort,", "tokens": ["Ich", "ver\u00b7mag", "schier", "nicht", "ein", "Wort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKNEG", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "So was taug, zu fassen,", "tokens": ["So", "was", "taug", ",", "zu", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "PWS", "VVFIN", "$,", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Bin als dem Verstand gebricht,", "tokens": ["Bin", "als", "dem", "Ver\u00b7stand", "ge\u00b7bricht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KOKOM", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Was zu thun sey weis ich nicht,", "tokens": ["Was", "zu", "thun", "sey", "weis", "ich", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PTKZU", "VVINF", "VAFIN", "PTKVZ", "PPER", "PTKNEG", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Noch was sey zu lassen.", "tokens": ["Noch", "was", "sey", "zu", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "VAFIN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.12": {"line.1": {"text": "Sein Verdienst hergegen steht", "tokens": ["Sein", "Ver\u00b7dienst", "her\u00b7ge\u00b7gen", "steht"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Vber MenschenWitz erh\u00f6ht,", "tokens": ["Vber", "Men\u00b7schen", "Witz", "er\u00b7h\u00f6ht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Tullius mag sprechen", "tokens": ["Tul\u00b7li\u00b7us", "mag", "spre\u00b7chen"], "token_info": ["word", "word", "word"], "pos": ["NE", "VMFIN", "VVINF"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Was selbst Rom best\u00fcrtzet macht,", "tokens": ["Was", "selbst", "Rom", "be\u00b7st\u00fcrt\u00b7zet", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "NE", "VVFIN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Hie wird es an Redens Pracht", "tokens": ["Hie", "wird", "es", "an", "Re\u00b7dens", "Pracht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "NN", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Warlich ihm gebrechen.", "tokens": ["War\u00b7lich", "ihm", "ge\u00b7bre\u00b7chen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.13": {"line.1": {"text": "Maro, Claudian, Papihn", "tokens": ["Ma\u00b7ro", ",", "Clau\u00b7di\u00b7an", ",", "Pa\u00b7pihn"], "token_info": ["word", "punct", "word", "punct", "word"], "pos": ["NE", "$,", "NE", "$,", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Werden hie den k\u00fcrtzern ziehn.", "tokens": ["Wer\u00b7den", "hie", "den", "k\u00fcrt\u00b7zern", "ziehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "W\u00fcrden sie nicht sagen,", "tokens": ["W\u00fcr\u00b7den", "sie", "nicht", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Lebten sie nur, ihre Zeit", "tokens": ["Leb\u00b7ten", "sie", "nur", ",", "ih\u00b7re", "Zeit"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "$,", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "H\u00e4tt in solcher Fertigheit", "tokens": ["H\u00e4tt", "in", "sol\u00b7cher", "Fer\u00b7tig\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Keinen Mann getragen?", "tokens": ["Kei\u00b7nen", "Mann", "ge\u00b7tra\u00b7gen", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.14": {"line.1": {"text": "Was f\u00fcr Leut ich je gekant,", "tokens": ["Was", "f\u00fcr", "Leut", "ich", "je", "ge\u00b7kant", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "NN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Welche Weisheit vnd Verstandt", "tokens": ["Wel\u00b7che", "Weis\u00b7heit", "vnd", "Ver\u00b7standt"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAT", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Billig mus erheben,", "tokens": ["Bil\u00b7lig", "mus", "er\u00b7he\u00b7ben", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "War dem die\u00df, dem das allein,", "tokens": ["War", "dem", "die\u00df", ",", "dem", "das", "al\u00b7lein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "PDS", "$,", "PRELS", "ART", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Keinem aber in gemein", "tokens": ["Kei\u00b7nem", "a\u00b7ber", "in", "ge\u00b7mein"], "token_info": ["word", "word", "word", "word"], "pos": ["PIS", "ADV", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Alles fast gegeben.", "tokens": ["Al\u00b7les", "fast", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "ADV", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.15": {"line.1": {"text": "Der ist reich von Wissenschafft,", "tokens": ["Der", "ist", "reich", "von", "Wis\u00b7sen\u00b7schafft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADJD", "APPR", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Doch im Leben tadelhafft,", "tokens": ["Doch", "im", "Le\u00b7ben", "ta\u00b7del\u00b7hafft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Der wird feig befunden,", "tokens": ["Der", "wird", "feig", "be\u00b7fun\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADJD", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Der hat keiner Sprachen Gunst,", "tokens": ["Der", "hat", "kei\u00b7ner", "Spra\u00b7chen", "Gunst", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PIAT", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Hier nur hatte sich mit Kunst", "tokens": ["Hier", "nur", "hat\u00b7te", "sich", "mit", "Kunst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VAFIN", "PRF", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Alles schier verbunden.", "tokens": ["Al\u00b7les", "schier", "ver\u00b7bun\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.16": {"line.1": {"text": "War sein Vrtheil oder Witz", "tokens": ["War", "sein", "Vrtheil", "o\u00b7der", "Witz"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPOSAT", "NN", "KON", "NN"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.2": {"text": "Nicht viel schneller als der Blitz,", "tokens": ["Nicht", "viel", "schnel\u00b7ler", "als", "der", "Blitz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADV", "ADJD", "KOKOM", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Den das Wetter schicket,", "tokens": ["Den", "das", "Wet\u00b7ter", "schi\u00b7cket", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Als der alles stracks begrieff,", "tokens": ["Als", "der", "al\u00b7les", "stracks", "be\u00b7grieff", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "PIS", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "War es noch so schwer vnd tieff", "tokens": ["War", "es", "noch", "so", "schwer", "vnd", "tieff"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADJD", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Was er nur erblicket.", "tokens": ["Was", "er", "nur", "er\u00b7bli\u00b7cket", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.17": {"line.1": {"text": "Worauff mancher sich bedenckt", "tokens": ["Wo\u00b7rauff", "man\u00b7cher", "sich", "be\u00b7denckt"], "token_info": ["word", "word", "word", "word"], "pos": ["PAV", "PIAT", "PRF", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Vnd in tausent wegen kr\u00e4nckt,", "tokens": ["Vnd", "in", "tau\u00b7sent", "we\u00b7gen", "kr\u00e4nckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "APPR", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Kuntt er stracks ergr\u00fcnden,", "tokens": ["Kuntt", "er", "stracks", "er\u00b7gr\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Vnd in Sachen, wie sie seyn,", "tokens": ["Vnd", "in", "Sa\u00b7chen", ",", "wie", "sie", "seyn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "$,", "PWAV", "PPER", "VAINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Stracks ohn Arbeit, Sorg vnd Pein", "tokens": ["Stracks", "ohn", "Ar\u00b7beit", ",", "Sorg", "vnd", "Pein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "APPR", "NN", "$,", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Einen Ausschlag finden.", "tokens": ["Ei\u00b7nen", "Aus\u00b7schlag", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.18": {"line.1": {"text": "Welches Buch war jhm nicht kunt", "tokens": ["Wel\u00b7ches", "Buch", "war", "jhm", "nicht", "kunt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAT", "NN", "VAFIN", "PPER", "PTKNEG", "PTKVZ"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Durch vnd durch bis auff den Grundt?", "tokens": ["Durch", "vnd", "durch", "bis", "auff", "den", "Grundt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KON", "APPR", "KON", "APPR", "ART", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Was ich m\u00f6chte lesen,", "tokens": ["Was", "ich", "m\u00f6ch\u00b7te", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Was ich nachschlug mit Begier,", "tokens": ["Was", "ich", "nach\u00b7schlug", "mit", "Be\u00b7gier", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Merckt' ich da\u00df er l\u00e4ngst vor mir", "tokens": ["Mer\u00b7ckt'", "ich", "da\u00df", "er", "l\u00e4ngst", "vor", "mir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "KOUS", "PPER", "ADV", "APPR", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "War daselbst gewesen", "tokens": ["War", "da\u00b7selbst", "ge\u00b7we\u00b7sen"], "token_info": ["word", "word", "word"], "pos": ["VAFIN", "PAV", "VAPP"], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.19": {"line.1": {"text": "Vnd in allen K\u00fcnsten zwar,", "tokens": ["Vnd", "in", "al\u00b7len", "K\u00fcns\u00b7ten", "zwar", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PIAT", "NN", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Darumb wir zu jhm auch gar", "tokens": ["Da\u00b7rumb", "wir", "zu", "jhm", "auch", "gar"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PAV", "PPER", "APPR", "PPER", "ADV", "ADV"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Als zur Schulen kamen,", "tokens": ["Als", "zur", "Schu\u00b7len", "ka\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPRART", "NN", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Vnd in der vnd jener Sach,", "tokens": ["Vnd", "in", "der", "vnd", "je\u00b7ner", "Sach", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "ART", "KON", "PDAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Als uns Wissenschafft gebrach,", "tokens": ["Als", "uns", "Wis\u00b7sen\u00b7schafft", "ge\u00b7brach", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Lehre von ihm nahmen.", "tokens": ["Leh\u00b7re", "von", "ihm", "nah\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPER", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.20": {"line.1": {"text": "Darumb f\u00fchrt umb Ihn Geschrey", "tokens": ["Da\u00b7rumb", "f\u00fchrt", "umb", "Ihn", "Ge\u00b7schrey"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PAV", "VVFIN", "APPR", "PPER", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Musica, Po\u00ebterey,", "tokens": ["Mu\u00b7si\u00b7ca", ",", "Po\u00ebte\u00b7rey", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.3": {"text": "Redkunst vnd dergleichen,", "tokens": ["Red\u00b7kunst", "vnd", "derg\u00b7lei\u00b7chen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "KON", "PIS", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Ja es tr\u00e4gt ohn Vnterscheid", "tokens": ["Ja", "es", "tr\u00e4gt", "ohn", "Vn\u00b7ter\u00b7scheid"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PTKANT", "PPER", "VVFIN", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Auch das Handwerk vmb Ihn Leid", "tokens": ["Auch", "das", "Hand\u00b7werk", "vmb", "Ihn", "Leid"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "APPR", "PPER", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Da\u00df er mus verbleichen.", "tokens": ["Da\u00df", "er", "mus", "ver\u00b7blei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VMFIN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.21": {"line.1": {"text": "Bleibt der Herr- und F\u00fcrsten-Standt", "tokens": ["Bleibt", "der", "Herr", "und", "F\u00fcrs\u00b7ten\u00b7Standt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "TRUNC", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Hie auch billig unbenant,", "tokens": ["Hie", "auch", "bil\u00b7lig", "un\u00b7be\u00b7nant", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "ADJD", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Welchem er f\u00fcr allen", "tokens": ["Wel\u00b7chem", "er", "f\u00fcr", "al\u00b7len"], "token_info": ["word", "word", "word", "word"], "pos": ["PWS", "PPER", "APPR", "PIAT"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Wegen seiner Gaben Schar,", "tokens": ["We\u00b7gen", "sei\u00b7ner", "Ga\u00b7ben", "Schar", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Die nicht aus zu sprechen war,", "tokens": ["Die", "nicht", "aus", "zu", "spre\u00b7chen", "war", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "PTKVZ", "PTKZU", "VVINF", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Allzeit wollgefallen?", "tokens": ["All\u00b7zeit", "woll\u00b7ge\u00b7fal\u00b7len", "?"], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VMFIN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.22": {"line.1": {"text": "Das Hoch Edle Hof-Gericht", "tokens": ["Das", "Hoch", "Ed\u00b7le", "Hof\u00b7Ge\u00b7richt"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Schweiget, weis ich, seiner nicht,", "tokens": ["Schwei\u00b7get", ",", "weis", "ich", ",", "sei\u00b7ner", "nicht", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["VVFIN", "$,", "KOUS", "PPER", "$,", "PPOSAT", "PTKNEG", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wird jhn hoch beklagen,", "tokens": ["Wird", "jhn", "hoch", "be\u00b7kla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADJD", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Vnd der F\u00fcrstlich Ober-Rath", "tokens": ["Vnd", "der", "F\u00fcrst\u00b7lich", "O\u00b7ber\u00b7Rath"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Der jhn wol gepr\u00fcfet hat,", "tokens": ["Der", "jhn", "wol", "ge\u00b7pr\u00fc\u00b7fet", "hat", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "ADV", "VVPP", "VAFIN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Selbs Leid umb Ihn tragen.", "tokens": ["Selbs", "Leid", "umb", "Ihn", "tra\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "APPR", "PPER", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.23": {"line.1": {"text": "Ja der Graff von Schwartzenbergk", "tokens": ["Ja", "der", "Graff", "von", "Schwart\u00b7zen\u00b7bergk"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PTKANT", "ART", "NN", "APPR", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Hat an seiner Tugend-Werck'", "tokens": ["Hat", "an", "sei\u00b7ner", "Tu\u00b7gen\u00b7d\u00b7Werck"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "PPOSAT", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.3": {"text": "Offtmals sich ergetzet,", "tokens": ["Offt\u00b7mals", "sich", "er\u00b7get\u00b7zet", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PRF", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Was? selbs vnser Haupt vnd Liecht,", "tokens": ["Was", "?", "selbs", "vn\u00b7ser", "Haupt", "vnd", "Liecht", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "$.", "ADV", "PPOSAT", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Friedrich Wilhelm, hat Ihn nicht", "tokens": ["Fried\u00b7rich", "Wil\u00b7helm", ",", "hat", "Ihn", "nicht"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["NE", "NE", "$,", "VAFIN", "PPER", "PTKNEG"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "F\u00fcr gemein gesch\u00e4tzet.", "tokens": ["F\u00fcr", "ge\u00b7mein", "ge\u00b7sch\u00e4t\u00b7zet", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "VVPP", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.24": {"line.1": {"text": "Vnd wo bleibt so mancher Mann", "tokens": ["Vnd", "wo", "bleibt", "so", "man\u00b7cher", "Mann"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "VVFIN", "ADV", "PIAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Den ich jetzt nicht nennen kan", "tokens": ["Den", "ich", "jetzt", "nicht", "nen\u00b7nen", "kan"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "PPER", "ADV", "PTKNEG", "VVINF", "VMFIN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Hier im gantzen Lande?", "tokens": ["Hier", "im", "gant\u00b7zen", "Lan\u00b7de", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Denn wer jrgends von Ihm wust'", "tokens": ["Denn", "wer", "jr\u00b7gends", "von", "Ihm", "wust'"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "ADV", "APPR", "PPER", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Hatte zu Ihm Lieb vnd Lust,", "tokens": ["Hat\u00b7te", "zu", "Ihm", "Lieb", "vnd", "Lust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "PPER", "NN", "KON", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Auch aus jedem Stande.", "tokens": ["Auch", "aus", "je\u00b7dem", "Stan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PIAT", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.25": {"line.1": {"text": "Sol ich Deutschland lassen stehn?", "tokens": ["Sol", "ich", "Deutschland", "las\u00b7sen", "stehn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "NE", "VVINF", "VVINF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Engelland f\u00fcr\u00fcber gehn?", "tokens": ["En\u00b7gel\u00b7land", "f\u00fc\u00b7r\u00fc\u00b7ber", "gehn", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "APPR", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Nichts von Frankreich melden?", "tokens": ["Nichts", "von", "Fran\u00b7kreich", "mel\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "APPR", "NE", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Nichts von Welschland, da die Kunst", "tokens": ["Nichts", "von", "Wel\u00b7schland", ",", "da", "die", "Kunst"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PIS", "APPR", "NN", "$,", "KOUS", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Ihn verkn\u00fcpfft durch Lieb vnd Gunst", "tokens": ["Ihn", "ver\u00b7kn\u00fcpfft", "durch", "Lieb", "vnd", "Gunst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Manchem wehrten Helden?", "tokens": ["Man\u00b7chem", "wehr\u00b7ten", "Hel\u00b7den", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.26": {"line.1": {"text": "Dennemarck und Schweden n\u00e4hrt", "tokens": ["Den\u00b7ne\u00b7marck", "und", "Schwe\u00b7den", "n\u00e4hrt"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "KON", "NE", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Leute die Ihn hoh und wehrt", "tokens": ["Leu\u00b7te", "die", "Ihn", "hoh", "und", "wehrt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ART", "PPER", "ADJD", "KON", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Vnd erkohren halten,", "tokens": ["Vnd", "er\u00b7koh\u00b7ren", "hal\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "VVINF", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Vnd ohn zweiffel vberall,", "tokens": ["Vnd", "ohn", "zweif\u00b7fel", "vbe\u00b7rall", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "ADV", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.5": {"text": "Wenn si h\u00f6ren diesen Fall,", "tokens": ["Wenn", "si", "h\u00f6\u00b7ren", "die\u00b7sen", "Fall", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "PDAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Schreckens-voll erkalten.", "tokens": ["Schre\u00b7ckens\u00b7voll", "er\u00b7kal\u00b7ten", "."], "token_info": ["word", "word", "punct"], "pos": ["ADJD", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.27": {"line.1": {"text": "Denn nicht aus zu sagen ist,", "tokens": ["Denn", "nicht", "aus", "zu", "sa\u00b7gen", "ist", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "PTKVZ", "PTKZU", "VVINF", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie er eilends ward erkiest", "tokens": ["Wie", "er", "ei\u00b7lends", "ward", "er\u00b7kiest"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "VAFIN", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wenn man Ihn nur h\u00f6rte,", "tokens": ["Wenn", "man", "Ihn", "nur", "h\u00f6r\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "ADV", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Stracks gewann er aller Hertz,", "tokens": ["Stracks", "ge\u00b7wann", "er", "al\u00b7ler", "Hertz", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Massen er durch Ernst und Schertz", "tokens": ["Mas\u00b7sen", "er", "durch", "Ernst", "und", "Schertz"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "PPER", "APPR", "NE", "KON", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Allzeit etwas lehrte.", "tokens": ["All\u00b7zeit", "et\u00b7was", "lehr\u00b7te", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.28": {"line.1": {"text": "O wie war doch seine Lust", "tokens": ["O", "wie", "war", "doch", "sei\u00b7ne", "Lust"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "KOKOM", "VAFIN", "ADV", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Zu bef\u00f6rtern, wie er wust,", "tokens": ["Zu", "be\u00b7f\u00f6r\u00b7tern", ",", "wie", "er", "wust", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "$,", "PWAV", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Alle Kunst vnd Tugend,", "tokens": ["Al\u00b7le", "Kunst", "vnd", "Tu\u00b7gend", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "KON", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Trew vnd flei\u00df ward nicht gespart,", "tokens": ["Trew", "vnd", "flei\u00df", "ward", "nicht", "ge\u00b7spart", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NN", "VAFIN", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Merckt er was von gutter Art", "tokens": ["Merckt", "er", "was", "von", "gut\u00b7ter", "Art"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "PIS", "APPR", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "In der lieben Jugend.", "tokens": ["In", "der", "lie\u00b7ben", "Ju\u00b7gend", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.29": {"line.1": {"text": "Mit was tieffer Niedrigheit", "tokens": ["Mit", "was", "tief\u00b7fer", "Nied\u00b7rig\u00b7heit"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PRELS", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Pflag er Gottes jederzeit", "tokens": ["Pflag", "er", "Got\u00b7tes", "je\u00b7der\u00b7zeit"], "token_info": ["word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Bey mir zu erwehnen,", "tokens": ["Bey", "mir", "zu", "er\u00b7weh\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Da es jhm im Hertzen nicht", "tokens": ["Da", "es", "jhm", "im", "Hert\u00b7zen", "nicht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PPER", "APPRART", "NN", "PTKNEG"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Hat an Andacht, im Gesicht", "tokens": ["Hat", "an", "An\u00b7dacht", ",", "im", "Ge\u00b7sicht"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "APPR", "NN", "$,", "APPRART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Nicht gefehlt an Thr\u00e4nen.", "tokens": ["Nicht", "ge\u00b7fehlt", "an", "Thr\u00e4\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVPP", "APPR", "NN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.30": {"line.1": {"text": "Sonst kam List und Heucheley", "tokens": ["Sonst", "kam", "List", "und", "Heu\u00b7che\u00b7ley"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "NE", "KON", "NN"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.2": {"text": "Gantz nicht seinem Hertzen bey,", "tokens": ["Gantz", "nicht", "sei\u00b7nem", "Hert\u00b7zen", "bey", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Das nur Warheit liebte,", "tokens": ["Das", "nur", "War\u00b7heit", "lieb\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "ADV", "NN", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Vnd mit wolbedachtem Rath", "tokens": ["Vnd", "mit", "wol\u00b7be\u00b7dach\u00b7tem", "Rath"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Alle Sachen die er that", "tokens": ["Al\u00b7le", "Sa\u00b7chen", "die", "er", "that"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "ART", "PPER", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Frey vnd frewdig \u00fcbte.", "tokens": ["Frey", "vnd", "frew\u00b7dig", "\u00fcb\u00b7te", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "ADJD", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.31": {"line.1": {"text": "Soll ich seine reiche Handt", "tokens": ["Soll", "ich", "sei\u00b7ne", "rei\u00b7che", "Handt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Die er hat an mich gewandt", "tokens": ["Die", "er", "hat", "an", "mich", "ge\u00b7wandt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PPER", "VAFIN", "APPR", "PPER", "VVPP"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Auch denn nicht erzehlen?", "tokens": ["Auch", "denn", "nicht", "er\u00b7zeh\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Nein, ich sorg', es werde mir", "tokens": ["Nein", ",", "ich", "sor\u00b7g'", ",", "es", "wer\u00b7de", "mir"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["PTKANT", "$,", "PPER", "VVFIN", "$,", "PPER", "VAFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "An verm\u00f6gen, zeit, Papier", "tokens": ["An", "ver\u00b7m\u00f6\u00b7gen", ",", "zeit", ",", "Pa\u00b7pier"], "token_info": ["word", "word", "punct", "word", "punct", "word"], "pos": ["APPR", "VVFIN", "$,", "NN", "$,", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Vnd an Worten fehlen.", "tokens": ["Vnd", "an", "Wor\u00b7ten", "feh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.32": {"line.1": {"text": "Vnd wer weis vorhin nicht schon", "tokens": ["Vnd", "wer", "weis", "vor\u00b7hin", "nicht", "schon"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "PTKVZ", "ADV", "PTKNEG", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Auch im gantzen Land' hievon?", "tokens": ["Auch", "im", "gant\u00b7zen", "Land'", "hie\u00b7von", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "ADJA", "NN", "PAV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Von so vielen Jahren", "tokens": ["Von", "so", "vie\u00b7len", "Jah\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADV", "PIAT", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Hatt es wol, nach meinem Wahn,", "tokens": ["Hatt", "es", "wol", ",", "nach", "mei\u00b7nem", "Wahn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "$,", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Was er stets bey mir gethan,", "tokens": ["Was", "er", "stets", "bey", "mir", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "APPR", "PPER", "VVPP", "$,"], "meter": "+-++--+", "measure": "iambic.tetra.chol"}, "line.6": {"text": "Auch ein Kind erfahren.", "tokens": ["Auch", "ein", "Kind", "er\u00b7fah\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "NN", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.33": {"line.1": {"text": "Vber das so h\u00e4ufft die Zahl", "tokens": ["Vber", "das", "so", "h\u00e4ufft", "die", "Zahl"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADV", "VVFIN", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Seiner Gutthat meine Qual,", "tokens": ["Sei\u00b7ner", "Gut\u00b7that", "mei\u00b7ne", "Qual", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Nichts kan ich erm\u00e4ssen,", "tokens": ["Nichts", "kan", "ich", "er\u00b7m\u00e4s\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PPER", "VVINF", "$,"], "meter": "+---+-", "measure": "dactylic.init"}, "line.4": {"text": "Stracks f\u00e4llt sein Gesicht mir ein,", "tokens": ["Stracks", "f\u00e4llt", "sein", "Ge\u00b7sicht", "mir", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPOSAT", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Stehend pflag er so zu seyn,", "tokens": ["Ste\u00b7hend", "pflag", "er", "so", "zu", "seyn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADV", "PTKZU", "VAINF", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "So ist er gesessen.", "tokens": ["So", "ist", "er", "ge\u00b7ses\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}}, "stanza.34": {"line.1": {"text": "Fraw, du sonst ein Tugend-Schild,", "tokens": ["Fraw", ",", "du", "sonst", "ein", "Tu\u00b7gen\u00b7dSchild", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PPER", "ADV", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Jetzt ein wahres Tr\u00fcbni\u00df-Bild,", "tokens": ["Jetzt", "ein", "wah\u00b7res", "Tr\u00fcb\u00b7ni\u00df\u00b7Bild", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Gro\u00df ist zwar dein Leiden,", "tokens": ["Gro\u00df", "ist", "zwar", "dein", "Lei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "PPOSAT", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Aber heisch nicht Trost von mir,", "tokens": ["A\u00b7ber", "heisch", "nicht", "Trost", "von", "mir", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PTKNEG", "NN", "APPR", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Denn ich weis mich selber hier", "tokens": ["Denn", "ich", "weis", "mich", "sel\u00b7ber", "hier"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "PTKVZ", "PPER", "ADV", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Mein nicht zu bescheiden.", "tokens": ["Mein", "nicht", "zu", "be\u00b7schei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "PTKNEG", "PTKZU", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}, "stanza.35": {"line.1": {"text": "Halt es diesmal mir zu gut.", "tokens": ["Halt", "es", "dies\u00b7mal", "mir", "zu", "gut", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "ADV", "PPER", "PTKA", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.2": {"text": "Schaw wie meiner Thr\u00e4nen Flut,", "tokens": ["Schaw", "wie", "mei\u00b7ner", "Thr\u00e4\u00b7nen", "Flut", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "KOKOM", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "So ich allzeit treibe,", "tokens": ["So", "ich", "all\u00b7zeit", "trei\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "ADV", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Mit der Tinten sich vermischt,", "tokens": ["Mit", "der", "Tin\u00b7ten", "sich", "ver\u00b7mischt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PRF", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Vnd mir von der Taffel wischt", "tokens": ["Vnd", "mir", "von", "der", "Taf\u00b7fel", "wischt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "APPR", "ART", "NN", "VVFIN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.6": {"text": "Alles was ich schreibe.", "tokens": ["Al\u00b7les", "was", "ich", "schrei\u00b7be", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "PWS", "PPER", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}}}}}