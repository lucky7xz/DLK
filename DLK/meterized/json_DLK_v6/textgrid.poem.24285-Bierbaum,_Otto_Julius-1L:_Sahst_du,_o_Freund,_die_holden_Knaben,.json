{"textgrid.poem.24285": {"metadata": {"author": {"name": "Bierbaum, Otto Julius", "birth": "N.A.", "death": "N.A."}, "title": "1L: Sahst du, o Freund, die holden Knaben,", "genre": "verse", "period": "N.A.", "pub_year": 1887, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Sahst du, o Freund, die holden Knaben,", "tokens": ["Sahst", "du", ",", "o", "Freund", ",", "die", "hol\u00b7den", "Kna\u00b7ben", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "FM", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die an der Kranzler-Ecke stehn,", "tokens": ["Die", "an", "der", "Kranz\u00b7le\u00b7rE\u00b7cke", "stehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Aus Seide rote Schlipse haben", "tokens": ["Aus", "Sei\u00b7de", "ro\u00b7te", "Schlip\u00b7se", "ha\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN", "VAFIN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und l\u00e4chelnd auf und nieder gehn?", "tokens": ["Und", "l\u00e4\u00b7chelnd", "auf", "und", "nie\u00b7der", "gehn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "PTKVZ", "KON", "PTKVZ", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Sie spitzen die gef\u00e4rbten Lippen", "tokens": ["Sie", "spit\u00b7zen", "die", "ge\u00b7f\u00e4rb\u00b7ten", "Lip\u00b7pen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und \u00e4ugeln sonderbar lasziv,", "tokens": ["Und", "\u00e4u\u00b7geln", "son\u00b7der\u00b7bar", "las\u00b7ziv", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und, kommst du ihnen nah, so tippen", "tokens": ["Und", ",", "kommst", "du", "ih\u00b7nen", "nah", ",", "so", "tip\u00b7pen"], "token_info": ["word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "$,", "VVFIN", "PPER", "PPER", "ADJD", "$,", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Sie dich wohl an und legen schief", "tokens": ["Sie", "dich", "wohl", "an", "und", "le\u00b7gen", "schief"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "PRF", "ADV", "PTKVZ", "KON", "VVINF", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Das K\u00f6pfchen mit gebrannten Haaren,", "tokens": ["Das", "K\u00f6pf\u00b7chen", "mit", "ge\u00b7brann\u00b7ten", "Haa\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihre Blicke himmeln dich", "tokens": ["Und", "ih\u00b7re", "Bli\u00b7cke", "him\u00b7meln", "dich"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Sehns\u00fcchtig an. Kurz, ihr Gebaren", "tokens": ["Sehn\u00b7s\u00fcch\u00b7tig", "an", ".", "Kurz", ",", "ihr", "Ge\u00b7ba\u00b7ren"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word"], "pos": ["ADJD", "PTKVZ", "$.", "ADJD", "$,", "PPOSAT", "NN"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.4": {"text": "Ist immerhin absonderlich.", "tokens": ["Ist", "im\u00b7mer\u00b7hin", "ab\u00b7son\u00b7der\u00b7lich", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Es ist nicht sch\u00f6n; ich geb es zu;", "tokens": ["Es", "ist", "nicht", "sch\u00f6n", ";", "ich", "geb", "es", "zu", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PTKNEG", "ADJD", "$.", "PPER", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wir wollen unserm Sch\u00f6pfer danken,", "tokens": ["Wir", "wol\u00b7len", "un\u00b7serm", "Sch\u00f6p\u00b7fer", "dan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df wir nicht so sind, ich und du;", "tokens": ["Da\u00df", "wir", "nicht", "so", "sind", ",", "ich", "und", "du", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "VAFIN", "$,", "PPER", "KON", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Doch nicht uns besser d\u00fcnken, meinen,", "tokens": ["Doch", "nicht", "uns", "bes\u00b7ser", "d\u00fcn\u00b7ken", ",", "mei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "PTKNEG", "PPER", "ADJD", "VVINF", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Es m\u00fc\u00dften alle sein wie wir.", "tokens": ["Es", "m\u00fc\u00df\u00b7ten", "al\u00b7le", "sein", "wie", "wir", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PIS", "VAINF", "KOKOM", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Hat nun die Liebe mehr als Einen", "tokens": ["Hat", "nun", "die", "Lie\u00b7be", "mehr", "als", "Ei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ART", "NN", "PIAT", "KOKOM", "ART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ausweg \u2013 jenun: so g\u00f6nn ihn ihr.", "tokens": ["Aus\u00b7weg", "\u2013", "je\u00b7nun", ":", "so", "g\u00f6nn", "ihn", "ihr", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "$(", "PDAT", "$.", "ADV", "VVFIN", "PPER", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Selbst das mu\u00df man mit Gleichmut tragen,", "tokens": ["Selbst", "das", "mu\u00df", "man", "mit", "Gleich\u00b7mut", "tra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDS", "VMFIN", "PIS", "APPR", "NN", "VVINF", "$,"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.2": {"text": "Da\u00df derlei Knaben (es ist b\u00f6s)", "tokens": ["Da\u00df", "der\u00b7lei", "Kna\u00b7ben", "(", "es", "ist", "b\u00f6s", ")"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "$(", "PPER", "VAFIN", "ADJD", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auf ihre Art die Leier schlagen,", "tokens": ["Auf", "ih\u00b7re", "Art", "die", "Lei\u00b7er", "schla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So scheu\u00dflich s\u00fc\u00df, so syrup\u00f6s,", "tokens": ["So", "scheu\u00df\u00b7lich", "s\u00fc\u00df", ",", "so", "sy\u00b7ru\u00b7p\u00f6s", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADJD", "ADJD", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Und da\u00df es Mode wird, zu schminken", "tokens": ["Und", "da\u00df", "es", "Mo\u00b7de", "wird", ",", "zu", "schmin\u00b7ken"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "KOUS", "PPER", "NN", "VAFIN", "$,", "PTKZU", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Lippen selbst der Poesie.", "tokens": ["Die", "Lip\u00b7pen", "selbst", "der", "Poe\u00b7sie", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Auch diese Mode wird versinken,", "tokens": ["Auch", "die\u00b7se", "Mo\u00b7de", "wird", "ver\u00b7sin\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PDAT", "NN", "VAFIN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Absurdit\u00e4ten dauern nie.", "tokens": ["Ab\u00b7sur\u00b7di\u00b7t\u00e4\u00b7ten", "dau\u00b7ern", "nie", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "ADV", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.8": {"line.1": {"text": "Das Zeug schmeckt bald auch denen fade,", "tokens": ["Das", "Zeug", "schmeckt", "bald", "auch", "de\u00b7nen", "fa\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADV", "ADV", "PDS", "VVFIN", "$,"], "meter": "-++-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Die jetzt dran schlecken: Zuckerkant,", "tokens": ["Die", "jetzt", "dran", "schle\u00b7cken", ":", "Zu\u00b7cker\u00b7kant", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "ADV", "PAV", "VVINF", "$.", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Lakritzensaft und Limonade", "tokens": ["La\u00b7krit\u00b7zen\u00b7saft", "und", "Li\u00b7mo\u00b7na\u00b7de"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wird auf die Dauer degoutant.", "tokens": ["Wird", "auf", "die", "Dau\u00b7er", "de\u00b7gou\u00b7tant", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}