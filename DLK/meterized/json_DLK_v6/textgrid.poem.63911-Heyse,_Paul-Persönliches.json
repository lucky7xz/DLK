{"textgrid.poem.63911": {"metadata": {"author": {"name": "Heyse, Paul", "birth": "N.A.", "death": "N.A."}, "title": "Pers\u00f6nliches", "genre": "verse", "period": "N.A.", "pub_year": 1872, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ich hab' erst sp\u00e4t mich emanzipiert", "tokens": ["Ich", "hab'", "erst", "sp\u00e4t", "mich", "e\u00b7man\u00b7zi\u00b7piert"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "PPER", "VVFIN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Und von mir selbst Besitz genommen.", "tokens": ["Und", "von", "mir", "selbst", "Be\u00b7sitz", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "PPER", "ADV", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nur wer die Piet\u00e4t verliert,", "tokens": ["Nur", "wer", "die", "Pi\u00b7e\u00b7t\u00e4t", "ver\u00b7liert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PWS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kann zu sich selber kommen.", "tokens": ["Kann", "zu", "sich", "sel\u00b7ber", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Mir ward ein Gl\u00fcck, das ich h\u00f6her sch\u00e4tzte,", "tokens": ["Mir", "ward", "ein", "Gl\u00fcck", ",", "das", "ich", "h\u00f6\u00b7her", "sch\u00e4tz\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,", "PRELS", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Als alles Gold in Kaliforniens Ebne:", "tokens": ["Als", "al\u00b7les", "Gold", "in", "Ka\u00b7li\u00b7for\u00b7ni\u00b7ens", "Eb\u00b7ne", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "APPR", "NE", "NE", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Ich hatte niemals Vorgesetzte", "tokens": ["Ich", "hat\u00b7te", "nie\u00b7mals", "Vor\u00b7ge\u00b7setz\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und niemals Untergebne.", "tokens": ["Und", "nie\u00b7mals", "Un\u00b7ter\u00b7geb\u00b7ne", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADV", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "\u00bbwarum h\u00e4ltst du dich uns so fern?", "tokens": ["\u00bb", "wa\u00b7rum", "h\u00e4ltst", "du", "dich", "uns", "so", "fern", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPER", "PRF", "PPER", "ADV", "ADJD", "$."], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.2": {"text": "Eine Lieb' ist der andern wert.\u00ab \u2013", "tokens": ["Ei\u00b7ne", "Lieb'", "ist", "der", "an\u00b7dern", "wert", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "VAFIN", "ART", "ADJA", "ADJD", "$.", "$(", "$("], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.3": {"text": "Ich w\u00fcrd' euch lieben herzlich gern,", "tokens": ["Ich", "w\u00fcrd'", "euch", "lie\u00b7ben", "herz\u00b7lich", "gern", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "VVFIN", "ADJD", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn ihr nur liebensw\u00fcrdig w\u00e4r't.", "tokens": ["Wenn", "ihr", "nur", "lie\u00b7bens\u00b7w\u00fcr\u00b7dig", "w\u00e4r'", "t."], "token_info": ["word", "word", "word", "word", "word", "abbreviation"], "pos": ["KOUS", "PPER", "ADV", "ADJD", "VAFIN", "NE"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Ich werde wohl dann und wann verstimmt,", "tokens": ["Ich", "wer\u00b7de", "wohl", "dann", "und", "wann", "ver\u00b7stimmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "KON", "PWAV", "VVPP", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn N\u00f6rgeln und M\u00e4keln kein Ende nimmt.", "tokens": ["Wenn", "N\u00f6r\u00b7geln", "und", "M\u00e4\u00b7keln", "kein", "En\u00b7de", "nimmt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "PIAT", "NN", "VVFIN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Dann mu\u00df ich von den Gr\u00f6\u00dften lesen,", "tokens": ["Dann", "mu\u00df", "ich", "von", "den", "Gr\u00f6\u00df\u00b7ten", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Wie's ihrer Zeit nicht besser gewesen.", "tokens": ["Wie's", "ih\u00b7rer", "Zeit", "nicht", "bes\u00b7ser", "ge\u00b7we\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPOSAT", "NN", "PTKNEG", "ADJD", "VAPP", "$."], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Auf einmal werd' ich still und heiter", "tokens": ["Auf", "ein\u00b7mal", "werd'", "ich", "still", "und", "hei\u00b7ter"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADV", "VAFIN", "PPER", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und treibe getrost mein Wesen weiter.", "tokens": ["Und", "trei\u00b7be", "ge\u00b7trost", "mein", "We\u00b7sen", "wei\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+--+-+-+-", "measure": "iambic.tetra.relaxed"}}, "stanza.5": {"line.1": {"text": "\u00bbauf diesen Mann hohnl\u00e4sterst du,", "tokens": ["\u00bb", "auf", "die\u00b7sen", "Mann", "hohn\u00b7l\u00e4s\u00b7terst", "du", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "PDAT", "NN", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der doch von dir mit Achtung spricht?\u00ab \u2013", "tokens": ["Der", "doch", "von", "dir", "mit", "Ach\u00b7tung", "spricht", "?", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "ADV", "APPR", "PPER", "APPR", "NN", "VVFIN", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Er hat vielleicht Grund dazu,", "tokens": ["Er", "hat", "viel\u00b7leicht", "Grund", "da\u00b7zu", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "PAV", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ich leider nicht.", "tokens": ["Ich", "lei\u00b7der", "nicht", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ADV", "PTKNEG", "$."], "meter": "-+-+", "measure": "iambic.di"}}, "stanza.6": {"line.1": {"text": "Bewahr in deinem Busen still,", "tokens": ["Be\u00b7wahr", "in", "dei\u00b7nem", "Bu\u00b7sen", "still", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was dir dein eigner D\u00e4mon g\u00f6nnte,", "tokens": ["Was", "dir", "dein", "eig\u00b7ner", "D\u00e4\u00b7mon", "g\u00f6nn\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PPOSAT", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da jedermann nur h\u00f6ren will,", "tokens": ["Da", "je\u00b7der\u00b7mann", "nur", "h\u00f6\u00b7ren", "will", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "ADV", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was er auch selbst sich sagen k\u00f6nnte.", "tokens": ["Was", "er", "auch", "selbst", "sich", "sa\u00b7gen", "k\u00f6nn\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADV", "ADV", "PRF", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Mir eine Elle zuzusetzen,", "tokens": ["Mir", "ei\u00b7ne", "El\u00b7le", "zu\u00b7zu\u00b7set\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Gel\u00e4ng's auch, k\u00e4me mir nicht in Sinn.", "tokens": ["Ge\u00b7l\u00e4ng's", "auch", ",", "k\u00e4\u00b7me", "mir", "nicht", "in", "Sinn", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADV", "$,", "VVFIN", "PPER", "PTKNEG", "APPR", "NN", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Das einzige, was an mir zu sch\u00e4tzen,", "tokens": ["Das", "ein\u00b7zi\u00b7ge", ",", "was", "an", "mir", "zu", "sch\u00e4t\u00b7zen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "$,", "PRELS", "APPR", "PPER", "PTKZU", "VVINF", "$,"], "meter": "--+-+-+-+-", "measure": "anapaest.init"}, "line.4": {"text": "Ist, da\u00df ich so und nicht anders bin.", "tokens": ["Ist", ",", "da\u00df", "ich", "so", "und", "nicht", "an\u00b7ders", "bin", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "ADV", "KON", "PTKNEG", "ADV", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.8": {"line.1": {"text": "Soll Ruhm mir bl\u00fchn, komm' er beizeit.", "tokens": ["Soll", "Ruhm", "mir", "bl\u00fchn", ",", "komm'", "er", "bei\u00b7zeit", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VMFIN", "NN", "PPER", "VVINF", "$,", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was hat die Nachwelt mir zu geben?", "tokens": ["Was", "hat", "die", "Nach\u00b7welt", "mir", "zu", "ge\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ich m\u00f6chte von meiner Unsterblichkeit", "tokens": ["Ich", "m\u00f6ch\u00b7te", "von", "mei\u00b7ner", "U\u00b7nsterb\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "APPR", "PPOSAT", "NN"], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Doch ein paar J\u00e4hrchen miterleben.", "tokens": ["Doch", "ein", "paar", "J\u00e4hr\u00b7chen", "mi\u00b7ter\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Gewisser Leute Bann und Acht", "tokens": ["Ge\u00b7wis\u00b7ser", "Leu\u00b7te", "Bann", "und", "Acht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "NN", "KON", "CARD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hat nie mich wundergenommen.", "tokens": ["Hat", "nie", "mich", "wun\u00b7der\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "PPER", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ich hab' ihnen den Verdru\u00df gemacht,", "tokens": ["Ich", "hab'", "ih\u00b7nen", "den", "Ver\u00b7dru\u00df", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ART", "NN", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Ohne sie durch die Welt zu kommen.", "tokens": ["Oh\u00b7ne", "sie", "durch", "die", "Welt", "zu", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "APPR", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}}, "stanza.10": {"line.1": {"text": "Ich machte mir keine Modellfigur,", "tokens": ["Ich", "mach\u00b7te", "mir", "kei\u00b7ne", "Mo\u00b7dell\u00b7fi\u00b7gur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PIAT", "NN", "$,"], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.2": {"text": "Mein Bildnis danach auszuf\u00fchren,", "tokens": ["Mein", "Bild\u00b7nis", "da\u00b7nach", "aus\u00b7zu\u00b7f\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PAV", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Um Kennerbeifall zu erhaschen.", "tokens": ["Um", "Ken\u00b7ner\u00b7bei\u00b7fall", "zu", "er\u00b7ha\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Stets gab ich Vollmacht der Natur", "tokens": ["Stets", "gab", "ich", "Voll\u00b7macht", "der", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "NN", "ART", "NN"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.5": {"text": "Und lie\u00df, froh, ihre Macht zu sp\u00fcren,", "tokens": ["Und", "lie\u00df", ",", "froh", ",", "ih\u00b7re", "Macht", "zu", "sp\u00fc\u00b7ren", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ADJD", "$,", "PPOSAT", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Mich mit mir selber \u00fcberraschen.", "tokens": ["Mich", "mit", "mir", "sel\u00b7ber", "\u00fc\u00b7berr\u00b7a\u00b7schen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Hab' doch in gut' und b\u00f6sen Tagen", "tokens": ["Hab'", "doch", "in", "gut'", "und", "b\u00f6\u00b7sen", "Ta\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "APPR", "ADJA", "KON", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mich redlich und honett betragen", "tokens": ["Mich", "red\u00b7lich", "und", "ho\u00b7nett", "be\u00b7tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "ADJD", "KON", "ADJD", "VVINF"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.3": {"text": "Und soll nun Pfaffen und Philister fragen,", "tokens": ["Und", "soll", "nun", "Pfaf\u00b7fen", "und", "Phi\u00b7lis\u00b7ter", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "ADV", "NN", "KON", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Ob auch mein sittlicher Instinkt", "tokens": ["Ob", "auch", "mein", "sitt\u00b7li\u00b7cher", "Ins\u00b7tinkt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Ihnen genugsam reinlich d\u00fcnkt?", "tokens": ["Ih\u00b7nen", "ge\u00b7nug\u00b7sam", "rein\u00b7lich", "d\u00fcnkt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "ADJD", "VVFIN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.12": {"line.1": {"text": "Halt' mich nicht, just f\u00fcr das Ma\u00df der Welt;", "tokens": ["Halt'", "mich", "nicht", ",", "just", "f\u00fcr", "das", "Ma\u00df", "der", "Welt", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPER", "PTKNEG", "$,", "ADV", "APPR", "ART", "NN", "ART", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Doch ", "tokens": ["Doch"], "token_info": ["word"], "pos": ["KON"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Hat durchgel\u00e4utert diesen Busen", "tokens": ["Hat", "durch\u00b7ge\u00b7l\u00e4u\u00b7tert", "die\u00b7sen", "Bu\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["VAFIN", "VVPP", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und ihn mit reinem Hauch geschwellt.", "tokens": ["Und", "ihn", "mit", "rei\u00b7nem", "Hauch", "ge\u00b7schwellt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Sonst hab' ich mir selbst Impulse gegeben;", "tokens": ["Sonst", "hab'", "ich", "mir", "selbst", "Im\u00b7pul\u00b7se", "ge\u00b7ge\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "PPER", "ADV", "NN", "VVPP", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Jetzt leb' ich nicht mehr, ich lasse mich leben.", "tokens": ["Jetzt", "leb'", "ich", "nicht", "mehr", ",", "ich", "las\u00b7se", "mich", "le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PTKNEG", "ADV", "$,", "PPER", "VVFIN", "PRF", "VVINF", "$."], "meter": "-+--+-+--+-", "measure": "iambic.tetra.relaxed"}}, "stanza.14": {"line.1": {"text": "Ich hinge wahrlich nicht so sehr", "tokens": ["Ich", "hin\u00b7ge", "wahr\u00b7lich", "nicht", "so", "sehr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PTKNEG", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "An diesem leidigen Leben,", "tokens": ["An", "die\u00b7sem", "lei\u00b7di\u00b7gen", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Wenn irgend sonst noch ein Mittel w\u00e4r',", "tokens": ["Wenn", "ir\u00b7gend", "sonst", "noch", "ein", "Mit\u00b7tel", "w\u00e4r'", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ADV", "ADV", "ART", "NN", "VAFIN", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Um allerlei zu erleben.", "tokens": ["Um", "al\u00b7ler\u00b7lei", "zu", "er\u00b7le\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "PIAT", "PTKZU", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Denn wenn auch m\u00e4nniglich bekannt,", "tokens": ["Denn", "wenn", "auch", "m\u00e4n\u00b7nig\u00b7lich", "be\u00b7kannt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "ADJD", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wie bitter oft das Leben schmeckt,", "tokens": ["Wie", "bit\u00b7ter", "oft", "das", "Le\u00b7ben", "schmeckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.7": {"text": "Und da\u00df die Welt sehr ennuyant,", "tokens": ["Und", "da\u00df", "die", "Welt", "sehr", "en\u00b7nu\u00b7yant", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Ward keine zweite doch entdeckt,", "tokens": ["Ward", "kei\u00b7ne", "zwei\u00b7te", "doch", "ent\u00b7deckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "ADJA", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Die auch nur halb so interessant.", "tokens": ["Die", "auch", "nur", "halb", "so", "in\u00b7ter\u00b7es\u00b7sant", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "ADJD", "ADV", "ADJD", "$."], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}}, "stanza.15": {"line.1": {"text": "Ich denke mit Gewissensbissen", "tokens": ["Ich", "den\u00b7ke", "mit", "Ge\u00b7wis\u00b7sens\u00b7bis\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zur\u00fcck, wie ich mein Lebenlang", "tokens": ["Zu\u00b7r\u00fcck", ",", "wie", "ich", "mein", "Le\u00b7ben\u00b7lang"], "token_info": ["word", "punct", "word", "word", "word", "word"], "pos": ["PTKVZ", "$,", "PWAV", "PPER", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Vorbeiging fastend an gewissen Bissen,", "tokens": ["Vor\u00b7bei\u00b7ging", "fas\u00b7tend", "an", "ge\u00b7wis\u00b7sen", "Bis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Die dann ein Schlechterer verschlang.", "tokens": ["Die", "dann", "ein", "Schlech\u00b7te\u00b7rer", "ver\u00b7schlang", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Wir haben uns gar nichts zu sagen;", "tokens": ["Wir", "ha\u00b7ben", "uns", "gar", "nichts", "zu", "sa\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "ADV", "PIS", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wie sollten wir uns nicht vertragen?", "tokens": ["Wie", "soll\u00b7ten", "wir", "uns", "nicht", "ver\u00b7tra\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VMFIN", "PPER", "PRF", "PTKNEG", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Mit Menschen bin ich tolerant,", "tokens": ["Mit", "Men\u00b7schen", "bin", "ich", "to\u00b7le\u00b7rant", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VAFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ob sie mich auch langweilen.", "tokens": ["Ob", "sie", "mich", "auch", "lang\u00b7wei\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ein schlechtes Buch fliegt an die Wand", "tokens": ["Ein", "schlech\u00b7tes", "Buch", "fliegt", "an", "die", "Wand"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Nach den ersten hundert Zeilen,", "tokens": ["Nach", "den", "ers\u00b7ten", "hun\u00b7dert", "Zei\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "CARD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Dieweil es B\u00fccher nicht verdrie\u00dft,", "tokens": ["Die\u00b7weil", "es", "B\u00fc\u00b7cher", "nicht", "ver\u00b7drie\u00dft", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wenn man sie nicht zu Ende liest.", "tokens": ["Wenn", "man", "sie", "nicht", "zu", "En\u00b7de", "liest", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PPER", "PTKNEG", "APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "\u00bbwas ist's f\u00fcr ein Mann? Wie ist er begabt?", "tokens": ["\u00bb", "was", "ist's", "f\u00fcr", "ein", "Mann", "?", "Wie", "ist", "er", "be\u00b7gabt", "?"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "APPR", "ART", "NN", "$.", "PWAV", "VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "Was leistet er, das ihm Ehre macht?\u00ab \u2013", "tokens": ["Was", "leis\u00b7tet", "er", ",", "das", "ihm", "Eh\u00b7re", "macht", "?", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$,", "PRELS", "PPER", "NN", "VVFIN", "$.", "$(", "$("], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Hab' wirklich nie dr\u00fcber nachgedacht,", "tokens": ["Hab'", "wirk\u00b7lich", "nie", "dr\u00fc\u00b7ber", "nach\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "ADV", "PAV", "VVPP", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hab' ihn nur schlechtweg lieb gehabt.", "tokens": ["Hab'", "ihn", "nur", "schlecht\u00b7weg", "lieb", "ge\u00b7habt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "\u00bbin der Zeitung las ich soeben", "tokens": ["\u00bb", "in", "der", "Zei\u00b7tung", "las", "ich", "soe\u00b7ben"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "APPR", "ART", "NN", "VVFIN", "PPER", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Ein sehr perfides Pasquill auf dich.\u00ab \u2013", "tokens": ["Ein", "sehr", "per\u00b7fi\u00b7des", "Pas\u00b7quill", "auf", "dich", ".", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "APPR", "PPER", "$.", "$(", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "So haben sie mir's schriftlich gegeben,", "tokens": ["So", "ha\u00b7ben", "sie", "mir's", "schrift\u00b7lich", "ge\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NE", "ADJD", "VVPP", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Da\u00df sie kleiner und schlechter sind, als ich.", "tokens": ["Da\u00df", "sie", "klei\u00b7ner", "und", "schlech\u00b7ter", "sind", ",", "als", "ich", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "KON", "ADJD", "VAFIN", "$,", "KOUS", "PPER", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}}, "stanza.20": {"line.1": {"text": "Was dem strebenden Flei\u00df gegl\u00fcckt,", "tokens": ["Was", "dem", "stre\u00b7ben\u00b7den", "Flei\u00df", "ge\u00b7gl\u00fcckt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "VVPP", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Wollte mir bald mi\u00dffallen.", "tokens": ["Woll\u00b7te", "mir", "bald", "mi\u00df\u00b7fal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "ADV", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Was mir dauernd das Herz entz\u00fcckt,", "tokens": ["Was", "mir", "dau\u00b7ernd", "das", "Herz", "ent\u00b7z\u00fcckt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ADJD", "ART", "NN", "VVPP", "$,"], "meter": "--+--+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Mu\u00dft' in den Scho\u00df mir fallen.", "tokens": ["Mu\u00dft'", "in", "den", "Scho\u00df", "mir", "fal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ART", "NN", "PPER", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.21": {"line.1": {"text": "Kein Trost in tatenlosem Leiden", "tokens": ["Kein", "Trost", "in", "ta\u00b7ten\u00b7lo\u00b7sem", "Lei\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ist, da\u00df ich r\u00fcstig einst geschafft.", "tokens": ["Ist", ",", "da\u00df", "ich", "r\u00fcs\u00b7tig", "einst", "ge\u00b7schafft", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "$,", "KOUS", "PPER", "ADJD", "ADV", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Seh' ich die Zeugen meiner alten Kraft,", "tokens": ["Seh'", "ich", "die", "Zeu\u00b7gen", "mei\u00b7ner", "al\u00b7ten", "Kraft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Fang' ich nur an, mich selber zu beneiden.", "tokens": ["Fang'", "ich", "nur", "an", ",", "mich", "sel\u00b7ber", "zu", "be\u00b7nei\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "ADV", "PTKVZ", "$,", "PPER", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.22": {"line.1": {"text": "\u00bbwarum mich nur das Gl\u00fcck nicht freut,", "tokens": ["\u00bb", "wa\u00b7rum", "mich", "nur", "das", "Gl\u00fcck", "nicht", "freut", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PWAV", "PPER", "ADV", "ART", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Trost f\u00fcr so viel Kummer beut! \u2013\u00ab", "tokens": ["Das", "Trost", "f\u00fcr", "so", "viel", "Kum\u00b7mer", "beut", "!", "\u2013", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "NN", "APPR", "ADV", "PIAT", "NN", "VVFIN", "$.", "$(", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Der Strahl, der Sturmgew\u00f6lk durchbricht,", "tokens": ["Der", "Strahl", ",", "der", "Sturm\u00b7ge\u00b7w\u00f6lk", "durch\u00b7bricht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Tut dir nicht wohl: die Sonne ", "tokens": ["Tut", "dir", "nicht", "wohl", ":", "die", "Son\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "PPER", "PTKNEG", "ADV", "$.", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.23": {"line.1": {"text": "Sonst hab' ich, wie die Gedanken kamen,", "tokens": ["Sonst", "hab'", "ich", ",", "wie", "die", "Ge\u00b7dan\u00b7ken", "ka\u00b7men", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "$,", "PWAV", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Sie rasch verbraucht im Augenblick.", "tokens": ["Sie", "rasch", "ver\u00b7braucht", "im", "Au\u00b7gen\u00b7blick", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "VVFIN", "APPRART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Jetzt leg' ich schon in Epigrammen", "tokens": ["Jetzt", "leg'", "ich", "schon", "in", "E\u00b7pig\u00b7ram\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein paar Notpfennige zur\u00fcck.", "tokens": ["Ein", "paar", "Not\u00b7pfen\u00b7ni\u00b7ge", "zu\u00b7r\u00fcck", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "\u00bbbeklagst dich, da\u00df Gespr\u00e4ch dir fehlt,", "tokens": ["\u00bb", "be\u00b7klagst", "dich", ",", "da\u00df", "Ge\u00b7spr\u00e4ch", "dir", "fehlt", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "VVFIN", "PPER", "$,", "KOUS", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und horchst du nicht und h\u00f6rst du nicht,", "tokens": ["Und", "horchst", "du", "nicht", "und", "h\u00f6rst", "du", "nicht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKNEG", "KON", "VVFIN", "PPER", "PTKNEG", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wie Berg und Wald so feinbeseelt", "tokens": ["Wie", "Berg", "und", "Wald", "so", "fein\u00b7be\u00b7seelt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "KON", "NN", "ADV", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "S\u00e4uselnd zu Ohr und Herzen spricht?\u00ab \u2013", "tokens": ["S\u00e4u\u00b7selnd", "zu", "Ohr", "und", "Her\u00b7zen", "spricht", "?", "\u00ab", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADJD", "APPR", "NN", "KON", "NN", "VVFIN", "$.", "$(", "$("], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.25": {"line.1": {"text": "Es klingt wohl sch\u00f6n, was hier und dort", "tokens": ["Es", "klingt", "wohl", "sch\u00f6n", ",", "was", "hier", "und", "dort"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "$,", "PRELS", "ADV", "KON", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Natur zu ihrem Kinde sagt,", "tokens": ["Na\u00b7tur", "zu", "ih\u00b7rem", "Kin\u00b7de", "sagt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch f\u00fchrt sie stets das gro\u00dfe Wort", "tokens": ["Doch", "f\u00fchrt", "sie", "stets", "das", "gro\u00b7\u00dfe", "Wort"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPER", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und gibt nicht Antwort, wenn man fragt.", "tokens": ["Und", "gibt", "nicht", "Ant\u00b7wort", ",", "wenn", "man", "fragt", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PTKNEG", "NN", "$,", "KOUS", "PIS", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.26": {"line.1": {"text": "Ach, wer versteht sein eigen Herz!", "tokens": ["Ach", ",", "wer", "ver\u00b7steht", "sein", "ei\u00b7gen", "Herz", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PWS", "VVFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein R\u00e4tsel ist dir's in die Brust geschaffen.", "tokens": ["Ein", "R\u00e4t\u00b7sel", "ist", "dir's", "in", "die", "Brust", "ge\u00b7schaf\u00b7fen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIS", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Heute schwer wie ein Berg von Erz", "tokens": ["Heu\u00b7te", "schwer", "wie", "ein", "Berg", "von", "Erz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "KOKOM", "ART", "NN", "APPR", "NN"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.4": {"text": "Will es dich in die Tiefe raffen;", "tokens": ["Will", "es", "dich", "in", "die", "Tie\u00b7fe", "raf\u00b7fen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "PRF", "APPR", "ART", "NN", "VVINF", "$."], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.5": {"text": "Morgen aller Schwere entbunden", "tokens": ["Mor\u00b7gen", "al\u00b7ler", "Schwe\u00b7re", "ent\u00b7bun\u00b7den"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "PIAT", "NN", "VVINF"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "Jauchzend lodert es wolkenw\u00e4rts,", "tokens": ["Jauch\u00b7zend", "lo\u00b7dert", "es", "wol\u00b7ken\u00b7w\u00e4rts", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPER", "ADV", "$,"], "meter": "+-+--+--", "measure": "trochaic.tri.relaxed"}, "line.7": {"text": "Und dann in gleichgeme\u00dfnen Stunden", "tokens": ["Und", "dann", "in", "gleich\u00b7ge\u00b7me\u00df\u00b7nen", "Stun\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Gelassen tr\u00e4gt es Lust und Schmerz.", "tokens": ["Ge\u00b7las\u00b7sen", "tr\u00e4gt", "es", "Lust", "und", "Schmerz", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Ach, wer beherrscht sein eigen Herz!", "tokens": ["Ach", ",", "wer", "be\u00b7herrscht", "sein", "ei\u00b7gen", "Herz", "!"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PWS", "VVFIN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "In jungen Jahren weint' ich viel", "tokens": ["In", "jun\u00b7gen", "Jah\u00b7ren", "weint'", "ich", "viel"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In jedem R\u00fchr- und Trauerspiel.", "tokens": ["In", "je\u00b7dem", "R\u00fchr", "und", "Trau\u00b7er\u00b7spiel", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "TRUNC", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Jetzt scheint mir das R\u00fchrendste auf Erden,", "tokens": ["Jetzt", "scheint", "mir", "das", "R\u00fch\u00b7rends\u00b7te", "auf", "Er\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "APPR", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Wenn gute Menschen gl\u00fccklich werden.", "tokens": ["Wenn", "gu\u00b7te", "Men\u00b7schen", "gl\u00fcck\u00b7lich", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADJA", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Lange leben ist keine Kunst,", "tokens": ["Lan\u00b7ge", "le\u00b7ben", "ist", "kei\u00b7ne", "Kunst", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "VAFIN", "PIAT", "NN", "$,"], "meter": "+-+--+-+", "measure": "glykoneus"}, "line.2": {"text": "Wird dir nur Zeit dazu gegeben.", "tokens": ["Wird", "dir", "nur", "Zeit", "da\u00b7zu", "ge\u00b7ge\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "NN", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch wer im Dichten, Wirken, Streben", "tokens": ["Doch", "wer", "im", "Dich\u00b7ten", ",", "Wir\u00b7ken", ",", "Stre\u00b7ben"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["KON", "PWS", "APPRART", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Es nie erlebt, sich selbst zu \u00fcberleben,", "tokens": ["Es", "nie", "er\u00b7lebt", ",", "sich", "selbst", "zu", "\u00fc\u00b7berl\u00b7e\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVPP", "$,", "PRF", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Der preise seiner Sterne Gunst.", "tokens": ["Der", "prei\u00b7se", "sei\u00b7ner", "Ster\u00b7ne", "Gunst", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}