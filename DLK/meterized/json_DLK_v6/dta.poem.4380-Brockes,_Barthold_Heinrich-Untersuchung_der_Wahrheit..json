{"dta.poem.4380": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Untersuchung der Wahrheit.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1743", "urn": "urn:nbn:de:kobv:b4-20083-6", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Die Wahrheit, in sich selbst betrachtet, hat die\u00df allein", "tokens": ["Die", "Wahr\u00b7heit", ",", "in", "sich", "selbst", "be\u00b7trach\u00b7tet", ",", "hat", "die\u00df", "al\u00b7lein"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "$,", "APPR", "PRF", "ADV", "VVPP", "$,", "VAFIN", "PDS", "ADV"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "zu ihrem Schatz,", "tokens": ["zu", "ih\u00b7rem", "Schatz", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.3": {"text": "Ja sie ist selber eigentlich nichts, als ", "tokens": ["Ja", "sie", "ist", "sel\u00b7ber", "ei\u00b7gent\u00b7lich", "nichts", ",", "als"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["PTKANT", "PPER", "VAFIN", "ADV", "ADV", "PIS", "$,", "KOUS"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Gegensatz.", "tokens": ["Ge\u00b7gen\u00b7satz", "."], "token_info": ["word", "punct"], "pos": ["NN", "$."], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Des Menschen Geists vern\u00fcnft\u2019ge Wahl, wenn er vom", "tokens": ["Des", "Men\u00b7schen", "Geists", "ver\u00b7n\u00fcnft'\u00b7ge", "Wahl", ",", "wenn", "er", "vom"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "NN", "ADJA", "NN", "$,", "KOUS", "PPER", "APPRART"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Jrrthum sich entfernet,", "tokens": ["Jrr\u00b7thum", "sich", "ent\u00b7fer\u00b7net", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PRF", "VVFIN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.7": {"text": "Ist Wahrheit, da man Jrrthum meiden, dem Rechte", "tokens": ["Ist", "Wahr\u00b7heit", ",", "da", "man", "Jrr\u00b7thum", "mei\u00b7den", ",", "dem", "Rech\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["VAFIN", "NN", "$,", "KOUS", "PIS", "NN", "VVINF", "$,", "ART", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.8": {"text": "beyzupflichten lernet.", "tokens": ["bey\u00b7zu\u00b7pflich\u00b7ten", "ler\u00b7net", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.9": {"text": "Nichts anders kann ich wesentlichs, ich mag sie, wie ich", "tokens": ["Nichts", "an\u00b7ders", "kann", "ich", "we\u00b7sent\u00b7lichs", ",", "ich", "mag", "sie", ",", "wie", "ich"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["PIS", "ADV", "VMFIN", "PPER", "PIS", "$,", "PPER", "VMFIN", "PPER", "$,", "PWAV", "PPER"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.10": {"text": "will, ergr\u00fcnden,", "tokens": ["will", ",", "er\u00b7gr\u00fcn\u00b7den", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VMFIN", "$,", "VVPP", "$,"], "meter": "+-+-", "measure": "trochaic.di"}, "line.11": {"text": "Sie noch so tief im Brunnen suchen, vom wahren Seyn", "tokens": ["Sie", "noch", "so", "tief", "im", "Brun\u00b7nen", "su\u00b7chen", ",", "vom", "wah\u00b7ren", "Seyn"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "ADV", "ADV", "ADJD", "APPRART", "NN", "VVINF", "$,", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "der Wahrheit finden.", "tokens": ["der", "Wahr\u00b7heit", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.13": {"text": "Denn was man von der Gottheit Selber, Sie sey die", "tokens": ["Denn", "was", "man", "von", "der", "Got\u00b7theit", "Sel\u00b7ber", ",", "Sie", "sey", "die"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "PWS", "PIS", "APPR", "ART", "NN", "NN", "$,", "PPER", "VAFIN", "ART"], "meter": "-+---+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "ew'ge Wahrheit, spricht,", "tokens": ["ew'\u00b7ge", "Wahr\u00b7heit", ",", "spricht", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["ADJA", "NN", "$,", "VVFIN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.15": {"text": "Ist ein ehrw\u00fcrdiges Geheimni\u00df, geh\u00f6rt zu unsern Schl\u00fcs-", "tokens": ["Ist", "ein", "ehr\u00b7w\u00fcr\u00b7di\u00b7ges", "Ge\u00b7heim\u00b7ni\u00df", ",", "ge\u00b7h\u00f6rt", "zu", "un\u00b7sern", "Schl\u00fcs"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "ADJA", "NN", "$,", "VVFIN", "APPR", "PPOSAT", "TRUNC"], "meter": "-+--+--+--+-+-+", "measure": "amphibrach.tetra.plus"}, "line.16": {"text": "sen nicht.", "tokens": ["sen", "nicht", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "PTKNEG", "$."], "meter": "-+", "measure": "iambic.single"}, "line.17": {"text": "Wir suchen nur, was die aus Hochmuht erzeugte Wahr-", "tokens": ["Wir", "su\u00b7chen", "nur", ",", "was", "die", "aus", "Hoch\u00b7muht", "er\u00b7zeug\u00b7te", "Wahr"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "$,", "PRELS", "ART", "APPR", "NN", "VVFIN", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "heit-Sucherey,", "tokens": ["heit\u00b7Su\u00b7che\u00b7rey", ","], "token_info": ["word", "punct"], "pos": ["PTKVZ", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.19": {"text": "Womit zu unsrer Zeit so viele sich breit zu machen suchen,", "tokens": ["Wo\u00b7mit", "zu", "uns\u00b7rer", "Zeit", "so", "vie\u00b7le", "sich", "breit", "zu", "ma\u00b7chen", "su\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPOSAT", "NN", "ADV", "PIS", "PRF", "ADJD", "PTKZU", "VVINF", "VVINF", "$,"], "meter": "-+-+-+-+--+-+-+-", "measure": "iambic.septa.relaxed"}, "line.20": {"text": "sey.", "tokens": ["sey", "."], "token_info": ["word", "punct"], "pos": ["VAFIN", "$."], "meter": "+", "measure": "single.up"}, "line.21": {"text": "Ich glaube, da\u00df im Gleichlaut eben des Wortes ", "tokens": ["Ich", "glau\u00b7be", ",", "da\u00df", "im", "Gleich\u00b7laut", "e\u00b7ben", "des", "Wor\u00b7tes"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "APPRART", "NN", "ADV", "ART", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.22": {"text": "heit, man sich irre,", "tokens": ["heit", ",", "man", "sich", "ir\u00b7re", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADJD", "$,", "PIS", "PRF", "VVFIN", "$,"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.23": {"text": "Und da\u00df man, mit erborgter Klarheit,", "tokens": ["Und", "da\u00df", "man", ",", "mit", "er\u00b7borg\u00b7ter", "Klar\u00b7heit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PIS", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.24": {"text": "Von der nur g\u00f6ttlich-ewigen, der eitlen Menschen dunkle", "tokens": ["Von", "der", "nur", "g\u00f6tt\u00b7lich\u00b7ewi\u00b7gen", ",", "der", "eit\u00b7len", "Men\u00b7schen", "dunk\u00b7le"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADV", "ADJA", "$,", "ART", "ADJA", "NN", "ADJA"], "meter": "-+-+-+--+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.25": {"text": "Wahrheit", "tokens": ["Wahr\u00b7heit"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.26": {"text": "In ungl\u00fccksel\u2019gem Gleichlaut schm\u00fccke, und die Jdeen", "tokens": ["In", "un\u00b7gl\u00fcck\u00b7sel'\u00b7gem", "Gleich\u00b7laut", "schm\u00fc\u00b7cke", ",", "und", "die", "Jdeen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "$,", "KON", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.27": {"text": "ganz verwirre.", "tokens": ["ganz", "ver\u00b7wir\u00b7re", "."], "token_info": ["word", "word", "punct"], "pos": ["ADV", "VVFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.28": {"text": "Wodurch wir denn, durch Stolz verf\u00fchrt, uns nicht allein", "tokens": ["Wo\u00b7durch", "wir", "denn", ",", "durch", "Stolz", "ver\u00b7f\u00fchrt", ",", "uns", "nicht", "al\u00b7lein"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "PPER", "ADV", "$,", "APPR", "NN", "VVPP", "$,", "PPER", "PTKNEG", "ADV"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.29": {"text": "so weit vergehn,", "tokens": ["so", "weit", "ver\u00b7gehn", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVINF", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.30": {"text": "Da\u00df wir uns, nach der von uns selbst erzeugten Wahrheit,", "tokens": ["Da\u00df", "wir", "uns", ",", "nach", "der", "von", "uns", "selbst", "er\u00b7zeug\u00b7ten", "Wahr\u00b7heit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "$,", "APPR", "ART", "APPR", "PPER", "ADV", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.31": {"text": "unterstehn,", "tokens": ["un\u00b7ter\u00b7stehn", ","], "token_info": ["word", "punct"], "pos": ["VVINF", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.32": {"text": "Die Brut von unserm eignen Hirn als eine Gottheit zu", "tokens": ["Die", "Brut", "von", "un\u00b7serm", "eig\u00b7nen", "Hirn", "als", "ei\u00b7ne", "Got\u00b7theit", "zu"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "PPOSAT", "ADJA", "NN", "KOKOM", "ART", "NN", "PTKZU"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.33": {"text": "erh\u00f6hn;", "tokens": ["er\u00b7h\u00f6hn", ";"], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.34": {"text": "Wir sprechen von der Menschen Wahrheit so, als ob diese", "tokens": ["Wir", "spre\u00b7chen", "von", "der", "Men\u00b7schen", "Wahr\u00b7heit", "so", ",", "als", "ob", "die\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "ART", "NN", "NN", "ADV", "$,", "KOKOM", "KOUS", "PDS"], "meter": "-+-+-+-+--+-+-", "measure": "iambic.hexa.relaxed"}, "line.35": {"text": "einerley", "tokens": ["ei\u00b7ner\u00b7ley"], "token_info": ["word"], "pos": ["PIAT"], "meter": "+-+", "measure": "trochaic.di"}, "line.36": {"text": "Mit der selbst\u00e4nd\u2019gen Wahrheit GOttes, und eben so zu", "tokens": ["Mit", "der", "selb\u00b7st\u00e4n\u00b7d'\u00b7gen", "Wahr\u00b7heit", "Got\u00b7tes", ",", "und", "e\u00b7ben", "so", "zu"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN", "NN", "$,", "KON", "ADV", "ADV", "APPR"], "meter": "--+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}, "line.37": {"text": "ehren sey.", "tokens": ["eh\u00b7ren", "sey", "."], "token_info": ["word", "word", "punct"], "pos": ["VVINF", "VAFIN", "$."], "meter": "+-+", "measure": "trochaic.di"}}}}}