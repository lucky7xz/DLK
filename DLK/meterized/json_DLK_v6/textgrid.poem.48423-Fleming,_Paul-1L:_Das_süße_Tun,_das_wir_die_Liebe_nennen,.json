{"textgrid.poem.48423": {"metadata": {"author": {"name": "Fleming, Paul", "birth": "N.A.", "death": "N.A."}, "title": "1L: Das s\u00fc\u00dfe Tun, das wir die Liebe nennen,", "genre": "verse", "period": "N.A.", "pub_year": 1624, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Das s\u00fc\u00dfe Tun, das wir die Liebe nennen,", "tokens": ["Das", "s\u00fc\u00b7\u00dfe", "Tun", ",", "das", "wir", "die", "Lie\u00b7be", "nen\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "den freien Dienst, den wundenlosen Streit,", "tokens": ["den", "frei\u00b7en", "Dienst", ",", "den", "wun\u00b7den\u00b7lo\u00b7sen", "Streit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "den besten Schmack, die Zuckerung der Zeit,", "tokens": ["den", "bes\u00b7ten", "Schmack", ",", "die", "Zu\u00b7cke\u00b7rung", "der", "Zeit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "den lieben Tod, das angenehme Brennen,", "tokens": ["den", "lie\u00b7ben", "Tod", ",", "das", "an\u00b7ge\u00b7neh\u00b7me", "Bren\u00b7nen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "und was wir sonst noch Bessers k\u00f6nnen kennen:", "tokens": ["und", "was", "wir", "sonst", "noch", "Bes\u00b7sers", "k\u00f6n\u00b7nen", "ken\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "ADV", "ADV", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "das leset hier, ihr Ha\u00df der Einsamkeit,", "tokens": ["das", "le\u00b7set", "hier", ",", "ihr", "Ha\u00df", "der", "Ein\u00b7sam\u00b7keit", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "$,", "PPOSAT", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "ihr edles Paar, die ihr gesonnen seid", "tokens": ["ihr", "ed\u00b7les", "Paar", ",", "die", "ihr", "ge\u00b7son\u00b7nen", "seid"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "PRELS", "PPER", "VVPP", "VAFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "zu gehen ein, was auch kein Tod kan trennen!", "tokens": ["zu", "ge\u00b7hen", "ein", ",", "was", "auch", "kein", "Tod", "kan", "tren\u00b7nen", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKZU", "VVINF", "PTKVZ", "$,", "PRELS", "ADV", "PIAT", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Gl\u00fcck zu! Gl\u00fcck zu! schreit meine Poesie,", "tokens": ["Gl\u00fcck", "zu", "!", "Gl\u00fcck", "zu", "!", "schreit", "mei\u00b7ne", "Poe\u00b7sie", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "PTKVZ", "$.", "NN", "PTKVZ", "$.", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.10": {"text": "wie schlecht sie ist. Zwar was ihr leset hie,", "tokens": ["wie", "schlecht", "sie", "ist", ".", "Zwar", "was", "ihr", "le\u00b7set", "hie", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PPER", "VAFIN", "$.", "ADV", "PWS", "PPER", "VVFIN", "ADV", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.11": {"text": "das ist nur Schrift und blo\u00dfes W\u00f6rterscherzen;", "tokens": ["das", "ist", "nur", "Schrift", "und", "blo\u00b7\u00dfes", "W\u00f6r\u00b7ter\u00b7scher\u00b7zen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADV", "NN", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "doch soll sie auch sein eine Zeigerin,", "tokens": ["doch", "soll", "sie", "auch", "sein", "ei\u00b7ne", "Zei\u00b7ge\u00b7rin", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "PPOSAT", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "da\u00df ich bereit euch aufzuwarten bin.", "tokens": ["da\u00df", "ich", "be\u00b7reit", "euch", "auf\u00b7zu\u00b7war\u00b7ten", "bin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJD", "PPER", "VVIZU", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "Den h\u00f6chsten Wundsch, den trag' ich noch im Herzen.", "tokens": ["Den", "h\u00f6chs\u00b7ten", "Wund\u00b7sch", ",", "den", "trag'", "ich", "noch", "im", "Her\u00b7zen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "ART", "VVFIN", "PPER", "ADV", "APPRART", "NN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "E.E. Gn. Gn.", "tokens": ["E.", "E.", "Gn", ".", "Gn", "."], "token_info": ["abbreviation", "abbreviation", "word", "punct", "word", "punct"], "pos": ["NE", "NE", "NE", "$.", "NN", "$."], "meter": "-+", "measure": "iambic.single"}, "line.2": {"text": "gehorsamber", "tokens": ["ge\u00b7hor\u00b7sam\u00b7ber"], "token_info": ["word"], "pos": ["NN"], "meter": "-+--", "measure": "dactylic.init"}}}}}