{"dta.poem.2165": {"metadata": {"author": {"name": "Brockes, Barthold Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Die verbesserte Welt.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1748", "urn": "urn:nbn:de:kobv:b4-200905198553", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Zum Denken scheint der Mensch geschaffen. Nun hat", "tokens": ["Zum", "Den\u00b7ken", "scheint", "der", "Mensch", "ge\u00b7schaf\u00b7fen", ".", "Nun", "hat"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPRART", "NN", "VVFIN", "ART", "NN", "VVPP", "$.", "ADV", "VAFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "die vordre Welt gedacht,", "tokens": ["die", "vor\u00b7dre", "Welt", "ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und, durch ihr so verschiednes Denken, viel Meynungen", "tokens": ["Und", ",", "durch", "ihr", "so", "ver\u00b7schied\u00b7nes", "Den\u00b7ken", ",", "viel", "Mey\u00b7nun\u00b7gen"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KON", "$,", "APPR", "PPER", "ADV", "ADJA", "NN", "$,", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "hervorgebracht;", "tokens": ["her\u00b7vor\u00b7ge\u00b7bracht", ";"], "token_info": ["word", "punct"], "pos": ["VVPP", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Wenn nun die jetzige vermerkt, wie oft die ersteren ge-", "tokens": ["Wenn", "nun", "die", "jet\u00b7zi\u00b7ge", "ver\u00b7merkt", ",", "wie", "oft", "die", "ers\u00b7te\u00b7ren", "ge"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "ADJA", "VVPP", "$,", "PWAV", "ADV", "ART", "ADJA", "TRUNC"], "meter": "-+--+--+-+-+-+-", "measure": "amphibrach.tri.plus"}, "line.6": {"text": "irrt,", "tokens": ["irrt", ","], "token_info": ["word", "punct"], "pos": ["VVFIN", "$,"], "meter": "+", "measure": "single.up"}, "line.7": {"text": "So scheinet, da\u00df doch diese Wahrheit, f\u00fcr uns, hier-", "tokens": ["So", "schei\u00b7net", ",", "da\u00df", "doch", "die\u00b7se", "Wahr\u00b7heit", ",", "f\u00fcr", "uns", ",", "hier"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "word"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "ADV", "PDAT", "NN", "$,", "APPR", "PPER", "$,", "TRUNC"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "aus entdecket wird:", "tokens": ["aus", "ent\u00b7de\u00b7cket", "wird", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "VVFIN", "VAFIN", "$."], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.9": {"text": "Da\u00df blo\u00df dadurch, da\u00df so viel Fehler, die wir erkannt,", "tokens": ["Da\u00df", "blo\u00df", "da\u00b7durch", ",", "da\u00df", "so", "viel", "Feh\u00b7ler", ",", "die", "wir", "er\u00b7kannt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PAV", "$,", "KOUS", "ADV", "PIAT", "NN", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "-+-+---+-+--+", "measure": "iambic.penta.chol"}, "line.10": {"text": "vermieden werden,", "tokens": ["ver\u00b7mie\u00b7den", "wer\u00b7den", ","], "token_info": ["word", "word", "punct"], "pos": ["VVPP", "VAINF", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.11": {"text": "Wir n\u00e4her zu der Wahrheit kommen, und da\u00df der Zu-", "tokens": ["Wir", "n\u00e4\u00b7her", "zu", "der", "Wahr\u00b7heit", "kom\u00b7men", ",", "und", "da\u00df", "der", "Zu"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADJD", "APPR", "ART", "NN", "VVINF", "$,", "KON", "KOUS", "ART", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "stand unsrer Erden", "tokens": ["stand", "uns\u00b7rer", "Er\u00b7den"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN"], "meter": "+--+-", "measure": "amphibrach.di.relaxed"}, "line.13": {"text": "Sich ganz gewi\u00df verbessert finde. Je mehr man Jrrweg", "tokens": ["Sich", "ganz", "ge\u00b7wi\u00df", "ver\u00b7bes\u00b7sert", "fin\u00b7de", ".", "Je", "mehr", "man", "Jrr\u00b7weg"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PRF", "ADV", "ADV", "VVPP", "VVFIN", "$.", "ADV", "ADV", "PIS", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.14": {"text": "meiden lernt,", "tokens": ["mei\u00b7den", "lernt", ","], "token_info": ["word", "word", "punct"], "pos": ["PIS", "VVFIN", "$,"], "meter": "+-+", "measure": "trochaic.di"}, "line.15": {"text": "Je mehr man Abweg\u2019 erst gewahr wird, und dann von ih-", "tokens": ["Je", "mehr", "man", "Ab\u00b7weg'", "erst", "ge\u00b7wahr", "wird", ",", "und", "dann", "von", "ih"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "PIS", "NN", "ADV", "ADJD", "VAFIN", "$,", "KON", "ADV", "APPR", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.16": {"text": "nen sich entfernt,", "tokens": ["nen", "sich", "ent\u00b7fernt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VVFIN", "PRF", "ADJD", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.17": {"text": "Je mehr er Nachricht von den Steigen, die abwerts lei-", "tokens": ["Je", "mehr", "er", "Nach\u00b7richt", "von", "den", "Stei\u00b7gen", ",", "die", "ab\u00b7werts", "lei"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "ADV", "PPER", "NN", "APPR", "ART", "NN", "$,", "PRELS", "ADV", "TRUNC"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.18": {"text": "ten, hat empfangen;", "tokens": ["ten", ",", "hat", "emp\u00b7fan\u00b7gen", ";"], "token_info": ["word", "punct", "word", "word", "punct"], "pos": ["FM", "$,", "VAFIN", "VVPP", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.19": {"text": "Je n\u00e4her wird ein Reisender zum rechten Weg und Zweck", "tokens": ["Je", "n\u00e4\u00b7her", "wird", "ein", "Rei\u00b7sen\u00b7der", "zum", "rech\u00b7ten", "Weg", "und", "Zweck"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "ADJD", "VAFIN", "ART", "NN", "APPRART", "ADJA", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.20": {"text": "gelangen.", "tokens": ["ge\u00b7lan\u00b7gen", "."], "token_info": ["word", "punct"], "pos": ["VVINF", "$."], "meter": "-+-", "measure": "amphibrach.single"}}}}}