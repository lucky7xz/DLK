{"textgrid.poem.57928": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Der Differenzbeschlu\u00df", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ihr lustigen B\u00fcrgervorsteher,", "tokens": ["Ihr", "lus\u00b7ti\u00b7gen", "B\u00fcr\u00b7ger\u00b7vor\u00b7ste\u00b7her", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Seid ihr alle zusammen?", "tokens": ["Seid", "ihr", "al\u00b7le", "zu\u00b7sam\u00b7men", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAIMP", "PPER", "PIS", "PTKVZ", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Ei so lasset uns fahren", "tokens": ["Ei", "so", "las\u00b7set", "uns", "fah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "VVFIN", "PPER", "VVINF"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Im Stra\u00dfenbahnwagen", "tokens": ["Im", "Stra\u00b7\u00dfen\u00b7bahn\u00b7wa\u00b7gen"], "token_info": ["word", "word"], "pos": ["APPRART", "NN"], "meter": "-+-+--", "measure": "unknown.measure.di"}, "line.5": {"text": "Zum Dreim\u00e4nnerquartier,", "tokens": ["Zum", "Drei\u00b7m\u00e4n\u00b7ner\u00b7quar\u00b7tier", ","], "token_info": ["word", "word", "punct"], "pos": ["APPRART", "NN", "$,"], "meter": "--+--+", "measure": "anapaest.di.plus"}, "line.6": {"text": "Lust'ge B\u00fcrgervorsteh'r seien wir.", "tokens": ["Lust'\u00b7ge", "B\u00fcr\u00b7ger\u00b7vor\u00b7steh'r", "sei\u00b7en", "wir", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PPER", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.2": {"line.1": {"text": "Unser Hauptmann hat uns wohl bedacht,", "tokens": ["Un\u00b7ser", "Haupt\u00b7mann", "hat", "uns", "wohl", "be\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.2": {"text": "Die Weinkarte hergebracht,", "tokens": ["Die", "Wein\u00b7kar\u00b7te", "her\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Den Bleistift zum Schreiben,", "tokens": ["Den", "Blei\u00b7stift", "zum", "Schrei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "So la\u00dft's uns denn treiben", "tokens": ["So", "la\u00dft's", "uns", "denn", "trei\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "VVINF"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Zu Lust und Pl\u00e4sier,", "tokens": ["Zu", "Lust", "und", "Pl\u00e4\u00b7sier", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Lust'ge B\u00fcrgervorsteh'r seien wir.", "tokens": ["Lust'\u00b7ge", "B\u00fcr\u00b7ger\u00b7vor\u00b7steh'r", "sei\u00b7en", "wir", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PPER", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Und als wir kamen in den Sitzungssaal,", "tokens": ["Und", "als", "wir", "ka\u00b7men", "in", "den", "Sit\u00b7zungs\u00b7saal", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Da gab es einen Mordskandal,", "tokens": ["Da", "gab", "es", "ei\u00b7nen", "Mords\u00b7kan\u00b7dal", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die beiden Parteien", "tokens": ["Die", "bei\u00b7den", "Par\u00b7tei\u00b7en"], "token_info": ["word", "word", "word"], "pos": ["ART", "PIAT", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Woll'n sich \u00fcberschreien:", "tokens": ["Woll'n", "sich", "\u00fc\u00b7bersc\u00b7hrei\u00b7en", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "PRF", "VVINF", "$."], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "Gilt es mir oder gilt es dir?", "tokens": ["Gilt", "es", "mir", "o\u00b7der", "gilt", "es", "dir", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "KON", "VVFIN", "PPER", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Lust'ge B\u00fcrgervorsteh'r seien wir.", "tokens": ["Lust'\u00b7ge", "B\u00fcr\u00b7ger\u00b7vor\u00b7steh'r", "sei\u00b7en", "wir", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "PPER", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.4": {"line.1": {"text": "Es hat sich das Bl\u00e4ttlein", "tokens": ["Es", "hat", "sich", "das", "Bl\u00e4tt\u00b7lein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "PRF", "ART", "NN"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.2": {"text": "Schon zweimal gewendet,", "tokens": ["Schon", "zwei\u00b7mal", "ge\u00b7wen\u00b7det", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Schon zweimal gewendet,", "tokens": ["Schon", "zwei\u00b7mal", "ge\u00b7wen\u00b7det", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Noch ist's nicht beendet,", "tokens": ["Noch", "ist's", "nicht", "be\u00b7en\u00b7det", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PTKNEG", "VVPP", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.5": {"text": "Neugierig ist die ganze Stadt,", "tokens": ["Neu\u00b7gie\u00b7rig", "ist", "die", "gan\u00b7ze", "Stadt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wo es diesmal eingeschlagen hat.", "tokens": ["Wo", "es", "dies\u00b7mal", "ein\u00b7ge\u00b7schla\u00b7gen", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADV", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.5": {"line.1": {"text": "O seht nur, wie so niedlich", "tokens": ["O", "seht", "nur", ",", "wie", "so", "nied\u00b7lich"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "VVFIN", "ADV", "$,", "PWAV", "ADV", "ADJD"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Wir hin und her schwanken,", "tokens": ["Wir", "hin", "und", "her", "schwan\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "KON", "ADV", "VVINF", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.3": {"text": "Fest stehn auf den Beinen", "tokens": ["Fest", "stehn", "auf", "den", "Bei\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "APPR", "ART", "NN"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Sieht kaum man noch einen,", "tokens": ["Sieht", "kaum", "man", "noch", "ei\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PIS", "ADV", "ART", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.5": {"text": "Dem Volk macht's Pl\u00e4sier,", "tokens": ["Dem", "Volk", "macht's", "Pl\u00e4\u00b7sier", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NN", "$,"], "meter": "-+-+-", "measure": "iambic.di"}, "line.6": {"text": "Lust'ge B\u00fcrgervorsteh'r, das sein wir.", "tokens": ["Lust'\u00b7ge", "B\u00fcr\u00b7ger\u00b7vor\u00b7steh'r", ",", "das", "sein", "wir", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "PRELS", "PPOSAT", "PPER", "$."], "meter": "+-+--+--+", "measure": "trochaic.tetra.relaxed"}}}}}