{"textgrid.poem.43117": {"metadata": {"author": {"name": "Christen, Ada", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wie seltsam! Unser feiger Muth", "genre": "verse", "period": "N.A.", "pub_year": 1870, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie seltsam! Unser feiger Muth", "tokens": ["Wie", "selt\u00b7sam", "!", "Un\u00b7ser", "fei\u00b7ger", "Muth"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "ADJD", "$.", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "L\u00e4\u00dft alles Elend uns tragen;", "tokens": ["L\u00e4\u00dft", "al\u00b7les", "E\u00b7lend", "uns", "tra\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "PPER", "VVINF", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "O h\u00e4tten wir doch den echten Muth,", "tokens": ["O", "h\u00e4t\u00b7ten", "wir", "doch", "den", "ech\u00b7ten", "Muth", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "ADV", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Das l\u00f6sende Wort zu sagen.", "tokens": ["Das", "l\u00f6\u00b7sen\u00b7de", "Wort", "zu", "sa\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}}, "stanza.2": {"line.1": {"text": "Wir laufen neben einander her", "tokens": ["Wir", "lau\u00b7fen", "ne\u00b7ben", "ein\u00b7an\u00b7der", "her"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "APPR", "PRF", "APZR"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und werden m\u00fcder und m\u00fcder;", "tokens": ["Und", "wer\u00b7den", "m\u00fc\u00b7der", "und", "m\u00fc\u00b7der", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Ich werde bl\u00e4sser und kr\u00e4nker stets", "tokens": ["Ich", "wer\u00b7de", "bl\u00e4s\u00b7ser", "und", "kr\u00e4n\u00b7ker", "stets"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "KON", "ADJD", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und du wirst k\u00e4lter und r\u00fcder.", "tokens": ["Und", "du", "wirst", "k\u00e4l\u00b7ter", "und", "r\u00fc\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.3": {"line.1": {"text": "O raffe dich auf und fasse Muth", "tokens": ["O", "raf\u00b7fe", "dich", "auf", "und", "fas\u00b7se", "Muth"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "PPER", "PTKVZ", "KON", "ADJA", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und sei zum letzten Mal ein Mann.", "tokens": ["Und", "sei", "zum", "letz\u00b7ten", "Mal", "ein", "Mann", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPRART", "ADJA", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was ich nicht l\u00e4nger tragen kann!", "tokens": ["Was", "ich", "nicht", "l\u00e4n\u00b7ger", "tra\u00b7gen", "kann", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "PTKNEG", "ADJD", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}