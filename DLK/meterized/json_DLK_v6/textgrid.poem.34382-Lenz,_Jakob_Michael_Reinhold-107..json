{"textgrid.poem.34382": {"metadata": {"author": {"name": "Lenz, Jakob Michael Reinhold", "birth": "N.A.", "death": "N.A."}, "title": "107.", "genre": "verse", "period": "N.A.", "pub_year": 1771, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Auf einen Menschenrumpf den Kopf des Pferdes passen,", "tokens": ["Auf", "ei\u00b7nen", "Men\u00b7schen\u00b7rumpf", "den", "Kopf", "des", "Pfer\u00b7des", "pas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ist wie Horaz uns lehrt, dem Dichter nicht verg\u00f6nnt.", "tokens": ["Ist", "wie", "Ho\u00b7raz", "uns", "lehrt", ",", "dem", "Dich\u00b7ter", "nicht", "ver\u00b7g\u00f6nnt", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KOKOM", "NE", "PPER", "VVFIN", "$,", "ART", "NN", "PTKNEG", "VVPP", "$."], "meter": "--+--+-+-+-+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Doch hat Homer, den man daf\u00fcr erkennt,", "tokens": ["Doch", "hat", "Ho\u00b7mer", ",", "den", "man", "da\u00b7f\u00fcr", "er\u00b7kennt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "NE", "$,", "PRELS", "PIS", "PAV", "VVFIN", "$,"], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}, "line.4": {"text": "Durch Cirzen seine Freund' in B\u00e4ren wandeln lassen.", "tokens": ["Durch", "Cir\u00b7zen", "sei\u00b7ne", "Freund'", "in", "B\u00e4\u00b7ren", "wan\u00b7deln", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PPOSAT", "NN", "APPR", "NN", "VVFIN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Hat er dabei gedacht? Hat er die edlen Rollen", "tokens": ["Hat", "er", "da\u00b7bei", "ge\u00b7dacht", "?", "Hat", "er", "die", "ed\u00b7len", "Rol\u00b7len"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PPER", "PAV", "VVPP", "$.", "VAFIN", "PPER", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Der Helden am Ily\u00df dadurch verspotten wollen?", "tokens": ["Der", "Hel\u00b7den", "am", "I\u00b7ly\u00df", "da\u00b7durch", "ver\u00b7spot\u00b7ten", "wol\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "NN", "PAV", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das ist undenkbar. Peinigen", "tokens": ["Das", "ist", "un\u00b7denk\u00b7bar", ".", "Pei\u00b7ni\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["PDS", "VAFIN", "ADJD", "$.", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Der S\u00e4nger aus M\u00e4onien", "tokens": ["Der", "S\u00e4n\u00b7ger", "aus", "M\u00e4o\u00b7ni\u00b7en"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "NN"], "meter": "-+--+--", "measure": "iambic.di.relaxed"}, "line.5": {"text": "Personen die er sch\u00e4tzt? ", "tokens": ["Per\u00b7so\u00b7nen", "die", "er", "sch\u00e4tzt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Der Fr\u00f6sch und M\u00e4usekrieg ist \u00e4hnliche Satyre.", "tokens": ["Der", "Fr\u00f6sch", "und", "M\u00e4u\u00b7se\u00b7krieg", "ist", "\u00e4hn\u00b7li\u00b7che", "Sa\u00b7ty\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VAFIN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Auf wen? Auf seine Freund'? Auf seine Feinde? Nein", "tokens": ["Auf", "wen", "?", "Auf", "sei\u00b7ne", "Freund'", "?", "Auf", "sei\u00b7ne", "Fein\u00b7de", "?", "Nein"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["APPR", "PWS", "$.", "APPR", "PPOSAT", "NN", "$.", "APPR", "PPOSAT", "NN", "$.", "PTKANT"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Mich deucht, es kann ein Fall, wo keines statt hat, seyn \u2013", "tokens": ["Mich", "deucht", ",", "es", "kann", "ein", "Fall", ",", "wo", "kei\u00b7nes", "statt", "hat", ",", "seyn", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PPER", "VMFIN", "ART", "NN", "$,", "PWAV", "PIS", "VVPP", "VAFIN", "$,", "VAINF", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Sonst w\u00e4r er selbst das gr\u00f6sseste der Thiere.", "tokens": ["Sonst", "w\u00e4r", "er", "selbst", "das", "gr\u00f6s\u00b7ses\u00b7te", "der", "Thie\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ART", "ADJA", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Man hat geschliffne Gl\u00e4ser die", "tokens": ["Man", "hat", "ge\u00b7schliff\u00b7ne", "Gl\u00e4\u00b7ser", "die"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VAFIN", "ADJA", "NN", "ART"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Uns selbst das Sch\u00f6nste so verzogen", "tokens": ["Uns", "selbst", "das", "Sch\u00f6ns\u00b7te", "so", "ver\u00b7zo\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "ART", "NN", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Verzerret weisen, da\u00df wir nie", "tokens": ["Ver\u00b7zer\u00b7ret", "wei\u00b7sen", ",", "da\u00df", "wir", "nie"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "VVINF", "$,", "KOUS", "PPER", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dran denken, dieses Bild ist vorsetzlich gelogen,", "tokens": ["Dran", "den\u00b7ken", ",", "die\u00b7ses", "Bild", "ist", "vor\u00b7setz\u00b7lich", "ge\u00b7lo\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVINF", "$,", "PDAT", "NN", "VAFIN", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-++--+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Um uns nach Kummer, Tr\u00e4nen, Wachen", "tokens": ["Um", "uns", "nach", "Kum\u00b7mer", ",", "Tr\u00e4\u00b7nen", ",", "Wa\u00b7chen"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word"], "pos": ["KOUI", "PPER", "APPR", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Durch ein recht herzlich biedres Lachen", "tokens": ["Durch", "ein", "recht", "herz\u00b7lich", "bie\u00b7dres", "La\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ADV", "ADJD", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Die Galle und die Milz ein wenig leicht zu machen.", "tokens": ["Die", "Gal\u00b7le", "und", "die", "Milz", "ein", "we\u00b7nig", "leicht", "zu", "ma\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "ART", "PIS", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Ein solcher Spiegel ist die Poesie", "tokens": ["Ein", "sol\u00b7cher", "Spie\u00b7gel", "ist", "die", "Poe\u00b7sie"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIAT", "NN", "VAFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Von einem launigten Genie.", "tokens": ["Von", "ei\u00b7nem", "lau\u00b7nig\u00b7ten", "Ge\u00b7nie", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+---+", "measure": "unknown.measure.tri"}}, "stanza.5": {"line.1": {"text": "Und hat man wohl auf dieser Erden", "tokens": ["Und", "hat", "man", "wohl", "auf", "die\u00b7ser", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "PIS", "ADV", "APPR", "PDAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Was lustigers gesehn, als \u2013 b\u00f6se drauf zu werden?", "tokens": ["Was", "lus\u00b7ti\u00b7gers", "ge\u00b7sehn", ",", "als", "\u2013", "b\u00f6\u00b7se", "drauf", "zu", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VVPP", "$,", "KOUS", "$(", "ADJD", "PAV", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ja auf den ", "tokens": ["Ja", "auf", "den"], "token_info": ["word", "word", "word"], "pos": ["PTKANT", "APPR", "ART"], "meter": "-+-", "measure": "amphibrach.single"}, "line.4": {"text": "Und sagt, der Spiegel sey nicht konisch,", "tokens": ["Und", "sagt", ",", "der", "Spie\u00b7gel", "sey", "nicht", "ko\u00b7nisch", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "ART", "NN", "VAFIN", "PTKNEG", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Er sey getreu, kurz der auf gut ", "tokens": ["Er", "sey", "ge\u00b7treu", ",", "kurz", "der", "auf", "gut"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "ADJD", "ART", "APPR", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Mit Gassenbubenschn\u00f6rkeln beizt.", "tokens": ["Mit", "Gas\u00b7sen\u00b7bu\u00b7ben\u00b7schn\u00f6r\u00b7keln", "beizt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Anwenden was ins gro\u00dfe Blaue", "tokens": ["An\u00b7wen\u00b7den", "was", "ins", "gro\u00b7\u00dfe", "Blau\u00b7e"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "PWS", "APPRART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Hineingeschrieben ward, sey's Lust\u2013, sey's Trauerspiel,", "tokens": ["Hin\u00b7ein\u00b7ge\u00b7schrie\u00b7ben", "ward", ",", "sey's", "Lust", "\u2013", ",", "sey's", "Trau\u00b7er\u00b7spiel", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$,", "ADJA", "NN", "$(", "$,", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Sey'n Laster vorgestellt, sey Thorheit, Schwachheit, Ziel", "tokens": ["Sey'n", "Las\u00b7ter", "vor\u00b7ge\u00b7stellt", ",", "sey", "Thor\u00b7heit", ",", "Schwach\u00b7heit", ",", "Ziel"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word"], "pos": ["PPOSAT", "NN", "VVPP", "$,", "VAFIN", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Der Uebertretungen, ist \u2013 da\u00df ich dir's vertraue", "tokens": ["Der", "Ue\u00b7bert\u00b7re\u00b7tun\u00b7gen", ",", "ist", "\u2013", "da\u00df", "ich", "dir's", "ver\u00b7trau\u00b7e"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "VAFIN", "$(", "KOUS", "PPER", "PIS", "VVFIN"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.5": {"text": "Bescheidner Philosoph! \u2013 des Ungeheurs am Nil", "tokens": ["Be\u00b7scheid\u00b7ner", "Phi\u00b7lo\u00b7soph", "!", "\u2013", "des", "Un\u00b7ge\u00b7heurs", "am", "Nil"], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "$.", "$(", "ART", "NN", "APPRART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Das schreiet wie ein Kind und Menschen frisset \u2013 Sache.", "tokens": ["Das", "schrei\u00b7et", "wie", "ein", "Kind", "und", "Men\u00b7schen", "fris\u00b7set", "\u2013", "Sa\u00b7che", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PDS", "VVFIN", "KOKOM", "ART", "NN", "KON", "NN", "VVFIN", "$(", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ist's denn des Messers Schuld, wenn ich's zum Mordschwerdt mache?", "tokens": ["Ist's", "denn", "des", "Mes\u00b7sers", "Schuld", ",", "wenn", "ich's", "zum", "Mord\u00b7schwerdt", "ma\u00b7che", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "ART", "NN", "NN", "$,", "KOUS", "PIS", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "Wozu die Messer \u00fcberhaupt?", "tokens": ["Wo\u00b7zu", "die", "Mes\u00b7ser", "\u00fc\u00b7ber\u00b7haupt", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ruft Orgon, kann man nicht mit blo\u00dfen H\u00e4nden essen?", "tokens": ["Ruft", "Or\u00b7gon", ",", "kann", "man", "nicht", "mit", "blo\u00b7\u00dfen", "H\u00e4n\u00b7den", "es\u00b7sen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NN", "$,", "VMFIN", "PIS", "PTKNEG", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Das steht den Herren frei. Doch uns erlaubt", "tokens": ["Das", "steht", "den", "Her\u00b7ren", "frei", ".", "Doch", "uns", "er\u00b7laubt"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ART", "NN", "PTKVZ", "$.", "KON", "PPER", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Wird's gleichfalls seyn, mit Tartarn nicht zu speisen,", "tokens": ["Wird's", "gleich\u00b7falls", "seyn", ",", "mit", "Tar\u00b7tarn", "nicht", "zu", "spei\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "VAINF", "$,", "APPR", "NN", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Die gar gerittnes Fleisch vom Sattel fressen,", "tokens": ["Die", "gar", "ge\u00b7ritt\u00b7nes", "Fleisch", "vom", "Sat\u00b7tel", "fres\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ADJA", "NN", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Mit Z\u00e4hnen das Halbrohe wild zerrei\u00dfen.", "tokens": ["Mit", "Z\u00e4h\u00b7nen", "das", "Hal\u00b7bro\u00b7he", "wild", "zer\u00b7rei\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ART", "NN", "ADJD", "VVINF", "$."], "meter": "-+--++-+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.8": {"line.1": {"text": "So geht's, da\u00df ich die Klinge nicht verliere,", "tokens": ["So", "geht's", ",", "da\u00df", "ich", "die", "Klin\u00b7ge", "nicht", "ver\u00b7lie\u00b7re", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "PPER", "ART", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Fast buchst\u00e4blich mit der Satyre.", "tokens": ["Fast", "buch\u00b7st\u00e4b\u00b7lich", "mit", "der", "Sa\u00b7ty\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "ART", "NN", "$."], "meter": "--+-+--+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "Es giebt Gelegenheiten gnug,", "tokens": ["Es", "giebt", "Ge\u00b7le\u00b7gen\u00b7hei\u00b7ten", "gnug", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wo sich der Menschenwitz verwirrte,", "tokens": ["Wo", "sich", "der", "Men\u00b7schen\u00b7witz", "ver\u00b7wirr\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Und weil noch nie ein Mensch erkannt hat, da\u00df er irrte,", "tokens": ["Und", "weil", "noch", "nie", "ein", "Mensch", "er\u00b7kannt", "hat", ",", "da\u00df", "er", "irr\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "ADV", "ART", "NN", "VVPP", "VAFIN", "$,", "KOUS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Den Edlen oft in schwere Fesseln schlug.", "tokens": ["Den", "Ed\u00b7len", "oft", "in", "schwe\u00b7re", "Fes\u00b7seln", "schlug", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.7": {"text": "Bei den geh\u00e4uften Widerspr\u00fcchen", "tokens": ["Bei", "den", "ge\u00b7h\u00e4uf\u00b7ten", "Wi\u00b7der\u00b7spr\u00fc\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Von Stellungen und Reibungen", "tokens": ["Von", "Stel\u00b7lun\u00b7gen", "und", "Rei\u00b7bun\u00b7gen"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN"], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.9": {"text": "Gabs immer Uebertreibungen", "tokens": ["Gabs", "im\u00b7mer", "Ue\u00b7bert\u00b7rei\u00b7bun\u00b7gen"], "token_info": ["word", "word", "word"], "pos": ["NE", "ADV", "NN"], "meter": "+--+-++-", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Und tausend Stoff zum L\u00e4cherlichen.", "tokens": ["Und", "tau\u00b7send", "Stoff", "zum", "L\u00e4\u00b7cher\u00b7li\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "NN", "APPRART", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.11": {"text": "W\u00e4r' da die Gei\u00dfel nicht, mit der ein G\u00f6tterarm", "tokens": ["W\u00e4r'", "da", "die", "Gei\u00b7\u00dfel", "nicht", ",", "mit", "der", "ein", "G\u00f6t\u00b7ter\u00b7arm"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["VAFIN", "ADV", "ART", "NN", "PTKNEG", "$,", "APPR", "PRELS", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Der Hauptstadt Tempel selbst gereinigt,", "tokens": ["Der", "Haupt\u00b7stadt", "Tem\u00b7pel", "selbst", "ge\u00b7rei\u00b7nigt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Wohin die Wucherer gelaufen", "tokens": ["Wo\u00b7hin", "die", "Wu\u00b7che\u00b7rer", "ge\u00b7lau\u00b7fen"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.14": {"text": "Um zu verkaufen und zu kaufen:", "tokens": ["Um", "zu", "ver\u00b7kau\u00b7fen", "und", "zu", "kau\u00b7fen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PTKZU", "VVINF", "KON", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.15": {"text": "Die edelste Natur, gepeinigt", "tokens": ["Die", "e\u00b7dels\u00b7te", "Na\u00b7tur", ",", "ge\u00b7pei\u00b7nigt"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "ADJA", "NN", "$,", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.16": {"text": "Erl\u00e4ge dem verw\u00fcnschten Schwarm", "tokens": ["Er\u00b7l\u00e4\u00b7ge", "dem", "ver\u00b7w\u00fcnschten", "Schwarm"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ART", "ADJA", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.17": {"text": "Vom Leiden und dem ewgen Harm,", "tokens": ["Vom", "Lei\u00b7den", "und", "dem", "ew\u00b7gen", "Harm", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "KON", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Womit uns Eigensinn und Wuth der Thorheit steinigt.", "tokens": ["Wo\u00b7mit", "uns", "Ei\u00b7gen\u00b7sinn", "und", "Wuth", "der", "Thor\u00b7heit", "stei\u00b7nigt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "NN", "KON", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.9": {"line.1": {"text": "Dergleichen Stimmungen zum voraus zu verh\u00fcten", "tokens": ["Derg\u00b7lei\u00b7chen", "Stim\u00b7mun\u00b7gen", "zum", "vo\u00b7raus", "zu", "ver\u00b7h\u00fc\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIS", "NN", "APPRART", "ADV", "PTKZU", "VVINF"], "meter": "-+-+--+-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Bleibt allemal auch Pflicht: denn wer kann sich gebieten,", "tokens": ["Bleibt", "al\u00b7le\u00b7mal", "auch", "Pflicht", ":", "denn", "wer", "kann", "sich", "ge\u00b7bie\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ADV", "NN", "$.", "KON", "PWS", "VMFIN", "PRF", "VVINF", "$,"], "meter": "-+-+-+-++--+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Da\u00df, wenn man Hand und Fu\u00df ihm in die Folter schr\u00e4nkt,", "tokens": ["Da\u00df", ",", "wenn", "man", "Hand", "und", "Fu\u00df", "ihm", "in", "die", "Fol\u00b7ter", "schr\u00e4nkt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "KOUS", "PIS", "NN", "KON", "NN", "PPER", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Er, wie gew\u00f6hnlich spricht und denkt.", "tokens": ["Er", ",", "wie", "ge\u00b7w\u00f6hn\u00b7lich", "spricht", "und", "denkt", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "$,", "PWAV", "ADJD", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Verbrechen selbst kann diese Pflicht, die kr\u00e4nkt,", "tokens": ["Ver\u00b7bre\u00b7chen", "selbst", "kann", "die\u00b7se", "Pflicht", ",", "die", "kr\u00e4nkt", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["NN", "ADV", "VMFIN", "PDAT", "NN", "$,", "PRELS", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.6": {"text": "Doch nur zu kr\u00e4nken scheint, um Kr\u00e4nkung vorzubeugen,", "tokens": ["Doch", "nur", "zu", "kr\u00e4n\u00b7ken", "scheint", ",", "um", "Kr\u00e4n\u00b7kung", "vor\u00b7zu\u00b7beu\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "PTKZU", "VVINF", "VVFIN", "$,", "KOUI", "NN", "VVIZU", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Abwenden, und dem Thor der Weisheit Pfade zeigen.", "tokens": ["Ab\u00b7wen\u00b7den", ",", "und", "dem", "Thor", "der", "Weis\u00b7heit", "Pfa\u00b7de", "zei\u00b7gen", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "KON", "ART", "NN", "ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.10": {"line.1": {"text": "Was ist begl\u00fcckender als wahre Gottesfurcht?", "tokens": ["Was", "ist", "be\u00b7gl\u00fc\u00b7cken\u00b7der", "als", "wah\u00b7re", "Got\u00b7tes\u00b7furcht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ADJD", "KOKOM", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Was tr\u00f6stender im Sterben und im Leben?", "tokens": ["Was", "tr\u00f6s\u00b7ten\u00b7der", "im", "Ster\u00b7ben", "und", "im", "Le\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADJD", "APPRART", "NN", "KON", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Was kann der Stirn, die Sorge kr\u00e4nkt und furcht,", "tokens": ["Was", "kann", "der", "Stirn", ",", "die", "Sor\u00b7ge", "kr\u00e4nkt", "und", "furcht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VMFIN", "ART", "NN", "$,", "ART", "NN", "VVFIN", "KON", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Das Siegel G\u00f6tterhauchs und Abkunft wiedergeben?", "tokens": ["Das", "Sie\u00b7gel", "G\u00f6t\u00b7ter\u00b7hauchs", "und", "Ab\u00b7kunft", "wie\u00b7der\u00b7ge\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Doch giebt's erb\u00e4rmlicher's wohl was in der Natur", "tokens": ["Doch", "giebt's", "er\u00b7b\u00e4rm\u00b7li\u00b7cher's", "wohl", "was", "in", "der", "Na\u00b7tur"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "PIS", "ADV", "PWS", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Als einen Menschen zu dem Affen", "tokens": ["Als", "ei\u00b7nen", "Men\u00b7schen", "zu", "dem", "Af\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Von unsrer Neigungen Gewohnheit umzuschaffen?", "tokens": ["Von", "uns\u00b7rer", "Nei\u00b7gun\u00b7gen", "Ge\u00b7wohn\u00b7heit", "um\u00b7zu\u00b7schaf\u00b7fen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "VVIZU", "$."], "meter": "-+-+---+-+-+-", "measure": "unknown.measure.penta"}, "line.8": {"text": "Und die ", "tokens": ["Und", "die"], "token_info": ["word", "word"], "pos": ["KON", "ART"], "meter": "+-", "measure": "trochaic.single"}, "line.9": {"text": "Doch L\u00e4nder \u2013 Welten schon mit Menschenblut beschwemmt,", "tokens": ["Doch", "L\u00e4n\u00b7der", "\u2013", "Wel\u00b7ten", "schon", "mit", "Men\u00b7schen\u00b7blut", "be\u00b7schwemmt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$(", "NN", "ADV", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Weil sie der kalte Ernst der Weisheit nie ged\u00e4mmt,", "tokens": ["Weil", "sie", "der", "kal\u00b7te", "Ernst", "der", "Weis\u00b7heit", "nie", "ge\u00b7d\u00e4mmt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "ADJA", "NN", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und seit der ersten Sonnenuhr", "tokens": ["Und", "seit", "der", "ers\u00b7ten", "Son\u00b7ne\u00b7nuhr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Ein Mensch der Gott zu seyn vom andern stets begehrte", "tokens": ["Ein", "Mensch", "der", "Gott", "zu", "seyn", "vom", "an\u00b7dern", "stets", "be\u00b7gehr\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ART", "NN", "PTKZU", "VAINF", "APPRART", "ADJA", "ADV", "ADJA"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Und allen seinen Zorn stets auf den Bruder leerte,", "tokens": ["Und", "al\u00b7len", "sei\u00b7nen", "Zorn", "stets", "auf", "den", "Bru\u00b7der", "leer\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "PPOSAT", "NN", "ADV", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Wenn ihm was Unrechts wo entfuhr.", "tokens": ["Wenn", "ihm", "was", "Un\u00b7rechts", "wo", "ent\u00b7fuhr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIS", "NN", "PWAV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Horaz nennt jedes Nachbild Vieh.", "tokens": ["Ho\u00b7raz", "nennt", "je\u00b7des", "Nach\u00b7bild", "Vieh", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "VVFIN", "PIAT", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Unrecht, scheints. Die Noth, die Sympathie", "tokens": ["Mit", "Un\u00b7recht", ",", "scheints", ".", "Die", "Noth", ",", "die", "Sym\u00b7pa\u00b7thie"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "word"], "pos": ["APPR", "NN", "$,", "VVFIN", "$.", "ART", "NN", "$,", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Zwingt hundert Selbstgenies auf Erden", "tokens": ["Zwingt", "hun\u00b7dert", "Selbst\u00b7ge\u00b7nies", "auf", "Er\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "CARD", "NN", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Nachbilder fremden Werths zu werden.", "tokens": ["Nach\u00b7bil\u00b7der", "frem\u00b7den", "Werths", "zu", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "PTKZU", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wer einen gleichen Weg zu gleicher Tagszeit macht,", "tokens": ["Wer", "ei\u00b7nen", "glei\u00b7chen", "Weg", "zu", "glei\u00b7cher", "Tags\u00b7zeit", "macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ein \u00e4hnliches Gesch\u00e4ft zu treiben hat, und Freunde", "tokens": ["Ein", "\u00e4hn\u00b7li\u00b7ches", "Ge\u00b7sch\u00e4ft", "zu", "trei\u00b7ben", "hat", ",", "und", "Freun\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "PTKZU", "VVINF", "VAFIN", "$,", "KON", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So wie der andre findt, der hat auf keine Feinde", "tokens": ["So", "wie", "der", "and\u00b7re", "findt", ",", "der", "hat", "auf", "kei\u00b7ne", "Fein\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "ART", "PIS", "VVFIN", "$,", "PRELS", "VAFIN", "APPR", "PIAT", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Die ihn den Affen nennen, Acht.", "tokens": ["Die", "ihn", "den", "Af\u00b7fen", "nen\u00b7nen", ",", "Acht", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "PPER", "ART", "NN", "VVINF", "$,", "CARD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Doch seine Neigungen nach fremden Modeln wandeln,", "tokens": ["Doch", "sei\u00b7ne", "Nei\u00b7gun\u00b7gen", "nach", "frem\u00b7den", "Mo\u00b7deln", "wan\u00b7deln", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "APPR", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.10": {"text": "Hei\u00dft, meiner Meinung nach, zu eignem Schaden handeln,", "tokens": ["Hei\u00dft", ",", "mei\u00b7ner", "Mei\u00b7nung", "nach", ",", "zu", "eig\u00b7nem", "Scha\u00b7den", "han\u00b7deln", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "PPOSAT", "NN", "APPR", "$,", "APPR", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Denn man verliert dadurch das was uns unterscheidt,", "tokens": ["Denn", "man", "ver\u00b7liert", "da\u00b7durch", "das", "was", "uns", "un\u00b7ter\u00b7scheidt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "PAV", "ART", "PRELS", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "All' unsern Menschenwerth und unsre Freudigkeit.", "tokens": ["All'", "un\u00b7sern", "Men\u00b7schen\u00b7werth", "und", "uns\u00b7re", "Freu\u00b7dig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "PPOSAT", "NN", "KON", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.13": {"text": "Der Eifrer aber will uns in Copey verwandeln", "tokens": ["Der", "Eif\u00b7rer", "a\u00b7ber", "will", "uns", "in", "Co\u00b7pey", "ver\u00b7wan\u00b7deln"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VMFIN", "PPER", "APPR", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Oft bey Verlust der Seligkeit.", "tokens": ["Oft", "bey", "Ver\u00b7lust", "der", "Se\u00b7lig\u00b7keit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.15": {"text": "Er nimmt uns dann das Bild, so Gott uns anerschaffen", "tokens": ["Er", "nimmt", "uns", "dann", "das", "Bild", ",", "so", "Gott", "uns", "a\u00b7ner\u00b7schaf\u00b7fen"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ART", "NN", "$,", "ADV", "NN", "PPER", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.16": {"text": "Und stempelt's um zum Bilde eines Affen. \u2013", "tokens": ["Und", "stem\u00b7pelt's", "um", "zum", "Bil\u00b7de", "ei\u00b7nes", "Af\u00b7fen", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "APPR", "APPRART", "NN", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.17": {"text": "Das hei\u00df' ich Afterfr\u00f6mmigkeit!", "tokens": ["Das", "hei\u00df'", "ich", "Af\u00b7ter\u00b7fr\u00f6m\u00b7mig\u00b7keit", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Die M\u00e4urer und die Moralisten", "tokens": ["Die", "M\u00e4u\u00b7rer", "und", "die", "Mo\u00b7ra\u00b7lis\u00b7ten"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "KON", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und viele selbstgenannte Christen", "tokens": ["Und", "vie\u00b7le", "selbst\u00b7ge\u00b7nann\u00b7te", "Chris\u00b7ten"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Schreyn wider Leidenschaft. Ihr Schreyn", "tokens": ["Schreyn", "wi\u00b7der", "Lei\u00b7den\u00b7schaft", ".", "Ihr", "Schreyn"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["NN", "APPR", "NN", "$.", "PPOSAT", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Soll einer Jugend, die noch au\u00dfer kleinen R\u00e4nken,", "tokens": ["Soll", "ei\u00b7ner", "Ju\u00b7gend", ",", "die", "noch", "au\u00b7\u00dfer", "klei\u00b7nen", "R\u00e4n\u00b7ken", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "$,", "PRELS", "ADV", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Verr\u00e4thereyn und Knabenschw\u00e4nken", "tokens": ["Ver\u00b7r\u00e4\u00b7the\u00b7reyn", "und", "Kna\u00b7ben\u00b7schw\u00e4n\u00b7ken"], "token_info": ["word", "word", "word"], "pos": ["NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Nicht wei\u00df, was f\u00fcr ein Ding die Leidenschaft doch ist,", "tokens": ["Nicht", "wei\u00df", ",", "was", "f\u00fcr", "ein", "Ding", "die", "Lei\u00b7den\u00b7schaft", "doch", "ist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "$,", "PRELS", "APPR", "ART", "NN", "ART", "NN", "ADV", "VAFIN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Erziehung, Bildung, Sch\u00f6pfung seyn.", "tokens": ["Er\u00b7zie\u00b7hung", ",", "Bil\u00b7dung", ",", "Sch\u00f6p\u00b7fung", "seyn", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Der Tisch, die Speise selbst wird nach Sophistereyn", "tokens": ["Der", "Tisch", ",", "die", "Spei\u00b7se", "selbst", "wird", "nach", "So\u00b7phis\u00b7te\u00b7reyn"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "$,", "ART", "NN", "ADV", "VAFIN", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.9": {"text": "Der hochgelehrten Herrn zu einem Probestein", "tokens": ["Der", "hoch\u00b7ge\u00b7lehr\u00b7ten", "Herrn", "zu", "ei\u00b7nem", "Pro\u00b7be\u00b7stein"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Verborgner Neigungen der Seele:", "tokens": ["Ver\u00b7borg\u00b7ner", "Nei\u00b7gun\u00b7gen", "der", "See\u00b7le", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ART", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.11": {"text": "Als ob es uns an andern Proben fehle?", "tokens": ["Als", "ob", "es", "uns", "an", "an\u00b7dern", "Pro\u00b7ben", "feh\u00b7le", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "KOUS", "PPER", "PRF", "APPR", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "O stilles Lied der Philomele,", "tokens": ["O", "stil\u00b7les", "Lied", "der", "Phi\u00b7lo\u00b7me\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJA", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.13": {"text": "Schmilz doch die Augenblendereyn", "tokens": ["Schmilz", "doch", "die", "Au\u00b7gen\u00b7blen\u00b7de\u00b7reyn"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.14": {"text": "Einmal zur Wahrheit um. Allein die Herrn sind Stein!", "tokens": ["Ein\u00b7mal", "zur", "Wahr\u00b7heit", "um", ".", "Al\u00b7lein", "die", "Herrn", "sind", "Stein", "!"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPRART", "NN", "PTKVZ", "$.", "ADV", "ART", "NN", "VAFIN", "NN", "$."], "meter": "+--+-+-+-+-+", "measure": "iambic.hexa.invert"}, "line.15": {"text": "Und wenn man ihnen sagt, ihr gro\u00dfen Raphaele", "tokens": ["Und", "wenn", "man", "ih\u00b7nen", "sagt", ",", "ihr", "gro\u00b7\u00dfen", "Ra\u00b7phae\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "KOUS", "PIS", "PPER", "VVFIN", "$,", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.16": {"text": "Habt die Natur noch nie belauscht, ihr saht vorbey", "tokens": ["Habt", "die", "Na\u00b7tur", "noch", "nie", "be\u00b7lauscht", ",", "ihr", "saht", "vor\u00b7bey"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "ADV", "ADV", "VVPP", "$,", "PPER", "VVFIN", "PTKVZ"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.17": {"text": "Durch Nebel eurer Tr\u00e4umerey", "tokens": ["Durch", "Ne\u00b7bel", "eu\u00b7rer", "Tr\u00e4u\u00b7me\u00b7rey"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.18": {"text": "Durch B\u00fccher, die nur eine Seite", "tokens": ["Durch", "B\u00fc\u00b7cher", ",", "die", "nur", "ei\u00b7ne", "Sei\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "NN", "$,", "PRELS", "ADV", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.19": {"text": "Des Herzens h\u00f6chstens aufgedeckt", "tokens": ["Des", "Her\u00b7zens", "h\u00f6chs\u00b7tens", "auf\u00b7ge\u00b7deckt"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.20": {"text": "Und hundert Seiten Dunst gefleckt \u2013", "tokens": ["Und", "hun\u00b7dert", "Sei\u00b7ten", "Dunst", "ge\u00b7fleckt", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "CARD", "NN", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.21": {"text": "Ihr nennet Eitelkeit, was Wohlthun, G\u00f6ttertugend \u2013", "tokens": ["Ihr", "nen\u00b7net", "Ei\u00b7tel\u00b7keit", ",", "was", "Wohl\u00b7thun", ",", "G\u00f6t\u00b7ter\u00b7tu\u00b7gend", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "NN", "$,", "PWS", "NN", "$,", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.22": {"text": "Gef\u00fchl hervorgebracht, ihr nennet toller Jugend", "tokens": ["Ge\u00b7f\u00fchl", "her\u00b7vor\u00b7ge\u00b7bracht", ",", "ihr", "nen\u00b7net", "tol\u00b7ler", "Ju\u00b7gend"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "VVPP", "$,", "PPER", "VVFIN", "ADJA", "NN"], "meter": "-+-+-+---+-+-", "measure": "unknown.measure.penta"}, "line.23": {"text": "Vergehungen mit Namen, da\u00df Verfolgung sich bereite,", "tokens": ["Ver\u00b7ge\u00b7hun\u00b7gen", "mit", "Na\u00b7men", ",", "da\u00df", "Ver\u00b7fol\u00b7gung", "sich", "be\u00b7rei\u00b7te", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$,", "KOUS", "NN", "PRF", "VVFIN", "$,"], "meter": "+-+--+-+-+-+-+-", "measure": "trochaic.septa.relaxed"}, "line.24": {"text": "Und Menschen, werth belohnt zu werden, S\u00fcnder,", "tokens": ["Und", "Men\u00b7schen", ",", "werth", "be\u00b7lohnt", "zu", "wer\u00b7den", ",", "S\u00fcn\u00b7der", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "NN", "$,", "ADJD", "VVPP", "PTKZU", "VAINF", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.25": {"text": "So hat Thorheit gespielt, und M\u00e4nner werden Kinder.", "tokens": ["So", "hat", "Thor\u00b7heit", "ge\u00b7spielt", ",", "und", "M\u00e4n\u00b7ner", "wer\u00b7den", "Kin\u00b7der", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "VVPP", "$,", "KON", "NN", "VAFIN", "NN", "$."], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}}, "stanza.13": {"line.1": {"text": "Theater \u2013 o beh\u00fcte Gott!", "tokens": ["The\u00b7a\u00b7ter", "\u2013", "o", "be\u00b7h\u00fc\u00b7te", "Gott", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "FM", "VVFIN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein gro\u00dfer Rousseau \u2013 zwar gelesen hab' ichs nie,", "tokens": ["Ein", "gro\u00b7\u00dfer", "Rous\u00b7se\u00b7au", "\u2013", "zwar", "ge\u00b7le\u00b7sen", "hab'", "ichs", "nie", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$(", "ADV", "VVPP", "VAFIN", "PIS", "ADV", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Allein er schrieb dagegen, mein' ich,", "tokens": ["Al\u00b7lein", "er", "schrieb", "da\u00b7ge\u00b7gen", ",", "mein'", "ich", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "PAV", "$,", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Kurz die Gelehrten all sind einig", "tokens": ["Kurz", "die", "Ge\u00b7lehr\u00b7ten", "all", "sind", "ei\u00b7nig"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJD", "ART", "NN", "PIAT", "VAFIN", "ADJD"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Theater ist Pedanterie.", "tokens": ["The\u00b7a\u00b7ter", "ist", "Pe\u00b7dan\u00b7te\u00b7rie", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Ein Edler stirbt. Man tanzt und lacht.", "tokens": ["Ein", "Ed\u00b7ler", "stirbt", ".", "Man", "tanzt", "und", "lacht", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "PIS", "VVFIN", "KON", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Glas zerbricht! Es wird ein Kriegsverh\u00f6r gehalten", "tokens": ["Ein", "Glas", "zer\u00b7bricht", "!", "Es", "wird", "ein", "Kriegs\u00b7ver\u00b7h\u00f6r", "ge\u00b7hal\u00b7ten"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVPP", "$.", "PPER", "VAFIN", "ART", "NN", "VVPP"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Und alle Stirnen stehn in Falten,", "tokens": ["Und", "al\u00b7le", "Stir\u00b7nen", "stehn", "in", "Fal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Als w\u00e4re dies des Erdballs letzte Nacht.", "tokens": ["Als", "w\u00e4\u00b7re", "dies", "des", "Erd\u00b7balls", "letz\u00b7te", "Nacht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PDS", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Der Knabe soll im Takt und nach der Trommel lernen", "tokens": ["Der", "Kna\u00b7be", "soll", "im", "Takt", "und", "nach", "der", "Trom\u00b7mel", "ler\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VMFIN", "APPRART", "NN", "KON", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und tanzen und verdaun. Die Mentore entfernen", "tokens": ["Und", "tan\u00b7zen", "und", "ver\u00b7daun", ".", "Die", "Men\u00b7to\u00b7re", "ent\u00b7fer\u00b7nen"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVINF", "KON", "VVINF", "$.", "ART", "NN", "VVINF"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Was mit dem Leben ihn bekannt zu machen schien.", "tokens": ["Was", "mit", "dem", "Le\u00b7ben", "ihn", "be\u00b7kannt", "zu", "ma\u00b7chen", "schien", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "ART", "NN", "PPER", "ADJD", "PTKZU", "VVINF", "VVFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.8": {"text": "Er sieht nur Kutschen-Complimenten,", "tokens": ["Er", "sieht", "nur", "Kut\u00b7schen\u00b7Com\u00b7pli\u00b7men\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "H\u00f6rt das Geschrei schulm\u00e4\u00dfiger Studenten,", "tokens": ["H\u00f6rt", "das", "Ge\u00b7schrei", "schul\u00b7m\u00e4\u00b7\u00dfi\u00b7ger", "Stu\u00b7den\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+--+-+--+--", "measure": "iambic.tetra.invert"}, "line.10": {"text": "Die \u00fcber Activ und Passiv", "tokens": ["Die", "\u00fc\u00b7ber", "Ac\u00b7tiv", "und", "Pas\u00b7siv"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.11": {"text": "Oft r\u00e4sonniren krumm und schief,", "tokens": ["Oft", "r\u00e4\u00b7son\u00b7ni\u00b7ren", "krumm", "und", "schief", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADJD", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.12": {"text": "Und dieses Drehewerk, der Mischmasch von Genien", "tokens": ["Und", "die\u00b7ses", "Dre\u00b7he\u00b7werk", ",", "der", "Mischmasch", "von", "Ge\u00b7ni\u00b7en"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PDAT", "NN", "$,", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.13": {"text": "Und Gassenhauerwitz, der Unsinn hei\u00dft \u2013 ", "tokens": ["Und", "Gas\u00b7sen\u00b7hau\u00b7er\u00b7witz", ",", "der", "Un\u00b7sinn", "hei\u00dft", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.14": {"text": "So schlage doch Merkur darein, den Wust zu enden.", "tokens": ["So", "schla\u00b7ge", "doch", "Mer\u00b7kur", "da\u00b7rein", ",", "den", "Wust", "zu", "en\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "NN", "PAV", "$,", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.15": {"line.1": {"text": "Ich bitte denn doch mir zu sagen,", "tokens": ["Ich", "bit\u00b7te", "denn", "doch", "mir", "zu", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ADV", "PPER", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ob die Moral, so vorgetragen", "tokens": ["Ob", "die", "Mo\u00b7ral", ",", "so", "vor\u00b7ge\u00b7tra\u00b7gen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "ART", "NN", "$,", "ADV", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie Shakespeare sie sinnlich macht:", "tokens": ["Wie", "Sha\u00b7ke\u00b7spe\u00b7a\u00b7re", "sie", "sinn\u00b7lich", "macht", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "PPER", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ob V\u00e4ter, die durch ihre frommen", "tokens": ["Ob", "V\u00e4\u00b7ter", ",", "die", "durch", "ih\u00b7re", "from\u00b7men"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "$,", "PRELS", "APPR", "PPOSAT", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Herzlieben S\u00f6hnchen in der Nacht", "tokens": ["Herz\u00b7lie\u00b7ben", "S\u00f6hn\u00b7chen", "in", "der", "Nacht"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Des Alters und der Noth, zuletzt um alles kommen,", "tokens": ["Des", "Al\u00b7ters", "und", "der", "Noth", ",", "zu\u00b7letzt", "um", "al\u00b7les", "kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$,", "ADV", "APPR", "PIS", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Ob Ehrgeiz, der mit Menschenblut geschmiert", "tokens": ["Ob", "Ehr\u00b7geiz", ",", "der", "mit", "Men\u00b7schen\u00b7blut", "ge\u00b7schmiert"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "$,", "PRELS", "APPR", "NN", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Von einer Klippe zu der andern", "tokens": ["Von", "ei\u00b7ner", "Klip\u00b7pe", "zu", "der", "an\u00b7dern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "ART", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.9": {"text": "Und endlich zum Schaffot durch Zaubereyen f\u00fchrt,", "tokens": ["Und", "end\u00b7lich", "zum", "Schaf\u00b7fot", "durch", "Zau\u00b7be\u00b7re\u00b7yen", "f\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPRART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.10": {"text": "Durch welche wir erziehn \u2013 ob Regeln ohne Zahl", "tokens": ["Durch", "wel\u00b7che", "wir", "er\u00b7ziehn", "\u2013", "ob", "Re\u00b7geln", "oh\u00b7ne", "Zahl"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["APPR", "PRELS", "PPER", "VVINF", "$(", "KOUS", "NN", "APPR", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Auf Pult und Kanzeln hergeschrien,", "tokens": ["Auf", "Pult", "und", "Kan\u00b7zeln", "her\u00b7ge\u00b7schri\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.12": {"text": "Ein junges Herz zu feinerer Moral", "tokens": ["Ein", "jun\u00b7ges", "Herz", "zu", "fei\u00b7ne\u00b7rer", "Mo\u00b7ral"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.13": {"text": "Und bessern Entschl\u00fcssen erziehen:", "tokens": ["Und", "bes\u00b7sern", "Ent\u00b7schl\u00fcs\u00b7sen", "er\u00b7zie\u00b7hen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "VVINF", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.14": {"text": "Als auf der hohen See von wirklichem Geschick", "tokens": ["Als", "auf", "der", "ho\u00b7hen", "See", "von", "wirk\u00b7li\u00b7chem", "Ge\u00b7schick"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ADJA", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.15": {"text": "Nicht blo\u00dfen Tr\u00e4umereyn \u2013 von Shakespeare ein St\u00fcck. \u2013", "tokens": ["Nicht", "blo\u00b7\u00dfen", "Tr\u00e4u\u00b7me\u00b7reyn", "\u2013", "von", "Sha\u00b7ke\u00b7spe\u00b7a\u00b7re", "ein", "St\u00fcck", ".", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["PTKNEG", "ADJA", "NN", "$(", "APPR", "NE", "ART", "NN", "$.", "$("], "meter": "-+-+-+-+--+--+", "measure": "iambic.hexa.relaxed"}}, "stanza.16": {"line.1": {"text": "Man lernt den Krieg, man lernet sich", "tokens": ["Man", "lernt", "den", "Krieg", ",", "man", "ler\u00b7net", "sich"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ART", "NN", "$,", "PIS", "VVFIN", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Das Halsband und die Degenkuppel schnallen.", "tokens": ["Das", "Hals\u00b7band", "und", "die", "De\u00b7gen\u00b7kup\u00b7pel", "schnal\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Man greift auch ans Gewehr und \u2013 ohne Noth la\u00df ich", "tokens": ["Man", "greift", "auch", "ans", "Ge\u00b7wehr", "und", "\u2013", "oh\u00b7ne", "Noth", "la\u00df", "ich"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "ADV", "APPRART", "NN", "KON", "$(", "APPR", "NN", "VVIMP", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Auf einen Burschen ders weit besser f\u00fchrt, um mich", "tokens": ["Auf", "ei\u00b7nen", "Bur\u00b7schen", "ders", "weit", "bes\u00b7ser", "f\u00fchrt", ",", "um", "mich"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "ADJD", "ADJD", "VVFIN", "$,", "KOUI", "PRF"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Vor Kutschen sehn zu lassen, Hiebe fallen,", "tokens": ["Vor", "Kut\u00b7schen", "sehn", "zu", "las\u00b7sen", ",", "Hie\u00b7be", "fal\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "PTKZU", "VVINF", "$,", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "F\u00fcnfhundert wen'ger eins mit einem Modschen Stock \u2013", "tokens": ["F\u00fcnf\u00b7hun\u00b7dert", "wen'\u00b7ger", "eins", "mit", "ei\u00b7nem", "Mod\u00b7schen", "Stock", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "PIS", "APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Das alles macht \u2013 mein feinrer Rock.", "tokens": ["Das", "al\u00b7les", "macht", "\u2013", "mein", "fein\u00b7rer", "Rock", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VVFIN", "$(", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Allein ihr Herrn seyd nie gelegen", "tokens": ["Al\u00b7lein", "ihr", "Herrn", "seyd", "nie", "ge\u00b7le\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "NN", "VAFIN", "ADV", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Nackt und blessirt wie Vater Kleist.", "tokens": ["Nackt", "und", "bles\u00b7sirt", "wie", "Va\u00b7ter", "Kleist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "KON", "VVFIN", "KOKOM", "NN", "NN", "$."], "meter": "+-++-+-+", "measure": "unknown.measure.penta"}, "line.3": {"text": "Ein feindlicher Soldat hat nie den gro\u00dfen Geist", "tokens": ["Ein", "feind\u00b7li\u00b7cher", "Sol\u00b7dat", "hat", "nie", "den", "gro\u00b7\u00dfen", "Geist"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Ins Zeit gebracht \u2013 und st\u00fcrbet ihr, so rei\u00dft", "tokens": ["Ins", "Zeit", "ge\u00b7bracht", "\u2013", "und", "st\u00fcr\u00b7bet", "ihr", ",", "so", "rei\u00dft"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word"], "pos": ["APPRART", "NN", "VVPP", "$(", "KON", "VVFIN", "PPER", "$,", "ADV", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Kein Hauptmann von den Feinden sich den Degen", "tokens": ["Kein", "Haupt\u00b7mann", "von", "den", "Fein\u00b7den", "sich", "den", "De\u00b7gen"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "APPR", "ART", "NN", "PRF", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Von seiner Seit' \u2013 und fleht um euren Geist.", "tokens": ["Von", "sei\u00b7ner", "Seit'", "\u2013", "und", "fleht", "um", "eu\u00b7ren", "Geist", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$(", "KON", "VVFIN", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.18": {"line.1": {"text": "Der Krieg ist keine Uhr, und dennoch ist er eine;", "tokens": ["Der", "Krieg", "ist", "kei\u00b7ne", "Uhr", ",", "und", "den\u00b7noch", "ist", "er", "ei\u00b7ne", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PIAT", "NN", "$,", "KON", "ADV", "VAFIN", "PPER", "ART", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bewegungen, so wir von Jugend auf gelernt,", "tokens": ["Be\u00b7we\u00b7gun\u00b7gen", ",", "so", "wir", "von", "Ju\u00b7gend", "auf", "ge\u00b7lernt", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "ADV", "PPER", "APPR", "NN", "APPR", "VVPP", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Die werden uns Natur und fallen oft ins Kleine.", "tokens": ["Die", "wer\u00b7den", "uns", "Na\u00b7tur", "und", "fal\u00b7len", "oft", "ins", "Klei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "NN", "KON", "VVFIN", "ADV", "APPRART", "ADJA", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Nur keiner sieht, da\u00df man [weit] davon entfernt,", "tokens": ["Nur", "kei\u00b7ner", "sieht", ",", "da\u00df", "man", "weit", "da\u00b7von", "ent\u00b7fernt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "PIS", "VVFIN", "$,", "KOUS", "PIS", "$(", "ADJD", "$(", "PAV", "VVPP", "$,"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Und so sind blind die F\u00fchrer ganzer Heerden.", "tokens": ["Und", "so", "sind", "blind", "die", "F\u00fch\u00b7rer", "gan\u00b7zer", "Heer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VAFIN", "ADJD", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}