{"textgrid.poem.37361": {"metadata": {"author": {"name": "Busch, Wilhelm", "birth": "N.A.", "death": "N.A."}, "title": "1L: Der Ruhm, wie alle Schwindelware,", "genre": "verse", "period": "N.A.", "pub_year": 1870, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Ruhm, wie alle Schwindelware,", "tokens": ["Der", "Ruhm", ",", "wie", "al\u00b7le", "Schwin\u00b7del\u00b7wa\u00b7re", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PWAV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "H\u00e4lt selten \u00fcber tausend Jahre.", "tokens": ["H\u00e4lt", "sel\u00b7ten", "\u00fc\u00b7ber", "tau\u00b7send", "Jah\u00b7re", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "CARD", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zumeist vergeht schon etwas eh'r", "tokens": ["Zu\u00b7meist", "ver\u00b7geht", "schon", "et\u00b7was", "eh'r"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "ADV", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Haltbarkeit und die Kul\u00f6r.", "tokens": ["Die", "Halt\u00b7bar\u00b7keit", "und", "die", "Ku\u00b7l\u00f6r", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ein Schmetterling voll Eleganz,", "tokens": ["Ein", "Schmet\u00b7ter\u00b7ling", "voll", "E\u00b7le\u00b7ganz", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Genannt der Ritter Schwalbenschwanz,", "tokens": ["Ge\u00b7nannt", "der", "Rit\u00b7ter", "Schwal\u00b7ben\u00b7schwanz", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "NE", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Exemplar von erster G\u00fcte,", "tokens": ["Ein", "Ex\u00b7emp\u00b7lar", "von", "ers\u00b7ter", "G\u00fc\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Begr\u00fc\u00dfte jede Doldenbl\u00fcte", "tokens": ["Be\u00b7gr\u00fc\u00df\u00b7te", "je\u00b7de", "Dol\u00b7den\u00b7bl\u00fc\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Und holte hier und holte da", "tokens": ["Und", "hol\u00b7te", "hier", "und", "hol\u00b7te", "da"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ADV", "KON", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sich Nektar und Ambrosia.", "tokens": ["Sich", "Nek\u00b7tar", "und", "A\u00b7mbro\u00b7sia", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "NN", "KON", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Mitunter macht er sich auch breit", "tokens": ["Mi\u00b7tun\u00b7ter", "macht", "er", "sich", "auch", "breit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "PRF", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In seiner ganzen Herrlichkeit", "tokens": ["In", "sei\u00b7ner", "gan\u00b7zen", "Herr\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und zeigt den Leuten seine Orden", "tokens": ["Und", "zeigt", "den", "Leu\u00b7ten", "sei\u00b7ne", "Or\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "ART", "NN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Und ist mit Recht ber\u00fchmt geworden.", "tokens": ["Und", "ist", "mit", "Recht", "be\u00b7r\u00fchmt", "ge\u00b7wor\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "APPR", "NN", "ADJD", "VAPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Die jungen M\u00e4dchen fanden dies", "tokens": ["Die", "jun\u00b7gen", "M\u00e4d\u00b7chen", "fan\u00b7den", "dies"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "PDS"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Entz\u00fcckend, goldig, reizend, s\u00fc\u00df.", "tokens": ["Ent\u00b7z\u00fc\u00b7ckend", ",", "gol\u00b7dig", ",", "rei\u00b7zend", ",", "s\u00fc\u00df", "."], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "ADJD", "$,", "VVPP", "$,", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Vergeblich schwenkten ihre M\u00fctzen", "tokens": ["Ver\u00b7geb\u00b7lich", "schwenk\u00b7ten", "ih\u00b7re", "M\u00fct\u00b7zen"], "token_info": ["word", "word", "word", "word"], "pos": ["ADJD", "VVFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Knaben, um ihn zu besitzen.", "tokens": ["Die", "Kna\u00b7ben", ",", "um", "ihn", "zu", "be\u00b7sit\u00b7zen", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "KOUI", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Sogar der Spatz hat zugeschnappt", "tokens": ["So\u00b7gar", "der", "Spatz", "hat", "zu\u00b7ge\u00b7schnappt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "VAFIN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und h\u00e4tt' ihn um ein Haar gehabt.", "tokens": ["Und", "h\u00e4tt'", "ihn", "um", "ein", "Haar", "ge\u00b7habt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "APPR", "ART", "NN", "VAPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Jetzt aber naht sich ein Student,", "tokens": ["Jetzt", "a\u00b7ber", "naht", "sich", "ein", "Stu\u00b7dent", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "PRF", "ART", "NN", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Der seine Winkelz\u00fcge kennt.", "tokens": ["Der", "sei\u00b7ne", "Win\u00b7kel\u00b7z\u00fc\u00b7ge", "kennt", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "In einem Netz mit engen Maschen", "tokens": ["In", "ei\u00b7nem", "Netz", "mit", "en\u00b7gen", "Ma\u00b7schen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "T\u00e4t er den Fl\u00fcchtigen erhaschen,", "tokens": ["T\u00e4t", "er", "den", "Fl\u00fcch\u00b7ti\u00b7gen", "er\u00b7ha\u00b7schen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "VVINF", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Und da derselbe ohne Tadel,", "tokens": ["Und", "da", "der\u00b7sel\u00b7be", "oh\u00b7ne", "Ta\u00b7del", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PDAT", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Spie\u00dft er ihn auf die hei\u00dfe Nadel.", "tokens": ["Spie\u00dft", "er", "ihn", "auf", "die", "hei\u00b7\u00dfe", "Na\u00b7del", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "So kam er unter Glas und Rahmen", "tokens": ["So", "kam", "er", "un\u00b7ter", "Glas", "und", "Rah\u00b7men"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mit Datum, Jahreszahl und Namen", "tokens": ["Mit", "Da\u00b7tum", ",", "Jah\u00b7res\u00b7zahl", "und", "Na\u00b7men"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und bleibt ber\u00fchmt und unvergessen,", "tokens": ["Und", "bleibt", "be\u00b7r\u00fchmt", "und", "un\u00b7ver\u00b7ges\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Bis ihn zuletzt die Motten fressen.", "tokens": ["Bis", "ihn", "zu\u00b7letzt", "die", "Mot\u00b7ten", "fres\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Man m\u00f6chte weinen, wenn man sieht,", "tokens": ["Man", "m\u00f6ch\u00b7te", "wei\u00b7nen", ",", "wenn", "man", "sieht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "VVINF", "$,", "KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df dies das Ende von dem Lied.", "tokens": ["Da\u00df", "dies", "das", "En\u00b7de", "von", "dem", "Lied", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PDS", "ART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}