{"textgrid.poem.46180": {"metadata": {"author": {"name": "Weckherlin, Georg Rodolf", "birth": "N.A.", "death": "N.A."}, "title": "1L: Empfindlicher ist kein verscheiden,", "genre": "verse", "period": "N.A.", "pub_year": 1618, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Empfindlicher ist kein verscheiden,", "tokens": ["Emp\u00b7find\u00b7li\u00b7cher", "ist", "kein", "ver\u00b7schei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PIAT", "ADJA", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "als von der liebsten abzuscheiden,", "tokens": ["als", "von", "der", "liebs\u00b7ten", "ab\u00b7zu\u00b7schei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "ADJA", "VVIZU", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "dan sunst der allgemeine tod", "tokens": ["dan", "sunst", "der", "all\u00b7ge\u00b7mei\u00b7ne", "tod"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "ART", "ADJA", "NN"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "vollendet alle pein und not,", "tokens": ["voll\u00b7en\u00b7det", "al\u00b7le", "pein", "und", "not", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "und pfleget die seel durch das sterben,", "tokens": ["und", "pfle\u00b7get", "die", "seel", "durch", "das", "ster\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "APPR", "PDS", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.6": {"text": "ein neues leben zu erwerben.", "tokens": ["ein", "neu\u00b7es", "le\u00b7ben", "zu", "er\u00b7wer\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "VVINF", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Wan aber zwei verliebte herzen", "tokens": ["Wan", "a\u00b7ber", "zwei", "ver\u00b7lieb\u00b7te", "her\u00b7zen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "CARD", "VVFIN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "sich scheiden, alsdan ihre schmerzen", "tokens": ["sich", "schei\u00b7den", ",", "als\u00b7dan", "ih\u00b7re", "schmer\u00b7zen"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PRF", "VVINF", "$,", "KOUS", "PPOSAT", "NN"], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "seind \u00fcberschmerzlich, und die pein,", "tokens": ["seind", "\u00fc\u00b7bersc\u00b7hmerz\u00b7lich", ",", "und", "die", "pein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "$,", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "zwar t\u00f6dlich, mu\u00df doch ewig sein.", "tokens": ["zwar", "t\u00f6d\u00b7lich", ",", "mu\u00df", "doch", "e\u00b7wig", "sein", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "VMFIN", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "weil nach dem scheiden und ableiben", "tokens": ["weil", "nach", "dem", "schei\u00b7den", "und", "ab\u00b7lei\u00b7ben"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "ADJA", "KON", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "sie tot, und lebendig doch, bleiben.", "tokens": ["sie", "tot", ",", "und", "le\u00b7ben\u00b7dig", "doch", ",", "blei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "ADJD", "$,", "KON", "ADJD", "ADV", "$,", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Zwo liebende geliebte seelen,", "tokens": ["Zwo", "lie\u00b7ben\u00b7de", "ge\u00b7lieb\u00b7te", "see\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "ADJA", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die ihre k\u00fc\u00df einander stehlen", "tokens": ["die", "ih\u00b7re", "k\u00fc\u00df", "ein\u00b7an\u00b7der", "steh\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "PPOSAT", "VVFIN", "PRF", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "genie\u00dfend der lieb s\u00fc\u00dfen treu,", "tokens": ["ge\u00b7nie\u00b7\u00dfend", "der", "lieb", "s\u00fc\u00b7\u00dfen", "treu", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "ART", "ADJD", "VVFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die k\u00f6nden sich ja nicht bekr\u00e4nken,", "tokens": ["die", "k\u00f6n\u00b7den", "sich", "ja", "nicht", "be\u00b7kr\u00e4n\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "PRF", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "vil weniger des tods gedenken,", "tokens": ["vil", "we\u00b7ni\u00b7ger", "des", "tods", "ge\u00b7den\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "als aller forcht und sorgen frei.", "tokens": ["als", "al\u00b7ler", "forcht", "und", "sor\u00b7gen", "frei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIAT", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Doch wie bald wird ihr trost ver\u00e4ndert,", "tokens": ["Doch", "wie", "bald", "wird", "ihr", "trost", "ver\u00b7\u00e4n\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "ADV", "VAFIN", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "wan von einander abges\u00f6ndert", "tokens": ["wan", "von", "ein\u00b7an\u00b7der", "ab\u00b7ge\u00b7s\u00f6n\u00b7dert"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "APPR", "PRF", "VVPP"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "ein jedes misset seine seel,", "tokens": ["ein", "je\u00b7des", "mis\u00b7set", "sei\u00b7ne", "seel", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "indem sie beed gr\u00fcn und verdorben,", "tokens": ["in\u00b7dem", "sie", "beed", "gr\u00fcn", "und", "ver\u00b7dor\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "ADJD", "KON", "VVPP", "$,"], "meter": "-+-++--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "beed lebendig und doch gestorben", "tokens": ["beed", "le\u00b7ben\u00b7dig", "und", "doch", "ge\u00b7stor\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "ADJD", "KON", "ADV", "VVPP"], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.6": {"text": "nicht sehend f\u00fchlen ihren fehl?", "tokens": ["nicht", "se\u00b7hend", "f\u00fch\u00b7len", "ih\u00b7ren", "fehl", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "ADJD", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "O lieb, wer kan dich recht beschreiben!", "tokens": ["O", "lieb", ",", "wer", "kan", "dich", "recht", "be\u00b7schrei\u00b7ben", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "$,", "PWS", "VMFIN", "PRF", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "du kanst beseelen und entleiben,", "tokens": ["du", "kanst", "be\u00b7see\u00b7len", "und", "ent\u00b7lei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "VVINF", "KON", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "vereinigen zu einer zeit", "tokens": ["ver\u00b7ei\u00b7ni\u00b7gen", "zu", "ei\u00b7ner", "zeit"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "kanst du mit streit lieb, mit lieb streit,", "tokens": ["kanst", "du", "mit", "streit", "lieb", ",", "mit", "lieb", "streit", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPR", "NN", "ADJD", "$,", "APPR", "ADJD", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "ja thorheit und verstand verm\u00e4hlen", "tokens": ["ja", "thor\u00b7heit", "und", "ver\u00b7stand", "ver\u00b7m\u00e4h\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "NN", "KON", "VVFIN", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "und dan beleiben und entseelen.", "tokens": ["und", "dan", "be\u00b7lei\u00b7ben", "und", "ent\u00b7see\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVPP", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Was aber kan man von dir klagen?", "tokens": ["Was", "a\u00b7ber", "kan", "man", "von", "dir", "kla\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ADV", "VMFIN", "PIS", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "was warheit kan man von dir sagen,", "tokens": ["was", "war\u00b7heit", "kan", "man", "von", "dir", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NN", "VMFIN", "PIS", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "o lieb, dan das, wa du wilt sein,", "tokens": ["o", "lieb", ",", "dan", "das", ",", "wa", "du", "wilt", "sein", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["FM", "ADJD", "$,", "ADV", "PDS", "$,", "PWAV", "PPER", "VMFIN", "VAINF", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.4": {"text": "da ist zugleich vil freud, vil pein:", "tokens": ["da", "ist", "zu\u00b7gleich", "vil", "freud", ",", "vil", "pein", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ADV", "VVFIN", "$,", "PIAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "nicht wei\u00df seind die, die sich verliebet,", "tokens": ["nicht", "wei\u00df", "seind", "die", ",", "die", "sich", "ver\u00b7lie\u00b7bet", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "VAFIN", "ART", "$,", "PRELS", "PRF", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "doch wird die witz durch lieb ge\u00fcbet.", "tokens": ["doch", "wird", "die", "witz", "durch", "lieb", "ge\u00b7\u00fc\u00b7bet", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "APPR", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Die lieb und torheit uns verdrie\u00dfet,", "tokens": ["Die", "lieb", "und", "tor\u00b7heit", "uns", "ver\u00b7drie\u00b7\u00dfet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "NN", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "doch ist die torheit so vers\u00fc\u00dfet,", "tokens": ["doch", "ist", "die", "tor\u00b7heit", "so", "ver\u00b7s\u00fc\u00b7\u00dfet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "da\u00df ihr kein wollust der welt gleich;", "tokens": ["da\u00df", "ihr", "kein", "wol\u00b7lust", "der", "welt", "gleich", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PIAT", "NN", "ART", "NN", "ADV", "$."], "meter": "++-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die welt, der torheit k\u00fcnigreich,", "tokens": ["die", "welt", ",", "der", "tor\u00b7heit", "k\u00fc\u00b7ni\u00b7greich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "wird von ihr und der lieb erhalten,", "tokens": ["wird", "von", "ihr", "und", "der", "lieb", "er\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "PPER", "KON", "ART", "ADJD", "VVINF", "$,"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.6": {"text": "sie beed die ganze welt verwalten.", "tokens": ["sie", "beed", "die", "gan\u00b7ze", "welt", "ver\u00b7wal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Ach, herzlieb, wan mich dein abwesen", "tokens": ["Ach", ",", "herz\u00b7lieb", ",", "wan", "mich", "dein", "ab\u00b7we\u00b7sen"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "VVFIN", "$,", "PWAV", "PPER", "PPOSAT", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "nicht lasset ferr von dir genesen,", "tokens": ["nicht", "las\u00b7set", "ferr", "von", "dir", "ge\u00b7ne\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKNEG", "VVFIN", "ADJD", "APPR", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "so find ich mich auch ohn verstand,", "tokens": ["so", "find", "ich", "mich", "auch", "ohn", "ver\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PRF", "ADV", "APPR", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wie ohn seel; es ist eine schand", "tokens": ["wie", "ohn", "seel", ";", "es", "ist", "ei\u00b7ne", "schand"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "APPR", "NE", "$.", "PPER", "VAFIN", "ART", "NN"], "meter": "+-+-++-+", "measure": "unknown.measure.penta"}, "line.5": {"text": "f\u00fcr uns beed, die wir herzlich lieben,", "tokens": ["f\u00fcr", "uns", "beed", ",", "die", "wir", "herz\u00b7lich", "lie\u00b7ben", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "VVFIN", "$,", "PRELS", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "und ohn verstand uns stets betr\u00fcben.", "tokens": ["und", "ohn", "ver\u00b7stand", "uns", "stets", "be\u00b7tr\u00fc\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "VVFIN", "PPER", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Ist dan lieb wie torheit zu schelten,", "tokens": ["Ist", "dan", "lieb", "wie", "tor\u00b7heit", "zu", "schel\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "KOKOM", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "so k\u00f6nden sie uns doch vergelten", "tokens": ["so", "k\u00f6n\u00b7den", "sie", "uns", "doch", "ver\u00b7gel\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VMFIN", "PPER", "PPER", "ADV", "VVINF"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "mit h\u00f6chster freud, trost, lob und lust,", "tokens": ["mit", "h\u00f6chs\u00b7ter", "freud", ",", "trost", ",", "lob", "und", "lust", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "VVFIN", "$,", "VVFIN", "$,", "NN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "wan zumal unsre seel und brust,", "tokens": ["wan", "zu\u00b7mal", "uns\u00b7re", "seel", "und", "brust", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPOSAT", "NN", "KON", "VVFIN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.5": {"text": "die stets mit lieb sich mehr entz\u00fcnden,", "tokens": ["die", "stets", "mit", "lieb", "sich", "mehr", "ent\u00b7z\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "APPR", "ADJD", "PRF", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "mehr s\u00fc\u00dfigkeit in narrheit finden.", "tokens": ["mehr", "s\u00fc\u00b7\u00dfig\u00b7keit", "in", "nar\u00b7rheit", "fin\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}