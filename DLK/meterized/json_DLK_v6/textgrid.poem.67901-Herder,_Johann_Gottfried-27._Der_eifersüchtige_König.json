{"textgrid.poem.67901": {"metadata": {"author": {"name": "Herder, Johann Gottfried", "birth": "N.A.", "death": "N.A."}, "title": "27. Der eifers\u00fcchtige K\u00f6nig", "genre": "verse", "period": "N.A.", "pub_year": 1773, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "An Christme\u00dffest, im Winter kalt,", "tokens": ["An", "Christ\u00b7me\u00df\u00b7fest", ",", "im", "Win\u00b7ter", "kalt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "$,", "APPRART", "NN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als Tafelrund begann:", "tokens": ["Als", "Ta\u00b7fel\u00b7rund", "be\u00b7gann", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da kam zu K\u00f6nigs Hof und Hall", "tokens": ["Da", "kam", "zu", "K\u00f6\u00b7nigs", "Hof", "und", "Hall"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "NN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Manch wackrer Ritter an.", "tokens": ["Manch", "wack\u00b7rer", "Rit\u00b7ter", "an", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Die K\u00f6nigin sah Feld hinaus", "tokens": ["Die", "K\u00f6\u00b7ni\u00b7gin", "sah", "Feld", "hin\u00b7aus"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "NN", "APZR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sah \u00fcber Schlosses Wall;", "tokens": ["Sah", "\u00fc\u00b7ber", "Schlos\u00b7ses", "Wall", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "NE", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Da sah sie, Junker Waters", "tokens": ["Da", "sah", "sie", ",", "Jun\u00b7ker", "Wa\u00b7ters"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "$,", "NE", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Kam reitend ab im Thal.", "tokens": ["Kam", "rei\u00b7tend", "ab", "im", "Thal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "ADJD", "PTKVZ", "APPRART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Sein L\u00e4ufer, der lief vor ihm her,", "tokens": ["Sein", "L\u00e4u\u00b7fer", ",", "der", "lief", "vor", "ihm", "her", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "VVFIN", "APPR", "PPER", "PTKVZ", "$,"], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "Sein Reuter ritt ihm nach:", "tokens": ["Sein", "Reu\u00b7ter", "ritt", "ihm", "nach", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NE", "VVFIN", "PPER", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Ein Mantel reich an rothem Gold,", "tokens": ["Ein", "Man\u00b7tel", "reich", "an", "ro\u00b7them", "Gold", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJD", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "War Wind- und Wetters Dach!", "tokens": ["War", "Win\u00b7d", "und", "Wet\u00b7ters", "Dach", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "TRUNC", "KON", "NN", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.4": {"line.1": {"text": "Und vorn am Rosse gl\u00e4nzte Gold,", "tokens": ["Und", "vorn", "am", "Ros\u00b7se", "gl\u00e4nz\u00b7te", "Gold", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPRART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dahinten Silber hell:", "tokens": ["Da\u00b7hin\u00b7ten", "Sil\u00b7ber", "hell", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Das Ro\u00df, das Junker Waters ritt,", "tokens": ["Das", "Ro\u00df", ",", "das", "Jun\u00b7ker", "Wa\u00b7ters", "ritt", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ging wie der Wind so schnell.", "tokens": ["Ging", "wie", "der", "Wind", "so", "schnell", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "KOKOM", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "\u00bbwer ist denn? sprach ein Rittersmann,", "tokens": ["\u00bb", "wer", "ist", "denn", "?", "sprach", "ein", "Rit\u00b7ters\u00b7mann", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["$(", "PWS", "VAFIN", "ADV", "$.", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "(zur K\u00f6nigin sprach er)", "tokens": ["(", "zur", "K\u00f6\u00b7ni\u00b7gin", "sprach", "er", ")"], "token_info": ["punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPRART", "NN", "VVFIN", "PPER", "$("], "meter": "-+-+--", "measure": "unknown.measure.di"}, "line.3": {"text": "Wer ist der sch\u00f6ne Junker dort,", "tokens": ["Wer", "ist", "der", "sch\u00f6\u00b7ne", "Jun\u00b7ker", "dort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "ADJA", "NN", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der reitet zu uns her?\u00ab", "tokens": ["Der", "rei\u00b7tet", "zu", "uns", "her", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PDS", "VVFIN", "APPR", "PPER", "PTKVZ", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "\u00bbwohl manchen Ritter und Fr\u00e4ulein auch", "tokens": ["\u00bb", "wohl", "man\u00b7chen", "Rit\u00b7ter", "und", "Fr\u00e4u\u00b7lein", "auch"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "PIAT", "NN", "KON", "NN", "ADV"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Hab ich mein' Tag gesehn;", "tokens": ["Hab", "ich", "mein'", "Tag", "ge\u00b7sehn", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch sch\u00f6ner als Junker Waters dort,", "tokens": ["Doch", "sch\u00f6\u00b7ner", "als", "Jun\u00b7ker", "Wa\u00b7ters", "dort", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "KOKOM", "NE", "NN", "ADV", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Hab ich nie nichts gesehn.\u00ab", "tokens": ["Hab", "ich", "nie", "nichts", "ge\u00b7sehn", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NN", "PPER", "ADV", "PIS", "VVPP", "$.", "$("], "meter": "---+-+", "measure": "unknown.measure.di"}}, "stanza.7": {"line.1": {"text": "Da brach des K\u00f6nigs Eifer aus,", "tokens": ["Da", "brach", "des", "K\u00f6\u00b7nigs", "Ei\u00b7fer", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "(denn eifernd war er sehr!)", "tokens": ["(", "denn", "ei\u00b7fernd", "war", "er", "sehr", "!", ")"], "token_info": ["punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KON", "VVPP", "VAFIN", "PPER", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "\u00bbund w\u00e4r er dreimal noch so sch\u00f6n", "tokens": ["\u00bb", "und", "w\u00e4r", "er", "drei\u00b7mal", "noch", "so", "sch\u00f6n"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "KON", "VAFIN", "PPER", "ADV", "ADV", "ADV", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sollt ichs dir doch seyn mehr.\u00ab", "tokens": ["Sollt", "ichs", "dir", "doch", "seyn", "mehr", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "PIS", "PPER", "ADV", "VAINF", "PIS", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "\u00bbkein Ritter ja, kein Fr\u00e4ulein nicht", "tokens": ["\u00bb", "kein", "Rit\u00b7ter", "ja", ",", "kein", "Fr\u00e4u\u00b7lein", "nicht"], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "PIAT", "NN", "PTKANT", "$,", "PIAT", "NN", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr seyd ja K\u00f6nig im Reich;", "tokens": ["Ihr", "seyd", "ja", "K\u00f6\u00b7nig", "im", "Reich", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "NN", "APPRART", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Im ganzen Schottland ist niemand", "tokens": ["Im", "gan\u00b7zen", "Schott\u00b7land", "ist", "nie\u00b7mand"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "ADJA", "NN", "VAFIN", "PIS"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Ja seinem K\u00f6nig gleich.\u00ab", "tokens": ["Ja", "sei\u00b7nem", "K\u00f6\u00b7nig", "gleich", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PTKANT", "PPOSAT", "NN", "ADV", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Doch was sie sagt \u2013 doch was sie th\u00e4t", "tokens": ["Doch", "was", "sie", "sagt", "\u2013", "doch", "was", "sie", "th\u00e4t"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "PWS", "PPER", "VVFIN", "$(", "KON", "PWS", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nichts stillte K\u00f6nigs Wuth;", "tokens": ["Nichts", "still\u00b7te", "K\u00f6\u00b7nigs", "Wuth", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "F\u00fcr die zwei Worte die sie sprach,", "tokens": ["F\u00fcr", "die", "zwei", "Wor\u00b7te", "die", "sie", "sprach", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "CARD", "NN", "ART", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Flo\u00df Junker Waters Blut.", "tokens": ["Flo\u00df", "Jun\u00b7ker", "Wa\u00b7ters", "Blut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Sie rissen ihn, sie zwangen ihm", "tokens": ["Sie", "ris\u00b7sen", "ihn", ",", "sie", "zwan\u00b7gen", "ihm"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In Ketten, Fu\u00df und Hand;", "tokens": ["In", "Ket\u00b7ten", ",", "Fu\u00df", "und", "Hand", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Sie rissen ihn, sie zwangen ihn,", "tokens": ["Sie", "ris\u00b7sen", "ihn", ",", "sie", "zwan\u00b7gen", "ihn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wo ihn kein Taglicht fand.", "tokens": ["Wo", "ihn", "kein", "Tag\u00b7licht", "fand", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PIAT", "NN", "VVFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "\u00bboft ritt ich ein in Sterlingschlo\u00df", "tokens": ["\u00bb", "oft", "ritt", "ich", "ein", "in", "Ster\u00b7ling\u00b7schlo\u00df"], "token_info": ["punct", "word", "word", "word", "word", "word", "word"], "pos": ["$(", "ADV", "VVFIN", "PPER", "ART", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bei Wetter und bei Wind;", "tokens": ["Bei", "Wet\u00b7ter", "und", "bei", "Wind", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch nie hatt' ich an Fu\u00df und Hand", "tokens": ["Doch", "nie", "hatt'", "ich", "an", "Fu\u00df", "und", "Hand"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "ADV", "VAFIN", "PPER", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was diese Ketten sind.", "tokens": ["Was", "die\u00b7se", "Ket\u00b7ten", "sind", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PDAT", "NN", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Oft ritt ich ein in Sterlingschlo\u00df", "tokens": ["Oft", "ritt", "ich", "ein", "in", "Ster\u00b7ling\u00b7schlo\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Bei Wetter und bei Sturm;", "tokens": ["Bei", "Wet\u00b7ter", "und", "bei", "Sturm", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Doch nimmer, nimmer fand ich mich", "tokens": ["Doch", "nim\u00b7mer", ",", "nim\u00b7mer", "fand", "ich", "mich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "ADV", "$,", "ADV", "VVFIN", "PPER", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Im finstern tiefen Thurm.\u00ab", "tokens": ["Im", "fins\u00b7tern", "tie\u00b7fen", "Thurm", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "ADJA", "ADJA", "NN", "$.", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Sie rissen ihn, sie zwangen ihn", "tokens": ["Sie", "ris\u00b7sen", "ihn", ",", "sie", "zwan\u00b7gen", "ihn"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "$,", "PPER", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zum Todesh\u00fcgel hin,", "tokens": ["Zum", "To\u00b7des\u00b7h\u00fc\u00b7gel", "hin", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "Und Ro\u00df und Knaben rissen sie", "tokens": ["Und", "Ro\u00df", "und", "Kna\u00b7ben", "ris\u00b7sen", "sie"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "NN", "KON", "NN", "VVFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zum Todesh\u00fcgel hin.", "tokens": ["Zum", "To\u00b7des\u00b7h\u00fc\u00b7gel", "hin", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.14": {"line.1": {"text": "Und was sie sagt und was sie th\u00e4t,", "tokens": ["Und", "was", "sie", "sagt", "und", "was", "sie", "th\u00e4t", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "PPER", "VVFIN", "KON", "PWS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Nichts stillte K\u00f6nigs Wuth:", "tokens": ["Nichts", "still\u00b7te", "K\u00f6\u00b7nigs", "Wuth", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.3": {"text": "F\u00fcr die zwei Worte, die sie sprach,", "tokens": ["F\u00fcr", "die", "zwei", "Wor\u00b7te", ",", "die", "sie", "sprach", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "CARD", "NN", "$,", "PRELS", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Flo\u00df Junker Waters Blut.", "tokens": ["Flo\u00df", "Jun\u00b7ker", "Wa\u00b7ters", "Blut", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "NE", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}