{"textgrid.poem.53711": {"metadata": {"author": {"name": "Tucholsky, Kurt", "birth": "N.A.", "death": "N.A."}, "title": "Letzte Fahrt", "genre": "verse", "period": "N.A.", "pub_year": 1912, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "An meinem Todestag \u2013 ich werd ihn nicht erleben \u2013", "tokens": ["An", "mei\u00b7nem", "To\u00b7des\u00b7tag", "\u2013", "ich", "werd", "ihn", "nicht", "er\u00b7le\u00b7ben", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$(", "PPER", "VAFIN", "PPER", "PTKNEG", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "da soll es mittags Rote Gr\u00fctze geben,", "tokens": ["da", "soll", "es", "mit\u00b7tags", "Ro\u00b7te", "Gr\u00fct\u00b7ze", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "mit einer fetten, wei\u00dfen Sahneschicht . . .", "tokens": ["mit", "ei\u00b7ner", "fet\u00b7ten", ",", "wei\u00b7\u00dfen", "Sah\u00b7ne\u00b7schicht", ".", ".", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct", "punct"], "pos": ["APPR", "ART", "ADJA", "$,", "ADJA", "NN", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Von wegen: Leibgericht.", "tokens": ["Von", "we\u00b7gen", ":", "Leib\u00b7ge\u00b7richt", "."], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["APPR", "APPR", "$.", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Mein Kind, der Ludolf, bohrt sich kleine Dinger", "tokens": ["Mein", "Kind", ",", "der", "Lu\u00b7dolf", ",", "bohrt", "sich", "klei\u00b7ne", "Din\u00b7ger"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "$,", "ART", "NE", "$,", "VVFIN", "PRF", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "aus seiner Nase \u2013 niemand haut ihm auf die Finger.", "tokens": ["aus", "sei\u00b7ner", "Na\u00b7se", "\u2013", "nie\u00b7mand", "haut", "ihm", "auf", "die", "Fin\u00b7ger", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "$(", "PIS", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Er strahlt, als einziger, im Trauerhaus.", "tokens": ["Er", "strahlt", ",", "als", "ein\u00b7zi\u00b7ger", ",", "im", "Trau\u00b7er\u00b7haus", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "KOUS", "ADJA", "$,", "APPRART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}, "line.4": {"text": "Und ich lieg da und denk: \u00bbAch, polk dich aus!\u00ab", "tokens": ["Und", "ich", "lieg", "da", "und", "denk", ":", "\u00bb", "Ach", ",", "polk", "dich", "aus", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "KON", "VVFIN", "$.", "$(", "ITJ", "$,", "VVIMP", "PPER", "PTKVZ", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Dann tragen M\u00e4nner mich vors Haus hinunter.", "tokens": ["Dann", "tra\u00b7gen", "M\u00e4n\u00b7ner", "mich", "vors", "Haus", "hin\u00b7un\u00b7ter", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJA", "NN", "PPER", "APPRART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Nun fa\u00dft der Karlchen die Blondine unter,", "tokens": ["Nun", "fa\u00dft", "der", "Karl\u00b7chen", "die", "Blon\u00b7di\u00b7ne", "un\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ART", "NN", "APPR", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "die mir zuletzt noch dies und jenes lieh . . .", "tokens": ["die", "mir", "zu\u00b7letzt", "noch", "dies", "und", "je\u00b7nes", "lieh", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ART", "PPER", "ADV", "ADV", "PDS", "KON", "PDS", "VVFIN", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Sie findet: Trauer kleidet sie.", "tokens": ["Sie", "fin\u00b7det", ":", "Trau\u00b7er", "klei\u00b7det", "sie", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "NN", "VVFIN", "PPER", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der Zug ruckt an. Und alle Damen,", "tokens": ["Der", "Zug", "ruckt", "an", ".", "Und", "al\u00b7le", "Da\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PTKVZ", "$.", "KON", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "die jemals, wenn was fehlte, zu mir kamen:", "tokens": ["die", "je\u00b7mals", ",", "wenn", "was", "fehl\u00b7te", ",", "zu", "mir", "ka\u00b7men", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "$,", "KOUS", "PIS", "VVFIN", "$,", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "vollz\u00e4hlig sind sie heut noch einmal da . . .", "tokens": ["voll\u00b7z\u00e4h\u00b7lig", "sind", "sie", "heut", "noch", "ein\u00b7mal", "da", ".", ".", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "ADV", "ADV", "ADV", "ADV", "$.", "$.", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und vorne rollt Papa.", "tokens": ["Und", "vor\u00b7ne", "rollt", "Pa\u00b7pa", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Da f\u00e4hrt die erste, die ich damals ohne", "tokens": ["Da", "f\u00e4hrt", "die", "ers\u00b7te", ",", "die", "ich", "da\u00b7mals", "oh\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "ADJA", "$,", "PRELS", "PPER", "ADV", "APPR"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "die leiseste Erfahrung k\u00fc\u00dfte \u2013 die Matrone", "tokens": ["die", "lei\u00b7ses\u00b7te", "Er\u00b7fah\u00b7rung", "k\u00fc\u00df\u00b7te", "\u2013", "die", "Mat\u00b7ro\u00b7ne"], "token_info": ["word", "word", "word", "word", "punct", "word", "word"], "pos": ["ART", "ADJA", "NN", "VVFIN", "$(", "ART", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "sitzt schlicht im Fond, mit kleinem Trauerhut.", "tokens": ["sitzt", "schlicht", "im", "Fond", ",", "mit", "klei\u00b7nem", "Trau\u00b7er\u00b7hut", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPRART", "NN", "$,", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Altmodisch war sie \u2013 aber sie war gut.", "tokens": ["Alt\u00b7mo\u00b7disch", "war", "sie", "\u2013", "a\u00b7ber", "sie", "war", "gut", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "PPER", "$(", "KON", "PPER", "VAFIN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Und Lotte! Lottchen mit dem kleinen Jungen!", "tokens": ["Und", "Lot\u00b7te", "!", "Lott\u00b7chen", "mit", "dem", "klei\u00b7nen", "Jun\u00b7gen", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NE", "$.", "NE", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Brieftr\u00e4ger jetzt! Wie ist mir der gelungen?", "tokens": ["Brief\u00b7tr\u00e4\u00b7ger", "jetzt", "!", "Wie", "ist", "mir", "der", "ge\u00b7lun\u00b7gen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "$.", "PWAV", "VAFIN", "PPER", "ART", "VVPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Ich sah ihn nie. Doch wo er immer schritt:", "tokens": ["Ich", "sah", "ihn", "nie", ".", "Doch", "wo", "er", "im\u00b7mer", "schritt", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "$.", "KON", "PWAV", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "mein Postscheck ging durch sechzehn Jahre mit.", "tokens": ["mein", "Post\u00b7scheck", "ging", "durch", "sech\u00b7zehn", "Jah\u00b7re", "mit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "CARD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.7": {"line.1": {"text": "Auf rotem samtnen Kissen, im Spaliere,", "tokens": ["Auf", "ro\u00b7tem", "samt\u00b7nen", "Kis\u00b7sen", ",", "im", "Spa\u00b7lie\u00b7re", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["APPR", "ADJA", "ADJA", "NN", "$,", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "da tragen feierlich zwei Reichswehroffiziere", "tokens": ["da", "tra\u00b7gen", "fei\u00b7er\u00b7lich", "zwei", "Reichs\u00b7wehr\u00b7of\u00b7fi\u00b7zie\u00b7re"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADJD", "CARD", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "die Orden durch die ganze Stadt", "tokens": ["die", "Or\u00b7den", "durch", "die", "gan\u00b7ze", "Stadt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "die mir mein Kaiser einst verliehen hat.", "tokens": ["die", "mir", "mein", "Kai\u00b7ser", "einst", "ver\u00b7lie\u00b7hen", "hat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PPOSAT", "NN", "ADV", "VVPP", "VAFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Und hinterm Sarg mit seinen Silberputten,", "tokens": ["Und", "hin\u00b7term", "Sarg", "mit", "sei\u00b7nen", "Sil\u00b7ber\u00b7put\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "da schreiten zwoundzwonzig Nutten \u2013", "tokens": ["da", "schrei\u00b7ten", "zwound\u00b7zwon\u00b7zig", "Nut\u00b7ten", "\u2013"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "CARD", "NN", "$("], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.3": {"text": "sie schluchzen innig und mit viel System.", "tokens": ["sie", "schluch\u00b7zen", "in\u00b7nig", "und", "mit", "viel", "Sys\u00b7tem", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "KON", "APPR", "PIAT", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Ich war zuletzt als Kunde sehr bequem.", "tokens": ["Ich", "war", "zu\u00b7letzt", "als", "Kun\u00b7de", "sehr", "be\u00b7quem", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "KOUS", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Das Ganze halt! Jetzt wird es dionysisch!", "tokens": ["Das", "Gan\u00b7ze", "halt", "!", "Jetzt", "wird", "es", "di\u00b7o\u00b7ny\u00b7sisch", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "ADV", "VAFIN", "PPER", "ADJD", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Nun singt ein Chor: Ich l\u00e4chle metaphysisch.", "tokens": ["Nun", "singt", "ein", "Chor", ":", "Ich", "l\u00e4ch\u00b7le", "me\u00b7ta\u00b7phy\u00b7sisch", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "PPER", "VVFIN", "ADJD", "$."], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Wie wird die schwarzgestrichne Kiste gro\u00df!", "tokens": ["Wie", "wird", "die", "schwarz\u00b7ge\u00b7strich\u00b7ne", "Kis\u00b7te", "gro\u00df", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ich schweige tief.", "tokens": ["Ich", "schwei\u00b7ge", "tief", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADJD", "$."], "meter": "-+-+", "measure": "iambic.di"}, "line.5": {"text": "Und bin mich endlich los.", "tokens": ["Und", "bin", "mich", "end\u00b7lich", "los", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "ADV", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}}}}