{"textgrid.poem.49672": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Prinzenexamen", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Auch Prinzen haben die Weisheit vonn\u00f6ten,", "tokens": ["Auch", "Prin\u00b7zen", "ha\u00b7ben", "die", "Weis\u00b7heit", "von\u00b7n\u00f6\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "VAFIN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Darum schickt man sie auf die Universit\u00e4ten,", "tokens": ["Da\u00b7rum", "schickt", "man", "sie", "auf", "die", "U\u00b7niv\u00b7er\u00b7si\u00b7t\u00e4\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PIS", "PPER", "APPR", "ART", "NN", "$,"], "meter": "--+--+-+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Damit hierorts ihr Verstand gedeiht.", "tokens": ["Da\u00b7mit", "hier\u00b7orts", "ihr", "Ver\u00b7stand", "ge\u00b7deiht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "So geschah es einem vor einiger Zeit.", "tokens": ["So", "ge\u00b7schah", "es", "ei\u00b7nem", "vor", "ei\u00b7ni\u00b7ger", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ART", "APPR", "PIAT", "NN", "$."], "meter": "--+-+--+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.2": {"line.1": {"text": "Aber nach Ablauf von nur zwei Jahren,", "tokens": ["A\u00b7ber", "nach", "Ab\u00b7lauf", "von", "nur", "zwei", "Jah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPR", "NN", "APPR", "ADV", "CARD", "NN", "$,"], "meter": "+--+--+-+-", "measure": "dactylic.di.plus"}, "line.2": {"text": "Von denen er das meiste auf der Eisenbahn gefahren,", "tokens": ["Von", "de\u00b7nen", "er", "das", "meis\u00b7te", "auf", "der", "Ei\u00b7sen\u00b7bahn", "ge\u00b7fah\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PPER", "PDS", "VVFIN", "APPR", "ART", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-+-+-+-", "measure": "iambic.septa"}, "line.3": {"text": "War des Prinzen Hoheit so klug,", "tokens": ["War", "des", "Prin\u00b7zen", "Ho\u00b7heit", "so", "klug", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "NN", "ADV", "ADJD", "$,"], "meter": "+-+-+--+", "measure": "iambic.tetra.chol"}, "line.4": {"text": "Da\u00df man fand, es sei nunmehr genug.", "tokens": ["Da\u00df", "man", "fand", ",", "es", "sei", "nun\u00b7mehr", "ge\u00b7nug", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$,", "PPER", "VAFIN", "ADV", "ADV", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.3": {"line.1": {"text": "Um jedoch den Schein zu vermeiden,", "tokens": ["Um", "je\u00b7doch", "den", "Schein", "zu", "ver\u00b7mei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "ADV", "ART", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+--+--+-", "measure": "amphibrach.tri"}, "line.2": {"text": "Als sei es anders bei den K\u00f6niglichen Hoheiten,", "tokens": ["Als", "sei", "es", "an\u00b7ders", "bei", "den", "K\u00f6\u00b7nig\u00b7li\u00b7chen", "Ho\u00b7hei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VAFIN", "PPER", "ADV", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+--+-", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Wie es bei den \u00fcbrigen Studiosis sei,", "tokens": ["Wie", "es", "bei", "den", "\u00fcb\u00b7ri\u00b7gen", "Stu\u00b7di\u00b7o\u00b7sis", "sei", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "APPR", "ART", "ADJA", "NN", "VAFIN", "$,"], "meter": "+-+-+--+-+-+", "measure": "trochaic.hexa.relaxed"}, "line.4": {"text": "Lie\u00df er sich zu einem Examen herbei.", "tokens": ["Lie\u00df", "er", "sich", "zu", "ei\u00b7nem", "E\u00b7xa\u00b7men", "her\u00b7bei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PRF", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Die Professores, welches dieses sollten wagen,", "tokens": ["Die", "Pro\u00b7fes\u00b7so\u00b7res", ",", "wel\u00b7ches", "die\u00b7ses", "soll\u00b7ten", "wa\u00b7gen", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "PDS", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Kamen herbei mit gro\u00dfem Zittern und Zagen,", "tokens": ["Ka\u00b7men", "her\u00b7bei", "mit", "gro\u00b7\u00dfem", "Zit\u00b7tern", "und", "Za\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "ADJA", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+--+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Sie scharrten dem\u00fctig mit dem Fu\u00df", "tokens": ["Sie", "scharr\u00b7ten", "de\u00b7m\u00fc\u00b7tig", "mit", "dem", "Fu\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADJD", "APPR", "ART", "NN"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Und entboten dem Prinzen ihren Gru\u00df.", "tokens": ["Und", "ent\u00b7bo\u00b7ten", "dem", "Prin\u00b7zen", "ih\u00b7ren", "Gru\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "--+--+-+-+", "measure": "anapaest.di.plus"}}, "stanza.5": {"line.1": {"text": "Der Herr Rektor machte den Anfang", "tokens": ["Der", "Herr", "Rek\u00b7tor", "mach\u00b7te", "den", "An\u00b7fang"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VVFIN", "ART", "NN"], "meter": "-++-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Und gab seiner Stimme einen sanften Klang,", "tokens": ["Und", "gab", "sei\u00b7ner", "Stim\u00b7me", "ei\u00b7nen", "sanf\u00b7ten", "Klang", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "ART", "ADJA", "NN", "$,"], "meter": "-+--+-+-+-+", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Indem er fragte mit ergebenem Ton:", "tokens": ["In\u00b7dem", "er", "frag\u00b7te", "mit", "er\u00b7ge\u00b7be\u00b7nem", "Ton", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.4": {"text": "\u00bbhoheit, was ist eine Konstitution?\u00ab", "tokens": ["\u00bb", "ho\u00b7heit", ",", "was", "ist", "ei\u00b7ne", "Kons\u00b7ti\u00b7tu\u00b7ti\u00b7on", "?", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "$,", "PWS", "VAFIN", "ART", "NN", "$.", "$("], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}}, "stanza.6": {"line.1": {"text": "Hier antwortete des Prinzen erlauchte", "tokens": ["Hier", "ant\u00b7wor\u00b7te\u00b7te", "des", "Prin\u00b7zen", "er\u00b7lauch\u00b7te"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ART", "NN", "VVFIN"], "meter": "-+-++-+--+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Person, wozu er l\u00e4ngere Zeit gebrauchte:", "tokens": ["Per\u00b7son", ",", "wo\u00b7zu", "er", "l\u00e4n\u00b7ge\u00b7re", "Zeit", "ge\u00b7brauch\u00b7te", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "PWAV", "PPER", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "\u00bbkonstitution ist, wenn das Volk stets tut,", "tokens": ["\u00bb", "kons\u00b7ti\u00b7tu\u00b7ti\u00b7on", "ist", ",", "wenn", "das", "Volk", "stets", "tut", ","], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "NE", "VAFIN", "$,", "KOUS", "ART", "NN", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.4": {"text": "Was uns h\u00f6chstselbst zu belieben geruht.\u00ab", "tokens": ["Was", "uns", "h\u00f6chst\u00b7selbst", "zu", "be\u00b7lie\u00b7ben", "ge\u00b7ruht", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "PPER", "ADV", "PTKZU", "VVINF", "VVPP", "$.", "$("], "meter": "+-+-+-+--+", "measure": "iambic.penta.chol"}}, "stanza.7": {"line.1": {"text": "\u00dcber diese Antwort des hohen Kandidaten", "tokens": ["\u00dc\u00b7ber", "die\u00b7se", "Ant\u00b7wort", "des", "ho\u00b7hen", "Kan\u00b7di\u00b7da\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PDAT", "NN", "ART", "ADJA", "NN"], "meter": "+-+-+--+-+-+-", "measure": "trochaic.hexa.relaxed"}, "line.2": {"text": "Konnten sich die Professores der Freude nicht entraten,", "tokens": ["Konn\u00b7ten", "sich", "die", "Pro\u00b7fes\u00b7so\u00b7res", "der", "Freu\u00b7de", "nicht", "ent\u00b7ra\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PRF", "ART", "NN", "ART", "NN", "PTKNEG", "VVFIN", "$,"], "meter": "+--+-+-+-+-+-+-", "measure": "iambic.septa.invert"}, "line.3": {"text": "Und es herrschte gro\u00dfe Verwundernis", "tokens": ["Und", "es", "herrschte", "gro\u00b7\u00dfe", "Ver\u00b7wun\u00b7der\u00b7nis"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VVFIN", "ADJA", "NN"], "meter": "---+--+-+", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "\u00dcber den filium principis.", "tokens": ["\u00dc\u00b7ber", "den", "fi\u00b7li\u00b7um", "prin\u00b7ci\u00b7pis", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "FM", "FM", "$."], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}}, "stanza.8": {"line.1": {"text": "Nun begann ein Professor zu fragen:", "tokens": ["Nun", "be\u00b7gann", "ein", "Pro\u00b7fes\u00b7sor", "zu", "fra\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "\u00bbbelieben Hoheit mir geneigtest zu sagen,", "tokens": ["\u00bb", "be\u00b7lie\u00b7ben", "Ho\u00b7heit", "mir", "ge\u00b7neig\u00b7test", "zu", "sa\u00b7gen", ","], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ADJA", "NN", "PPER", "VVPP", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Welche Befugnisse man kennt", "tokens": ["Wel\u00b7che", "Be\u00b7fug\u00b7nis\u00b7se", "man", "kennt"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAT", "NN", "PIS", "VVFIN"], "meter": "+--+---+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Als eigent\u00fcmlich dem Parlament?\u00ab", "tokens": ["Als", "ei\u00b7gen\u00b7t\u00fcm\u00b7lich", "dem", "Par\u00b7la\u00b7ment", "?", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["KOUS", "ADJD", "ART", "NN", "$.", "$("], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}}, "stanza.9": {"line.1": {"text": "Hier antwortete der Prinz: \u00bbHerr Professer,", "tokens": ["Hier", "ant\u00b7wor\u00b7te\u00b7te", "der", "Prinz", ":", "\u00bb", "Herr", "Pro\u00b7fes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "$.", "$(", "NN", "NN", "$,"], "meter": "-+--+-++-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Je weniger es solche gibt, desto besser,", "tokens": ["Je", "we\u00b7ni\u00b7ger", "es", "sol\u00b7che", "gibt", ",", "des\u00b7to", "bes\u00b7ser", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "ADV", "PPER", "PIS", "VVFIN", "$,", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Weil der Untertan dadurch beirrt", "tokens": ["Weil", "der", "Un\u00b7ter\u00b7tan", "da\u00b7durch", "be\u00b7irrt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "ART", "NN", "PAV", "VVFIN"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Im Betreffe seines Gehorsames wird.\u00ab", "tokens": ["Im", "Be\u00b7tref\u00b7fe", "sei\u00b7nes", "Ge\u00b7hor\u00b7sa\u00b7mes", "wird", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "NN", "VAFIN", "$.", "$("], "meter": "+-+-+--+--+", "measure": "trochaic.penta.relaxed"}}, "stanza.10": {"line.1": {"text": "Auch dieses Mal konnten nicht unterdr\u00fccken", "tokens": ["Auch", "die\u00b7ses", "Mal", "konn\u00b7ten", "nicht", "un\u00b7ter\u00b7dr\u00fc\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PDAT", "NN", "VMFIN", "PTKNEG", "VVINF"], "meter": "-+-++--+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Die Herren Professores ihr helles Entz\u00fccken,", "tokens": ["Die", "Her\u00b7ren", "Pro\u00b7fes\u00b7so\u00b7res", "ihr", "hel\u00b7les", "Ent\u00b7z\u00fc\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Und sie haben sodann unverweilt", "tokens": ["Und", "sie", "ha\u00b7ben", "so\u00b7dann", "un\u00b7ver\u00b7weilt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VAFIN", "ADV", "ADJD"], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}, "line.4": {"text": "Dem Prinzen das Reifezeugnis erteilt.", "tokens": ["Dem", "Prin\u00b7zen", "das", "Rei\u00b7fe\u00b7zeug\u00b7nis", "er\u00b7teilt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+--+-+--+", "measure": "iambic.tetra.relaxed"}}, "stanza.11": {"line.1": {"text": "Hieraus ist es als bewiesen erschienen:", "tokens": ["Hier\u00b7aus", "ist", "es", "als", "be\u00b7wie\u00b7sen", "er\u00b7schie\u00b7nen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "KOUS", "ADJA", "VVINF", "$."], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Wenn einer als Doktor will sein Brot verdienen,", "tokens": ["Wenn", "ei\u00b7ner", "als", "Dok\u00b7tor", "will", "sein", "Brot", "ver\u00b7die\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "KOKOM", "NN", "VMFIN", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Braucht er ", "tokens": ["Braucht", "er"], "token_info": ["word", "word"], "pos": ["VVFIN", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "F\u00fcr einen K\u00f6nig reichen schon ", "tokens": ["F\u00fcr", "ei\u00b7nen", "K\u00f6\u00b7nig", "rei\u00b7chen", "schon"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "VVFIN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}