{"textgrid.poem.49683": {"metadata": {"author": {"name": "Thoma, Ludwig", "birth": "N.A.", "death": "N.A."}, "title": "Bange Zweifel", "genre": "verse", "period": "N.A.", "pub_year": 1894, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Das Korn steht hoch und seine \u00c4hren bl\u00fch'n;", "tokens": ["Das", "Korn", "steht", "hoch", "und", "sei\u00b7ne", "\u00c4h\u00b7ren", "bl\u00fch'n", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ADJD", "KON", "PPOSAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Mit zarten Halmen sprie\u00dft der junge Haber;", "tokens": ["Mit", "zar\u00b7ten", "Hal\u00b7men", "sprie\u00dft", "der", "jun\u00b7ge", "Ha\u00b7ber", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Es strotzt der Weizen, \u00fcppig, dunkelgr\u00fcn;", "tokens": ["Es", "strotzt", "der", "Wei\u00b7zen", ",", "\u00fcp\u00b7pig", ",", "dun\u00b7kel\u00b7gr\u00fcn", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "ADJD", "$,", "ADJD", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Man sieht mit Freude rings den Segen \u2013 aber \u2013", "tokens": ["Man", "sieht", "mit", "Freu\u00b7de", "rings", "den", "Se\u00b7gen", "\u2013", "a\u00b7ber", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PIS", "VVFIN", "APPR", "NN", "ADV", "ART", "NN", "$(", "ADV", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Ein Seufzer ringt sich doch dem Deutschen los.", "tokens": ["Ein", "Seuf\u00b7zer", "ringt", "sich", "doch", "dem", "Deut\u00b7schen", "los", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PRF", "ADV", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "'s ist ja recht sch\u00f6n und gut, wenn wir im Frieden", "tokens": ["'s", "ist", "ja", "recht", "sch\u00f6n", "und", "gut", ",", "wenn", "wir", "im", "Frie\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "ADJD", "ADJD", "KON", "ADJD", "$,", "KOUS", "PPER", "APPRART", "NN"], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.3": {"text": "Den Reichtum ernten, den der Mutterscho\u00df", "tokens": ["Den", "Reich\u00b7tum", "ern\u00b7ten", ",", "den", "der", "Mut\u00b7ter\u00b7scho\u00df"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "VVINF", "$,", "PRELS", "ART", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Der Erde gab. Indessen \u2013 tja \u2013 vermieden", "tokens": ["Der", "Er\u00b7de", "gab", ".", "In\u00b7des\u00b7sen", "\u2013", "tja", "\u2013", "ver\u00b7mie\u00b7den"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["ART", "NN", "VVFIN", "$.", "NN", "$(", "FM.la", "$(", "VVPP"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Wir nicht am Ende doch den heil'chen Krieg", "tokens": ["Wir", "nicht", "am", "En\u00b7de", "doch", "den", "heil'\u00b7chen", "Krieg"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "PTKNEG", "APPRART", "NN", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Zum Schaden uns'rer nazjonalen Ehre??", "tokens": ["Zum", "Scha\u00b7den", "un\u00b7s'\u00b7rer", "naz\u00b7jo\u00b7na\u00b7len", "Eh\u00b7re", "??"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "PPOSAT", "ADJA", "NN", "$."], "meter": "-+-+--+-+-+-", "measure": "iambic.penta.relaxed"}, "line.3": {"text": "Die Frage ist, ob das Prestiesche stieg,", "tokens": ["Die", "Fra\u00b7ge", "ist", ",", "ob", "das", "Pre\u00b7stie\u00b7sche", "stieg", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$,", "KOUS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ob es nicht fiel trotz uns'rer scharfen Wehre?", "tokens": ["Ob", "es", "nicht", "fiel", "trotz", "un\u00b7s'\u00b7rer", "schar\u00b7fen", "Weh\u00b7re", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "VVFIN", "APPR", "PPOSAT", "ADJA", "NN", "$."], "meter": "+--+-+--+-+-", "measure": "iambic.penta.invert"}}, "stanza.4": {"line.1": {"text": "Wir lassen rings um diese sch\u00f6ne Welt", "tokens": ["Wir", "las\u00b7sen", "rings", "um", "die\u00b7se", "sch\u00f6\u00b7ne", "Welt"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Das trotzig blaue deutsche Auge rollen;", "tokens": ["Das", "trot\u00b7zig", "blau\u00b7e", "deut\u00b7sche", "Au\u00b7ge", "rol\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "ADJA", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Versteht man auch \u2013 die Frage ist gestellt \u2013,", "tokens": ["Ver\u00b7steht", "man", "auch", "\u2013", "die", "Fra\u00b7ge", "ist", "ge\u00b7stellt", "\u2013", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "PIS", "ADV", "$(", "ART", "NN", "VAFIN", "VVPP", "$(", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df wir mal anders k\u00f6nnen, wenn wir wollen?", "tokens": ["Da\u00df", "wir", "mal", "an\u00b7ders", "k\u00f6n\u00b7nen", ",", "wenn", "wir", "wol\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADV", "ADV", "VMFIN", "$,", "KOUS", "PPER", "VMFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Wie steht's in Asjen mit der Bagdadbahn?", "tokens": ["Wie", "steht's", "in", "As\u00b7jen", "mit", "der", "Bag\u00b7dad\u00b7bahn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "APPR", "NE", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Was macht in China uns're Einflu\u00dfsph\u00e4re?", "tokens": ["Was", "macht", "in", "Chi\u00b7na", "un\u00b7s'\u00b7re", "Ein\u00b7flu\u00df\u00b7sph\u00e4\u00b7re", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "NE", "PPOSAT", "NN", "$."], "meter": "-+----+-+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Hegt irgendeine Gro\u00dfmacht diesen Wahn,", "tokens": ["Hegt", "ir\u00b7gend\u00b7ei\u00b7ne", "Gro\u00df\u00b7macht", "die\u00b7sen", "Wahn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Da\u00df an der Sonne unser Platz nicht w\u00e4re?", "tokens": ["Da\u00df", "an", "der", "Son\u00b7ne", "un\u00b7ser", "Platz", "nicht", "w\u00e4\u00b7re", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "APPR", "ART", "NN", "PPOSAT", "NN", "PTKNEG", "VAFIN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Herrscht links und rechts von uns, herrscht \u00fcberall", "tokens": ["Herrscht", "links", "und", "rechts", "von", "uns", ",", "herrscht", "\u00fc\u00b7be\u00b7rall"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["NN", "ADV", "KON", "ADV", "APPR", "PPER", "$,", "VVFIN", "ADV"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Die rechte Ehrfurcht vor der deutschen Gr\u00f6\u00dfe?", "tokens": ["Die", "rech\u00b7te", "Ehr\u00b7furcht", "vor", "der", "deut\u00b7schen", "Gr\u00f6\u00b7\u00dfe", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "APPR", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Sonst n\u00e4mlich braust ein Ruf wie Donnerhall,", "tokens": ["Sonst", "n\u00e4m\u00b7lich", "braust", "ein", "Ruf", "wie", "Don\u00b7ner\u00b7hall", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VVFIN", "ART", "NN", "KOKOM", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Und 's w\u00e4re Zeit, da\u00df man das Schwert entbl\u00f6\u00dfe.", "tokens": ["Und", "'s", "w\u00e4\u00b7re", "Zeit", ",", "da\u00df", "man", "das", "Schwert", "ent\u00b7bl\u00f6\u00b7\u00dfe", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VAFIN", "NN", "$,", "KOUS", "PIS", "ART", "NN", "VVFIN", "$."], "meter": "--+-+-+-+-+-", "measure": "anapaest.init"}}, "stanza.7": {"line.1": {"text": "Die deutsche Tatkraft ist just zentenar,", "tokens": ["Die", "deut\u00b7sche", "Tat\u00b7kraft", "ist", "just", "zen\u00b7te\u00b7nar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+--+--+", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Auch Richard Wachner ward uns ja geboren,", "tokens": ["Auch", "Ric\u00b7hard", "Wach\u00b7ner", "ward", "uns", "ja", "ge\u00b7bo\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NE", "NN", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Und jeder \u00e4chte Deutsche f\u00fchlt, er war,", "tokens": ["Und", "je\u00b7der", "\u00e4ch\u00b7te", "Deut\u00b7sche", "f\u00fchlt", ",", "er", "war", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "PIAT", "ADJA", "NN", "VVFIN", "$,", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Er ist und bleibt zum H\u00f6chsten auserkoren.", "tokens": ["Er", "ist", "und", "bleibt", "zum", "H\u00f6chs\u00b7ten", "au\u00b7ser\u00b7ko\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "KON", "VVFIN", "APPRART", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Man trinkt sein Bier. Die heil'che Flamme gl\u00fcht,", "tokens": ["Man", "trinkt", "sein", "Bier", ".", "Die", "heil'\u00b7che", "Flam\u00b7me", "gl\u00fcht", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPOSAT", "NN", "$.", "ART", "ADJA", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Sie greift um sich in rasender Verbreitung,", "tokens": ["Sie", "greift", "um", "sich", "in", "ra\u00b7sen\u00b7der", "Ver\u00b7brei\u00b7tung", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "PRF", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Denn jeglicher Gedanke, der hier spr\u00fcht,", "tokens": ["Denn", "jeg\u00b7li\u00b7cher", "Ge\u00b7dan\u00b7ke", ",", "der", "hier", "spr\u00fcht", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "$,", "PRELS", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.4": {"text": "Ist ein Extrakt aus abonnierter Zeitung.", "tokens": ["Ist", "ein", "Ex\u00b7trakt", "aus", "a\u00b7bon\u00b7nier\u00b7ter", "Zei\u00b7tung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}}}}