{"dta.poem.848": {"metadata": {"author": {"name": "Gryphius, Andreas", "birth": "N.A.", "death": "N.A."}, "title": "XxXV.  Auff den Sontag der von der Welt  \n gehasseten Warheit/ oder  Exaudi,  \n Johan. 15.", "genre": "Lyrik, Drama", "period": "N.A.", "pub_year": "1650", "urn": "urn:nbn:de:kobv:b4-20218-7", "language": ["de:0.99"], "booktitle": "Gryphius, Andreas: Teutsche Reim-Gedichte. Frankfurt (Main), 1650."}, "poem": {"stanza.1": {"line.1": {"text": "Hjer bilde dir nichts eyn/ al\u00df Geisseln/ Strick vnd Band ", "tokens": ["Hjer", "bil\u00b7de", "dir", "nichts", "eyn", "/", "al\u00df", "Geis\u00b7seln", "/", "Strick", "vnd", "Band"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "PIS", "ART", "$(", "KOUS", "NN", "$(", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Al\u00df Zangen/ Schwerd vnd Todt/ dafern du Christo trewe", "tokens": ["Al\u00df", "Zan\u00b7gen", "/", "Schwerd", "vnd", "Todt", "/", "da\u00b7fern", "du", "Chris\u00b7to", "tre\u00b7we"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "$(", "NN", "KON", "NN", "$(", "KOUS", "PPER", "NE", "NE"], "meter": "-+-+-+-+--+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Wer ", "tokens": ["Wer"], "token_info": ["word"], "pos": ["PWS"], "meter": "-", "measure": "single.down"}, "line.4": {"text": "Die warheit sagen wil/ kracht offt in lichtem brand:", "tokens": ["Die", "war\u00b7heit", "sa\u00b7gen", "wil", "/", "kracht", "offt", "in", "lich\u00b7tem", "brand", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVINF", "VMFIN", "$(", "VVFIN", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Dein Seeligmacher selbst trug nichts dann Ha\u00df vnd", "tokens": ["Dein", "See\u00b7lig\u00b7ma\u00b7cher", "selbst", "trug", "nichts", "dann", "Ha\u00df", "vnd"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "ADV", "VVFIN", "PIS", "ADV", "NN", "KON"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Schand ", "tokens": ["Schand"], "token_info": ["word"], "pos": ["NN"], "meter": "+", "measure": "single.up"}, "line.3": {"text": "Al\u00df Schmach vnd Creutz zu lohn: Wen dunckts dann/ lieb-", "tokens": ["Al\u00df", "Schmach", "vnd", "Creutz", "zu", "lohn", ":", "Wen", "dunckts", "dann", "/", "lie\u00b7b"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word"], "pos": ["KOUS", "NN", "KON", "NN", "PTKZU", "VVINF", "$.", "PWS", "VVFIN", "ADV", "$(", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "ster/ newe ", "tokens": ["ster", "/", "ne\u00b7we"], "token_info": ["word", "punct", "word"], "pos": ["ADJD", "$(", "ADJA"], "meter": "+-+", "measure": "trochaic.di"}, "line.5": {"text": "Da\u00df offt der Christen Schar wie gar nicht n\u00fctze sprewe", "tokens": ["Da\u00df", "offt", "der", "Chris\u00b7ten", "Schar", "wie", "gar", "nicht", "n\u00fct\u00b7ze", "spre\u00b7we"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "ART", "ADJA", "NN", "KOKOM", "ADV", "PTKNEG", "VVFIN", "NE"], "meter": "-+-+-+-+-+--+", "measure": "iambic.hexa.chol.strict"}, "line.6": {"text": "Wird vom verfolgungs Nord gest\u00fcrmet au\u00df dem Land.", "tokens": ["Wird", "vom", "ver\u00b7fol\u00b7gungs", "Nord", "ge\u00b7st\u00fcr\u00b7met", "au\u00df", "dem", "Land", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPRART", "ADJA", "NN", "VVPP", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "Was machts? als da\u00df die Welt den Vatter nie erkennet:", "tokens": ["Was", "machts", "?", "als", "da\u00df", "die", "Welt", "den", "Vat\u00b7ter", "nie", "er\u00b7ken\u00b7net", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "$.", "KOKOM", "KOUS", "ART", "NN", "ART", "NN", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Vnd meynt jhr toller Zorn/ der so pocht/ w\u00fcrgt vnd bre\u00f1et", "tokens": ["Vnd", "meynt", "jhr", "tol\u00b7ler", "Zorn", "/", "der", "so", "pocht", "/", "w\u00fcrgt", "vnd", "bre\u00f1et"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KON", "VVFIN", "PPOSAT", "ADJA", "NN", "$(", "ART", "ADV", "VVFIN", "$(", "VVFIN", "KON", "VVFIN"], "meter": "-+-+-+--++-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Sey di\u00df was nur allein den h\u00f6chsten GOTT ergetzt.", "tokens": ["Sey", "di\u00df", "was", "nur", "al\u00b7lein", "den", "h\u00f6chs\u00b7ten", "GoTT", "er\u00b7getzt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PRELS", "ADV", "ADV", "ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Doch sey getrost/ der Geist/ der alle zeugen lehret", "tokens": ["Doch", "sey", "ge\u00b7trost", "/", "der", "Geist", "/", "der", "al\u00b7le", "zeu\u00b7gen", "leh\u00b7ret"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "VVPP", "$(", "ART", "NN", "$(", "ART", "PIS", "VVFIN", "VVFIN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Zeugt/ da\u00df wer bi\u00df zum Pfal den HERREN ", "tokens": ["Zeugt", "/", "da\u00df", "wer", "bi\u00df", "zum", "Pfal", "den", "HeR\u00b7REN"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["NE", "$(", "KOUS", "PWS", "ADV", "APPRART", "NN", "ART", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Werd au\u00df dem Creutz ins Reich/ au\u00df hohn in Lohn ver-", "tokens": ["Werd", "au\u00df", "dem", "Creutz", "ins", "Reich", "/", "au\u00df", "hohn", "in", "Lohn", "ver"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "APPR", "ART", "NN", "APPRART", "NN", "$(", "APPR", "NN", "APPR", "NN", "TRUNC"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "setzt. ", "tokens": ["setzt", "."], "token_info": ["word", "punct"], "pos": ["VVFIN", "$."], "meter": "+", "measure": "single.up"}}}}}