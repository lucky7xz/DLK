{"textgrid.poem.43926": {"metadata": {"author": {"name": "G\u00fcnther, Johann Christian", "birth": "N.A.", "death": "N.A."}, "title": "1L: Nun warthe, Flavia, das will ich dir gedencken!", "genre": "verse", "period": "N.A.", "pub_year": 1709, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Nun warthe, Flavia, das will ich dir gedencken!", "tokens": ["Nun", "wart\u00b7he", ",", "Fla\u00b7via", ",", "das", "will", "ich", "dir", "ge\u00b7den\u00b7cken", "!"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "$,", "NE", "$,", "PDS", "VMFIN", "PPER", "PPER", "NN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Du kennst den schmerzlichen Verdru\u00df,", "tokens": ["Du", "kennst", "den", "schmerz\u00b7li\u00b7chen", "Ver\u00b7dru\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn Lieb und Sehnsucht warthen mu\u00df,", "tokens": ["Wenn", "Lieb", "und", "Sehn\u00b7sucht", "wart\u00b7hen", "mu\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und kanst mich so empfindlich kr\u00e4ncken.", "tokens": ["Und", "kanst", "mich", "so", "emp\u00b7find\u00b7lich", "kr\u00e4n\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PPER", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ich weis ja nicht, woran ich bin,", "tokens": ["Ich", "weis", "ja", "nicht", ",", "wo\u00b7ran", "ich", "bin", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "PTKVZ", "ADV", "PTKNEG", "$,", "PWAV", "PPER", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ob Falschheit oder Noth dir Fu\u00df und Willen binde.", "tokens": ["Ob", "Falschheit", "o\u00b7der", "Noth", "dir", "Fu\u00df", "und", "Wil\u00b7len", "bin\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "KON", "NN", "PPER", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+--+-+-+-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Hier schick ich bey der kahlen Linde", "tokens": ["Hier", "schick", "ich", "bey", "der", "kah\u00b7len", "Lin\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Aus Eifer und aus Angst so Fluch als Seufzer hin.", "tokens": ["Aus", "Ei\u00b7fer", "und", "aus", "Angst", "so", "Fluch", "als", "Seuf\u00b7zer", "hin", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NN", "ADV", "NN", "KOUS", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.2": {"line.1": {"text": "Du nennst mir Zeit und Ort, du schwierst mir, gleich zu kommen;", "tokens": ["Du", "nennst", "mir", "Zeit", "und", "Ort", ",", "du", "schwierst", "mir", ",", "gleich", "zu", "kom\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "NN", "KON", "NN", "$,", "PPER", "VVFIN", "PPER", "$,", "ADV", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Ich lausch, ich zehl, ich hoff und fleh,", "tokens": ["Ich", "lausch", ",", "ich", "zehl", ",", "ich", "hoff", "und", "fleh", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "$,", "PPER", "ADJD", "$,", "PPER", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das Mondlicht hat, so viel ich seh,", "tokens": ["Das", "Mond\u00b7licht", "hat", ",", "so", "viel", "ich", "seh", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "$,", "ADV", "ADV", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Fast um ein Vierthel zugenommen.", "tokens": ["Fast", "um", "ein", "Vier\u00b7thel", "zu\u00b7ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Es teuscht mich Schatten, Hahn und Wind,", "tokens": ["Es", "teu\u00b7scht", "mich", "Schat\u00b7ten", ",", "Hahn", "und", "Wind", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "NN", "$,", "NE", "KON", "NN", "$,"], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.6": {"text": "Ich mein, ich seh dein Bild, so sind es nur Gedancken, Kind!", "tokens": ["Ich", "mein", ",", "ich", "seh", "dein", "Bild", ",", "so", "sind", "es", "nur", "Ge\u00b7dan\u00b7cken", ",", "Kind", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "PPOSAT", "$,", "PPER", "VVFIN", "PPOSAT", "NN", "$,", "ADV", "VAFIN", "PPER", "ADV", "NN", "$,", "NN", "$."], "meter": "-+-+-+-+-+-+-+", "measure": "iambic.septa"}, "line.7": {"text": "Und regt sich was um Strauch und Plancken,", "tokens": ["Und", "regt", "sich", "was", "um", "Strauch", "und", "Plan\u00b7cken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PRF", "PRELS", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "So schleich und zisch ich nur: Ach, kommstu? Komm, mein", "tokens": ["So", "schleich", "und", "zisch", "ich", "nur", ":", "Ach", ",", "komms\u00b7tu", "?", "Komm", ",", "mein"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["ADV", "ADJD", "KON", "VVFIN", "PPER", "ADV", "$.", "ITJ", "$,", "VVFIN", "$.", "VVFIN", "$,", "PPOSAT"], "meter": "-+-+--+-+-+", "measure": "iambic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "Die Nacht ist niemands Freund. Sie ist vielleicht erschrocken?", "tokens": ["Die", "Nacht", "ist", "nie\u00b7mands", "Freund", ".", "Sie", "ist", "viel\u00b7leicht", "er\u00b7schro\u00b7cken", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "NN", "$.", "PPER", "VAFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Verliebte ficht kein Blendwerk an.", "tokens": ["Ver\u00b7lieb\u00b7te", "ficht", "kein", "Blend\u00b7werk", "an", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PIAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Die Mutter ist nicht Schuld daran,", "tokens": ["Die", "Mut\u00b7ter", "ist", "nicht", "Schuld", "da\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKNEG", "NN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Denn jezo ruhn Gestrick und Rocken.", "tokens": ["Denn", "je\u00b7zo", "ruhn", "Ge\u00b7strick", "und", "Ro\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVINF", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Wie, wenn das M\u00e4gdgen untreu w\u00e4r?", "tokens": ["Wie", ",", "wenn", "das", "M\u00e4gd\u00b7gen", "un\u00b7treu", "w\u00e4r", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "$,", "KOUS", "ART", "NN", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Dies kenn ich auch zu gut, es thut mir nichts zum Po\u00dfen.", "tokens": ["Dies", "kenn", "ich", "auch", "zu", "gut", ",", "es", "thut", "mir", "nichts", "zum", "Po\u00b7\u00dfen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "ADV", "PTKA", "ADJD", "$,", "PPER", "VVFIN", "PPER", "PIS", "APPRART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So geh und mach ich tausend Glo\u00dfen", "tokens": ["So", "geh", "und", "mach", "ich", "tau\u00b7send", "Glo\u00b7\u00dfen"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "KON", "VVFIN", "PPER", "CARD", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Und sinne doch umsonst mit Unruh hin und her.", "tokens": ["Und", "sin\u00b7ne", "doch", "um\u00b7sonst", "mit", "Un\u00b7ruh", "hin", "und", "her", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "APPR", "NN", "PTKVZ", "KON", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.4": {"line.1": {"text": "Ach, warum lies ich dich doch einmahl aus den Armen?", "tokens": ["Ach", ",", "wa\u00b7rum", "lies", "ich", "dich", "doch", "ein\u00b7mahl", "aus", "den", "Ar\u00b7men", "?"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "$,", "PWAV", "VVFIN", "PPER", "PRF", "ADV", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Mein Weinen schmelzt und mehrt den Teich;", "tokens": ["Mein", "Wei\u00b7nen", "schmelzt", "und", "mehrt", "den", "Teich", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "KON", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Ich werd auf einmahl grau und bleich,", "tokens": ["Ich", "werd", "auf", "ein\u00b7mahl", "grau", "und", "bleich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es m\u00f6chte Stern und Stein erbarmen.", "tokens": ["Es", "m\u00f6ch\u00b7te", "Stern", "und", "Stein", "er\u00b7bar\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Ach, sollte morgen doch das Ei\u00df", "tokens": ["Ach", ",", "soll\u00b7te", "mor\u00b7gen", "doch", "das", "Ei\u00df"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ITJ", "$,", "VMFIN", "ADV", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die traurende Gestalt dir noch im Spiegel zeigen!", "tokens": ["Die", "trau\u00b7ren\u00b7de", "Ge\u00b7stalt", "dir", "noch", "im", "Spie\u00b7gel", "zei\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "PPER", "ADV", "APPRART", "NN", "VVINF", "$."], "meter": "-+---+-+-+-+-", "measure": "dactylic.init"}, "line.7": {"text": "Du w\u00fcrdest vor Erschr\u00f6ckn\u00fc\u00df schweigen,", "tokens": ["Du", "w\u00fcr\u00b7dest", "vor", "Er\u00b7schr\u00f6c\u00b7kn\u00fc\u00df", "schwei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Indem wohl deine Schuld nicht einen Vorwand weis.", "tokens": ["In\u00b7dem", "wohl", "dei\u00b7ne", "Schuld", "nicht", "ei\u00b7nen", "Vor\u00b7wand", "weis", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "PPOSAT", "NN", "PTKNEG", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.5": {"line.1": {"text": "Du scherzest wohl nicht gar? Das will ich ja nicht hofen,", "tokens": ["Du", "scher\u00b7zest", "wohl", "nicht", "gar", "?", "Das", "will", "ich", "ja", "nicht", "ho\u00b7fen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "PTKNEG", "ADV", "$.", "PDS", "VMFIN", "PPER", "ADV", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Es k\u00e4m uns beiden hoch zu stehn.", "tokens": ["Es", "k\u00e4m", "uns", "bei\u00b7den", "hoch", "zu", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PIAT", "ADJD", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Was h\u00f6r ich dort vor Th\u00fcren gehn?", "tokens": ["Was", "h\u00f6r", "ich", "dort", "vor", "Th\u00fc\u00b7ren", "gehn", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "ADV", "APPR", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Was seh ich vor ein Fenster ofen?", "tokens": ["Was", "seh", "ich", "vor", "ein", "Fens\u00b7ter", "o\u00b7fen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Hilf Himmel! Welcher Anblick f\u00e4llt?", "tokens": ["Hilf", "Him\u00b7mel", "!", "Wel\u00b7cher", "An\u00b7blick", "f\u00e4llt", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$.", "PWAT", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Ist dies nicht Scandors Haar? Ist dies nicht meine Sch\u00f6ne?", "tokens": ["Ist", "dies", "nicht", "Scan\u00b7dors", "Haar", "?", "Ist", "dies", "nicht", "mei\u00b7ne", "Sch\u00f6\u00b7ne", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDS", "PTKNEG", "NE", "NN", "$.", "VAFIN", "PDS", "PTKNEG", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "So hastu, listige Syrene,", "tokens": ["So", "has\u00b7tu", ",", "lis\u00b7ti\u00b7ge", "Sy\u00b7re\u00b7ne", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "$,", "ADJA", "NN", "$,"], "meter": "-+-+--++-", "measure": "iambic.tetra.relaxed"}, "line.8": {"text": "O Ansehn voller Schimpf, mich darum hergestellt?", "tokens": ["O", "An\u00b7sehn", "vol\u00b7ler", "Schimpf", ",", "mich", "da\u00b7rum", "her\u00b7ge\u00b7stellt", "?"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NN", "ADJA", "NN", "$,", "PRF", "PAV", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Den Streich verge\u00df ich nicht, es sey denn nach der Strafe.", "tokens": ["Den", "Streich", "ver\u00b7ge\u00df", "ich", "nicht", ",", "es", "sey", "denn", "nach", "der", "Stra\u00b7fe", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "PPER", "PTKNEG", "$,", "PPER", "VAFIN", "ADV", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Die Rache sey von nun an scharf", "tokens": ["Die", "Ra\u00b7che", "sey", "von", "nun", "an", "scharf"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VAFIN", "APPR", "ADV", "APZR", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und gebe, wo ich w\u00fcntschen darf,", "tokens": ["Und", "ge\u00b7be", ",", "wo", "ich", "w\u00fcnt\u00b7schen", "darf", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "PWAV", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df eure Brunst den Tag verschlafe.", "tokens": ["Da\u00df", "eu\u00b7re", "Brunst", "den", "Tag", "ver\u00b7schla\u00b7fe", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.5": {"text": "Das Schr\u00f6cken mache Spiel und Ku\u00df,", "tokens": ["Das", "Schr\u00f6\u00b7cken", "ma\u00b7che", "Spiel", "und", "Ku\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Die Hize deinen Leib, die Ohnmacht ihn zu Schanden,", "tokens": ["Die", "Hi\u00b7ze", "dei\u00b7nen", "Leib", ",", "die", "Ohn\u00b7macht", "ihn", "zu", "Schan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "$,", "ART", "NN", "PPER", "APPR", "NN", "$,"], "meter": "-+-+-+--+--+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Bis, wenn du trostlos aufgestanden,", "tokens": ["Bis", ",", "wenn", "du", "trost\u00b7los", "auf\u00b7ge\u00b7stan\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "$,", "KOUS", "PPER", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.8": {"text": "Dein eigner Mund mir selbst die Thorheit beichten mu\u00df.", "tokens": ["Dein", "eig\u00b7ner", "Mund", "mir", "selbst", "die", "Thor\u00b7heit", "beich\u00b7ten", "mu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "PPER", "ADV", "ART", "NN", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}