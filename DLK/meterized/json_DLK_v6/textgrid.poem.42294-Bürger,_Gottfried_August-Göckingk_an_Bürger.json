{"textgrid.poem.42294": {"metadata": {"author": {"name": "B\u00fcrger, Gottfried August", "birth": "N.A.", "death": "N.A."}, "title": "G\u00f6ckingk an B\u00fcrger", "genre": "verse", "period": "N.A.", "pub_year": 1775, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Verdammte Versemacherei!", "tokens": ["Ver\u00b7damm\u00b7te", "Ver\u00b7se\u00b7ma\u00b7che\u00b7rei", "!"], "token_info": ["word", "word", "punct"], "pos": ["ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Was hast du angerichtet?", "tokens": ["Was", "hast", "du", "an\u00b7ge\u00b7rich\u00b7tet", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Uns unsers Lebens einz'gen Mai", "tokens": ["Uns", "un\u00b7sers", "Le\u00b7bens", "einz'\u00b7gen", "Mai"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "PPOSAT", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zum Kuckuck hingedichtet?", "tokens": ["Zum", "Ku\u00b7ckuck", "hin\u00b7ge\u00b7dich\u00b7tet", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Gevatter ", "tokens": ["Ge\u00b7vat\u00b7ter"], "token_info": ["word"], "pos": ["NN"], "meter": "-+-", "measure": "amphibrach.single"}, "line.2": {"text": "Sind wir nicht brave Thoren,", "tokens": ["Sind", "wir", "nicht", "bra\u00b7ve", "Tho\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PTKNEG", "ADJA", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df wir, durch selbgemachte Qual,", "tokens": ["Da\u00df", "wir", ",", "durch", "selb\u00b7ge\u00b7mach\u00b7te", "Qual", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Den sch\u00f6nen Mai verloren?", "tokens": ["Den", "sch\u00f6\u00b7nen", "Mai", "ver\u00b7lo\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Was hat man von dem Dichten? Hum!", "tokens": ["Was", "hat", "man", "von", "dem", "Dich\u00b7ten", "?", "Hum", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "VAFIN", "PIS", "APPR", "ART", "NN", "$.", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vielleicht das bi\u00dfchen Ehre:", "tokens": ["Viel\u00b7leicht", "das", "bi\u00df\u00b7chen", "Eh\u00b7re", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Gekannt zu sein von Publikum? \u2013", "tokens": ["Ge\u00b7kannt", "zu", "sein", "von", "Pub\u00b7li\u00b7kum", "?", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVPP", "PTKZU", "VAINF", "APPR", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ich dachte, was mir w\u00e4re!", "tokens": ["Ich", "dach\u00b7te", ",", "was", "mir", "w\u00e4\u00b7re", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "$,", "PWS", "PPER", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Mag sein, da\u00df man bei Tafel spricht,", "tokens": ["Mag", "sein", ",", "da\u00df", "man", "bei", "Ta\u00b7fel", "spricht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "VAINF", "$,", "KOUS", "PIS", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wann den durchlauchten B\u00e4uchen", "tokens": ["Wann", "den", "durch\u00b7lauch\u00b7ten", "B\u00e4u\u00b7chen"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Die Zeit lang w\u00e4hrt: Ist B\u00fcrger nicht", "tokens": ["Die", "Zeit", "lang", "w\u00e4hrt", ":", "Ist", "B\u00fcr\u00b7ger", "nicht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "NN", "ADJD", "VVFIN", "$.", "VAFIN", "NN", "PTKNEG"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Amtmann zu Altengleichen?", "tokens": ["Amt\u00b7mann", "zu", "Al\u00b7ten\u00b7glei\u00b7chen", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}}, "stanza.5": {"line.1": {"text": "Ein Fr\u00e4ulein thut dir wohl sogar", "tokens": ["Ein", "Fr\u00e4u\u00b7lein", "thut", "dir", "wohl", "so\u00b7gar"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "VVFIN", "PPER", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Gnad' und fragt nicht minder:", "tokens": ["Die", "Gnad'", "und", "fragt", "nicht", "min\u00b7der", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "VVFIN", "PTKNEG", "ADV", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Tr\u00e4gt denn der B\u00fcrger eignes Haar?", "tokens": ["Tr\u00e4gt", "denn", "der", "B\u00fcr\u00b7ger", "eig\u00b7nes", "Haar", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Hat er schon Frau und Kinder?", "tokens": ["Hat", "er", "schon", "Frau", "und", "Kin\u00b7der", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "NN", "KON", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Ein Amtsauditor geht, bepackt", "tokens": ["Ein", "Amt\u00b7sau\u00b7di\u00b7tor", "geht", ",", "be\u00b7packt"], "token_info": ["word", "word", "word", "punct", "word"], "pos": ["ART", "NN", "VVFIN", "$,", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit deinem Buch, zu Sch\u00f6nen", "tokens": ["Mit", "dei\u00b7nem", "Buch", ",", "zu", "Sch\u00f6\u00b7nen"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "$,", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und lieset, da\u00df der Balken knackt", "tokens": ["Und", "lie\u00b7set", ",", "da\u00df", "der", "Bal\u00b7ken", "knackt"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "$,", "KOUS", "ART", "NN", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und alle Fenster dr\u00f6hnen.", "tokens": ["Und", "al\u00b7le", "Fens\u00b7ter", "dr\u00f6h\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Das h\u00f6rt denn ein Student und schreit:", "tokens": ["Das", "h\u00f6rt", "denn", "ein", "Stu\u00b7dent", "und", "schreit", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "ADV", "ART", "NN", "KON", "ADJD", "$."], "meter": "-+--+--+", "measure": "prosodiakos"}, "line.2": {"text": "\u00bbund wohnt' er bei den Sternen!", "tokens": ["\u00bb", "und", "wohnt'", "er", "bei", "den", "Ster\u00b7nen", "!"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "KON", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ich mu\u00df \u2013 ist Altengleichen weit? \u2013", "tokens": ["Ich", "mu\u00df", "\u2013", "ist", "Al\u00b7ten\u00b7glei\u00b7chen", "weit", "?", "\u2013"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PPER", "VMFIN", "$(", "VAFIN", "NN", "ADJD", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mu\u00df B\u00fcrgern kennen lernen.\u00ab", "tokens": ["Mu\u00df", "B\u00fcr\u00b7gern", "ken\u00b7nen", "ler\u00b7nen", ".", "\u00ab"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["VMFIN", "NN", "VVINF", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Und eh' Herr B\u00fcrger sich's versieht", "tokens": ["Und", "eh'", "Herr", "B\u00fcr\u00b7ger", "sich's", "ver\u00b7sieht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "NN", "NN", "NE", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "K\u00f6mmt mein Signor geritten,", "tokens": ["K\u00f6mmt", "mein", "Sig\u00b7nor", "ge\u00b7rit\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPOSAT", "NN", "VVPP", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.3": {"text": "Und B\u00fcrger, f\u00fcr sein herrlich Lied,", "tokens": ["Und", "B\u00fcr\u00b7ger", ",", "f\u00fcr", "sein", "herr\u00b7lich", "Lied", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "$,", "APPR", "PPOSAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mu\u00df ihn zum Essen bitten.", "tokens": ["Mu\u00df", "ihn", "zum", "Es\u00b7sen", "bit\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PPER", "APPRART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Da schlingt er nun den Truthahn ein,", "tokens": ["Da", "schlingt", "er", "nun", "den", "Trut\u00b7hahn", "ein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "ART", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den du mir aufbewahrtest,", "tokens": ["Den", "du", "mir", "auf\u00b7be\u00b7wahr\u00b7test", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und trinkt, \u2013 hol' ihn der Fuchs! \u2013 den Wein,", "tokens": ["Und", "trinkt", ",", "\u2013", "hol'", "ihn", "der", "Fuchs", "!", "\u2013", "den", "Wein", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "$(", "VVIMP", "PPER", "ART", "NE", "$.", "$(", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Den du f\u00fcr mich erspartest.", "tokens": ["Den", "du", "f\u00fcr", "mich", "er\u00b7spar\u00b7test", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "PPER", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Er r\u00fchmt dir ba\u00df sein gutes Herz,", "tokens": ["Er", "r\u00fchmt", "dir", "ba\u00df", "sein", "gu\u00b7tes", "Herz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Will Freundschaft mit dir treiben,", "tokens": ["Will", "Freund\u00b7schaft", "mit", "dir", "trei\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "NN", "APPR", "PPER", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und droht sogar \u2013 o H\u00f6llenschmerz! \u2013", "tokens": ["Und", "droht", "so\u00b7gar", "\u2013", "o", "H\u00f6l\u00b7len\u00b7schmerz", "!", "\u2013"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["KON", "VVFIN", "ADV", "$(", "FM", "NN", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Recht oft an dich zu schreiben.", "tokens": ["Recht", "oft", "an", "dich", "zu", "schrei\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Das macht: Manch ehrliches Journal", "tokens": ["Das", "macht", ":", "Manch", "ehr\u00b7li\u00b7ches", "Jour\u00b7nal"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["PDS", "VVFIN", "$.", "PIAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Lie\u00df laut dein Lob erschallen;", "tokens": ["Lie\u00df", "laut", "dein", "Lob", "er\u00b7schal\u00b7len", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Allein, wann las denn wohl einmal", "tokens": ["Al\u00b7lein", ",", "wann", "las", "denn", "wohl", "ein\u00b7mal"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "PWAV", "VVFIN", "ADV", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Herr B\u00fcrger Eins von allen?", "tokens": ["Herr", "B\u00fcr\u00b7ger", "Eins", "von", "al\u00b7len", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "NN", "APPR", "PIAT", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Und lie\u00df' ich dich in Kupfer, schier", "tokens": ["Und", "lie\u00df'", "ich", "dich", "in", "Kup\u00b7fer", ",", "schier"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["KON", "VVFIN", "PPER", "PRF", "APPR", "NN", "$,", "ADJA"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Von ", "tokens": ["Von"], "token_info": ["word"], "pos": ["APPR"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "Hilft dir es etwas, wenn von dir", "tokens": ["Hilft", "dir", "es", "et\u00b7was", ",", "wenn", "von", "dir"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "PPER", "PPER", "ADV", "$,", "KOUS", "APPR", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Leut' ein Weilchen sprechen?", "tokens": ["Die", "Leut'", "ein", "Weil\u00b7chen", "spre\u00b7chen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.13": {"line.1": {"text": "Was hast du von dem allen? Sklav!", "tokens": ["Was", "hast", "du", "von", "dem", "al\u00b7len", "?", "Sklav", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "APPR", "ART", "PIAT", "$.", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wenn ich's zusammenpresse,", "tokens": ["Wenn", "ich's", "zu\u00b7sam\u00b7men\u00b7pres\u00b7se", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "VVFIN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Was ist es, als: Despotenschlaf", "tokens": ["Was", "ist", "es", ",", "als", ":", "Des\u00b7po\u00b7ten\u00b7schlaf"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word"], "pos": ["PWS", "VAFIN", "PPER", "$,", "KOUS", "$.", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und Inquisiten-Bl\u00e4sse?", "tokens": ["Und", "In\u00b7qui\u00b7si\u00b7ten\u00b7Bl\u00e4s\u00b7se", "?"], "token_info": ["word", "word", "punct"], "pos": ["KON", "NN", "$."], "meter": "+-+--+-", "measure": "pherekrateus"}}, "stanza.14": {"line.1": {"text": "H\u00f6r' auf! Ich gab mein Herz dir hin,", "tokens": ["H\u00f6r'", "auf", "!", "Ich", "gab", "mein", "Herz", "dir", "hin", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKVZ", "$.", "PPER", "VVFIN", "PPOSAT", "NN", "PPER", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Eh' du ein Blatt geschrieben;", "tokens": ["Eh'", "du", "ein", "Blatt", "ge\u00b7schrie\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "H\u00f6r' auf! Und die Frau Amtmannin", "tokens": ["H\u00f6r'", "auf", "!", "Und", "die", "Frau", "Amt\u00b7man\u00b7nin"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "PTKVZ", "$.", "KON", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wird dich noch lieber lieben.", "tokens": ["Wird", "dich", "noch", "lie\u00b7ber", "lie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ADV", "ADV", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.15": {"line.1": {"text": "H\u00f6r' auf! Als Dichter kennt man dich,", "tokens": ["H\u00f6r'", "auf", "!", "Als", "Dich\u00b7ter", "kennt", "man", "dich", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PTKVZ", "$.", "KOUS", "NN", "VVFIN", "PIS", "PPER", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als Mensch lebst du verborgen;", "tokens": ["Als", "Mensch", "lebst", "du", "ver\u00b7bor\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVFIN", "PPER", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Kein Christenkind bek\u00fcmmert sich", "tokens": ["Kein", "Chris\u00b7ten\u00b7kind", "be\u00b7k\u00fcm\u00b7mert", "sich"], "token_info": ["word", "word", "word", "word"], "pos": ["PIAT", "NN", "VVFIN", "PRF"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Um alle deine Sorgen.", "tokens": ["Um", "al\u00b7le", "dei\u00b7ne", "Sor\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUI", "PIAT", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.16": {"line.1": {"text": "Ja! solltest du auch den Homer", "tokens": ["Ja", "!", "soll\u00b7test", "du", "auch", "den", "Ho\u00b7mer"], "token_info": ["word", "punct", "word", "word", "word", "word", "word"], "pos": ["PTKANT", "$.", "VMFIN", "PPER", "ADV", "ART", "NE"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "In Jamben \u00fcbersetzen,", "tokens": ["In", "Jam\u00b7ben", "\u00fc\u00b7bers\u00b7et\u00b7zen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Drob werden dich kein Haarbreit mehr", "tokens": ["Drob", "wer\u00b7den", "dich", "kein", "Haar\u00b7breit", "mehr"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "PIAT", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Herrn Minister sch\u00e4tzen.", "tokens": ["Die", "Herrn", "Mi\u00b7nis\u00b7ter", "sch\u00e4t\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.17": {"line.1": {"text": "Du w\u00fcrdest dennoch nach wie vor", "tokens": ["Du", "w\u00fcr\u00b7dest", "den\u00b7noch", "nach", "wie", "vor"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "KOKOM", "APPR"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Amtmann zu Gleichen bleiben;", "tokens": ["Amt\u00b7mann", "zu", "Glei\u00b7chen", "blei\u00b7ben", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "NN", "VVINF", "$."], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.3": {"text": "Drum, trauter B\u00fcrger, sei kein Thor,", "tokens": ["Drum", ",", "trau\u00b7ter", "B\u00fcr\u00b7ger", ",", "sei", "kein", "Thor", ","], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PAV", "$,", "ADJA", "NN", "$,", "VAFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und trinke, statt zu schreiben.", "tokens": ["Und", "trin\u00b7ke", ",", "statt", "zu", "schrei\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "$,", "KOUI", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}