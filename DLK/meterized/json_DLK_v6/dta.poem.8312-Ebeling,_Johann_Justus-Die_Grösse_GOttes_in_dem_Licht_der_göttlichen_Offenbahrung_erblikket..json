{"dta.poem.8312": {"metadata": {"author": {"name": "Ebeling, Johann Justus", "birth": "N.A.", "death": "N.A."}, "title": "Die Gr\u00f6sse GOttes in dem  \n Licht der g\u00f6ttlichen Offenbahrung  \n erblikket.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1747", "urn": "urn:nbn:de:kobv:b4-200905198782", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Grosser GOtt! in deinem Lichte", "tokens": ["Gros\u00b7ser", "Gott", "!", "in", "dei\u00b7nem", "Lich\u00b7te"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["ADJA", "NN", "$.", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sehn wir deine Herrlichkeit,", "tokens": ["Sehn", "wir", "dei\u00b7ne", "Herr\u00b7lich\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Dein verborgnes Angesichte", "tokens": ["Dein", "ver\u00b7borg\u00b7nes", "An\u00b7ge\u00b7sich\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Das mit Glanz und Pracht bestreut,", "tokens": ["Das", "mit", "Glanz", "und", "Pracht", "be\u00b7streut", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "NN", "KON", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Dein unendlich grosses Wesen,", "tokens": ["Dein", "un\u00b7end\u00b7lich", "gros\u00b7ses", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJD", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Ist am deutlichsten zu lesen,", "tokens": ["Ist", "am", "deut\u00b7lichs\u00b7ten", "zu", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPRART", "ADJA", "PTKZU", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "In der Schrift, die uns das zeigt,", "tokens": ["In", "der", "Schrift", ",", "die", "uns", "das", "zeigt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "$,", "PRELS", "PPER", "PDS", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Was uns die Natur verschweigt.", "tokens": ["Was", "uns", "die", "Na\u00b7tur", "ver\u00b7schweigt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.2": {"line.1": {"text": "Darin hast du uns beschrieben,", "tokens": ["Da\u00b7rin", "hast", "du", "uns", "be\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PRF", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.2": {"text": "Deiner GOttheit Majest\u00e4t,", "tokens": ["Dei\u00b7ner", "Got\u00b7theit", "Ma\u00b7jes\u00b7t\u00e4t", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Die der Mensch durch Furcht und Lieben,", "tokens": ["Die", "der", "Mensch", "durch", "Furcht", "und", "Lie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "APPR", "NN", "KON", "ADJA", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ueber alle Ding erh\u00f6ht;", "tokens": ["Ue\u00b7ber", "al\u00b7le", "Ding", "er\u00b7h\u00f6ht", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Darin hast du uns gesaget,", "tokens": ["Da\u00b7rin", "hast", "du", "uns", "ge\u00b7sa\u00b7get", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VAFIN", "PPER", "PPER", "VVPP", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.6": {"text": "Wornach sonst vergeblich fraget", "tokens": ["Wor\u00b7nach", "sonst", "ver\u00b7geb\u00b7lich", "fra\u00b7get"], "token_info": ["word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ADJD", "VVFIN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Unser Wiz, der das nicht trift,", "tokens": ["Un\u00b7ser", "Wiz", ",", "der", "das", "nicht", "trift", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PRELS", "PDS", "PTKNEG", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Was uns lehrt die heilge Schrift.", "tokens": ["Was", "uns", "lehrt", "die", "heil\u00b7ge", "Schrift", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.3": {"line.1": {"text": "Le\u00df ich nach wie du die Erde,", "tokens": ["Le\u00df", "ich", "nach", "wie", "du", "die", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "APPR", "PWAV", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Und den Himmel hast formirt,", "tokens": ["Und", "den", "Him\u00b7mel", "hast", "for\u00b7mirt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und wie dein allm\u00e4chtig Werde,", "tokens": ["Und", "wie", "dein", "all\u00b7m\u00e4ch\u00b7tig", "Wer\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "PPOSAT", "ADJD", "VAFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Aus dem Nichts das hergef\u00fchrt,", "tokens": ["Aus", "dem", "Nichts", "das", "her\u00b7ge\u00b7f\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PDS", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Was wir in der Welt erblikken,", "tokens": ["Was", "wir", "in", "der", "Welt", "er\u00b7blik\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "--+-+-+-", "measure": "anapaest.init"}, "line.6": {"text": "So kan uns das gleich eindr\u00fckken:", "tokens": ["So", "kan", "uns", "das", "gleich", "ein\u00b7dr\u00fck\u00b7ken", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "PDS", "ADV", "VVINF", "$."], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Durch ein Wort hat dargestellt.", "tokens": ["Durch", "ein", "Wort", "hat", "dar\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.4": {"line.1": {"text": "Le\u00df ich diese Welt-Geschichte,", "tokens": ["Le\u00df", "ich", "die\u00b7se", "Welt\u00b7Ge\u00b7schich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die uns Moses hat erz\u00e4hlt,", "tokens": ["Die", "uns", "Mo\u00b7ses", "hat", "er\u00b7z\u00e4hlt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "NE", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie nach deines Geists Berichte,", "tokens": ["Wie", "nach", "dei\u00b7nes", "Geists", "Be\u00b7rich\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "PPOSAT", "NN", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Alles weislich auserw\u00e4hlt,", "tokens": ["Al\u00b7les", "weis\u00b7lich", "au\u00b7ser\u00b7w\u00e4hlt", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PIS", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Was in diesem Kreis zu finden:", "tokens": ["Was", "in", "die\u00b7sem", "Kreis", "zu", "fin\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "APPR", "PDAT", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "So kan ich gar bald ergr\u00fcnden,", "tokens": ["So", "kan", "ich", "gar", "bald", "er\u00b7gr\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Da\u00df du seist ein grosser ", "tokens": ["Da\u00df", "du", "seist", "ein", "gros\u00b7ser"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "VAFIN", "ART", "ADJA"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.8": {"text": "Und ein weiser Zebaoth.", "tokens": ["Und", "ein", "wei\u00b7ser", "Ze\u00b7bao\u00b7th", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.5": {"line.1": {"text": "Als der Urstof aller Dinge,", "tokens": ["Als", "der", "Ur\u00b7stof", "al\u00b7ler", "Din\u00b7ge", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PIAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Aus des Nichtes Dunkelheit,", "tokens": ["Aus", "des", "Nich\u00b7tes", "Dun\u00b7kel\u00b7heit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Aus dem Reich der M\u00f6glichkeit", "tokens": ["Aus", "dem", "Reich", "der", "M\u00f6g\u00b7lich\u00b7keit"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "W\u00fcrklich war hervor gekommen,", "tokens": ["W\u00fcrk\u00b7lich", "war", "her\u00b7vor", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VAFIN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.5": {"text": "Hast du jegliches genommen,", "tokens": ["Hast", "du", "jeg\u00b7li\u00b7ches", "ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PIS", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "In die Ordnung eingebracht,", "tokens": ["In", "die", "Ord\u00b7nung", "ein\u00b7ge\u00b7bracht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Die die Welt uns herrlich macht.", "tokens": ["Die", "die", "Welt", "uns", "herr\u00b7lich", "macht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "PPER", "ADJD", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "In den aufgeschriebnen Werken", "tokens": ["In", "den", "auf\u00b7ge\u00b7schrieb\u00b7nen", "Wer\u00b7ken"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Deiner Sch\u00f6pfung, k\u00f6nnen wir,", "tokens": ["Dei\u00b7ner", "Sch\u00f6p\u00b7fung", ",", "k\u00f6n\u00b7nen", "wir", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VMFIN", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Deine Gr\u00f6sse auch bemerken,", "tokens": ["Dei\u00b7ne", "Gr\u00f6s\u00b7se", "auch", "be\u00b7mer\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Deine Herrlichkeit und Zier;", "tokens": ["Dei\u00b7ne", "Herr\u00b7lich\u00b7keit", "und", "Zier", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "KON", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Wenn ich die Beschreibung sehe,", "tokens": ["Wenn", "ich", "die", "Be\u00b7schrei\u00b7bung", "se\u00b7he", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Find ich auch von deiner H\u00f6he", "tokens": ["Find", "ich", "auch", "von", "dei\u00b7ner", "H\u00f6\u00b7he"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Ein recht majest\u00e4tisch Bild,", "tokens": ["Ein", "recht", "ma\u00b7jes\u00b7t\u00e4\u00b7tisch", "Bild", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Das mein Herz mit Ehrfurcht f\u00fcllt.", "tokens": ["Das", "mein", "Herz", "mit", "Ehr\u00b7furcht", "f\u00fcllt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PPOSAT", "NN", "APPR", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.7": {"line.1": {"text": "Dadurch lerne ich erkennen,", "tokens": ["Da\u00b7durch", "ler\u00b7ne", "ich", "er\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df du seist ein Jehovah,", "tokens": ["Da\u00df", "du", "seist", "ein", "Je\u00b7ho\u00b7vah", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VAFIN", "ART", "NN", "$,"], "meter": "+-+-+--", "measure": "unknown.measure.tri"}, "line.3": {"text": "Der nur etwas darf benennen,", "tokens": ["Der", "nur", "et\u00b7was", "darf", "be\u00b7nen\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PIS", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "So ist es zugleich auch da;", "tokens": ["So", "ist", "es", "zu\u00b7gleich", "auch", "da", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "ADV", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Auf den Wink mu\u00df das geschehen,", "tokens": ["Auf", "den", "Wink", "mu\u00df", "das", "ge\u00b7sche\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VMFIN", "PDS", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Was du wilt, das ist zu sehen.", "tokens": ["Was", "du", "wilt", ",", "das", "ist", "zu", "se\u00b7hen", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VMFIN", "$,", "PDS", "VAFIN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Le\u00df ich das; so f\u00e4llt mir ein,", "tokens": ["Le\u00df", "ich", "das", ";", "so", "f\u00e4llt", "mir", "ein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "PPER", "PDS", "$.", "ADV", "VVFIN", "PPER", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Deine Macht mu\u00df herrlich seyn.", "tokens": ["Dei\u00b7ne", "Macht", "mu\u00df", "herr\u00b7lich", "seyn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VMFIN", "ADJD", "VAINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.8": {"line.1": {"text": "Dein Wort hat uns das entdekket,", "tokens": ["Dein", "Wort", "hat", "uns", "das", "ent\u00b7dek\u00b7ket", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VAFIN", "PPER", "PDS", "VVFIN", "$,"], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}, "line.2": {"text": "Was der menschlichen Vernunft,", "tokens": ["Was", "der", "menschli\u00b7chen", "Ver\u00b7nunft", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "ADJA", "NN", "$,"], "meter": "+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Ganz verborgen, ganz verstekket,", "tokens": ["Ganz", "ver\u00b7bor\u00b7gen", ",", "ganz", "vers\u00b7tek\u00b7ket", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVPP", "$,", "ADV", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Und was aller Weisen Zunft", "tokens": ["Und", "was", "al\u00b7ler", "Wei\u00b7sen", "Zunft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PWS", "PIAT", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Durch den Wiz nicht kan ausfinden,", "tokens": ["Durch", "den", "Wiz", "nicht", "kan", "aus\u00b7fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "PTKNEG", "VMFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Wenn sie sich auch unterwinden,", "tokens": ["Wenn", "sie", "sich", "auch", "un\u00b7ter\u00b7win\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "In die Tieffen einzugehn,", "tokens": ["In", "die", "Tief\u00b7fen", "ein\u00b7zu\u00b7gehn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VVIZU", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Da wir dich im Dunklen sehn.", "tokens": ["Da", "wir", "dich", "im", "Dunk\u00b7len", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPRART", "NN", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.9": {"line.1": {"text": "Aber alles was wir lesen", "tokens": ["A\u00b7ber", "al\u00b7les", "was", "wir", "le\u00b7sen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PIS", "PWS", "PPER", "VVINF"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Das geoffenbahret ist,", "tokens": ["Das", "ge\u00b7of\u00b7fen\u00b7bah\u00b7ret", "ist", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDS", "VVPP", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Lehret uns das du ein Wesen,", "tokens": ["Leh\u00b7ret", "uns", "das", "du", "ein", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "PPER", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Grosser Herrlichkeiten bist,", "tokens": ["Gros\u00b7ser", "Herr\u00b7lich\u00b7kei\u00b7ten", "bist", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJA", "NN", "VAFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Das Unendlich, g\u00fctig, weise,", "tokens": ["Das", "Un\u00b7end\u00b7lich", ",", "g\u00fc\u00b7tig", ",", "wei\u00b7se", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["PDS", "ADJD", "$,", "ADJD", "$,", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Das in keine Schranken, Kreise", "tokens": ["Das", "in", "kei\u00b7ne", "Schran\u00b7ken", ",", "Krei\u00b7se"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["PDS", "APPR", "PIAT", "NN", "$,", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Einer Zeit wird einger\u00fckt,", "tokens": ["Ei\u00b7ner", "Zeit", "wird", "ein\u00b7ge\u00b7r\u00fckt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Noch an einem Ort umstrikt.", "tokens": ["Noch", "an", "ei\u00b7nem", "Ort", "um\u00b7strikt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.10": {"line.1": {"text": "In dem Lichte das uns scheinet,", "tokens": ["In", "dem", "Lich\u00b7te", "das", "uns", "schei\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Sehn wir dich auf deinem Thron", "tokens": ["Sehn", "wir", "dich", "auf", "dei\u00b7nem", "Thron"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "PRF", "APPR", "PPOSAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Anders als Vernunft es meinet,", "tokens": ["An\u00b7ders", "als", "Ver\u00b7nunft", "es", "mei\u00b7net", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "KOUS", "NN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Da wir dich als Vater, Sohn", "tokens": ["Da", "wir", "dich", "als", "Va\u00b7ter", ",", "Sohn"], "token_info": ["word", "word", "word", "word", "word", "punct", "word"], "pos": ["KOUS", "PPER", "PRF", "KOUS", "NN", "$,", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Und als heilgen Geist verehren,", "tokens": ["Und", "als", "heil\u00b7gen", "Geist", "ver\u00b7eh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Weil uns heilge M\u00e4nner lehren,", "tokens": ["Weil", "uns", "heil\u00b7ge", "M\u00e4n\u00b7ner", "leh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ADJA", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Da\u00df GOtt der dreieinig heist", "tokens": ["Da\u00df", "Gott", "der", "drei\u00b7ei\u00b7nig", "heist"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "ART", "CARD", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.8": {"text": "Vater, Sohn und heilger Geist.", "tokens": ["Va\u00b7ter", ",", "Sohn", "und", "heil\u00b7ger", "Geist", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "KON", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.11": {"line.1": {"text": "Dies Geheimnis kan uns lehren,", "tokens": ["Dies", "Ge\u00b7heim\u00b7nis", "kan", "uns", "leh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "VMFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df der gr\u00f6sseste Verstand,", "tokens": ["Da\u00df", "der", "gr\u00f6s\u00b7ses\u00b7te", "Ver\u00b7stand", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Den nicht fasset, den wir ehren,", "tokens": ["Den", "nicht", "fas\u00b7set", ",", "den", "wir", "eh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PTKNEG", "VVFIN", "$,", "PRELS", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Du bleibst ihn doch unbekant;", "tokens": ["Du", "bleibst", "ihn", "doch", "un\u00b7be\u00b7kant", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ADV", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Kein Wiz kan dich recht ermessen,", "tokens": ["Kein", "Wiz", "kan", "dich", "recht", "er\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VMFIN", "PRF", "ADJD", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Dieses zeugt von deinen Gr\u00f6ssen,", "tokens": ["Die\u00b7ses", "zeugt", "von", "dei\u00b7nen", "Gr\u00f6s\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Die auch keiner fa\u00dflich kennt,", "tokens": ["Die", "auch", "kei\u00b7ner", "fa\u00df\u00b7lich", "kennt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "PIS", "ADJD", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Da des Wortes Leuchte brennt.", "tokens": ["Da", "des", "Wor\u00b7tes", "Leuch\u00b7te", "brennt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "NN", "VVFIN", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.12": {"line.1": {"text": "Auch die tieffen Dunkelheiten,", "tokens": ["Auch", "die", "tief\u00b7fen", "Dun\u00b7kel\u00b7hei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die der Offenbahrung Licht,", "tokens": ["Die", "der", "Of\u00b7fen\u00b7bah\u00b7rung", "Licht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Uns noch l\u00e4\u00dft ohn Wiederstreiten,", "tokens": ["Uns", "noch", "l\u00e4\u00dft", "ohn", "Wie\u00b7der\u00b7strei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "VVFIN", "APPR", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Zeigen uns wie im Gesicht,", "tokens": ["Zei\u00b7gen", "uns", "wie", "im", "Ge\u00b7sicht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "KOKOM", "APPRART", "NN", "$,"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.5": {"text": "Da\u00df du seist ein ", "tokens": ["Da\u00df", "du", "seist", "ein"], "token_info": ["word", "word", "word", "word"], "pos": ["KOUS", "PPER", "VAFIN", "ART"], "meter": "-+-+", "measure": "iambic.di"}, "line.6": {"text": "Den wir nicht vollkommen kennen,", "tokens": ["Den", "wir", "nicht", "voll\u00b7kom\u00b7men", "ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "PTKNEG", "ADJD", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Und warum? weil wir zu klein,", "tokens": ["Und", "wa\u00b7rum", "?", "weil", "wir", "zu", "klein", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWAV", "$.", "KOUS", "PPER", "PTKA", "ADJD", "$,"], "meter": "--+-+-+", "measure": "anapaest.init"}, "line.8": {"text": "Gegen deine Gr\u00f6sse seyn.", "tokens": ["Ge\u00b7gen", "dei\u00b7ne", "Gr\u00f6s\u00b7se", "seyn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VAINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.13": {"line.1": {"text": "Lesen wir wie du regierest,", "tokens": ["Le\u00b7sen", "wir", "wie", "du", "re\u00b7gie\u00b7rest", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "PWAV", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie das Licht gleichsam dein Kleid,", "tokens": ["Wie", "das", "Licht", "gleich\u00b7sam", "dein", "Kleid", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADJD", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und der Herrschaft Scepter f\u00fchrest,", "tokens": ["Und", "der", "Herr\u00b7schaft", "Scep\u00b7ter", "f\u00fch\u00b7rest", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Mit G\u00fct und Gerechtigkeit:", "tokens": ["Mit", "G\u00fct", "und", "Ge\u00b7rech\u00b7tig\u00b7keit", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "NN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "So mu\u00df man zugleich gestehen,", "tokens": ["So", "mu\u00df", "man", "zu\u00b7gleich", "ge\u00b7ste\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "ADV", "VVPP", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Man kan dieses nicht ansehen,", "tokens": ["Man", "kan", "die\u00b7ses", "nicht", "an\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VMFIN", "PDS", "PTKNEG", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Ohne da\u00df man deutlich sp\u00fcrt,", "tokens": ["Oh\u00b7ne", "da\u00df", "man", "deut\u00b7lich", "sp\u00fcrt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "PIS", "ADJD", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Wie du herrlich bist geziert.", "tokens": ["Wie", "du", "herr\u00b7lich", "bist", "ge\u00b7ziert", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "ADJD", "VAFIN", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.14": {"line.1": {"text": "Gros ist deine Wunderg\u00fcte,", "tokens": ["Gros", "ist", "dei\u00b7ne", "Wun\u00b7der\u00b7g\u00fc\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wenn man drauf das Herze lenkt,", "tokens": ["Wenn", "man", "drauf", "das", "Her\u00b7ze", "lenkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PIS", "PAV", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und mit forschenden Gem\u00fcte", "tokens": ["Und", "mit", "for\u00b7schen\u00b7den", "Ge\u00b7m\u00fc\u00b7te"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "APPR", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Nach dem Wort der Schrift bedenkt,", "tokens": ["Nach", "dem", "Wort", "der", "Schrift", "be\u00b7denkt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Wie du trachtest vor die S\u00fcnden,", "tokens": ["Wie", "du", "trach\u00b7test", "vor", "die", "S\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VVFIN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Heilungs-Mittel zu erfinden,", "tokens": ["Hei\u00b7lungs\u00b7Mit\u00b7tel", "zu", "er\u00b7fin\u00b7den", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Da du die, die dich betr\u00fcbt,", "tokens": ["Da", "du", "die", ",", "die", "dich", "be\u00b7tr\u00fcbt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "$,", "PRELS", "PPER", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Als ein Vater doch geliebt.", "tokens": ["Als", "ein", "Va\u00b7ter", "doch", "ge\u00b7liebt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ADV", "VVPP", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.15": {"line.1": {"text": "Wenn wir deinen Rathschlus h\u00f6ren,", "tokens": ["Wenn", "wir", "dei\u00b7nen", "Rath\u00b7schlus", "h\u00f6\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Von der Menschen Seeligkeit,", "tokens": ["Von", "der", "Men\u00b7schen", "See\u00b7lig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und die s\u00fcssen Warheits-Lehren", "tokens": ["Und", "die", "s\u00fcs\u00b7sen", "Wa\u00b7rheits\u00b7Leh\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Von der frohen Gnadenzeit:", "tokens": ["Von", "der", "fro\u00b7hen", "Gna\u00b7den\u00b7zeit", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Und die Ordnung drin betrachten,", "tokens": ["Und", "die", "Ord\u00b7nung", "drin", "be\u00b7trach\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Wornach sich der Mensch mu\u00df achten:", "tokens": ["Wor\u00b7nach", "sich", "der", "Mensch", "mu\u00df", "ach\u00b7ten", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PRF", "ART", "NN", "VMFIN", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "So erkennt man mehr und mehr,", "tokens": ["So", "er\u00b7kennt", "man", "mehr", "und", "mehr", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "ADV", "KON", "ADV", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Da\u00df GOtt sei ein grosser Herr.", "tokens": ["Da\u00df", "Gott", "sei", "ein", "gros\u00b7ser", "Herr", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VAFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.16": {"line.1": {"text": "Gros ist er, dieweil er heilig,", "tokens": ["Gros", "ist", "er", ",", "die\u00b7weil", "er", "hei\u00b7lig", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "VAFIN", "PPER", "$,", "KOUS", "PPER", "ADJD", "$,"], "meter": "+---+-+-", "measure": "dactylic.init"}, "line.2": {"text": "Und der S\u00fcnde ewig feind;", "tokens": ["Und", "der", "S\u00fcn\u00b7de", "e\u00b7wig", "feind", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "NN", "ADJD", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Die vor ihm als was abscheulich,", "tokens": ["Die", "vor", "ihm", "als", "was", "ab\u00b7scheu\u00b7lich", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPER", "KOUS", "PIS", "ADJD", "$,"], "meter": "-+-+-+--", "measure": "unknown.measure.tri"}, "line.4": {"text": "Gros weil er der S\u00fcnder Freund", "tokens": ["Gros", "weil", "er", "der", "S\u00fcn\u00b7der", "Freund"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NE", "KOUS", "PPER", "ART", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Die in seinen Gnaden-Armen,", "tokens": ["Die", "in", "sei\u00b7nen", "Gna\u00b7den\u00b7Ar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Einzig finden ihr Erbarmen,", "tokens": ["Ein\u00b7zig", "fin\u00b7den", "ihr", "Er\u00b7bar\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "VVFIN", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Wenn sie gl\u00e4ubig in der Bu\u00df", "tokens": ["Wenn", "sie", "gl\u00e4u\u00b7big", "in", "der", "Bu\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ADJD", "APPR", "ART", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Jhren Sch\u00f6pfer falln zu Fu\u00df.", "tokens": ["Ih\u00b7ren", "Sch\u00f6p\u00b7fer", "falln", "zu", "Fu\u00df", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.17": {"line.1": {"text": "Sein gerechter Eifer lodert,", "tokens": ["Sein", "ge\u00b7rech\u00b7ter", "Ei\u00b7fer", "lo\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wider das, was b\u00f6se heist,", "tokens": ["Wi\u00b7der", "das", ",", "was", "b\u00f6\u00b7se", "heist", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPR", "PDS", "$,", "PWS", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Der stets die Vergeltung fodert,", "tokens": ["Der", "stets", "die", "Ver\u00b7gel\u00b7tung", "fo\u00b7dert", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADV", "ART", "NN", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Die er im Gericht beschleust;", "tokens": ["Die", "er", "im", "Ge\u00b7richt", "be\u00b7schleust", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Auch hieraus kan man ermessen,", "tokens": ["Auch", "hier\u00b7aus", "kan", "man", "er\u00b7mes\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PAV", "VMFIN", "PIS", "VVINF", "$,"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Seiner Eigenschaften Gr\u00f6ssen,", "tokens": ["Sei\u00b7ner", "Ei\u00b7gen\u00b7schaf\u00b7ten", "Gr\u00f6s\u00b7sen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Die unendlich und nach Recht,", "tokens": ["Die", "un\u00b7end\u00b7lich", "und", "nach", "Recht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "KON", "APPR", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Straffen einen S\u00fcnden Knecht.", "tokens": ["Straf\u00b7fen", "ei\u00b7nen", "S\u00fcn\u00b7den", "Knecht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "NE", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.18": {"line.1": {"text": "Sieht man aber dahingegen,", "tokens": ["Sieht", "man", "a\u00b7ber", "da\u00b7hin\u00b7ge\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PAV", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Seine weise G\u00fcte an,", "tokens": ["Sei\u00b7ne", "wei\u00b7se", "G\u00fc\u00b7te", "an", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "PTKVZ", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Die den Fluch in einen Seegen,", "tokens": ["Die", "den", "Fluch", "in", "ei\u00b7nen", "See\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wunderbar verwandeln kan,", "tokens": ["Wun\u00b7der\u00b7bar", "ver\u00b7wan\u00b7deln", "kan", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "VVINF", "VMFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Da\u00df das Recht nicht wird verlezzet,", "tokens": ["Da\u00df", "das", "Recht", "nicht", "wird", "ver\u00b7lez\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PTKNEG", "VAFIN", "VVPP", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "So wird unser Herz erg\u00f6zzet,", "tokens": ["So", "wird", "un\u00b7ser", "Herz", "er\u00b7g\u00f6z\u00b7zet", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Da\u00df ", "tokens": ["Da\u00df"], "token_info": ["word"], "pos": ["KOUS"], "meter": "+", "measure": "single.up"}, "line.8": {"text": "Seine Liebe zu erh\u00f6hn.", "tokens": ["Sei\u00b7ne", "Lie\u00b7be", "zu", "er\u00b7h\u00f6hn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.19": {"line.1": {"text": "In den weisen Bund der Gnaden,", "tokens": ["In", "den", "wei\u00b7sen", "Bund", "der", "Gna\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Den die Schrift uns kund gemacht,", "tokens": ["Den", "die", "Schrift", "uns", "kund", "ge\u00b7macht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "PPER", "PTKVZ", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Ist das Mittel vor den Schaden", "tokens": ["Ist", "das", "Mit\u00b7tel", "vor", "den", "Scha\u00b7den"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "ART", "NN", "APPR", "ART", "NN"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Unsrer Seel woll ausgedacht,", "tokens": ["Uns\u00b7rer", "Seel", "woll", "aus\u00b7ge\u00b7dacht", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Wenn ich ", "tokens": ["Wenn", "ich"], "token_info": ["word", "word"], "pos": ["KOUS", "PPER"], "meter": "++", "measure": "spondeus"}, "line.6": {"text": "Zu der S\u00fcnder Heil bedenke:", "tokens": ["Zu", "der", "S\u00fcn\u00b7der", "Heil", "be\u00b7den\u00b7ke", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "NN", "VVFIN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "So wird auch die Majest\u00e4t,", "tokens": ["So", "wird", "auch", "die", "Ma\u00b7jes\u00b7t\u00e4t", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ADV", "ART", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Unsers ", "tokens": ["Un\u00b7sers"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.20": {"line.1": {"text": "Da\u00df uns Christus von den B\u00f6sen,", "tokens": ["Da\u00df", "uns", "Chris\u00b7tus", "von", "den", "B\u00f6\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "NE", "APPR", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Nach des Vaters Gnaden-Raht", "tokens": ["Nach", "des", "Va\u00b7ters", "Gna\u00b7den\u00b7Raht"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "M\u00fcssen durch den Tod erl\u00f6sen,", "tokens": ["M\u00fcs\u00b7sen", "durch", "den", "Tod", "er\u00b7l\u00f6\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NE", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Welches er bewiesen hat;", "tokens": ["Wel\u00b7ches", "er", "be\u00b7wie\u00b7sen", "hat", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWS", "PPER", "VVPP", "VAFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Zeigt uns einmahl seine G\u00fcte,", "tokens": ["Zeigt", "uns", "ein\u00b7mahl", "sei\u00b7ne", "G\u00fc\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Ferner dr\u00fckts uns ins Gem\u00fcte,", "tokens": ["Fer\u00b7ner", "dr\u00fckts", "uns", "ins", "Ge\u00b7m\u00fc\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPRART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Es sei die Gerechtigkeit", "tokens": ["Es", "sei", "die", "Ge\u00b7rech\u00b7tig\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ART", "NN"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Zebaoths Vollkommenheit.", "tokens": ["Ze\u00b7baoths", "Voll\u00b7kom\u00b7men\u00b7heit", "."], "token_info": ["word", "word", "punct"], "pos": ["NE", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}}, "stanza.21": {"line.1": {"text": "Diese grossen Eigenschaften,", "tokens": ["Die\u00b7se", "gros\u00b7sen", "Ei\u00b7gen\u00b7schaf\u00b7ten", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["PDAT", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Die das h\u00f6chste Wesen ziern,", "tokens": ["Die", "das", "h\u00f6chs\u00b7te", "We\u00b7sen", "zi\u00b7ern", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.3": {"text": "Die in ", "tokens": ["Die", "in"], "token_info": ["word", "word"], "pos": ["ART", "APPR"], "meter": "-+", "measure": "iambic.single"}, "line.4": {"text": "K\u00f6nnen uns leicht dahin f\u00fchrn", "tokens": ["K\u00f6n\u00b7nen", "uns", "leicht", "da\u00b7hin", "f\u00fchrn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPER", "ADJD", "PAV", "VVINF"], "meter": "+--+--+", "measure": "dactylic.tri"}, "line.5": {"text": "Da\u00df wir ", "tokens": ["Da\u00df", "wir"], "token_info": ["word", "word"], "pos": ["KOUS", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.6": {"text": "Seine Gr\u00f6sse daher leiten,", "tokens": ["Sei\u00b7ne", "Gr\u00f6s\u00b7se", "da\u00b7her", "lei\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "PAV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Welche sich der ganzen Welt,", "tokens": ["Wel\u00b7che", "sich", "der", "gan\u00b7zen", "Welt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PRF", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Darin deutlich dargestellt.", "tokens": ["Da\u00b7rin", "deut\u00b7lich", "dar\u00b7ge\u00b7stellt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PAV", "ADJD", "VVPP", "$."], "meter": "--+-+-+", "measure": "anapaest.init"}}, "stanza.22": {"line.1": {"text": "Sehen wir die grossen Wunder,", "tokens": ["Se\u00b7hen", "wir", "die", "gros\u00b7sen", "Wun\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "In dem neuen Testament", "tokens": ["In", "dem", "neu\u00b7en", "Tes\u00b7ta\u00b7ment"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Wie des Glaubens reger Zunder,", "tokens": ["Wie", "des", "Glau\u00b7bens", "re\u00b7ger", "Zun\u00b7der", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Durch ein g\u00f6ttlich Licht entbrennt;", "tokens": ["Durch", "ein", "g\u00f6tt\u00b7lich", "Licht", "ent\u00b7brennt", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJD", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Wie durch der Apostel Zungen,", "tokens": ["Wie", "durch", "der", "A\u00b7pos\u00b7tel", "Zun\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Christi Lehre durchgedrungen:", "tokens": ["Chris\u00b7ti", "Leh\u00b7re", "durch\u00b7ge\u00b7drun\u00b7gen", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["NE", "NN", "VVPP", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "So sehn wir erstaunend an,", "tokens": ["So", "sehn", "wir", "er\u00b7stau\u00b7nend", "an", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "VVPP", "PTKVZ", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.8": {"text": "Was die Macht des H\u00f6chsten kan.", "tokens": ["Was", "die", "Macht", "des", "H\u00f6chs\u00b7ten", "kan", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "ART", "NN", "ART", "NN", "VMFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.23": {"line.1": {"text": "Auch aus denen Gnadenwerken,", "tokens": ["Auch", "aus", "de\u00b7nen", "Gna\u00b7den\u00b7wer\u00b7ken", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PRELS", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Kan ein achtsames Gem\u00fcth,", "tokens": ["Kan", "ein", "acht\u00b7sa\u00b7mes", "Ge\u00b7m\u00fcth", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "ADJA", "NN", "$,"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.3": {"text": "Unsers ", "tokens": ["Un\u00b7sers"], "token_info": ["word"], "pos": ["NN"], "meter": "+-", "measure": "trochaic.single"}, "line.4": {"text": "Seine Weisheit, seine G\u00fct.", "tokens": ["Sei\u00b7ne", "Weis\u00b7heit", ",", "sei\u00b7ne", "G\u00fct", "."], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "PPOSAT", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Da in denen Finsternissen,", "tokens": ["Da", "in", "de\u00b7nen", "Fins\u00b7ter\u00b7nis\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "PRELS", "NN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Seine Lichter leuchten m\u00fcssen,", "tokens": ["Sei\u00b7ne", "Lich\u00b7ter", "leuch\u00b7ten", "m\u00fcs\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVINF", "VMINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Wird die Blindheit selbst verkl\u00e4rt,", "tokens": ["Wird", "die", "Blind\u00b7heit", "selbst", "ver\u00b7kl\u00e4rt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "NN", "ADV", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Welche ", "tokens": ["Wel\u00b7che"], "token_info": ["word"], "pos": ["PWAT"], "meter": "+-", "measure": "trochaic.single"}}, "stanza.24": {"line.1": {"text": "Alles was die Schrift uns lehret,", "tokens": ["Al\u00b7les", "was", "die", "Schrift", "uns", "leh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PRELS", "ART", "NN", "PPER", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie sein Geist die Menschen f\u00fchrt,", "tokens": ["Wie", "sein", "Geist", "die", "Men\u00b7schen", "f\u00fchrt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Und zum ewgen Heil bekehret,", "tokens": ["Und", "zum", "ew\u00b7gen", "Heil", "be\u00b7keh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "APPRART", "ADJA", "NN", "VVFIN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie er ihre Seelen ziert:", "tokens": ["Wie", "er", "ih\u00b7re", "See\u00b7len", "ziert", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "PPOSAT", "NN", "VVFIN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Alles das giebt zu erkennen,", "tokens": ["Al\u00b7les", "das", "giebt", "zu", "er\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PDS", "VVFIN", "PTKZU", "VVINF", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.6": {"text": "Weil er durch des Wortes Kraft,", "tokens": ["Weil", "er", "durch", "des", "Wor\u00b7tes", "Kraft", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.7": {"text": "Uns ein neues Herze schaft.", "tokens": ["Uns", "ein", "neu\u00b7es", "Her\u00b7ze", "schaft", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ART", "ADJA", "VVFIN", "ADJD", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.25": {"line.1": {"text": "Lesen wir von k\u00fcnftgen Dingen,", "tokens": ["Le\u00b7sen", "wir", "von", "k\u00fcnft\u00b7gen", "Din\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPR", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Wie uns ", "tokens": ["Wie", "uns"], "token_info": ["word", "word"], "pos": ["PWAV", "PPER"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Durch den Glauben werde bringen,", "tokens": ["Durch", "den", "Glau\u00b7ben", "wer\u00b7de", "brin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "VAFIN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Wie er uns nach dieser Zeit", "tokens": ["Wie", "er", "uns", "nach", "die\u00b7ser", "Zeit"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PPER", "PRF", "APPR", "PDAT", "NN"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "Werde als die Gnaden-Sonne,", "tokens": ["Wer\u00b7de", "als", "die", "Gna\u00b7den\u00b7Son\u00b7ne", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "KOKOM", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Dort erfreun mit Himmels-Wonne:", "tokens": ["Dort", "er\u00b7freun", "mit", "Him\u00b7mels\u00b7Won\u00b7ne", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "APPR", "NN", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "So zeigt auch die Ewigkeit,", "tokens": ["So", "zeigt", "auch", "die", "E\u00b7wig\u00b7keit", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.26": {"line.1": {"text": "Kan der Frommen ewigs Leben,", "tokens": ["Kan", "der", "From\u00b7men", "e\u00b7wigs", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Uns von ", "tokens": ["Uns", "von"], "token_info": ["word", "word"], "pos": ["PPER", "APPR"], "meter": "+-", "measure": "trochaic.single"}, "line.3": {"text": "Einen grossen Abdruk geben,", "tokens": ["Ei\u00b7nen", "gros\u00b7sen", "Ab\u00b7druk", "ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "So wird er auch drin erh\u00f6ht,", "tokens": ["So", "wird", "er", "auch", "drin", "er\u00b7h\u00f6ht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "ADV", "VVPP", "$,"], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}, "line.5": {"text": "Wenn wir uns die Qual der H\u00f6llen,", "tokens": ["Wenn", "wir", "uns", "die", "Qual", "der", "H\u00f6l\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "ART", "NN", "ART", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Als der b\u00f6sen Straf vorstellen,", "tokens": ["Als", "der", "b\u00f6\u00b7sen", "Straf", "vor\u00b7stel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "Weil er als ein Zebaoth,", "tokens": ["Weil", "er", "als", "ein", "Ze\u00b7bao\u00b7th", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "KOUS", "ART", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.8": {"text": "Damit die Verkehrten droht.", "tokens": ["Da\u00b7mit", "die", "Ver\u00b7kehr\u00b7ten", "droht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "ART", "NN", "VVFIN", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}, "stanza.27": {"line.1": {"text": "Alles was wir also lesen,", "tokens": ["Al\u00b7les", "was", "wir", "al\u00b7so", "le\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "PWS", "PPER", "ADV", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Was sein Wort als wahr ausspricht,", "tokens": ["Was", "sein", "Wort", "als", "wahr", "aus\u00b7spricht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "PPOSAT", "NN", "KOKOM", "ADJD", "VVPP", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Zeiget uns von seinem Wesen,", "tokens": ["Zei\u00b7get", "uns", "von", "sei\u00b7nem", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Ein recht herrlich Angesicht.", "tokens": ["Ein", "recht", "herr\u00b7lich", "An\u00b7ge\u00b7sicht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJD", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "M\u00f6chten diese Warheits-Lehren,", "tokens": ["M\u00f6ch\u00b7ten", "die\u00b7se", "Wa\u00b7rheits\u00b7Leh\u00b7ren", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["VMFIN", "PDAT", "NN", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.6": {"text": "Darauf unser Herze kehren,", "tokens": ["Da\u00b7rauf", "un\u00b7ser", "Her\u00b7ze", "keh\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "PPOSAT", "NN", "VVINF", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.7": {"text": "Da\u00df wir d\u00e4chten: ", "tokens": ["Da\u00df", "wir", "d\u00e4ch\u00b7ten", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.8": {"text": "Wir sind elend, blind und blos!", "tokens": ["Wir", "sind", "e\u00b7lend", ",", "blind", "und", "blos", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADJD", "$,", "ADJD", "KON", "ADV", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.28": {"line.1": {"text": "Alsdenn w\u00fcrden wir erkennen,", "tokens": ["Als\u00b7denn", "w\u00fcr\u00b7den", "wir", "er\u00b7ken\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.2": {"text": "Da\u00df er alles und wir nur", "tokens": ["Da\u00df", "er", "al\u00b7les", "und", "wir", "nur"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "PIS", "KON", "PPER", "ADV"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.3": {"text": "Erdenw\u00fcrmer zu benennen,", "tokens": ["Er\u00b7den\u00b7w\u00fcr\u00b7mer", "zu", "be\u00b7nen\u00b7nen", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PTKZU", "VVINF", "$,"], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.4": {"text": "Die des Sch\u00f6pfers breite Spur,", "tokens": ["Die", "des", "Sch\u00f6p\u00b7fers", "brei\u00b7te", "Spur", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ART", "NN", "ADJA", "NN", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.5": {"text": "In der Demuth hier ansehen,", "tokens": ["In", "der", "De\u00b7muth", "hier", "an\u00b7se\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "NN", "ADV", "VVINF", "$,"], "meter": "+--+--+-", "measure": "dactylic.tri"}, "line.6": {"text": "Deren Gr\u00f6\u00df wir nicht verstehen;", "tokens": ["De\u00b7ren", "Gr\u00f6\u00df", "wir", "nicht", "ver\u00b7ste\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "PPER", "PTKNEG", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}, "line.7": {"text": "So w\u00fcrd unser Wallspruch seyn:", "tokens": ["So", "w\u00fcrd", "un\u00b7ser", "Wall\u00b7spruch", "seyn", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPOSAT", "NN", "VAINF", "$."], "meter": "-+--+-+", "measure": "iambic.tri.relaxed"}}}}}