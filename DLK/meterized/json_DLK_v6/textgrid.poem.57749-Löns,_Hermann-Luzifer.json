{"textgrid.poem.57749": {"metadata": {"author": {"name": "L\u00f6ns, Hermann", "birth": "N.A.", "death": "N.A."}, "title": "Luzifer", "genre": "verse", "period": "N.A.", "pub_year": 1890, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Am Tage des j\u00fcngsten Gerichtes war es,", "tokens": ["Am", "Ta\u00b7ge", "des", "j\u00fcng\u00b7sten", "Ge\u00b7rich\u00b7tes", "war", "es", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ART", "ADJA", "NN", "VAFIN", "PPER", "$,"], "meter": "-+--++-+-+-", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Am letzten Tage des letzten Jahres;", "tokens": ["Am", "letz\u00b7ten", "Ta\u00b7ge", "des", "letz\u00b7ten", "Jah\u00b7res", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Mit weithin h\u00f6rbarem Donnerknall", "tokens": ["Mit", "weit\u00b7hin", "h\u00f6r\u00b7ba\u00b7rem", "Don\u00b7ner\u00b7knall"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ADV", "ADJA", "NN"], "meter": "-+-+--+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Zersprang der morsche Erdenball.", "tokens": ["Zer\u00b7sprang", "der", "mor\u00b7sche", "Er\u00b7den\u00b7ball", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Alle, die dort vorhanden waren,", "tokens": ["Al\u00b7le", ",", "die", "dort", "vor\u00b7han\u00b7den", "wa\u00b7ren", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "$,", "PRELS", "ADV", "ADJD", "VAFIN", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.2": {"text": "Sind sofort gen Himmel gefahren;", "tokens": ["Sind", "so\u00b7fort", "gen", "Him\u00b7mel", "ge\u00b7fah\u00b7ren", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "APPR", "NN", "VVPP", "$."], "meter": "+-+-+--+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Sie scharten sich um Gottes Thron,", "tokens": ["Sie", "schar\u00b7ten", "sich", "um", "Got\u00b7tes", "Thron", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PRF", "APPR", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Strafe erharrend oder Lohn.", "tokens": ["Stra\u00b7fe", "er\u00b7har\u00b7rend", "o\u00b7der", "Lohn", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "VVPP", "KON", "NN", "$."], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}}, "stanza.3": {"line.1": {"text": "Die Frommen, die ganz vorne standen,", "tokens": ["Die", "From\u00b7men", ",", "die", "ganz", "vor\u00b7ne", "stan\u00b7den", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Zu ihrer Entr\u00fcstung pl\u00f6tzlich fanden,", "tokens": ["Zu", "ih\u00b7rer", "Ent\u00b7r\u00fcs\u00b7tung", "pl\u00f6tz\u00b7lich", "fan\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "ADJD", "VVFIN", "$,"], "meter": "+---+-+-+-", "measure": "dactylic.init"}, "line.3": {"text": "Da\u00df auch Satan bei ihnen sei;", "tokens": ["Da\u00df", "auch", "Sa\u00b7tan", "bei", "ih\u00b7nen", "sei", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "NN", "APPR", "PPER", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das war ihnen gar nicht einerlei.", "tokens": ["Das", "war", "ih\u00b7nen", "gar", "nicht", "ei\u00b7ner\u00b7lei", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "PTKNEG", "ADV", "$."], "meter": "----+-+-+", "measure": "unknown.measure.tri"}}, "stanza.4": {"line.1": {"text": "Lie\u00dfen darum bei dem Herren fragen,", "tokens": ["Lie\u00b7\u00dfen", "da\u00b7rum", "bei", "dem", "Her\u00b7ren", "fra\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PAV", "APPR", "ART", "NN", "VVINF", "$,"], "meter": "+-+-+-+-+-", "measure": "trochaic.penta"}, "line.2": {"text": "Was er eigentlich t\u00e4te sagen", "tokens": ["Was", "er", "ei\u00b7gent\u00b7lich", "t\u00e4\u00b7te", "sa\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "PPER", "ADV", "VVFIN", "VVINF"], "meter": "---+-+-+-", "measure": "unknown.measure.tri"}, "line.3": {"text": "Zu dieser Frechheit sonder Ma\u00df,", "tokens": ["Zu", "die\u00b7ser", "Frech\u00b7heit", "son\u00b7der", "Ma\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die sich herausn\u00e4hme Satanas.", "tokens": ["Die", "sich", "her\u00b7aus\u00b7n\u00e4h\u00b7me", "Sa\u00b7ta\u00b7nas", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "PRF", "ADJA", "NN", "$."], "meter": "+-+-+-+-+", "measure": "trochaic.penta"}}, "stanza.5": {"line.1": {"text": "Der Herr sprach ohne viel Federlesen:", "tokens": ["Der", "Herr", "sprach", "oh\u00b7ne", "viel", "Fe\u00b7der\u00b7le\u00b7sen", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "PIAT", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "\u00bbauf mein Gehei\u00df ist er Satan gewesen;", "tokens": ["\u00bb", "auf", "mein", "Ge\u00b7hei\u00df", "ist", "er", "Sa\u00b7tan", "ge\u00b7we\u00b7sen", ";"], "token_info": ["punct", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "PPOSAT", "NN", "VAFIN", "PPER", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "H\u00e4tte die S\u00fcnde gehabt nicht Platz,", "tokens": ["H\u00e4t\u00b7te", "die", "S\u00fcn\u00b7de", "ge\u00b7habt", "nicht", "Platz", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "VAPP", "PTKNEG", "NN", "$,"], "meter": "+--+--+-+", "measure": "dactylic.di.plus"}, "line.4": {"text": "Wo w\u00e4re geblieben der Gegensatz?", "tokens": ["Wo", "w\u00e4\u00b7re", "ge\u00b7blie\u00b7ben", "der", "Ge\u00b7gen\u00b7satz", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "VVPP", "ART", "NN", "$."], "meter": "-+--+--+-+", "measure": "amphibrach.tri.plus"}}, "stanza.6": {"line.1": {"text": "W\u00e4re kein Satan zur Erde gekommen,", "tokens": ["W\u00e4\u00b7re", "kein", "Sa\u00b7tan", "zur", "Er\u00b7de", "ge\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIAT", "NN", "APPRART", "NN", "VVPP", "$,"], "meter": "+--+--+--+-", "measure": "dactylic.tetra"}, "line.2": {"text": "G\u00e4b's keine Sch\u00e4cher, g\u00e4b's keine Frommen.", "tokens": ["G\u00e4b's", "kei\u00b7ne", "Sch\u00e4\u00b7cher", ",", "g\u00e4b's", "kei\u00b7ne", "From\u00b7men", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "PIAT", "NN", "$,", "NE", "PIAT", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Ihr Erzengel, \u00f6ffnet eure Reih'n:", "tokens": ["Ihr", "Er\u00b7zen\u00b7gel", ",", "\u00f6ff\u00b7net", "eu\u00b7re", "Reih'n", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+--+-+-+", "measure": "iambic.tetra.relaxed"}, "line.4": {"text": "Fortan soll er wieder Luzifer sein!\u00ab", "tokens": ["For\u00b7tan", "soll", "er", "wie\u00b7der", "Lu\u00b7zi\u00b7fer", "sein", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["NE", "VMFIN", "PPER", "ADV", "NE", "VAINF", "$.", "$("], "meter": "+-+-+--+-+", "measure": "trochaic.penta.relaxed"}}}}}