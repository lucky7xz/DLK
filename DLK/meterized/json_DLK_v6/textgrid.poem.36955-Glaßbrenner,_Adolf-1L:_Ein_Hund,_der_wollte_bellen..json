{"textgrid.poem.36955": {"metadata": {"author": {"name": "Gla\u00dfbrenner, Adolf", "birth": "N.A.", "death": "N.A."}, "title": "1L: Ein Hund, der wollte bellen.", "genre": "verse", "period": "N.A.", "pub_year": 1843, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ein Hund, der wollte bellen.", "tokens": ["Ein", "Hund", ",", "der", "woll\u00b7te", "bel\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Ein Hund, der wollte bellen.", "tokens": ["Ein", "Hund", ",", "der", "woll\u00b7te", "bel\u00b7len", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bellen, knurren!", "tokens": ["Bel\u00b7len", ",", "knur\u00b7ren", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Murren, blaffen! Ein Hund, der wollte bellen.", "tokens": ["Mur\u00b7ren", ",", "blaf\u00b7fen", "!", "Ein", "Hund", ",", "der", "woll\u00b7te", "bel\u00b7len", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVINF", "$.", "ART", "NN", "$,", "PRELS", "VMFIN", "VVINF", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Der Hausherr wollt's nicht dulden.", "tokens": ["Der", "Haus\u00b7herr", "wollt's", "nicht", "dul\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Der Hausherr wollt's nicht dulden.", "tokens": ["Der", "Haus\u00b7herr", "wollt's", "nicht", "dul\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NE", "PTKNEG", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Dulden, leiden!", "tokens": ["Dul\u00b7den", ",", "lei\u00b7den", "!"], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVINF", "$."], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Wollen, wi\u00dfen! Der Hausherr wollt's nicht dulden.", "tokens": ["Wol\u00b7len", ",", "wi\u00b7\u00dfen", "!", "Der", "Haus\u00b7herr", "wollt's", "nicht", "dul\u00b7den", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVINF", "$.", "ART", "NN", "NE", "PTKNEG", "VVINF", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.3": {"line.1": {"text": "Canaille, bell' gen Andre!", "tokens": ["Ca\u00b7nail\u00b7le", ",", "bell'", "gen", "And\u00b7re", "!"], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "APPR", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Nicht gegen mich, gen Andre!", "tokens": ["Nicht", "ge\u00b7gen", "mich", ",", "gen", "And\u00b7re", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PTKNEG", "APPR", "PPER", "$,", "APPR", "PIS", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Belle, blaffe", "tokens": ["Bel\u00b7le", ",", "blaf\u00b7fe"], "token_info": ["word", "punct", "word"], "pos": ["NE", "$,", "ADJA"], "meter": "+-+-", "measure": "trochaic.di"}, "line.4": {"text": "Gegen Andre! Sonst halte deine Schnauze!", "tokens": ["Ge\u00b7gen", "And\u00b7re", "!", "Sonst", "hal\u00b7te", "dei\u00b7ne", "Schnau\u00b7ze", "!"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIS", "$.", "NN", "VVFIN", "PPOSAT", "NN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Er gab dem Hund den Maulkorb.", "tokens": ["Er", "gab", "dem", "Hund", "den", "Maul\u00b7korb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Er gab dem Hund den Maulkorb.", "tokens": ["Er", "gab", "dem", "Hund", "den", "Maul\u00b7korb", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Maulkorb, Censur,", "tokens": ["Maul\u00b7korb", ",", "Cen\u00b7sur", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,"], "meter": "+--+", "measure": "iambic.di.chol"}, "line.4": {"text": "Censur, Maulkorb, Er gab dem Hund den Maulkorb.", "tokens": ["Cen\u00b7sur", ",", "Maul\u00b7korb", ",", "Er", "gab", "dem", "Hund", "den", "Maul\u00b7korb", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "PPER", "VVFIN", "ART", "NN", "ART", "NN", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}}, "stanza.5": {"line.1": {"text": "Dann ist das Thier gestorben.", "tokens": ["Dann", "ist", "das", "Thier", "ge\u00b7stor\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "So gut, als wie gestorben.", "tokens": ["So", "gut", ",", "als", "wie", "ge\u00b7stor\u00b7ben", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "$,", "KOUS", "KOKOM", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Gestorben, verschieden,", "tokens": ["Ge\u00b7stor\u00b7ben", ",", "ver\u00b7schie\u00b7den", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["NN", "$,", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Verrecket, crepiret, Dann ist das Thier gestorben.", "tokens": ["Ver\u00b7re\u00b7cket", ",", "cre\u00b7pi\u00b7ret", ",", "Dann", "ist", "das", "Thier", "ge\u00b7stor\u00b7ben", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$,", "VVFIN", "$,", "ADV", "VAFIN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.6": {"line.1": {"text": "Sie setzten ihm ein Denkmal.", "tokens": ["Sie", "setz\u00b7ten", "ihm", "ein", "Denk\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Nat\u00fcrlich wohl ein Denkmal.", "tokens": ["Na\u00b7t\u00fcr\u00b7lich", "wohl", "ein", "Denk\u00b7mal", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Denkmal, Monument,", "tokens": ["Denk\u00b7mal", ",", "Mo\u00b7nu\u00b7ment", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["ADV", "$,", "NN", "$,"], "meter": "+-+-+", "measure": "trochaic.tri"}, "line.4": {"text": "Grabstein, Leichenstein, Sie setzten ihm ein Denkmal.", "tokens": ["Grab\u00b7stein", ",", "Lei\u00b7chen\u00b7stein", ",", "Sie", "setz\u00b7ten", "ihm", "ein", "Denk\u00b7mal", "."], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "PPER", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}}, "stanza.7": {"line.1": {"text": "Darauf da stand geschrieben,", "tokens": ["Da\u00b7rauf", "da", "stand", "ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Darauf da stand geschrieben,", "tokens": ["Da\u00b7rauf", "da", "stand", "ge\u00b7schrie\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PAV", "ADV", "VVFIN", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Gekritzelt, geklieret,", "tokens": ["Ge\u00b7krit\u00b7zelt", ",", "ge\u00b7klie\u00b7ret", ","], "token_info": ["word", "punct", "word", "punct"], "pos": ["VVPP", "$,", "VVPP", "$,"], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.4": {"text": "Gegraben, geschmieret, Darauf da stand geschrieben:", "tokens": ["Ge\u00b7gra\u00b7ben", ",", "ge\u00b7schmie\u00b7ret", ",", "Da\u00b7rauf", "da", "stand", "ge\u00b7schrie\u00b7ben", ":"], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVPP", "$,", "PAV", "ADV", "VVFIN", "VVPP", "$."], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}}, "stanza.8": {"line.1": {"text": "Der arme Hund, der ist nun todt,", "tokens": ["Der", "ar\u00b7me", "Hund", ",", "der", "ist", "nun", "todt", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "PRELS", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Den Hausherrn holt die Schwerenoth!", "tokens": ["Den", "Haus\u00b7herrn", "holt", "die", "Schwe\u00b7re\u00b7noth", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wir wollen's ihm hiermit nur sagen,", "tokens": ["Wir", "wol\u00b7len's", "ihm", "hier\u00b7mit", "nur", "sa\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "PAV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df wir nicht mehr den Maulkorb tragen!", "tokens": ["Da\u00df", "wir", "nicht", "mehr", "den", "Maul\u00b7korb", "tra\u00b7gen", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PTKNEG", "ADV", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}