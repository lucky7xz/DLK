{"textgrid.poem.41357": {"metadata": {"author": {"name": "Hagedorn, Friedrich von", "birth": "N.A.", "death": "N.A."}, "title": "Die Thiere", "genre": "verse", "period": "N.A.", "pub_year": 1731, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Der Freiheit unverf\u00e4lschte Triebe", "tokens": ["Der", "Frei\u00b7heit", "un\u00b7ver\u00b7f\u00e4lschte", "Trie\u00b7be"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Erh\u00f6hn den Werth der Wahrheitliebe,", "tokens": ["Er\u00b7h\u00f6hn", "den", "Werth", "der", "Wahr\u00b7heit\u00b7lie\u00b7be", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die deine Seele stark gemacht.", "tokens": ["Die", "dei\u00b7ne", "See\u00b7le", "stark", "ge\u00b7macht", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dein gl\u00fccklicher Verstand durchdringt in edler Eile", "tokens": ["Dein", "gl\u00fcck\u00b7li\u00b7cher", "Ver\u00b7stand", "durch\u00b7dringt", "in", "ed\u00b7ler", "Ei\u00b7le"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "ADJA", "NN", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Den Nebel grauer Vorurtheile,", "tokens": ["Den", "Ne\u00b7bel", "grau\u00b7er", "Vor\u00b7urt\u00b7hei\u00b7le", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Des schulgelehrten P\u00f6bels Nacht.", "tokens": ["Des", "schul\u00b7ge\u00b7lehr\u00b7ten", "P\u00f6\u00b7bels", "Nacht", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Was Haller und die Wahrheit preisen,", "tokens": ["Was", "Hal\u00b7ler", "und", "die", "Wahr\u00b7heit", "prei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "NE", "KON", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Mein Freund! das wagst du zu beweisen:", "tokens": ["Mein", "Freund", "!", "das", "wagst", "du", "zu", "be\u00b7wei\u00b7sen", ":"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "PDS", "VVFIN", "PPER", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "\u00bbwer frei darf denken, denket wohl.\u00ab", "tokens": ["\u00bb", "wer", "frei", "darf", "den\u00b7ken", ",", "den\u00b7ket", "wohl", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "PWS", "ADJD", "VMFIN", "VVINF", "$,", "VVFIN", "ADV", "$.", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "La\u00df deinen Ausspruch mich vertraulich \u00fcberf\u00fchren,", "tokens": ["La\u00df", "dei\u00b7nen", "Aus\u00b7spruch", "mich", "ver\u00b7trau\u00b7lich", "\u00fc\u00b7berf\u00b7\u00fch\u00b7ren", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PPOSAT", "NN", "PPER", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ob ich die Urtheilskraft in Thieren", "tokens": ["Ob", "ich", "die", "Ur\u00b7theils\u00b7kraft", "in", "Thie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PPER", "ART", "NN", "APPR", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Bejahen oder l\u00e4ugnen soll.", "tokens": ["Be\u00b7ja\u00b7hen", "o\u00b7der", "l\u00e4ug\u00b7nen", "soll", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "KON", "VVINF", "VMFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Zwo Ratzen, die der Mangel plagte,", "tokens": ["Zwo", "Rat\u00b7zen", ",", "die", "der", "Man\u00b7gel", "plag\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PRELS", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und hungrig aus den L\u00f6chern jagte,", "tokens": ["Und", "hung\u00b7rig", "aus", "den", "L\u00f6\u00b7chern", "jag\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJD", "APPR", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Entdeckten unverhofft ein Ei.", "tokens": ["Ent\u00b7deck\u00b7ten", "un\u00b7ver\u00b7hofft", "ein", "Ei", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Das Ei war ihnen g'nug. Es wissen viele Weisen,", "tokens": ["Das", "Ei", "war", "ih\u00b7nen", "g'\u00b7nug", ".", "Es", "wis\u00b7sen", "vie\u00b7le", "Wei\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPER", "ADV", "$.", "PPER", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+--+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.5": {"text": "Ein Manzel selbst, da\u00df, die zu speisen,", "tokens": ["Ein", "Man\u00b7zel", "selbst", ",", "da\u00df", ",", "die", "zu", "spei\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADV", "$,", "KOUS", "$,", "PRELS", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Kein gro\u00dfes Mahl vonn\u00f6then sei.", "tokens": ["Kein", "gro\u00b7\u00dfes", "Mahl", "von\u00b7n\u00f6\u00b7then", "sei", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "ADJA", "NN", "VVINF", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Sie wollen froh zum Essen schreiten;", "tokens": ["Sie", "wol\u00b7len", "froh", "zum", "Es\u00b7sen", "schrei\u00b7ten", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADJD", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Allein, es l\u00e4\u00dft sich jetzt von weiten", "tokens": ["Al\u00b7lein", ",", "es", "l\u00e4\u00dft", "sich", "jetzt", "von", "wei\u00b7ten"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "PPER", "VVFIN", "PRF", "ADV", "APPR", "ADJA"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der Erbfeind ihres Volkes sehn.", "tokens": ["Der", "Erb\u00b7feind", "ih\u00b7res", "Vol\u00b7kes", "sehn", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es schleicht ein Fuchs heran; und guter Rath wird theuer,", "tokens": ["Es", "schleicht", "ein", "Fuchs", "he\u00b7ran", ";", "und", "gu\u00b7ter", "Rath", "wird", "theu\u00b7er", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NE", "PTKVZ", "$.", "KON", "ADJA", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Er fri\u00dft die Ratzen, und s\u00e4uft Eier;", "tokens": ["Er", "fri\u00dft", "die", "Rat\u00b7zen", ",", "und", "s\u00e4uft", "Ei\u00b7er", ";"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ART", "NN", "$,", "KON", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wie l\u00e4\u00dft sich's unberaubt entgehn?", "tokens": ["Wie", "l\u00e4\u00dft", "sich's", "un\u00b7be\u00b7raubt", "ent\u00b7gehn", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PIS", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Die eine legt sich auf den R\u00fccken", "tokens": ["Die", "ei\u00b7ne", "legt", "sich", "auf", "den", "R\u00fc\u00b7cken"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "PIS", "VVFIN", "PRF", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und h\u00e4lt mit unverwandten Blicken", "tokens": ["Und", "h\u00e4lt", "mit", "un\u00b7ver\u00b7wand\u00b7ten", "Bli\u00b7cken"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das Ei mit ihren Pfoten fest.", "tokens": ["Das", "Ei", "mit", "ih\u00b7ren", "Pfo\u00b7ten", "fest", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "PPOSAT", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die andre wei\u00df darauf, mit gl\u00fccklichem Bem\u00fchen,", "tokens": ["Die", "and\u00b7re", "wei\u00df", "da\u00b7rauf", ",", "mit", "gl\u00fcck\u00b7li\u00b7chem", "Be\u00b7m\u00fc\u00b7hen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "PIS", "VVFIN", "PAV", "$,", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Sie bei dem Schwanze fortzuziehen;", "tokens": ["Sie", "bei", "dem", "Schwan\u00b7ze", "fort\u00b7zu\u00b7zie\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und so erreichen sie das Nest.", "tokens": ["Und", "so", "er\u00b7rei\u00b7chen", "sie", "das", "Nest", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "VVFIN", "PPER", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wer lehret aus gewissen Gr\u00fcnden,", "tokens": ["Wer", "leh\u00b7ret", "aus", "ge\u00b7wis\u00b7sen", "Gr\u00fcn\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Da\u00df Thiere blo\u00dferdings empfinden?", "tokens": ["Da\u00df", "Thie\u00b7re", "blo\u00b7\u00dfer\u00b7dings", "emp\u00b7fin\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Hat hier die Ratze nicht gedacht?", "tokens": ["Hat", "hier", "die", "Rat\u00b7ze", "nicht", "ge\u00b7dacht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "NN", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Verrieth die Rettungsart, die sie so wohl erlesen,", "tokens": ["Ver\u00b7rieth", "die", "Ret\u00b7tungs\u00b7art", ",", "die", "sie", "so", "wohl", "er\u00b7le\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "NN", "$,", "PRELS", "PPER", "ADV", "ADV", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "So sch\u00f6n vollf\u00fchrt, kein geistig Wesen,", "tokens": ["So", "sch\u00f6n", "voll\u00b7f\u00fchrt", ",", "kein", "geis\u00b7tig", "We\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "VVPP", "$,", "PIAT", "ADJD", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Das zweifelt, forscht, und Schl\u00fcsse macht?", "tokens": ["Das", "zwei\u00b7felt", ",", "forscht", ",", "und", "Schl\u00fcs\u00b7se", "macht", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "$,", "VVFIN", "$,", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Zeigt sich in keines Thieres R\u00e4nken", "tokens": ["Zeigt", "sich", "in", "kei\u00b7nes", "Thie\u00b7res", "R\u00e4n\u00b7ken"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PRF", "APPR", "PIAT", "NN", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die Kraft, was m\u00f6glich ist, zu denken,", "tokens": ["Die", "Kraft", ",", "was", "m\u00f6g\u00b7lich", "ist", ",", "zu", "den\u00b7ken", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "PRELS", "ADJD", "VAFIN", "$,", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Des Menschen Leitstern, der Verstand?", "tokens": ["Des", "Men\u00b7schen", "Leits\u00b7tern", ",", "der", "Ver\u00b7stand", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Kennt man von ihrem Thun noch keine tiefre Quelle,", "tokens": ["Kennt", "man", "von", "ih\u00b7rem", "Thun", "noch", "kei\u00b7ne", "tief\u00b7re", "Quel\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "APPR", "PPOSAT", "NN", "ADV", "PIAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Als die Erwartung solcher F\u00e4lle,", "tokens": ["Als", "die", "Er\u00b7war\u00b7tung", "sol\u00b7cher", "F\u00e4l\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die jedes andern \u00e4hnlich fand?", "tokens": ["Die", "je\u00b7des", "an\u00b7dern", "\u00e4hn\u00b7lich", "fand", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PIAT", "ADJA", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Die besten Mittel weislich w\u00e4hlen,", "tokens": ["Die", "bes\u00b7ten", "Mit\u00b7tel", "weis\u00b7lich", "w\u00e4h\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Durch Klugheit nie den Zweck verfehlen,", "tokens": ["Durch", "Klug\u00b7heit", "nie", "den", "Zweck", "ver\u00b7feh\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "ADV", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das kann der stolze Mensch allein.", "tokens": ["Das", "kann", "der", "stol\u00b7ze", "Mensch", "al\u00b7lein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VMFIN", "ART", "ADJA", "NN", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Pflegt diese Fertigkeit nicht Thieren beizuwohnen?", "tokens": ["Pflegt", "die\u00b7se", "Fer\u00b7tig\u00b7keit", "nicht", "Thie\u00b7ren", "bei\u00b7zu\u00b7woh\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "PDAT", "NN", "PTKNEG", "NN", "VVIZU", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Warum denn m\u00fcssen die Huronen", "tokens": ["Wa\u00b7rum", "denn", "m\u00fcs\u00b7sen", "die", "Hu\u00b7ro\u00b7nen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "VMFIN", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Durch Biber-Witz besch\u00e4met sein?", "tokens": ["Durch", "Bi\u00b7ber\u00b7Witz", "be\u00b7sch\u00e4\u00b7met", "sein", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVPP", "VAINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Wann f\u00fcrchterliche Fluten schwellen,", "tokens": ["Wann", "f\u00fcrch\u00b7ter\u00b7li\u00b7che", "Flu\u00b7ten", "schwel\u00b7len", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wann die Gewalt vereinter Quellen", "tokens": ["Wann", "die", "Ge\u00b7walt", "ver\u00b7ein\u00b7ter", "Quel\u00b7len"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "ADJA", "NN"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Um Quebec w\u00fchlt, und Felder fri\u00dft;", "tokens": ["Um", "Que\u00b7bec", "w\u00fchlt", ",", "und", "Fel\u00b7der", "fri\u00dft", ";"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["KOUI", "NE", "VVFIN", "$,", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So wird im Strom ein Haus durch Biber aufgef\u00fchret,", "tokens": ["So", "wird", "im", "Strom", "ein", "Haus", "durch", "Bi\u00b7ber", "auf\u00b7ge\u00b7f\u00fch\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "APPRART", "NN", "ART", "NN", "APPR", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "An dem der Strom die Kraft verlieret,", "tokens": ["An", "dem", "der", "Strom", "die", "Kraft", "ver\u00b7lie\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ART", "NN", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Das rund, umpf\u00e4hlt und sicher ist.", "tokens": ["Das", "rund", ",", "ump\u00b7f\u00e4hlt", "und", "si\u00b7cher", "ist", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJD", "$,", "VVFIN", "KON", "ADJD", "VAFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Die Vorderf\u00fc\u00dfe scheinen H\u00e4nde,", "tokens": ["Die", "Vor\u00b7der\u00b7f\u00fc\u00b7\u00dfe", "schei\u00b7nen", "H\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Und flechten aus den Binsen W\u00e4nde,", "tokens": ["Und", "flech\u00b7ten", "aus", "den", "Bin\u00b7sen", "W\u00e4n\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Die auf sechs festen St\u00fctzen stehn.", "tokens": ["Die", "auf", "sechs", "fes\u00b7ten", "St\u00fct\u00b7zen", "stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "APPR", "CARD", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es kann ihr Wunderbau ein dreifach Stockwerk zeigen,", "tokens": ["Es", "kann", "ihr", "Wun\u00b7der\u00b7bau", "ein", "drei\u00b7fach", "Stock\u00b7werk", "zei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPOSAT", "NN", "ART", "ADJA", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und jeder Biber h\u00f6her steigen,", "tokens": ["Und", "je\u00b7der", "Bi\u00b7ber", "h\u00f6\u00b7her", "stei\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wann Eis und Wellen weiter gehn.", "tokens": ["Wann", "Eis", "und", "Wel\u00b7len", "wei\u00b7ter", "gehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NN", "KON", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Sie w\u00e4hlen nahe Pappelweiden,", "tokens": ["Sie", "w\u00e4h\u00b7len", "na\u00b7he", "Pap\u00b7pel\u00b7wei\u00b7den", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Die sie mit scharfem Zahn durchschneiden:", "tokens": ["Die", "sie", "mit", "schar\u00b7fem", "Zahn", "durch\u00b7schnei\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPER", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Doch ihre M\u00fche wird verk\u00fcrzt,", "tokens": ["Doch", "ih\u00b7re", "M\u00fc\u00b7he", "wird", "ver\u00b7k\u00fcrzt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VAFIN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und sie erwarten stets den Beistand starker Winde,", "tokens": ["Und", "sie", "er\u00b7war\u00b7ten", "stets", "den", "Bei\u00b7stand", "star\u00b7ker", "Win\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "VVFIN", "ADV", "ART", "NN", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Der pl\u00f6tzlich in die Wasserschl\u00fcnde", "tokens": ["Der", "pl\u00f6tz\u00b7lich", "in", "die", "Was\u00b7ser\u00b7schl\u00fcn\u00b7de"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "APPR", "ART", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die halb durchnagten St\u00e4mme st\u00fcrzt.", "tokens": ["Die", "halb", "durch\u00b7nag\u00b7ten", "St\u00e4m\u00b7me", "st\u00fcrzt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJD", "ADJA", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Es werden die, so Arbeit hassen,", "tokens": ["Es", "wer\u00b7den", "die", ",", "so", "Ar\u00b7beit", "has\u00b7sen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "$,", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der Schmach und Faulheit \u00fcberlassen,", "tokens": ["Der", "Schmach", "und", "Faul\u00b7heit", "\u00fc\u00b7ber\u00b7las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und man verbannt sie aus dem Staat.", "tokens": ["Und", "man", "ver\u00b7bannt", "sie", "aus", "dem", "Staat", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIS", "VVFIN", "PPER", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ein \u00e4chter Biber mu\u00df sein Amt getreu verwalten,", "tokens": ["Ein", "\u00e4ch\u00b7ter", "Bi\u00b7ber", "mu\u00df", "sein", "Amt", "ge\u00b7treu", "ver\u00b7wal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VMFIN", "PPOSAT", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Bald bauen, und bald Wache halten,", "tokens": ["Bald", "bau\u00b7en", ",", "und", "bald", "Wa\u00b7che", "hal\u00b7ten", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVINF", "$,", "KON", "ADV", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Und melden, wann ein Mensch sich naht.", "tokens": ["Und", "mel\u00b7den", ",", "wann", "ein", "Mensch", "sich", "naht", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVINF", "$,", "PWAV", "ART", "NN", "PRF", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "Wer war der Plato dieser Thiere?", "tokens": ["Wer", "war", "der", "Pla\u00b7to", "die\u00b7ser", "Thie\u00b7re", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "ART", "NE", "PDAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Wer lehrte sie, was ich hier sp\u00fcre:", "tokens": ["Wer", "lehr\u00b7te", "sie", ",", "was", "ich", "hier", "sp\u00fc\u00b7re", ":"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "$,", "PWS", "PPER", "ADV", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kunst, Ordnung, Witz, Bedachtsamkeit?", "tokens": ["Kunst", ",", "Ord\u00b7nung", ",", "Witz", ",", "Be\u00b7dacht\u00b7sam\u00b7keit", "?"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Soll man die F\u00e4higkeit, wodurch sie dieses k\u00f6nnen,", "tokens": ["Soll", "man", "die", "F\u00e4\u00b7hig\u00b7keit", ",", "wo\u00b7durch", "sie", "die\u00b7ses", "k\u00f6n\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "PIS", "ART", "NN", "$,", "PWAV", "PPER", "PDS", "VMFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Gef\u00fcgter Theile Wirkung nennen?", "tokens": ["Ge\u00b7f\u00fcg\u00b7ter", "Thei\u00b7le", "Wir\u00b7kung", "nen\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "NN", "VVINF", "$."], "meter": "-+-+--+--", "measure": "iambic.tri.relaxed"}, "line.6": {"text": "Wo ist ein Uhrwerk so gescheidt?", "tokens": ["Wo", "ist", "ein", "Uhr\u00b7werk", "so", "ge\u00b7scheidt", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VAFIN", "ART", "NN", "ADV", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Entdeckt man weiter nichts an ihnen,", "tokens": ["Ent\u00b7deckt", "man", "wei\u00b7ter", "nichts", "an", "ih\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "ADV", "PIS", "APPR", "PPER", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Als die Bewegung der Maschinen,", "tokens": ["Als", "die", "Be\u00b7we\u00b7gung", "der", "Ma\u00b7schi\u00b7nen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "NN", "ART", "NN", "$,"], "meter": "++-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der Urtheil und Bewu\u00dftsein fehlt?", "tokens": ["Der", "Ur\u00b7theil", "und", "Be\u00b7wu\u00df\u00b7tsein", "fehlt", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Cartesius bejaht's; doch ist im Recht zu geben?", "tokens": ["Car\u00b7te\u00b7sius", "be\u00b7jaht's", ";", "doch", "ist", "im", "Recht", "zu", "ge\u00b7ben", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$.", "ADV", "VAFIN", "APPRART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+-+-+-+-+-", "measure": "trochaic.hexa"}, "line.5": {"text": "Die Wahrheit mag den Zweifel heben,", "tokens": ["Die", "Wahr\u00b7heit", "mag", "den", "Zwei\u00b7fel", "he\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VMFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Die Frankreichs Ph\u00e4drus uns erz\u00e4hlt.", "tokens": ["Die", "Fran\u00b7kreichs", "Ph\u00e4d\u00b7rus", "uns", "er\u00b7z\u00e4hlt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "NE", "PPER", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Aurorens Feind, ein Freund der N\u00e4chte,", "tokens": ["Au\u00b7ro\u00b7rens", "Feind", ",", "ein", "Freund", "der", "N\u00e4ch\u00b7te", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NE", "NN", "$,", "ART", "NN", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Ein Thier aus traurigem Geschlechte,", "tokens": ["Ein", "Thier", "aus", "trau\u00b7ri\u00b7gem", "Ge\u00b7schlech\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ein Kauz, der schlauste B\u00f6sewicht,", "tokens": ["Ein", "Kauz", ",", "der", "schlaus\u00b7te", "B\u00f6\u00b7se\u00b7wicht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ward in dem Nest ertappt; das steckte voller M\u00e4use,", "tokens": ["Ward", "in", "dem", "Nest", "er\u00b7tappt", ";", "das", "steck\u00b7te", "vol\u00b7ler", "M\u00e4u\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "VVPP", "$.", "ART", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Die waren feist, und hatten Speise,", "tokens": ["Die", "wa\u00b7ren", "feist", ",", "und", "hat\u00b7ten", "Spei\u00b7se", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "ADJD", "$,", "KON", "VAFIN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Doch ihre F\u00fc\u00dfe fand man nicht.", "tokens": ["Doch", "ih\u00b7re", "F\u00fc\u00b7\u00dfe", "fand", "man", "nicht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "VVFIN", "PIS", "PTKNEG", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.16": {"line.1": {"text": "Sie wurden hier vom Kauz ern\u00e4hret,", "tokens": ["Sie", "wur\u00b7den", "hier", "vom", "Kauz", "er\u00b7n\u00e4h\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Der ihre Br\u00fcder l\u00e4ngst verzehret,", "tokens": ["Der", "ih\u00b7re", "Br\u00fc\u00b7der", "l\u00e4ngst", "ver\u00b7zeh\u00b7ret", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Und nun f\u00fcr sie den Weizen stahl.", "tokens": ["Und", "nun", "f\u00fcr", "sie", "den", "Wei\u00b7zen", "stahl", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "PPER", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Aus Vorsicht l\u00e4hmt' er sie, weil, die er sonst gefangen,", "tokens": ["Aus", "Vor\u00b7sicht", "l\u00e4hmt'", "er", "sie", ",", "weil", ",", "die", "er", "sonst", "ge\u00b7fan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PPER", "PPER", "$,", "KOUS", "$,", "PRELS", "PPER", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ihm wieder unverhofft entgangen:", "tokens": ["Ihm", "wie\u00b7der", "un\u00b7ver\u00b7hofft", "ent\u00b7gan\u00b7gen", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Jetzt fra\u00df er sie, nach sichrer Wahl.", "tokens": ["Jetzt", "fra\u00df", "er", "sie", ",", "nach", "sich\u00b7rer", "Wahl", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "PPER", "$,", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Hat dieser Schlecker nichts ermessen?", "tokens": ["Hat", "die\u00b7ser", "Schle\u00b7cker", "nichts", "er\u00b7mes\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PDAT", "NN", "PIS", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "Auf einmal alles aufzufressen,", "tokens": ["Auf", "ein\u00b7mal", "al\u00b7les", "auf\u00b7zu\u00b7fres\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADV", "PIS", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Das war zu ungesund, zu viel.", "tokens": ["Das", "war", "zu", "un\u00b7ge\u00b7sund", ",", "zu", "viel", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PTKA", "ADJD", "$,", "PTKA", "PIS", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er spart; er will die Maus, eh' er sie m\u00e4stet, l\u00e4hmen,", "tokens": ["Er", "spart", ";", "er", "will", "die", "Maus", ",", "eh'", "er", "sie", "m\u00e4s\u00b7tet", ",", "l\u00e4h\u00b7men", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["PPER", "VVFIN", "$.", "PPER", "VMFIN", "ART", "NN", "$,", "KOUS", "PPER", "PPER", "VVFIN", "$,", "VVFIN", "$,"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Und ihr zur Flucht die Mittel nehmen.", "tokens": ["Und", "ihr", "zur", "Flucht", "die", "Mit\u00b7tel", "neh\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "APPRART", "NN", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.6": {"text": "Wie kam's, da\u00df er darauf verfiel?", "tokens": ["Wie", "kam's", ",", "da\u00df", "er", "da\u00b7rauf", "ver\u00b7fiel", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "NE", "$,", "KOUS", "PPER", "PAV", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}