{"dta.poem.2714": {"metadata": {"author": {"name": "M\u00fchlpfort, Heinrich", "birth": "N.A.", "death": "N.A."}, "title": "Pindarische Ode  \n  Bey Beerdigung Fr. A. M. S. g. v. V. den  \n 20. Augusti 1678.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1686", "urn": "urn:nbn:de:kobv:b4-20414-7", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Wenn itzt das Jahr betagt erscheint/", "tokens": ["Wenn", "itzt", "das", "Jahr", "be\u00b7tagt", "er\u00b7scheint", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ADV", "ART", "NN", "VVPP", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Lufft wird frisch/ der Himmel weint/", "tokens": ["Die", "Lufft", "wird", "frisch", "/", "der", "Him\u00b7mel", "weint", "/"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$(", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und uns die langen N\u00e4chte decken/", "tokens": ["Und", "uns", "die", "lan\u00b7gen", "N\u00e4ch\u00b7te", "de\u00b7cken", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So falbt der B\u00e4ume gr\u00fcnes Haar;", "tokens": ["So", "falbt", "der", "B\u00e4u\u00b7me", "gr\u00fc\u00b7nes", "Haar", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ART", "NN", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Wo vor das sch\u00f6nste Blumwerck war", "tokens": ["Wo", "vor", "das", "sch\u00f6ns\u00b7te", "Blum\u00b7werck", "war"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "APPR", "ART", "ADJA", "NN", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Da siht man nichts als w\u00fcste Hecken.", "tokens": ["Da", "siht", "man", "nichts", "als", "w\u00fcs\u00b7te", "He\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PIS", "PIS", "KOKOM", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Der Apffel so den Baum\u2019 geziert", "tokens": ["Der", "Apf\u00b7fel", "so", "den", "Baum'", "ge\u00b7ziert"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADV", "ART", "NN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Und seinen Purpur hochgef\u00fchrt", "tokens": ["Und", "sei\u00b7nen", "Pur\u00b7pur", "hoch\u00b7ge\u00b7f\u00fchrt"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "NN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "F\u00e4llt ab/ das Gra\u00df verwelckt/ und die so volle Traube", "tokens": ["F\u00e4llt", "ab", "/", "das", "Gra\u00df", "ver\u00b7welckt", "/", "und", "die", "so", "vol\u00b7le", "Trau\u00b7be"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PTKVZ", "$(", "ART", "NN", "VVPP", "$(", "KON", "ART", "ADV", "ADJA", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Harrt auff des Wintzers Hand/ die sie vom Stocke raube:", "tokens": ["Harrt", "auff", "des", "Wint\u00b7zers", "Hand", "/", "die", "sie", "vom", "Sto\u00b7cke", "rau\u00b7be", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "NN", "$(", "PRELS", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Denn ist die Lust und alle Liebligkeit", "tokens": ["Denn", "ist", "die", "Lust", "und", "al\u00b7le", "Lieb\u00b7lig\u00b7keit"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "VAFIN", "ART", "NN", "KON", "PIAT", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Von uns hinweg und nur betr\u00fcbte Zeit.", "tokens": ["Von", "uns", "hin\u00b7weg", "und", "nur", "be\u00b7tr\u00fcb\u00b7te", "Zeit", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPER", "PTKVZ", "KON", "ADV", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "So gehts mit unsrer Lebens-Frist", "tokens": ["So", "gehts", "mit", "uns\u00b7rer", "Le\u00b7bens\u00b7Frist"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die einem Herbst gantz \u00e4hnlich ist", "tokens": ["Die", "ei\u00b7nem", "Herbst", "gantz", "\u00e4hn\u00b7lich", "ist"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "ART", "NN", "ADV", "ADJD", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Wenn unsre reiffe Jahre kommen/", "tokens": ["Wenn", "uns\u00b7re", "reif\u00b7fe", "Jah\u00b7re", "kom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPOSAT", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "So mercket man wie allgemach", "tokens": ["So", "mer\u00b7cket", "man", "wie", "all\u00b7ge\u00b7mach"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "KOKOM", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Das traurig\u2019 Alter schleichet nach/", "tokens": ["Das", "trau\u00b7rig'", "Al\u00b7ter", "schlei\u00b7chet", "nach", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VVFIN", "NN", "VVFIN", "APPR", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und hat uns Safft und Krafft genommen.", "tokens": ["Und", "hat", "uns", "Safft", "und", "Krafft", "ge\u00b7nom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Der Jugend Sch\u00f6nheit ist dahin", "tokens": ["Der", "Ju\u00b7gend", "Sch\u00f6n\u00b7heit", "ist", "da\u00b7hin"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NN", "NN", "VAFIN", "PAV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Der frische Muth/ der gr\u00fcne Sinn", "tokens": ["Der", "fri\u00b7sche", "Muth", "/", "der", "gr\u00fc\u00b7ne", "Sinn"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Die bl\u00fchende Gestalt/ des edlen Geistes Funcken", "tokens": ["Die", "bl\u00fc\u00b7hen\u00b7de", "Ge\u00b7stalt", "/", "des", "ed\u00b7len", "Geis\u00b7tes", "Fun\u00b7cken"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ART", "ADJA", "NN", "$(", "ART", "ADJA", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Der Gaben Treffligkeit sind allbereit entsuncken.", "tokens": ["Der", "Ga\u00b7ben", "Tref\u00b7flig\u00b7keit", "sind", "all\u00b7be\u00b7reit", "ent\u00b7sun\u00b7cken", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Und die wir vor so jung/ so sch\u00f6n und roth/", "tokens": ["Und", "die", "wir", "vor", "so", "jung", "/", "so", "sch\u00f6n", "und", "roth", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "PPER", "APPR", "ADV", "ADJD", "$(", "ADV", "ADJD", "KON", "ADJD", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Erwarten nun nichts anders als den Tod.", "tokens": ["Er\u00b7war\u00b7ten", "nun", "nichts", "an\u00b7ders", "als", "den", "Tod", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "PIS", "ADV", "KOKOM", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Von B\u00e4umen gibt der Herbst/ wir vom Gem\u00fcthe Fr\u00fcchte/", "tokens": ["Von", "B\u00e4u\u00b7men", "gibt", "der", "Herbst", "/", "wir", "vom", "Ge\u00b7m\u00fc\u00b7the", "Fr\u00fcch\u00b7te", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "ART", "NN", "$(", "PPER", "APPRART", "NN", "NN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Er kan so tr\u00e4chtig nimmer stehn/", "tokens": ["Er", "kan", "so", "tr\u00e4ch\u00b7tig", "nim\u00b7mer", "stehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "ADJD", "ADV", "VVINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Als wir geziert mit Tugend gehn/", "tokens": ["Als", "wir", "ge\u00b7ziert", "mit", "Tu\u00b7gend", "gehn", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "APPR", "NN", "VVINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und suchen in der Welt ein ewiges Ger\u00fcchte.", "tokens": ["Und", "su\u00b7chen", "in", "der", "Welt", "ein", "e\u00b7wi\u00b7ges", "Ge\u00b7r\u00fcch\u00b7te", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Ob Sch\u00f6nheit schon vergeht trit Klugheit an die statt.", "tokens": ["Ob", "Sch\u00f6n\u00b7heit", "schon", "ver\u00b7geht", "trit", "Klug\u00b7heit", "an", "die", "statt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "VVFIN", "VVFIN", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Ein hoch und edles Blut das seineu Ursprung hat", "tokens": ["Ein", "hoch", "und", "ed\u00b7les", "Blut", "das", "sei\u00b7neu", "Ur\u00b7sprung", "hat"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "ADJD", "KON", "ADJA", "NN", "ART", "PPOSAT", "NN", "VAFIN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Von der gestirnten H\u00f6h\u2019", "tokens": ["Von", "der", "ge\u00b7stirn\u00b7ten", "H\u00f6h'"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.8": {"text": "Reis\u2019t wieder zu den Sternen/", "tokens": ["Reis't", "wie\u00b7der", "zu", "den", "Ster\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Und ob der Leib vergeh\u2019/", "tokens": ["Und", "ob", "der", "Leib", "ver\u00b7geh'", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "Heist es die Welt doch lernen/", "tokens": ["Heist", "es", "die", "Welt", "doch", "ler\u00b7nen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "ART", "NN", "ADV", "VVINF", "$("], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.11": {"text": "Da\u00df ihm ihr Innbegrieff zu klein f\u00fcr seine Thaten.", "tokens": ["Da\u00df", "ihm", "ihr", "Inn\u00b7be\u00b7grieff", "zu", "klein", "f\u00fcr", "sei\u00b7ne", "Tha\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "PTKA", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Ja da\u00df ein himmlisch Geist", "tokens": ["Ja", "da\u00df", "ein", "himm\u00b7lisch", "Geist"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PTKANT", "KOUS", "ART", "ADJD", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "Mehr Sch\u00e4tze f\u00fchrt und weist/", "tokens": ["Mehr", "Sch\u00e4t\u00b7ze", "f\u00fchrt", "und", "weist", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PIAT", "NN", "VVFIN", "KON", "VVFIN", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.14": {"text": "Als Trauben in dem Stock und K\u00f6rner in Granaten.", "tokens": ["Als", "Trau\u00b7ben", "in", "dem", "Stock", "und", "K\u00f6r\u00b7ner", "in", "Gra\u00b7na\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "APPR", "ART", "NN", "KON", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+--++-", "measure": "iambic.hexa.relaxed"}}, "stanza.4": {"line.1": {"text": "Sind Menschen nun ein Bild der Zeit/", "tokens": ["Sind", "Men\u00b7schen", "nun", "ein", "Bild", "der", "Zeit", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "ADV", "ART", "NN", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Voll Unbestand und Fl\u00fcchtigkeit/", "tokens": ["Voll", "Un\u00b7be\u00b7stand", "und", "Fl\u00fcch\u00b7tig\u00b7keit", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADJD", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Gebaut aus schlechtem Leim und Erden?", "tokens": ["Ge\u00b7baut", "aus", "schlech\u00b7tem", "Leim", "und", "Er\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVPP", "APPR", "ADJA", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "War di\u00df ein Thon und rother Klo\u00df", "tokens": ["War", "di\u00df", "ein", "Thon", "und", "ro\u00b7ther", "Klo\u00df"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VAFIN", "PDS", "ART", "NN", "KON", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Drein GOtt den ersten Athem go\u00df/", "tokens": ["Drein", "Gott", "den", "ers\u00b7ten", "A\u00b7them", "go\u00df", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und hie\u00df ihn einen Menschen werden?", "tokens": ["Und", "hie\u00df", "ihn", "ei\u00b7nen", "Men\u00b7schen", "wer\u00b7den", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "ART", "NN", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Verkehrt uns denn des Todes Raub", "tokens": ["Ver\u00b7kehrt", "uns", "denn", "des", "To\u00b7des", "Raub"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPER", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Auch wieder nur in Asch\u2019 und Staub?", "tokens": ["Auch", "wie\u00b7der", "nur", "in", "Asch'", "und", "Staub", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Hilfft nichts der Jahre May/ des Alters Witz und Gaben?", "tokens": ["Hilfft", "nichts", "der", "Jah\u00b7re", "May", "/", "des", "Al\u00b7ters", "Witz", "und", "Ga\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "ART", "NN", "NE", "$(", "ART", "NN", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Wird bald ein kleines Kind und bald ein Grei\u00df begraben?", "tokens": ["Wird", "bald", "ein", "klei\u00b7nes", "Kind", "und", "bald", "ein", "Grei\u00df", "be\u00b7gra\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "KON", "ADV", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Schallt stets die Stimm so uns ins Ohre schreyt:", "tokens": ["Schallt", "stets", "die", "Stimm", "so", "uns", "ins", "Oh\u00b7re", "schreyt", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "ART", "NN", "ADV", "PPER", "APPRART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Der Mensch ist nur ein Bild und Spiel der Zeit?", "tokens": ["Der", "Mensch", "ist", "nur", "ein", "Bild", "und", "Spiel", "der", "Zeit", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "ART", "NN", "KON", "NN", "ART", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "So mu\u00df er auff was bessers schaun/", "tokens": ["So", "mu\u00df", "er", "auff", "was", "bes\u00b7sers", "schaun", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "APPR", "PRELS", "ADV", "VVINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und ihm ein Angedencken baun/", "tokens": ["Und", "ihm", "ein", "An\u00b7ge\u00b7den\u00b7cken", "baun", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ART", "NN", "VVINF", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Das nicht der Zeiten Zahn verletzet;", "tokens": ["Das", "nicht", "der", "Zei\u00b7ten", "Zahn", "ver\u00b7let\u00b7zet", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PTKNEG", "ART", "NN", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Ein Leben da\u00df die Tugend ziert/", "tokens": ["Ein", "Le\u00b7ben", "da\u00df", "die", "Tu\u00b7gend", "ziert", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KOUS", "ART", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Das unter Ehr und Ruhm gef\u00fchrt/", "tokens": ["Das", "un\u00b7ter", "Ehr", "und", "Ruhm", "ge\u00b7f\u00fchrt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "APPR", "NN", "KON", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Wird von der Nach-Welt hochgesch\u00e4tzet:", "tokens": ["Wird", "von", "der", "Nach\u00b7Welt", "hoch\u00b7ge\u00b7sch\u00e4t\u00b7zet", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "NN", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "Denn Tugend/ Wei\u00dfheit und Verstand", "tokens": ["Denn", "Tu\u00b7gend", "/", "Wei\u00df\u00b7heit", "und", "Ver\u00b7stand"], "token_info": ["word", "word", "punct", "word", "word", "word"], "pos": ["KON", "NN", "$(", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Sind unsrer Sinnen sch\u00f6nes Pfand/", "tokens": ["Sind", "uns\u00b7rer", "Sin\u00b7nen", "sch\u00f6\u00b7nes", "Pfand", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Und wer hier wohl gelebt/ der kan auch fr\u00f6lich sterben.", "tokens": ["Und", "wer", "hier", "wohl", "ge\u00b7lebt", "/", "der", "kan", "auch", "fr\u00f6\u00b7lich", "ster\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PWS", "ADV", "ADV", "VVPP", "$(", "ART", "VMFIN", "ADV", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Die Handvoll Fleisch und Blut mag wie sie wil verderben/", "tokens": ["Die", "Hand\u00b7voll", "Fleisch", "und", "Blut", "mag", "wie", "sie", "wil", "ver\u00b7der\u00b7ben", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NN", "VMFIN", "KOKOM", "PPER", "VMFIN", "VVFIN", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Ob dieses alte Kleid bricht und zerschleust", "tokens": ["Ob", "die\u00b7ses", "al\u00b7te", "Kleid", "bricht", "und", "zer\u00b7schleust"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "PDAT", "ADJA", "NN", "VVFIN", "KON", "VVFIN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "So zieret dort der Himmel unsren Geist.", "tokens": ["So", "zie\u00b7ret", "dort", "der", "Him\u00b7mel", "un\u00b7sren", "Geist", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "ART", "NN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Der Tugend G\u00f6ttligkeit ist nicht allein zuschauen/", "tokens": ["Der", "Tu\u00b7gend", "G\u00f6tt\u00b7lig\u00b7keit", "ist", "nicht", "al\u00b7lein", "zu\u00b7schau\u00b7en", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "VAFIN", "PTKNEG", "ADV", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Bey M\u00e4nnern denen sie verm\u00e4hlt.", "tokens": ["Bey", "M\u00e4n\u00b7nern", "de\u00b7nen", "sie", "ver\u00b7m\u00e4hlt", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "PRELS", "PPER", "VVPP", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Es sind den Sternen zugezehlt/", "tokens": ["Es", "sind", "den", "Ster\u00b7nen", "zu\u00b7ge\u00b7zehlt", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Heldinnen jener Zeit und Kronen von den Frauen.", "tokens": ["Hel\u00b7din\u00b7nen", "je\u00b7ner", "Zeit", "und", "Kro\u00b7nen", "von", "den", "Frau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PDAT", "NN", "KON", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Man preist Aspasien von Demuth/ Scham und Zucht/", "tokens": ["Man", "preist", "As\u00b7pa\u00b7si\u00b7en", "von", "De\u00b7muth", "/", "Scham", "und", "Zucht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "NE", "APPR", "NN", "$(", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Hat nicht Hipparchia die Wei\u00dfheit ausgesucht?", "tokens": ["Hat", "nicht", "Hip\u00b7par\u00b7chia", "die", "Wei\u00df\u00b7heit", "aus\u00b7ge\u00b7sucht", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PTKNEG", "NE", "ART", "NN", "VVPP", "$."], "meter": "+-+-+-+-+-+", "measure": "trochaic.hexa"}, "line.7": {"text": "Es bleibt ", "tokens": ["Es", "bleibt"], "token_info": ["word", "word"], "pos": ["PPER", "VVFIN"], "meter": "-+", "measure": "iambic.single"}, "line.8": {"text": "Der Keuschheit Schlo\u00df und Riegel/", "tokens": ["Der", "Keuschheit", "Schlo\u00df", "und", "Rie\u00b7gel", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NN", "$("], "meter": "--+-+-", "measure": "anapaest.init"}, "line.9": {"text": "Und einer s\u00fcssen Eh\u2019", "tokens": ["Und", "ei\u00b7ner", "s\u00fcs\u00b7sen", "Eh'"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "ART", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "Die Livia ein Spiegel;", "tokens": ["Die", "Li\u00b7via", "ein", "Spie\u00b7gel", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "NN", "$."], "meter": "-+--+-", "measure": "amphibrach.di.relaxed"}, "line.11": {"text": "Wird auch die j\u00fcngre Welt ein gr\u00f6sser Wunder lesen/", "tokens": ["Wird", "auch", "die", "j\u00fcng\u00b7re", "Welt", "ein", "gr\u00f6s\u00b7ser", "Wun\u00b7der", "le\u00b7sen", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ART", "ADJA", "NN", "ART", "ADJA", "NN", "VVINF", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.12": {"text": "Als wie auff ihrem Thron", "tokens": ["Als", "wie", "auff", "ih\u00b7rem", "Thron"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "KOKOM", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "Bey Scepter und bey Kron/", "tokens": ["Bey", "Scep\u00b7ter", "und", "bey", "Kron", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "APPR", "NE", "$("], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.14": {"text": "Begl\u00fcckt Elisabeth die K\u00f6nigin gewesen?", "tokens": ["Be\u00b7gl\u00fcckt", "E\u00b7lis\u00b7a\u00b7beth", "die", "K\u00f6\u00b7ni\u00b7gin", "ge\u00b7we\u00b7sen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "NE", "ART", "NN", "VAPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.7": {"line.1": {"text": "So ists die Tugend bleibt das Licht/", "tokens": ["So", "ists", "die", "Tu\u00b7gend", "bleibt", "das", "Licht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "ART", "NN", "VVFIN", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So durch des Grabes Schw\u00e4rtze bricht/", "tokens": ["So", "durch", "des", "Gra\u00b7bes", "Schw\u00e4rt\u00b7ze", "bricht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Und neuen Schein der Grufft gewehret.", "tokens": ["Und", "neu\u00b7en", "Schein", "der", "Grufft", "ge\u00b7weh\u00b7ret", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ART", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Obschon die Frau von ", "tokens": ["Ob\u00b7schon", "die", "Frau", "von"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "ART", "NN", "APPR"], "meter": "-+-+-", "measure": "iambic.di"}, "line.5": {"text": "Jtzt nichts als Racht umschlossen h\u00e4lt/", "tokens": ["Jtzt", "nichts", "als", "Racht", "um\u00b7schlos\u00b7sen", "h\u00e4lt", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PIS", "KOKOM", "NN", "VVINF", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Und F\u00e4uln\u00fc\u00df ihren Leib verzehret/", "tokens": ["Und", "F\u00e4ul\u00b7n\u00fc\u00df", "ih\u00b7ren", "Leib", "ver\u00b7zeh\u00b7ret", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "PPOSAT", "NN", "VVFIN", "$("], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "So wird doch nicht der Tugend Glantz", "tokens": ["So", "wird", "doch", "nicht", "der", "Tu\u00b7gend", "Glantz"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "ADV", "PTKNEG", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Der Ahnen Ruhm und Sieges-Krantz/", "tokens": ["Der", "Ah\u00b7nen", "Ruhm", "und", "Sie\u00b7ges\u00b7Krantz", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "KON", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Und andre Gaben mehr aus dem Ged\u00e4chtn\u00fc\u00df kommen.", "tokens": ["Und", "and\u00b7re", "Ga\u00b7ben", "mehr", "aus", "dem", "Ge\u00b7d\u00e4cht\u00b7n\u00fc\u00df", "kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADJA", "NN", "ADV", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.10": {"text": "Es hat den schlechten Theil der Tod nur weggenommen/", "tokens": ["Es", "hat", "den", "schlech\u00b7ten", "Theil", "der", "Tod", "nur", "weg\u00b7ge\u00b7nom\u00b7men", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "NN", "ART", "NN", "ADV", "VVPP", "$("], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Die Seele wei\u00df von der Verwesung nicht/", "tokens": ["Die", "See\u00b7le", "wei\u00df", "von", "der", "Ver\u00b7we\u00b7sung", "nicht", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "ART", "NN", "PTKNEG", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "Und schwebet nun in jenem grossen Licht.", "tokens": ["Und", "schwe\u00b7bet", "nun", "in", "je\u00b7nem", "gros\u00b7sen", "Licht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "APPR", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.8": {"line.1": {"text": "Hoch-Edler Herr es eilt zur Ruh", "tokens": ["Hoch\u00b7Ed\u00b7ler", "Herr", "es", "eilt", "zur", "Ruh"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADJA", "NN", "PPER", "VVFIN", "APPRART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die wertheste Frau Mutter zu/", "tokens": ["Die", "wert\u00b7hes\u00b7te", "Frau", "Mut\u00b7ter", "zu", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "PTKZU", "$("], "meter": "-++--+-+", "measure": "iambic.tetra.relaxed"}, "line.3": {"text": "Und f\u00e4llt wie itzt das Laub von B\u00e4umen.", "tokens": ["Und", "f\u00e4llt", "wie", "itzt", "das", "Laub", "von", "B\u00e4u\u00b7men", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "KOKOM", "ADV", "ART", "NN", "APPR", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.4": {"text": "Gleich wie ein L\u00e4uffer auff der Flucht", "tokens": ["Gleich", "wie", "ein", "L\u00e4uf\u00b7fer", "auff", "der", "Flucht"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "KOKOM", "ART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.5": {"text": "Der seiner Reisen Endziel sucht/", "tokens": ["Der", "sei\u00b7ner", "Rei\u00b7sen", "End\u00b7ziel", "sucht", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "PPOSAT", "NN", "NN", "VVFIN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.6": {"text": "Sich nicht wird bey dem Wirth vers\u00e4umen:", "tokens": ["Sich", "nicht", "wird", "bey", "dem", "Wirth", "ver\u00b7s\u00e4u\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PTKNEG", "VAFIN", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.7": {"text": "So eilt sie aus dem Thr\u00e4nen-Thal/", "tokens": ["So", "eilt", "sie", "aus", "dem", "Thr\u00e4\u00b7nen\u00b7Thal", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "APPR", "ART", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.8": {"text": "Zu dem erlauchten Freuden-Saal/", "tokens": ["Zu", "dem", "er\u00b7lauch\u00b7ten", "Freu\u00b7den\u00b7Saal", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.9": {"text": "Gott da von Angesicht zu Angesicht zu schauen.", "tokens": ["Gott", "da", "von", "An\u00b7ge\u00b7sicht", "zu", "An\u00b7ge\u00b7sicht", "zu", "schau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "APPR", "NN", "APPR", "NN", "PTKZU", "VVINF", "$."], "meter": "+--+-+-+-+-+-", "measure": "iambic.hexa.invert"}, "line.10": {"text": "Es wird der Mensch wie Gra\u00df und Blumen abgehauen.", "tokens": ["Es", "wird", "der", "Mensch", "wie", "Gra\u00df", "und", "Blu\u00b7men", "ab\u00b7ge\u00b7hau\u00b7en", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "KOKOM", "NN", "KON", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.11": {"text": "Doch wie es drauff der Fr\u00fchling neu gewehrt", "tokens": ["Doch", "wie", "es", "drauff", "der", "Fr\u00fch\u00b7ling", "neu", "ge\u00b7wehrt"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "PWAV", "PPER", "PAV", "ART", "NN", "ADJD", "VVPP"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.12": {"text": "So sol auch einst seyn unser Leib verkl\u00e4rt.", "tokens": ["So", "sol", "auch", "einst", "seyn", "un\u00b7ser", "Leib", "ver\u00b7kl\u00e4rt", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "ADV", "ADV", "VAINF", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.9": {"line.1": {"text": "Verbleiben unbewegt bey seiner Mutter Leichen", "tokens": ["Ver\u00b7blei\u00b7ben", "un\u00b7be\u00b7wegt", "bey", "sei\u00b7ner", "Mut\u00b7ter", "Lei\u00b7chen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADJD", "APPR", "PPOSAT", "NN", "NN"], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "St\u00fcnd einem harten Felsen an/", "tokens": ["St\u00fcnd", "ei\u00b7nem", "har\u00b7ten", "Fel\u00b7sen", "an", "/"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Doch hat er was er sol gethan/", "tokens": ["Doch", "hat", "er", "was", "er", "sol", "ge\u00b7than", "/"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PWS", "PPER", "VMFIN", "VVPP", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Wenn er ihr Thr\u00e4nen wird zum letzten Opffer reichen.", "tokens": ["Wenn", "er", "ihr", "Thr\u00e4\u00b7nen", "wird", "zum", "letz\u00b7ten", "Opf\u00b7fer", "rei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPOSAT", "NN", "VAFIN", "APPRART", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}, "line.5": {"text": "Di\u00df heischet die Natur/ di\u00df fodert Schuld und Pflicht", "tokens": ["Di\u00df", "hei\u00b7schet", "die", "Na\u00b7tur", "/", "di\u00df", "fo\u00b7dert", "Schuld", "und", "Pflicht"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["PDS", "VVFIN", "ART", "NN", "$(", "PDS", "VVFIN", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.6": {"text": "Und diese rinnen schon aus seiner Augen-Licht.", "tokens": ["Und", "die\u00b7se", "rin\u00b7nen", "schon", "aus", "sei\u00b7ner", "Au\u00b7gen\u00b7Licht", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PDS", "VVFIN", "ADV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.7": {"text": "Hingegen wenn das Leid", "tokens": ["Hin\u00b7ge\u00b7gen", "wenn", "das", "Leid"], "token_info": ["word", "word", "word", "word"], "pos": ["ADV", "KOUS", "ART", "NN"], "meter": "+--+-+", "measure": "iambic.tri.invert"}, "line.8": {"text": "Sich etwas hat geleget/", "tokens": ["Sich", "et\u00b7was", "hat", "ge\u00b7le\u00b7get", "/"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PRF", "PIS", "VAFIN", "VVPP", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.9": {"text": "Und er die Sterbligkeit", "tokens": ["Und", "er", "die", "Ster\u00b7blig\u00b7keit"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "ART", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.10": {"text": "In seinem Sinn erweget", "tokens": ["In", "sei\u00b7nem", "Sinn", "er\u00b7we\u00b7get"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.11": {"text": "Wie Auff- und Unter-Gang einander stets umbfangen;", "tokens": ["Wie", "Auf\u00b7f", "und", "Un\u00b7ter\u00b7Gang", "ein\u00b7an\u00b7der", "stets", "umb\u00b7fan\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "TRUNC", "KON", "NN", "ADV", "ADV", "VVINF", "$."], "meter": "-+--+-+-+-+-+-", "measure": "iambic.hexa.relaxed"}, "line.12": {"text": "So wird er frey gestehn", "tokens": ["So", "wird", "er", "frey", "ge\u00b7stehn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "VVPP"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.13": {"text": "&q;Da\u00df die von hinnen gehn", "tokens": ["&", "q", ";", "Da\u00df", "die", "von", "hin\u00b7nen", "gehn"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["$(", "FM.la", "$.", "KOUS", "ART", "APPR", "ADV", "VVINF"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.14": {"text": "&q;Den Anfang wahrer Lust/ der Schmertzen End erlangen.", "tokens": ["&", "q", ";", "Den", "An\u00b7fang", "wah\u00b7rer", "Lust", "/", "der", "Schmert\u00b7zen", "End", "er\u00b7lan\u00b7gen", "."], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "FM.la", "$.", "ART", "NN", "ADJA", "NN", "$(", "ART", "NN", "NN", "VVINF", "$."], "meter": "+-+-+-+-+-+-+-", "measure": "trochaic.septa"}}}}}