{"textgrid.poem.31718": {"metadata": {"author": {"name": "Weerth, Georg", "birth": "N.A.", "death": "N.A."}, "title": "15.", "genre": "verse", "period": "N.A.", "pub_year": 1839, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Herr K\u00f6nig, Ihr, in Gold und Samt,", "tokens": ["Herr", "K\u00f6\u00b7nig", ",", "Ihr", ",", "in", "Gold", "und", "Samt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "NN", "$,", "PPER", "$,", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr seid ein hochgepreister!", "tokens": ["Ihr", "seid", "ein", "hoch\u00b7ge\u00b7preis\u00b7ter", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sagt, habt Ihr nicht ein kleines Amt", "tokens": ["Sagt", ",", "habt", "Ihr", "nicht", "ein", "klei\u00b7nes", "Amt"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "VAFIN", "PPER", "PTKNEG", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als Obertrinkemeister?", "tokens": ["Als", "O\u00b7bert\u00b7rin\u00b7ke\u00b7meis\u00b7ter", "?"], "token_info": ["word", "word", "punct"], "pos": ["KOUS", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Studieren t\u00e4t ich manches Jahr", "tokens": ["Stu\u00b7die\u00b7ren", "t\u00e4t", "ich", "man\u00b7ches", "Jahr"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NN", "VVFIN", "PPER", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Am Neckar und am Rheine", "tokens": ["Am", "Ne\u00b7ckar", "und", "am", "Rhei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "KON", "APPRART", "NE"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und an der Mosel und der Ahr", "tokens": ["Und", "an", "der", "Mo\u00b7sel", "und", "der", "Ahr"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "KON", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "In rot und wei\u00dfem Weine.", "tokens": ["In", "rot", "und", "wei\u00b7\u00dfem", "Wei\u00b7ne", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJD", "KON", "ADJA", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Beim L\u00f6wenwirte an der Lahn", "tokens": ["Beim", "L\u00f6\u00b7wen\u00b7wir\u00b7te", "an", "der", "Lahn"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPRART", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und seiner sch\u00f6nen Schwester", "tokens": ["Und", "sei\u00b7ner", "sch\u00f6\u00b7nen", "Schwes\u00b7ter"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Hab ich mein Geld und Gut vertan", "tokens": ["Hab", "ich", "mein", "Geld", "und", "Gut", "ver\u00b7tan"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "PPER", "PPOSAT", "NN", "KON", "ADJD", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und blieb dort zw\u00f6lf Semester!", "tokens": ["Und", "blieb", "dort", "zw\u00f6lf", "Se\u00b7mes\u00b7ter", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "CARD", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Bis mein Examen kam heran \u2013", "tokens": ["Bis", "mein", "E\u00b7xa\u00b7men", "kam", "he\u00b7ran", "\u2013"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "PTKVZ", "$("], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Da war Herr Hans gar flei\u00dfig:", "tokens": ["Da", "war", "Herr", "Hans", "gar", "flei\u00b7\u00dfig", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "NE", "ADV", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der F\u00e4sser gr\u00f6\u00dftes stach er an", "tokens": ["Der", "F\u00e4s\u00b7ser", "gr\u00f6\u00df\u00b7tes", "stach", "er", "an"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "VVFIN", "PPER", "PTKVZ"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vom Jahre vierunddrei\u00dfig.", "tokens": ["Vom", "Jah\u00b7re", "vie\u00b7rund\u00b7drei\u00b7\u00dfig", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["APPRART", "NN", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Aus allen Schenken nah und fern", "tokens": ["Aus", "al\u00b7len", "Schen\u00b7ken", "nah", "und", "fern"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PIAT", "NN", "ADJD", "KON", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Erschienen vor den Toren", "tokens": ["Er\u00b7schie\u00b7nen", "vor", "den", "To\u00b7ren"], "token_info": ["word", "word", "word", "word"], "pos": ["NN", "APPR", "ART", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der Fakult\u00e4t gelahrte Herrn", "tokens": ["Der", "Fa\u00b7kul\u00b7t\u00e4t", "ge\u00b7lahr\u00b7te", "Herrn"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Und spitzten ihre Ohren.", "tokens": ["Und", "spitz\u00b7ten", "ih\u00b7re", "Oh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Und ich dozierte blitzgeschwind", "tokens": ["Und", "ich", "do\u00b7zier\u00b7te", "blitz\u00b7ge\u00b7schwind"], "token_info": ["word", "word", "word", "word"], "pos": ["KON", "PPER", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Und wies vor allen Dingen,", "tokens": ["Und", "wies", "vor", "al\u00b7len", "Din\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Da\u00df K\u00f6lner Schoppen kleiner sind", "tokens": ["Da\u00df", "K\u00f6l\u00b7ner", "Schop\u00b7pen", "klei\u00b7ner", "sind"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "NN", "NN", "ADJD", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als die zu Mainz und Bingen,", "tokens": ["Als", "die", "zu", "Mainz", "und", "Bin\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "ART", "APPR", "NE", "KON", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Und da\u00df hier Simrock, der Poet,", "tokens": ["Und", "da\u00df", "hier", "Sim\u00b7rock", ",", "der", "Po\u00b7et", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["KON", "KOUS", "ADV", "NE", "$,", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Als Winzer auch zu schauen,", "tokens": ["Als", "Win\u00b7zer", "auch", "zu", "schau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "NN", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wenn er zum Menzenberge geht,", "tokens": ["Wenn", "er", "zum", "Men\u00b7zen\u00b7ber\u00b7ge", "geht", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "APPRART", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sein Drachenblut zu bauen. \u2013", "tokens": ["Sein", "Dra\u00b7chen\u00b7blut", "zu", "bau\u00b7en", ".", "\u2013"], "token_info": ["word", "word", "word", "word", "punct", "punct"], "pos": ["PPOSAT", "NN", "PTKZU", "VVINF", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Mein r\u00f6misch Glas, so hell und rein,", "tokens": ["Mein", "r\u00f6\u00b7misch", "Glas", ",", "so", "hell", "und", "rein", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "$,", "ADV", "ADJD", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So gr\u00fcn und bunt gekr\u00e4uselt,", "tokens": ["So", "gr\u00fcn", "und", "bunt", "ge\u00b7kr\u00e4u\u00b7selt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "KON", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Erhub ein besseres Latein", "tokens": ["Er\u00b7hub", "ein", "bes\u00b7se\u00b7res", "La\u00b7tein"], "token_info": ["word", "word", "word", "word"], "pos": ["VVFIN", "ART", "ADJA", "NN"], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}, "line.4": {"text": "Als Cicero ges\u00e4uselt.", "tokens": ["Als", "Ci\u00b7ce\u00b7ro", "ge\u00b7s\u00e4u\u00b7selt", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Da schrieb man mein Diploma gut", "tokens": ["Da", "schrieb", "man", "mein", "Dip\u00b7lo\u00b7ma", "gut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PIS", "PPOSAT", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auf Pergament und Leder", "tokens": ["Auf", "Per\u00b7ga\u00b7ment", "und", "Le\u00b7der"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "NN", "KON", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und steckte auf den Doktorhut", "tokens": ["Und", "steck\u00b7te", "auf", "den", "Dok\u00b7tor\u00b7hut"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mir eine Pfauenfeder.", "tokens": ["Mir", "ei\u00b7ne", "Pfau\u00b7en\u00b7fe\u00b7der", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["PPER", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Die Bauern aus dem Binger Loch", "tokens": ["Die", "Bau\u00b7ern", "aus", "dem", "Bin\u00b7ger", "Loch"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ART", "NN", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hab ich zum Schmaus genommen;", "tokens": ["Hab", "ich", "zum", "Schmaus", "ge\u00b7nom\u00b7men", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "APPRART", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Doch bin ich, leider, nimmer noch", "tokens": ["Doch", "bin", "ich", ",", "lei\u00b7der", ",", "nim\u00b7mer", "noch"], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "word"], "pos": ["KON", "VAFIN", "PPER", "$,", "ADV", "$,", "ADV", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auf gr\u00fcnen Zweig gekommen.", "tokens": ["Auf", "gr\u00fc\u00b7nen", "Zweig", "ge\u00b7kom\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ADJA", "NN", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Drum K\u00f6nig, Ihr, in Gold und Samt,", "tokens": ["Drum", "K\u00f6\u00b7nig", ",", "Ihr", ",", "in", "Gold", "und", "Samt", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PAV", "NN", "$,", "PPER", "$,", "APPR", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Ihr hoch und sehr gepreister,", "tokens": ["Ihr", "hoch", "und", "sehr", "ge\u00b7preis\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADJD", "KON", "ADV", "ADJA", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Sagt, habt Ihr nicht ein kleines Amt", "tokens": ["Sagt", ",", "habt", "Ihr", "nicht", "ein", "klei\u00b7nes", "Amt"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "$,", "VAFIN", "PPER", "PTKNEG", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Als Obertrinkemeister?", "tokens": ["Als", "O\u00b7bert\u00b7rin\u00b7ke\u00b7meis\u00b7ter", "?"], "token_info": ["word", "word", "punct"], "pos": ["KOUS", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Gebt mir, soviel ein ehrlich Mann", "tokens": ["Gebt", "mir", ",", "so\u00b7viel", "ein", "ehr\u00b7lich", "Mann"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["VVIMP", "PPER", "$,", "ADV", "ART", "ADJD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit W\u00fcrde wei\u00df zu fassen,", "tokens": ["Mit", "W\u00fcr\u00b7de", "wei\u00df", "zu", "fas\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und habt Ihr keine Lust \u2013 wohlan,", "tokens": ["Und", "habt", "Ihr", "kei\u00b7ne", "Lust", "\u2013", "wo\u00b7hlan", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["KON", "VAFIN", "PPER", "PIAT", "NN", "$(", "ADV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So m\u00f6gt Ihr's bleiben lassen.", "tokens": ["So", "m\u00f6gt", "Ih\u00b7r's", "blei\u00b7ben", "las\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PIS", "VVINF", "VVINF", "$."], "meter": "+-+-+-+-", "measure": "trochaic.tetra"}}}}}