{"dta.poem.12781": {"metadata": {"author": {"name": "Hofmannswaldau, Christian Hofmann von", "birth": "N.A.", "death": "N.A."}, "title": "Der zeitlich-sterbende und ewig-lebende  \n ph\u00f6nix,  \n Herrn M. J. C. G. rc.  \n  D.  V. A.", "genre": "Lyrik", "period": "N.A.", "pub_year": "1710", "urn": "urn:nbn:de:kobv:b4-20284-0", "language": ["de:0.99"], "booktitle": "N.A."}, "poem": {"stanza.1": {"line.1": {"text": "Die hoffnung ist nun aus,", "tokens": ["Die", "hoff\u00b7nung", "ist", "nun", "aus", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADV", "PTKVZ", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Das war die letzte flamme", "tokens": ["Das", "war", "die", "letz\u00b7te", "flam\u00b7me"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "VAFIN", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Aus Geyers edlem stamme,", "tokens": ["Aus", "Ge\u00b7yers", "ed\u00b7lem", "stam\u00b7me", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJA", "VVFIN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Ein stifft vom gantzen hau\u00df,", "tokens": ["Ein", "stifft", "vom", "gant\u00b7zen", "hau\u00df", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "APPRART", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Das zwar die schwestern zieren", "tokens": ["Das", "zwar", "die", "schwes\u00b7tern", "zie\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PDS", "ADV", "ART", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Doch nicht den namen f\u00fchren.", "tokens": ["Doch", "nicht", "den", "na\u00b7men", "f\u00fch\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PTKNEG", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.2": {"line.1": {"text": "Es ist zwar l\u00e4ngst geschehn,", "tokens": ["Es", "ist", "zwar", "l\u00e4ngst", "ge\u00b7schehn", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Da\u00df aus der Geyer orden", "tokens": ["Da\u00df", "aus", "der", "Ge\u00b7yer", "or\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KOUS", "APPR", "ART", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ein eintzler ph\u00f6nix worden;", "tokens": ["Ein", "eintz\u00b7ler", "ph\u00f6\u00b7nix", "wor\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "VAPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Mehr hat man nicht gesehn:", "tokens": ["Mehr", "hat", "man", "nicht", "ge\u00b7sehn", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PIS", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Doch da er muste sterben,", "tokens": ["Doch", "da", "er", "mus\u00b7te", "ster\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Lie\u00df er auch einen erben.", "tokens": ["Lie\u00df", "er", "auch", "ei\u00b7nen", "er\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ADV", "PIS", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.3": {"line.1": {"text": "Ich ziele hier auf dich,", "tokens": ["Ich", "zie\u00b7le", "hier", "auf", "dich", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "PPER", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Du ph\u00f6nix dieser lande!", "tokens": ["Du", "ph\u00f6\u00b7nix", "die\u00b7ser", "lan\u00b7de", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "NE", "PDAT", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Du adler in dem stande,", "tokens": ["Du", "ad\u00b7ler", "in", "dem", "stan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "NN", "APPR", "ART", "ADJA", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Den GOtt beh\u00e4lt vor sich!", "tokens": ["Den", "Gott", "be\u00b7h\u00e4lt", "vor", "sich", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "APPR", "PRF", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Du Nathan! dessen gaben", "tokens": ["Du", "Na\u00b7than", "!", "des\u00b7sen", "ga\u00b7ben"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["PPER", "NE", "$.", "PDS", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Wir itzt an Spenern haben.", "tokens": ["Wir", "itzt", "an", "Spe\u00b7nern", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "ADV", "APPR", "NN", "VAFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.4": {"line.1": {"text": "Du hattest einen Sohn,", "tokens": ["Du", "hat\u00b7test", "ei\u00b7nen", "Sohn", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Mein Geyer! erster ehe,", "tokens": ["Mein", "Ge\u00b7yer", "!", "ers\u00b7ter", "e\u00b7he", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$.", "ADV", "KOUS", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Der aber bald, o wehe!", "tokens": ["Der", "a\u00b7ber", "bald", ",", "o", "we\u00b7he", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ART", "ADV", "ADV", "$,", "FM", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Eilt aus der welt davon.", "tokens": ["Eilt", "aus", "der", "welt", "da\u00b7von", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Er h\u00e4tte sonst auf erden", "tokens": ["Er", "h\u00e4t\u00b7te", "sonst", "auf", "er\u00b7den"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Ein ph\u00f6nix k\u00f6nnen werden.", "tokens": ["Ein", "ph\u00f6\u00b7nix", "k\u00f6n\u00b7nen", "wer\u00b7den", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VMFIN", "VAINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.5": {"line.1": {"text": "Wie r\u00fchmlich war sein flei\u00df:", "tokens": ["Wie", "r\u00fchm\u00b7lich", "war", "sein", "flei\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Wie wust\u2019 er sein studiren", "tokens": ["Wie", "wust'", "er", "sein", "stu\u00b7di\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWAV", "VVFIN", "PPER", "PPOSAT", "ADJA"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "So n\u00fctzlich fort zu f\u00fchren!", "tokens": ["So", "n\u00fctz\u00b7lich", "fort", "zu", "f\u00fch\u00b7ren", "!"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PTKVZ", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Die tugend war sein prei\u00df:", "tokens": ["Die", "tu\u00b7gend", "war", "sein", "prei\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Sein lorbeer und eypressen", "tokens": ["Sein", "lor\u00b7beer", "und", "ey\u00b7pres\u00b7sen"], "token_info": ["word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "KON", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Sind bey mir unvergessen.", "tokens": ["Sind", "bey", "mir", "un\u00b7ver\u00b7ges\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "PPER", "ADJD", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.6": {"line.1": {"text": "Es bliebe nicht darbey:", "tokens": ["Es", "blie\u00b7be", "nicht", "dar\u00b7bey", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PTKNEG", "PAV", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Wir konten uns getrauen", "tokens": ["Wir", "kon\u00b7ten", "uns", "ge\u00b7trau\u00b7en"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "PPER", "VVPP"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Am andern sohn zu schauen,", "tokens": ["Am", "an\u00b7dern", "sohn", "zu", "schau\u00b7en", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da\u00df er ein ph\u00f6nix sey;", "tokens": ["Da\u00df", "er", "ein", "ph\u00f6\u00b7nix", "sey", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NE", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "An dem des vaters gaben", "tokens": ["An", "dem", "des", "va\u00b7ters", "ga\u00b7ben"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "ART", "ART", "NN", "VVFIN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Wir solten wieder haben.", "tokens": ["Wir", "sol\u00b7ten", "wie\u00b7der", "ha\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ADV", "VAINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.7": {"line.1": {"text": "Er war von guter art,", "tokens": ["Er", "war", "von", "gu\u00b7ter", "art", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Des himmels reine flammen", "tokens": ["Des", "him\u00b7mels", "rei\u00b7ne", "flam\u00b7men"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Verbunden sich zusammen,", "tokens": ["Ver\u00b7bun\u00b7den", "sich", "zu\u00b7sam\u00b7men", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["NN", "PRF", "PTKVZ", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Als er geboren ward:", "tokens": ["Als", "er", "ge\u00b7bo\u00b7ren", "ward", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVPP", "VAFIN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Carpzoys und Geyers namen,", "tokens": ["Car\u00b7pzoys", "und", "Ge\u00b7yers", "na\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NE", "KON", "NE", "NN", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Sind der gerechten saamen.", "tokens": ["Sind", "der", "ge\u00b7rech\u00b7ten", "saa\u00b7men", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ART", "ADJA", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.8": {"line.1": {"text": "Ein solcher doppel-stamm", "tokens": ["Ein", "sol\u00b7cher", "dop\u00b7pel\u00b7stamm"], "token_info": ["word", "word", "word"], "pos": ["ART", "PIAT", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Kan gute fr\u00fcchte bringen;", "tokens": ["Kan", "gu\u00b7te", "fr\u00fcch\u00b7te", "brin\u00b7gen", ";"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VMFIN", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Ein ph\u00f6nix mu\u00df sich schwingen", "tokens": ["Ein", "ph\u00f6\u00b7nix", "mu\u00df", "sich", "schwin\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ART", "NE", "VMFIN", "PRF", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Aus einer solchen flamm:", "tokens": ["Aus", "ei\u00b7ner", "sol\u00b7chen", "flamm", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "PIAT", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Hier ist es auch geschehen;", "tokens": ["Hier", "ist", "es", "auch", "ge\u00b7sche\u00b7hen", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADV", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Wer hat es nicht gesehen?", "tokens": ["Wer", "hat", "es", "nicht", "ge\u00b7se\u00b7hen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VAFIN", "PPER", "PTKNEG", "VVPP", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.9": {"line.1": {"text": "Wer wei\u00df nicht, wie er schon", "tokens": ["Wer", "wei\u00df", "nicht", ",", "wie", "er", "schon"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PTKNEG", "$,", "PWAV", "PPER", "ADV"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Der tugend sich ergeben,", "tokens": ["Der", "tu\u00b7gend", "sich", "er\u00b7ge\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PRF", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Bey seines Vaters leben", "tokens": ["Bey", "sei\u00b7nes", "Va\u00b7ters", "le\u00b7ben"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Der wohlgerathne Sohn?", "tokens": ["Der", "wohl\u00b7ge\u00b7rath\u00b7ne", "Sohn", "?"], "token_info": ["word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Man sah in jungen jahren", "tokens": ["Man", "sah", "in", "jun\u00b7gen", "jah\u00b7ren"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PIS", "VVFIN", "APPR", "ADJA", "NN"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Jhn mit den Musen paaren.", "tokens": ["Jhn", "mit", "den", "Mu\u00b7sen", "paa\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.10": {"line.1": {"text": "Der Mutter treue hand", "tokens": ["Der", "Mut\u00b7ter", "treu\u00b7e", "hand"], "token_info": ["word", "word", "word", "word"], "pos": ["ART", "NN", "ADJA", "NN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "F\u00fchrt ihn zu unsern linden;", "tokens": ["F\u00fchrt", "ihn", "zu", "un\u00b7sern", "lin\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "APPR", "PPOSAT", "ADJA", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Wo gleiche kunst zu finden,", "tokens": ["Wo", "glei\u00b7che", "kunst", "zu", "fin\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJA", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Hat sie ihn hingesandt:", "tokens": ["Hat", "sie", "ihn", "hin\u00b7ge\u00b7sandt", ":"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "PPER", "VVPP", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "T\u00fcbingen, Stra\u00dfburg, Gie\u00dfen,", "tokens": ["T\u00fc\u00b7bin\u00b7gen", ",", "Stra\u00df\u00b7burg", ",", "Gie\u00b7\u00dfen", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NE", "$,", "NN", "$,"], "meter": "+--+-+-", "measure": "iambic.tri.invert"}, "line.6": {"text": "Lie\u00df nectar auf ihn fliessen.", "tokens": ["Lie\u00df", "nec\u00b7tar", "auf", "ihn", "flies\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.11": {"line.1": {"text": "Nun war es an der zeit,", "tokens": ["Nun", "war", "es", "an", "der", "zeit", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Nachdem er wiederkommen,", "tokens": ["Nach\u00b7dem", "er", "wie\u00b7der\u00b7kom\u00b7men", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Und trefflich zugenommen,", "tokens": ["Und", "treff\u00b7lich", "zu\u00b7ge\u00b7nom\u00b7men", ","], "token_info": ["word", "word", "word", "punct"], "pos": ["KON", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Da\u00df er sie auch erfreut:", "tokens": ["Da\u00df", "er", "sie", "auch", "er\u00b7freut", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PPER", "ADV", "ADJD", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Allein sie geht im leide;", "tokens": ["Al\u00b7lein", "sie", "geht", "im", "lei\u00b7de", ";"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "APPRART", "VVFIN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Wo bleibet ihre freude?", "tokens": ["Wo", "blei\u00b7bet", "ih\u00b7re", "freu\u00b7de", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PPOSAT", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}, "stanza.12": {"line.1": {"text": "Gott hat es selbst gethan,", "tokens": ["Gott", "hat", "es", "selbst", "ge\u00b7than", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VAFIN", "PPER", "ADV", "VVPP", "$,"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.2": {"text": "Wor\u00fcber sie will klagen,", "tokens": ["Wo\u00b7r\u00fc\u00b7ber", "sie", "will", "kla\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPER", "VMFIN", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.3": {"text": "Jhr ph\u00f6nix l\u00e4st sich tragen", "tokens": ["Ihr", "ph\u00f6\u00b7nix", "l\u00e4st", "sich", "tra\u00b7gen"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "NE", "VVFIN", "PRF", "VVINF"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.4": {"text": "Zu seinem pelican:", "tokens": ["Zu", "sei\u00b7nem", "pe\u00b7li\u00b7can", ":"], "token_info": ["word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NE", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.5": {"text": "Der wird ihn einst verj\u00fcngen,", "tokens": ["Der", "wird", "ihn", "einst", "ver\u00b7j\u00fcn\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "VAFIN", "PPER", "ADV", "VVINF", "$,"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.6": {"text": "Und wieder zu ihr bringen.", "tokens": ["Und", "wie\u00b7der", "zu", "ihr", "brin\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "PPER", "VVINF", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}}}}}