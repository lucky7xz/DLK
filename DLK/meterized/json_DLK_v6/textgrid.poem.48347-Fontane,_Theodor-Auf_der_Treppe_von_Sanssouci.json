{"textgrid.poem.48347": {"metadata": {"author": {"name": "Fontane, Theodor", "birth": "N.A.", "death": "N.A."}, "title": "Auf der Treppe von Sanssouci", "genre": "verse", "period": "N.A.", "pub_year": 1858, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Von Marly kommend und der Friedenskirche,", "tokens": ["Von", "Mar\u00b7ly", "kom\u00b7mend", "und", "der", "Frie\u00b7dens\u00b7kir\u00b7che", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NE", "ADJD", "KON", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Hin am Bassin (es pl\u00e4tscherte kein Springstrahl)", "tokens": ["Hin", "am", "Bas\u00b7sin", "(", "es", "pl\u00e4t\u00b7scher\u00b7te", "kein", "Springs\u00b7trahl", ")"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$(", "PPER", "VVFIN", "PIAT", "NN", "$("], "meter": "+--+-+---+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Stieg ich treppan; die Sterne blinkten, blitzten,", "tokens": ["Stieg", "ich", "trep\u00b7pan", ";", "die", "Ster\u00b7ne", "blink\u00b7ten", ",", "blitz\u00b7ten", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "PPER", "VVINF", "$.", "ART", "NN", "VVFIN", "$,", "VVFIN", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.4": {"text": "Und auf den Stufenaufbau der Terrasse", "tokens": ["Und", "auf", "den", "Stu\u00b7fe\u00b7nauf\u00b7bau", "der", "Ter\u00b7ras\u00b7se"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KON", "APPR", "ART", "NN", "ART", "NN"], "meter": "-+-+--+--+-", "measure": "iambic.tetra.relaxed"}, "line.5": {"text": "Warf Baum und Strauchwerk seine d\u00fcnnen Schatten,", "tokens": ["Warf", "Baum", "und", "Strauch\u00b7werk", "sei\u00b7ne", "d\u00fcn\u00b7nen", "Schat\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "NE", "KON", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Durchsichtige, wie Schatten nur von Schatten.", "tokens": ["Durch\u00b7sich\u00b7ti\u00b7ge", ",", "wie", "Schat\u00b7ten", "nur", "von", "Schat\u00b7ten", "."], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJA", "$,", "PWAV", "NN", "ADV", "APPR", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Rings tiefe Stille, selbst der Wache Schritt", "tokens": ["Rings", "tie\u00b7fe", "Stil\u00b7le", ",", "selbst", "der", "Wa\u00b7che", "Schritt"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["NN", "ADJA", "NN", "$,", "ADV", "ART", "NN", "NN"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.8": {"text": "Blieb lautlos auf dem \u00fcberreiften Boden,", "tokens": ["Blieb", "laut\u00b7los", "auf", "dem", "\u00fc\u00b7berr\u00b7eif\u00b7ten", "Bo\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADJD", "APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Und nur von rechts her, von der Stadt her\u00fcber,", "tokens": ["Und", "nur", "von", "rechts", "her", ",", "von", "der", "Stadt", "her\u00b7\u00fc\u00b7ber", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "APPR", "ADV", "PTKVZ", "$,", "APPR", "ART", "NN", "ADV", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Erscholl das Glockenspiel.", "tokens": ["Er\u00b7scholl", "das", "Glo\u00b7cken\u00b7spiel", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["ADJD", "ART", "NN", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.11": {"text": "Nun schwieg auch das,", "tokens": ["Nun", "schwieg", "auch", "das", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "ADV", "PDS", "$,"], "meter": "-+-+", "measure": "iambic.di"}, "line.12": {"text": "Und als mein Auge, das auf kurze Weile", "tokens": ["Und", "als", "mein", "Au\u00b7ge", ",", "das", "auf", "kur\u00b7ze", "Wei\u00b7le"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["KON", "KOUS", "PPOSAT", "NN", "$,", "PRELS", "APPR", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Dem Ohr gefolgt war, wieder vorw\u00e4rts blickte,", "tokens": ["Dem", "Ohr", "ge\u00b7folgt", "war", ",", "wie\u00b7der", "vor\u00b7w\u00e4rts", "blick\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVPP", "VAFIN", "$,", "ADV", "ADV", "VVFIN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.14": {"text": "Trat aus dem Buschwerk, und ich schrak zusammen,", "tokens": ["Trat", "aus", "dem", "Busc\u00b7hwerk", ",", "und", "ich", "schrak", "zu\u00b7sam\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ART", "NN", "$,", "KON", "PPER", "VVFIN", "PTKVZ", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Er selbst, im Frackrock, hinter ihm das Windspiel", "tokens": ["Er", "selbst", ",", "im", "Frack\u00b7rock", ",", "hin\u00b7ter", "ihm", "das", "Wind\u00b7spiel"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "ADV", "$,", "APPRART", "NN", "$,", "APPR", "PPER", "ART", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "(biche, wenn nicht alles t\u00e4uschte), dazu Kr\u00fcckstock", "tokens": ["(", "bi\u00b7che", ",", "wenn", "nicht", "al\u00b7les", "t\u00e4uschte", ")", ",", "da\u00b7zu", "Kr\u00fcck\u00b7stock"], "token_info": ["punct", "word", "punct", "word", "word", "word", "word", "punct", "punct", "word", "word"], "pos": ["$(", "VVFIN", "$,", "KOUS", "PTKNEG", "PIS", "VVFIN", "$(", "$,", "PAV", "NN"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.17": {"text": "Und Hut und Stern. Bei Gott, es war der K\u00f6nig.", "tokens": ["Und", "Hut", "und", "Stern", ".", "Bei", "Gott", ",", "es", "war", "der", "K\u00f6\u00b7nig", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "NN", "KON", "NN", "$.", "APPR", "NN", "$,", "PPER", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.2": {"line.1": {"text": "Was tun? Ich dacht' an Umkehr; doch sein Auge,", "tokens": ["Was", "tun", "?", "Ich", "dacht'", "an", "Um\u00b7kehr", ";", "doch", "sein", "Au\u00b7ge", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PWS", "VVINF", "$.", "PPER", "VVFIN", "APPR", "NN", "$.", "ADV", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Das ", "tokens": ["Das"], "token_info": ["word"], "pos": ["PDS"], "meter": "-", "measure": "single.down"}, "line.3": {"text": "So hielt ich denn und machte Front.", "tokens": ["So", "hielt", "ich", "denn", "und", "mach\u00b7te", "Front", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "KON", "VVFIN", "NE", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "\u00bbwie hei\u00dft Er?\u00ab", "tokens": ["\u00bb", "wie", "hei\u00dft", "Er", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "PWAV", "VVFIN", "PPER", "$.", "$("], "meter": "+--", "measure": "dactylic.init"}, "line.5": {"text": "Ich stotterte was hin.", "tokens": ["Ich", "stot\u00b7ter\u00b7te", "was", "hin", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PIS", "PTKVZ", "$."], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.6": {"text": "\u00bbund sein Metier?\u00ab", "tokens": ["\u00bb", "und", "sein", "Me\u00b7tier", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KON", "PPOSAT", "NN", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.7": {"text": "\u00bbschriftsteller, Majest\u00e4t. Ich mache Verse!\u00ab", "tokens": ["\u00bb", "schrift\u00b7stel\u00b7ler", ",", "Ma\u00b7jes\u00b7t\u00e4t", ".", "Ich", "ma\u00b7che", "Ver\u00b7se", "!", "\u00ab"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "$,", "NN", "$.", "PPER", "VVFIN", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.3": {"line.1": {"text": "Der K\u00f6nig l\u00e4chelte: \u00bbNun h\u00f6r' Er, Herr,", "tokens": ["Der", "K\u00f6\u00b7nig", "l\u00e4\u00b7chel\u00b7te", ":", "\u00bb", "Nun", "h\u00f6r'", "Er", ",", "Herr", ","], "token_info": ["word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$.", "$(", "ADV", "VVFIN", "PPER", "$,", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Ich will's Ihm glauben; keiner ist der Tor,", "tokens": ["Ich", "will's", "Ihm", "glau\u00b7ben", ";", "kei\u00b7ner", "ist", "der", "Tor", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "PPER", "VVFIN", "$.", "PIS", "VAFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.3": {"text": "Sich dieses Zeichens ohne Not zu r\u00fchmen,", "tokens": ["Sich", "die\u00b7ses", "Zei\u00b7chens", "oh\u00b7ne", "Not", "zu", "r\u00fch\u00b7men", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PRF", "PDAT", "NN", "APPR", "NN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Dergleichen sagt nur, wer es sagen mu\u00df,", "tokens": ["Derg\u00b7lei\u00b7chen", "sagt", "nur", ",", "wer", "es", "sa\u00b7gen", "mu\u00df", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "ADV", "$,", "PWS", "PPER", "VVINF", "VMFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.5": {"text": "Der Spott ist sicher, zweifelhaft das andre.", "tokens": ["Der", "Spott", "ist", "si\u00b7cher", ",", "zwei\u00b7fel\u00b7haft", "das", "and\u00b7re", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "ADJD", "ART", "ADJA", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Po\u00e8te allemand! Ja, ja, Berlin wird Weltstadt.", "tokens": ["Po\u00e8te", "al\u00b7le\u00b7mand", "!", "Ja", ",", "ja", ",", "Ber\u00b7lin", "wird", "Welt\u00b7stadt", "."], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PIS", "$.", "PTKANT", "$,", "PTKANT", "$,", "NE", "VAFIN", "NN", "$."], "meter": "-+--+-++-+-", "measure": "iambic.penta.relaxed"}, "line.7": {"text": "Nun aber sag' Er mir, ich les' da t\u00e4glich", "tokens": ["Nun", "a\u00b7ber", "sag'", "Er", "mir", ",", "ich", "les'", "da", "t\u00e4g\u00b7lich"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "ADV", "VVFIN", "PPER", "PPER", "$,", "PPER", "VVFIN", "ADV", "ADJD"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "(verzeih' Er, aber Federvieh und Borste", "tokens": ["(", "ver\u00b7zeih'", "Er", ",", "a\u00b7ber", "Fe\u00b7der\u00b7vieh", "und", "Bors\u00b7te"], "token_info": ["punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["$(", "VVIMP", "PPER", "$,", "KON", "NN", "KON", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.9": {"text": "Wohnt auf demselben Hof und h\u00e4lt Gemeinschaft),", "tokens": ["Wohnt", "auf", "dem\u00b7sel\u00b7ben", "Hof", "und", "h\u00e4lt", "Ge\u00b7mein\u00b7schaft", ")", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["VVFIN", "APPR", "PDAT", "NN", "KON", "VVFIN", "NN", "$(", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Ich les' da t\u00e4glich jetzt in den Gazetten", "tokens": ["Ich", "les'", "da", "t\u00e4g\u00b7lich", "jetzt", "in", "den", "Ga\u00b7zet\u00b7ten"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ADJD", "ADV", "APPR", "ART", "NN"], "meter": "-+-+-+---+-", "measure": "unknown.measure.tetra"}, "line.11": {"text": "Von Menzelfest und siebzigstem Geburtstag,", "tokens": ["Von", "Men\u00b7zel\u00b7fest", "und", "sieb\u00b7zigs\u00b7tem", "Ge\u00b7burts\u00b7tag", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+---+-", "measure": "unknown.measure.tetra"}, "line.12": {"text": "Ausstellung von Tableaux und von Peint\u00fcren", "tokens": ["Aus\u00b7stel\u00b7lung", "von", "Tab\u00b7le\u00b7a\u00b7ux", "und", "von", "Pein\u00b7t\u00fc\u00b7ren"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "APPR", "NE", "KON", "APPR", "NN"], "meter": "-+--+--+-+-+-", "measure": "amphibrach.tri.plus"}, "line.13": {"text": "Und \u00e4hnlichem. Ein gro\u00dfer L\u00e4rm. Eh bien, Herr,", "tokens": ["Und", "\u00e4hn\u00b7li\u00b7chem", ".", "Ein", "gro\u00b7\u00dfer", "L\u00e4rm", ".", "Eh", "bi\u00b7en", ",", "Herr", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "punct"], "pos": ["KON", "ADJA", "$.", "ART", "ADJA", "NN", "$.", "NN", "NE", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.14": {"text": "Was soll das? Kennt Er Menzel? Wer ist Menzel?\u00ab", "tokens": ["Was", "soll", "das", "?", "Kennt", "Er", "Men\u00b7zel", "?", "Wer", "ist", "Men\u00b7zel", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["PWS", "VMFIN", "PDS", "$.", "VVFIN", "PPER", "NN", "$.", "PWS", "VAFIN", "NN", "$.", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.4": {"line.1": {"text": "Und dabei flog ein Zug um seinen Mund,", "tokens": ["Und", "da\u00b7bei", "flog", "ein", "Zug", "um", "sei\u00b7nen", "Mund", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PAV", "VVFIN", "ART", "NN", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.2": {"text": "Als wiss' er selber Antwort auf die Frage.", "tokens": ["Als", "wiss'", "er", "sel\u00b7ber", "Ant\u00b7wort", "auf", "die", "Fra\u00b7ge", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "VVFIN", "PPER", "ADV", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "\u00bbzu Gnaden Majest\u00e4t\u00ab, begann ich z\u00f6gernd,", "tokens": ["\u00bb", "zu", "Gna\u00b7den", "Ma\u00b7jes\u00b7t\u00e4t", "\u00ab", ",", "be\u00b7gann", "ich", "z\u00f6\u00b7gernd", ","], "token_info": ["punct", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["$(", "APPR", "NN", "NN", "$(", "$,", "VVFIN", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.2": {"text": "\u00bbdie Frag' ist schwer, das ist ein Doktorthema;", "tokens": ["\u00bb", "die", "Frag'", "ist", "schwer", ",", "das", "ist", "ein", "Dok\u00b7tor\u00b7the\u00b7ma", ";"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["$(", "ART", "NN", "VAFIN", "ADJD", "$,", "PDS", "VAFIN", "ART", "NN", "$."], "meter": "-+-+-+----+", "measure": "unknown.measure.tetra"}, "line.3": {"text": "Mein Wissen reicht bis Pierer nur und Brockhaus.", "tokens": ["Mein", "Wis\u00b7sen", "reicht", "bis", "Pie\u00b7rer", "nur", "und", "Brock\u00b7haus", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "VVFIN", "APPR", "NN", "ADV", "KON", "NN", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "Ja, wer ist Menzel? Menzel ist sehr vieles,", "tokens": ["Ja", ",", "wer", "ist", "Men\u00b7zel", "?", "Men\u00b7zel", "ist", "sehr", "vie\u00b7les", ","], "token_info": ["word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["PTKANT", "$,", "PWS", "VAFIN", "NN", "$.", "NN", "VAFIN", "ADV", "PIS", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Um nicht zu sagen alles; mind'stens ist er", "tokens": ["Um", "nicht", "zu", "sa\u00b7gen", "al\u00b7les", ";", "min\u00b7d'\u00b7stens", "ist", "er"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["KOUI", "PTKNEG", "PTKZU", "VVINF", "PIS", "$.", "ADV", "VAFIN", "PPER"], "meter": "-+-+-+-+-++-", "measure": "unknown.measure.hexa"}, "line.6": {"text": "Die ganze Arche No\u00e4h, Tier und Menschen:", "tokens": ["Die", "gan\u00b7ze", "Ar\u00b7che", "No\u00e4h", ",", "Tier", "und", "Men\u00b7schen", ":"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NE", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+--+-+-", "measure": "iambic.tetra.relaxed"}, "line.7": {"text": "Putth\u00fchner, G\u00e4nse, Papagei'n und Enten,", "tokens": ["Putt\u00b7h\u00fch\u00b7ner", ",", "G\u00e4n\u00b7se", ",", "Pa\u00b7pa\u00b7gei'n", "und", "En\u00b7ten", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "$,", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.8": {"text": "Schwerin und Seydlitz, Leopold von Dessau,", "tokens": ["Schwe\u00b7rin", "und", "Seyd\u00b7litz", ",", "Leo\u00b7pold", "von", "Des\u00b7sau", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "KON", "NE", "$,", "NE", "APPR", "NE", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.9": {"text": "Der alte Zieten, Ammen, Schlosserjungen,", "tokens": ["Der", "al\u00b7te", "Zie\u00b7ten", ",", "Am\u00b7men", ",", "Schlos\u00b7ser\u00b7jun\u00b7gen", ","], "token_info": ["word", "word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ART", "ADJA", "NN", "$,", "NN", "$,", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.10": {"text": "Kathol'sche Kirchen, italien'sche Pl\u00e4tze,", "tokens": ["Ka\u00b7thol'\u00b7sche", "Kir\u00b7chen", ",", "i\u00b7ta\u00b7li\u00b7en'\u00b7sche", "Pl\u00e4t\u00b7ze", ","], "token_info": ["word", "word", "punct", "word", "word", "punct"], "pos": ["ADJA", "NN", "$,", "ADJA", "NN", "$,"], "meter": "+--+--+-+-+-", "measure": "dactylic.di.plus"}, "line.11": {"text": "Schuhschnallen, Bronzen, Walz- und Eisenwerke,", "tokens": ["Schuh\u00b7schnal\u00b7len", ",", "Bron\u00b7zen", ",", "Wa\u00b7lz", "und", "Ei\u00b7sen\u00b7wer\u00b7ke", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "TRUNC", "KON", "NN", "$,"], "meter": "+--+--+-+-+-", "measure": "dactylic.di.plus"}, "line.12": {"text": "Stadtr\u00e4te mit und ohne goldne Kette,", "tokens": ["Stadt\u00b7r\u00e4\u00b7te", "mit", "und", "oh\u00b7ne", "gold\u00b7ne", "Ket\u00b7te", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "KON", "APPR", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Minister, mi\u00dfgestimmt in Kaschmirhosen,", "tokens": ["Mi\u00b7nis\u00b7ter", ",", "mi\u00df\u00b7ge\u00b7stimmt", "in", "Kaschmir\u00b7ho\u00b7sen", ","], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$,", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.14": {"text": "Strau\u00dffedern, Hofball, Hummermayonnaise,", "tokens": ["Strau\u00df\u00b7fe\u00b7dern", ",", "Hof\u00b7ball", ",", "Hum\u00b7mer\u00b7ma\u00b7yon\u00b7nai\u00b7se", ","], "token_info": ["word", "punct", "word", "punct", "word", "punct"], "pos": ["NN", "$,", "NN", "$,", "NN", "$,"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.15": {"text": "Der Kaiser, Moltke, Gr\u00e4fin Hacke, Bismarck \u2013\u00ab", "tokens": ["Der", "Kai\u00b7ser", ",", "Molt\u00b7ke", ",", "Gr\u00e4\u00b7fin", "Ha\u00b7cke", ",", "Bis\u00b7marck", "\u2013", "\u00ab"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "punct", "word", "punct", "punct"], "pos": ["ART", "NN", "$,", "NE", "$,", "NN", "NE", "$,", "NE", "$(", "$("], "meter": "-+-+-+-+--+", "measure": "iambic.penta.chol"}, "line.16": {"text": "\u00bboutrier' Er nicht.\u00ab", "tokens": ["\u00bb", "ou\u00b7trier'", "Er", "nicht", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "VVIMP", "PPER", "PTKNEG", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.17": {"text": "\u00bbich spreche nur die Wahrheit.", "tokens": ["\u00bb", "ich", "spre\u00b7che", "nur", "die", "Wahr\u00b7heit", "."], "token_info": ["punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "PPER", "VVFIN", "ADV", "ART", "NN", "$."], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.18": {"text": "Bescheidne Wahrheit nur. Er durchstudierte", "tokens": ["Be\u00b7scheid\u00b7ne", "Wahr\u00b7heit", "nur", ".", "Er", "durch\u00b7stu\u00b7dier\u00b7te"], "token_info": ["word", "word", "word", "punct", "word", "word"], "pos": ["ADJA", "NN", "ADV", "$.", "PPER", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.19": {"text": "Die gro\u00df' und kleine Welt; was kreucht und fleucht,", "tokens": ["Die", "gro\u00df'", "und", "klei\u00b7ne", "Welt", ";", "was", "kreucht", "und", "fleucht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "KON", "ADJA", "NN", "$.", "PWS", "VVFIN", "KON", "VVFIN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.20": {"text": "Er gibt es uns im Spiegelbilde wieder.", "tokens": ["Er", "gibt", "es", "uns", "im", "Spie\u00b7gel\u00b7bil\u00b7de", "wie\u00b7der", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PRF", "APPRART", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.21": {"text": "Am liebsten aber (und mir schwoll der Kamm,", "tokens": ["Am", "liebs\u00b7ten", "a\u00b7ber", "(", "und", "mir", "schwoll", "der", "Kamm", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "ADV", "$(", "KON", "PPER", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}, "line.22": {"text": "Ich war im Gang, \u203ajetzt oder niemals\u2039 dacht' ich),", "tokens": ["Ich", "war", "im", "Gang", ",", "\u203a", "jetzt", "o\u00b7der", "nie\u00b7mals", "\u2039", "dacht'", "ich", ")", ","], "token_info": ["word", "word", "word", "word", "punct", "punct", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "APPRART", "NN", "$,", "$(", "ADV", "KON", "ADV", "$(", "VVFIN", "PPER", "$(", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.23": {"text": "Am liebsten aber gibt ", "tokens": ["Am", "liebs\u00b7ten", "a\u00b7ber", "gibt"], "token_info": ["word", "word", "word", "word"], "pos": ["PTKA", "ADJD", "ADV", "VVFIN"], "meter": "-+-+-+", "measure": "iambic.tri"}, "line.24": {"text": "Die ", "tokens": ["Die"], "token_info": ["word"], "pos": ["ART"], "meter": "-", "measure": "single.down"}, "line.25": {"text": "Im Rundsaal, vom Plafond her, strahlt der Lustre,", "tokens": ["Im", "Rund\u00b7saal", ",", "vom", "Pla\u00b7fond", "her", ",", "strahlt", "der", "Lust\u00b7re", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "$,", "APPRART", "NN", "PTKVZ", "$,", "VVFIN", "ART", "NN", "$,"], "meter": "-+--+--+-+-", "measure": "amphibrach.tri.plus"}, "line.26": {"text": "Siebartig golden blinkt der St\u00fchle Flechtwerk,", "tokens": ["Sieb\u00b7ar\u00b7tig", "gol\u00b7den", "blinkt", "der", "St\u00fch\u00b7le", "Flecht\u00b7werk", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "ADJA", "VVFIN", "ART", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.27": {"text": "Biche (\u203akomm, mein Bichechen\u2039) streift die Tischtuchecke,", "tokens": ["Bi\u00b7che", "(", "\u203a", "komm", ",", "mein", "Bi\u00b7ch\u00b7e\u00b7chen", "\u2039", ")", "streift", "die", "Tischtu\u00b7che\u00b7cke", ","], "token_info": ["word", "punct", "punct", "word", "punct", "word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["NN", "$(", "$(", "VVFIN", "$,", "PPOSAT", "NN", "$(", "$(", "VVFIN", "ART", "NN", "$,"], "meter": "+-+-+-+-+-+-+", "measure": "trochaic.septa"}, "line.28": {"text": "Champagner perlt, und auf der Mei\u00dfner Schale", "tokens": ["Cham\u00b7pag\u00b7ner", "perlt", ",", "und", "auf", "der", "Mei\u00df\u00b7ner", "Scha\u00b7le"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word"], "pos": ["NE", "VVFIN", "$,", "KON", "APPR", "ART", "ADJA", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.29": {"text": "Liegt, schon zerpfl\u00fcckt, die Pontac-Apfelsine ...\u00ab", "tokens": ["Liegt", ",", "schon", "zer\u00b7pfl\u00fcckt", ",", "die", "Pon\u00b7tac\u00b7Ap\u00b7fel\u00b7si\u00b7ne", "...", "\u00ab"], "token_info": ["word", "punct", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["VVFIN", "$,", "ADV", "VVPP", "$,", "ART", "NN", "$(", "$("], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "\u00bbnun lass' Er nur. Ich wei\u00df schon.\u00ab", "tokens": ["\u00bb", "nun", "lass'", "Er", "nur", ".", "Ich", "wei\u00df", "schon", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADV", "VVFIN", "PPER", "ADV", "$.", "PPER", "VVFIN", "ADV", "$.", "$("], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.2": {"text": "Und er l\u00fcpfte", "tokens": ["Und", "er", "l\u00fcpf\u00b7te"], "token_info": ["word", "word", "word"], "pos": ["KON", "PPER", "VVFIN"], "meter": "+-+-", "measure": "trochaic.di"}, "line.3": {"text": "Den Hut und ging. Doch sieh, nur wenig Schritte,", "tokens": ["Den", "Hut", "und", "ging", ".", "Doch", "sieh", ",", "nur", "we\u00b7nig", "Schrit\u00b7te", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "KON", "VVFIN", "$.", "KON", "VVFIN", "$,", "ADV", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "So hielt er wieder, wandte sich und winkte", "tokens": ["So", "hielt", "er", "wie\u00b7der", ",", "wand\u00b7te", "sich", "und", "wink\u00b7te"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ADV", "$,", "VVFIN", "PRF", "KON", "VVFIN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.5": {"text": "Mich an die Seit' ihm. \u00bbH\u00f6r Er, Herr; ein Wort noch:", "tokens": ["Mich", "an", "die", "Seit'", "ihm", ".", "\u00bb", "H\u00f6r", "Er", ",", "Herr", ";", "ein", "Wort", "noch", ":"], "token_info": ["word", "word", "word", "word", "word", "punct", "punct", "word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "APPR", "ART", "NN", "PPER", "$.", "$(", "VVIMP", "PPER", "$,", "NN", "$.", "ART", "NN", "ADV", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Er hat bestanden; so lala. Denn wiss' Er,", "tokens": ["Er", "hat", "be\u00b7stan\u00b7den", ";", "so", "la\u00b7la", ".", "Denn", "wiss'", "Er", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "$.", "ADV", "NE", "$.", "KON", "VVFIN", "PPER", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.7": {"text": "Ich kenne Menzel wie mich selbst und w\u00e4r' ihm", "tokens": ["Ich", "ken\u00b7ne", "Men\u00b7zel", "wie", "mich", "selbst", "und", "w\u00e4r'", "ihm"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "NN", "KOKOM", "PPER", "ADV", "KON", "VAFIN", "PPER"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.8": {"text": "Erkenntlich gern. Emaille-Uhr? Tabati\u00e8re?", "tokens": ["Er\u00b7kennt\u00b7lich", "gern", ".", "E\u00b7mail\u00b7le\u00b7Uhr", "?", "Ta\u00b7ba\u00b7ti\u00e8\u00b7re", "?"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct"], "pos": ["ADJD", "ADV", "$.", "NN", "$.", "NN", "$."], "meter": "-+-+-+-+--+-", "measure": "iambic.penta.relaxed"}, "line.9": {"text": "Vielleicht ein Solitaire? Was macht ihm Spa\u00df wohl?\u00ab", "tokens": ["Viel\u00b7leicht", "ein", "So\u00b7li\u00b7tai\u00b7re", "?", "Was", "macht", "ihm", "Spa\u00df", "wohl", "?", "\u00ab"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ADV", "ART", "NN", "$.", "PWS", "VVFIN", "PPER", "NN", "ADV", "$.", "$("], "meter": "-+-+-+--+-+-", "measure": "iambic.penta.relaxed"}}, "stanza.7": {"line.1": {"text": "\u00bbach, Majest\u00e4t, was soll ihm Freude machen?", "tokens": ["\u00bb", "ach", ",", "Ma\u00b7jes\u00b7t\u00e4t", ",", "was", "soll", "ihm", "Freu\u00b7de", "ma\u00b7chen", "?"], "token_info": ["punct", "word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["$(", "ITJ", "$,", "NN", "$,", "PWS", "VMFIN", "PPER", "NN", "VVINF", "$."], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "Er hat vollauf von G\u00fctern dieser Erde,", "tokens": ["Er", "hat", "vol\u00b7lauf", "von", "G\u00fc\u00b7tern", "die\u00b7ser", "Er\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "APPR", "NN", "PDAT", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.3": {"text": "Hat Ansehn, Ehre, Titel, Ordenskreuze", "tokens": ["Hat", "An\u00b7sehn", ",", "Eh\u00b7re", ",", "Ti\u00b7tel", ",", "Or\u00b7dens\u00b7kreu\u00b7ze"], "token_info": ["word", "word", "punct", "word", "punct", "word", "punct", "word"], "pos": ["VAFIN", "VVINF", "$,", "NN", "$,", "NN", "$,", "NN"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.4": {"text": "(pour le m\u00e9rite, nat\u00fcrlich Friedensklasse),", "tokens": ["(", "pour", "le", "m\u00e9ri\u00b7te", ",", "na\u00b7t\u00fcr\u00b7lich", "Frie\u00b7dens\u00b7klas\u00b7se", ")", ","], "token_info": ["punct", "word", "word", "word", "punct", "word", "word", "punct", "punct"], "pos": ["$(", "FM", "FM", "FM", "$,", "ADV", "NN", "$(", "$,"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.5": {"text": "Hat Freunde, Mut und Gl\u00fcck, und was die Hauptsach',", "tokens": ["Hat", "Freun\u00b7de", ",", "Mut", "und", "Gl\u00fcck", ",", "und", "was", "die", "Haupt\u00b7sach'", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "NN", "$,", "NN", "KON", "NN", "$,", "KON", "PWS", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.6": {"text": "Hat seine ", "tokens": ["Hat", "sei\u00b7ne"], "token_info": ["word", "word"], "pos": ["VAFIN", "PPOSAT"], "meter": "-+-", "measure": "amphibrach.single"}, "line.7": {"text": "\u00bbund fehlt ihm nichts?\u00ab", "tokens": ["\u00bb", "und", "fehlt", "ihm", "nichts", "?", "\u00ab"], "token_info": ["punct", "word", "word", "word", "word", "punct", "punct"], "pos": ["$(", "KON", "VVFIN", "PPER", "PIS", "$.", "$("], "meter": "-+-+", "measure": "iambic.di"}, "line.8": {"text": "\u00bbrein gar nichts.\u00ab", "tokens": ["\u00bb", "rein", "gar", "nichts", ".", "\u00ab"], "token_info": ["punct", "word", "word", "word", "punct", "punct"], "pos": ["$(", "ADJD", "ADV", "PIS", "$.", "$("], "meter": "+-+", "measure": "trochaic.di"}, "line.9": {"text": "\u00bbna, das ist brav. Comme philosophe! Das lob' ich", "tokens": ["\u00bb", "na", ",", "das", "ist", "brav", ".", "Com\u00b7me", "phi\u00b7lo\u00b7so\u00b7phe", "!", "Das", "lob'", "ich"], "token_info": ["punct", "word", "punct", "word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["$(", "ITJ", "$,", "PDS", "VAFIN", "ADJD", "$.", "FM.la", "FM.la", "$.", "PDS", "VVFIN", "PPER"], "meter": "+-+-+-+-++-+-", "measure": "unknown.measure.septa"}, "line.10": {"text": "Und will nicht st\u00f6ren. Aber ", "tokens": ["Und", "will", "nicht", "st\u00f6\u00b7ren", ".", "A\u00b7ber"], "token_info": ["word", "word", "word", "word", "punct", "word"], "pos": ["KON", "VMFIN", "PTKNEG", "VVINF", "$.", "KON"], "meter": "-+-+-+-", "measure": "iambic.tri"}, "line.11": {"text": "Ich l\u00fcd' ihn ein (er mag die Zeit bestimmen,", "tokens": ["Ich", "l\u00fcd'", "ihn", "ein", "(", "er", "mag", "die", "Zeit", "be\u00b7stim\u00b7men", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "ART", "$(", "PPER", "VMFIN", "ART", "NN", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.12": {"text": "Ein Jahrer zehne will ich gern noch warten),", "tokens": ["Ein", "Jah\u00b7rer", "zeh\u00b7ne", "will", "ich", "gern", "noch", "war\u00b7ten", ")", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct", "punct"], "pos": ["ART", "NN", "VVFIN", "VMFIN", "PPER", "ADV", "ADV", "VVINF", "$(", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.13": {"text": "Ich l\u00fcd' ihn ein nach Sanssouci; sie nennen's", "tokens": ["Ich", "l\u00fcd'", "ihn", "ein", "nach", "Sans\u00b7sou\u00b7ci", ";", "sie", "nen\u00b7nen's"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "ART", "APPR", "NE", "$.", "PPER", "NE"], "meter": "-+-+-+--+-+", "measure": "iambic.penta.relaxed"}, "line.14": {"text": "Dort find't er alte Freunde: Gen'ral Stille,", "tokens": ["Dort", "find't", "er", "al\u00b7te", "Freun\u00b7de", ":", "Gen'\u00b7ral", "Stil\u00b7le", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "ADJA", "NN", "$.", "NN", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.15": {"text": "Graf Rotenburg, die ganze Tafelrunde,", "tokens": ["Graf", "Ro\u00b7ten\u00b7burg", ",", "die", "gan\u00b7ze", "Ta\u00b7fel\u00b7run\u00b7de", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NE", "NE", "$,", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.16": {"text": "Nur Herr von Voltaire fehlt seit Anno 70;", "tokens": ["Nur", "Herr", "von", "Vol\u00b7tai\u00b7re", "fehlt", "seit", "An\u00b7no", "70", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "number", "punct"], "pos": ["ADV", "NN", "APPR", "NE", "VVFIN", "APPR", "NN", "CARD", "$."], "meter": "-+----+--+", "measure": "iambic.tri.chol"}, "line.17": {"text": "Franzose, rapplig. ", "tokens": ["Fran\u00b7zo\u00b7se", ",", "rapp\u00b7lig", "."], "token_info": ["word", "punct", "word", "punct"], "pos": ["NE", "$,", "ADJD", "$."], "meter": "-+-+-", "measure": "iambic.di"}, "line.18": {"text": "Ich bin Sein gn\u00e4d'ger K\u00f6nig. Serviteur!\u00ab", "tokens": ["Ich", "bin", "Sein", "gn\u00e4d'\u00b7ger", "K\u00f6\u00b7nig", ".", "Ser\u00b7vi\u00b7teur", "!", "\u00ab"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct", "punct"], "pos": ["PPER", "VAFIN", "PPOSAT", "ADJA", "NN", "$.", "NN", "$.", "$("], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}}}}