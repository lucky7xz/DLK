{"textgrid.poem.61057": {"metadata": {"author": {"name": "Geibel, Emanuel", "birth": "N.A.", "death": "N.A."}, "title": "Die junge Nonne", "genre": "verse", "period": "N.A.", "pub_year": 1833, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Ach Gott, was hat mein Vater, was meine Mutter gedacht,", "tokens": ["Ach", "Gott", ",", "was", "hat", "mein", "Va\u00b7ter", ",", "was", "mei\u00b7ne", "Mut\u00b7ter", "ge\u00b7dacht", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ITJ", "NN", "$,", "PWS", "VAFIN", "PPOSAT", "NN", "$,", "PRELS", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+--+-+--+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Da\u00df sie mich zu den Nonnen in das Kloster gebracht!", "tokens": ["Da\u00df", "sie", "mich", "zu", "den", "Non\u00b7nen", "in", "das", "Klos\u00b7ter", "ge\u00b7bracht", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "PRF", "APPR", "ART", "NN", "APPR", "ART", "NN", "VVPP", "$."], "meter": "--+--+-+-+--+", "measure": "anapaest.di.plus"}, "line.3": {"text": "Nun darf ich nimmer lachen und mu\u00df im Schleier gehn,", "tokens": ["Nun", "darf", "ich", "nim\u00b7mer", "la\u00b7chen", "und", "mu\u00df", "im", "Schlei\u00b7er", "gehn", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "VVINF", "KON", "VMFIN", "APPRART", "NN", "VVINF", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Und darf kein liebend Herze mein Herze verstehn.", "tokens": ["Und", "darf", "kein", "lie\u00b7bend", "Her\u00b7ze", "mein", "Her\u00b7ze", "ver\u00b7stehn", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "PIAT", "ADJD", "VVFIN", "PPOSAT", "VVFIN", "VVINF", "$."], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}}, "stanza.2": {"line.1": {"text": "Sie haben abgeschnitten mein langes schwarzes Haar,", "tokens": ["Sie", "ha\u00b7ben", "ab\u00b7ge\u00b7schnit\u00b7ten", "mein", "lan\u00b7ges", "schwar\u00b7zes", "Haar", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "VVPP", "PPOSAT", "ADJA", "ADJA", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Hat keiner sich erbarmet meiner sechzehn Jahr;", "tokens": ["Hat", "kei\u00b7ner", "sich", "er\u00b7bar\u00b7met", "mei\u00b7ner", "sech\u00b7zehn", "Jahr", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PIS", "PRF", "VVFIN", "PPOSAT", "CARD", "NN", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Ich bin schon so betr\u00fcbt und bin doch noch so jung,", "tokens": ["Ich", "bin", "schon", "so", "be\u00b7tr\u00fcbt", "und", "bin", "doch", "noch", "so", "jung", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "ADV", "ADV", "VVPP", "KON", "VAFIN", "ADV", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.4": {"text": "Und hat die Welt der Freuden doch f\u00fcr alle genung.", "tokens": ["Und", "hat", "die", "Welt", "der", "Freu\u00b7den", "doch", "f\u00fcr", "al\u00b7le", "ge\u00b7nung", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VAFIN", "ART", "NN", "ART", "NN", "ADV", "APPR", "PIAT", "NN", "$."], "meter": "-+-+-+-+-+-+-", "measure": "alexandrine.iambic.hexa"}}, "stanza.3": {"line.1": {"text": "An meiner Zelle Fenster baun die V\u00f6gelein,", "tokens": ["An", "mei\u00b7ner", "Zel\u00b7le", "Fens\u00b7ter", "baun", "die", "V\u00f6\u00b7ge\u00b7lein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "NN", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Da m\u00f6cht' ich oft mit ihnen so frei und lustig sein;", "tokens": ["Da", "m\u00f6cht'", "ich", "oft", "mit", "ih\u00b7nen", "so", "frei", "und", "lus\u00b7tig", "sein", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VMFIN", "PPER", "ADV", "APPR", "PPER", "ADV", "ADJD", "KON", "ADJD", "VAINF", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Ich h\u00f6be meine Fl\u00fcgel und f\u00e4nde wohl den Steg", "tokens": ["Ich", "h\u00f6\u00b7be", "mei\u00b7ne", "Fl\u00fc\u00b7gel", "und", "f\u00e4n\u00b7de", "wohl", "den", "Steg"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPOSAT", "NN", "KON", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Weit \u00fcber alle T\u00fcrme und Kl\u00f6ster hinweg.", "tokens": ["Weit", "\u00fc\u00b7ber", "al\u00b7le", "T\u00fcr\u00b7me", "und", "Kl\u00f6s\u00b7ter", "hin\u00b7weg", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADJD", "APPR", "PIAT", "NN", "KON", "NN", "PTKVZ", "$."], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}}, "stanza.4": {"line.1": {"text": "Und wenn der Abend d\u00e4mmert, und dunkelt die Nacht,", "tokens": ["Und", "wenn", "der", "A\u00b7bend", "d\u00e4m\u00b7mert", ",", "und", "dun\u00b7kelt", "die", "Nacht", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "ART", "NN", "VVFIN", "$,", "KON", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.2": {"text": "Hab' ich vieltausendmal an meinen Schatz gedacht;", "tokens": ["Hab'", "ich", "viel\u00b7tau\u00b7send\u00b7mal", "an", "mei\u00b7nen", "Schatz", "ge\u00b7dacht", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPER", "ADV", "APPR", "PPOSAT", "NN", "VVPP", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.3": {"text": "Nun bin ich eine Nonne, mein Schatz ist so weit,", "tokens": ["Nun", "bin", "ich", "ei\u00b7ne", "Non\u00b7ne", ",", "mein", "Schatz", "ist", "so", "weit", ","], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ART", "NN", "$,", "PPOSAT", "NN", "VAFIN", "ADV", "ADJD", "$,"], "meter": "-+-+-+--+--+", "measure": "iambic.penta.relaxed"}, "line.4": {"text": "Drum flie\u00dfen meine Tr\u00e4nen allezeit.", "tokens": ["Drum", "flie\u00b7\u00dfen", "mei\u00b7ne", "Tr\u00e4\u00b7nen", "al\u00b7le\u00b7zeit", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PAV", "VVFIN", "PPOSAT", "NN", "ADV", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.5": {"line.1": {"text": "Es flie\u00dfen wohl die Wellen mitsammen in das Meer,", "tokens": ["Es", "flie\u00b7\u00dfen", "wohl", "die", "Wel\u00b7len", "mit\u00b7sam\u00b7men", "in", "das", "Meer", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN", "VVIZU", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.2": {"text": "Es fliegen mitsammen die V\u00f6gel dr\u00fcber her,", "tokens": ["Es", "flie\u00b7gen", "mit\u00b7sam\u00b7men", "die", "V\u00f6\u00b7gel", "dr\u00fc\u00b7ber", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVINF", "VVFIN", "ART", "NN", "PAV", "PTKVZ", "$,"], "meter": "-+--+--+-+-+", "measure": "amphibrach.tri.plus"}, "line.3": {"text": "Der Tag hat seine Sonne, die Nacht den Sternenschein;", "tokens": ["Der", "Tag", "hat", "sei\u00b7ne", "Son\u00b7ne", ",", "die", "Nacht", "den", "Ster\u00b7nen\u00b7schein", ";"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PPOSAT", "NN", "$,", "ART", "NN", "ART", "NN", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.4": {"text": "Nur ich mu\u00df alle Stunden einsam sein.", "tokens": ["Nur", "ich", "mu\u00df", "al\u00b7le", "Stun\u00b7den", "ein\u00b7sam", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VMFIN", "PIAT", "NN", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+", "measure": "iambic.penta"}}, "stanza.6": {"line.1": {"text": "Ich wollt', sie l\u00e4uteten im Kreuzgang erst um mich", "tokens": ["Ich", "wollt'", ",", "sie", "l\u00e4u\u00b7te\u00b7ten", "im", "Kreuz\u00b7gang", "erst", "um", "mich"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "$,", "PPER", "VVFIN", "APPRART", "NN", "ADV", "APPR", "PPER"], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}, "line.2": {"text": "Und tr\u00fcgen mit den Kerzen mich still und feierlich;", "tokens": ["Und", "tr\u00fc\u00b7gen", "mit", "den", "Ker\u00b7zen", "mich", "still", "und", "fei\u00b7er\u00b7lich", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "APPR", "ART", "NN", "PPER", "ADJD", "KON", "ADJD", "$."], "meter": "-+-+-+--+-+-+", "measure": "iambic.hexa.relaxed"}, "line.3": {"text": "Da w\u00e4r' ich los auf einmal von aller Not und Pein", "tokens": ["Da", "w\u00e4r'", "ich", "los", "auf", "ein\u00b7mal", "von", "al\u00b7ler", "Not", "und", "Pein"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "APPR", "ADV", "APPR", "PIAT", "NN", "KON", "NN"], "meter": "--+-+-+-+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Und d\u00fcrfte mit den Engeln wieder fr\u00f6hlich sein.", "tokens": ["Und", "d\u00fcrf\u00b7te", "mit", "den", "En\u00b7geln", "wie\u00b7der", "fr\u00f6h\u00b7lich", "sein", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VMFIN", "APPR", "ART", "NN", "ADV", "ADJD", "VAINF", "$."], "meter": "-+-+-+-+-+-+", "measure": "alexandrine.iambic.hexa"}}}}}