{"textgrid.poem.57329": {"metadata": {"author": {"name": "Klopstock, Friedrich Gottlieb", "birth": "N.A.", "death": "N.A."}, "title": "1L: Lang' erwarteten wir, du w\u00fcrdest Deutschlands", "genre": "verse", "period": "N.A.", "pub_year": 1782, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Lang' erwarteten wir, du w\u00fcrdest Deutschlands", "tokens": ["Lang'", "er\u00b7war\u00b7te\u00b7ten", "wir", ",", "du", "w\u00fcr\u00b7dest", "Deutschlands"], "token_info": ["word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "VVFIN", "PPER", "$,", "PPER", "VAFIN", "NE"], "meter": "+-+--+-+-+", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Muse sch\u00fctzen, auch so mit Ruhm dich kr\u00f6nen;", "tokens": ["Mu\u00b7se", "sch\u00fct\u00b7zen", ",", "auch", "so", "mit", "Ruhm", "dich", "kr\u00f6\u00b7nen", ";"], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VVINF", "$,", "ADV", "ADV", "APPR", "NN", "PPER", "VVINF", "$."], "meter": "+-+-+--+-+-", "measure": "sapphicusminor"}, "line.3": {"text": "Durch den sch\u00f6neren Lorber", "tokens": ["Durch", "den", "sch\u00f6\u00b7ne\u00b7ren", "Lor\u00b7ber"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "ADJA", "NN"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Decken des anderen Blut!", "tokens": ["De\u00b7cken", "des", "an\u00b7de\u00b7ren", "Blut", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["NN", "ART", "ADJA", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.2": {"line.1": {"text": "Gleimen sandte sie dir, und sandte Ramlern,", "tokens": ["Glei\u00b7men", "sand\u00b7te", "sie", "dir", ",", "und", "sand\u00b7te", "Ram\u00b7lern", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["NN", "VVFIN", "PPER", "PPER", "$,", "KON", "ADJA", "NN", "$,"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Dich zu fragen. Und du? Dass sie ihr Auge", "tokens": ["Dich", "zu", "fra\u00b7gen", ".", "Und", "du", "?", "Dass", "sie", "ihr", "Au\u00b7ge"], "token_info": ["word", "word", "word", "punct", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PPER", "PTKZU", "VVINF", "$.", "KON", "PPER", "$.", "KOUS", "PPER", "PPOSAT", "NN"], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Niedersenkte, die Wang' ihr", "tokens": ["Nie\u00b7der\u00b7senk\u00b7te", ",", "die", "Wang'", "ihr"], "token_info": ["word", "punct", "word", "word", "word"], "pos": ["VVFIN", "$,", "ART", "NN", "PPOSAT"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Flamte von r\u00f6therer Scham!", "tokens": ["Flam\u00b7te", "von", "r\u00f6\u00b7the\u00b7rer", "Scham", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "ADJA", "NN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.3": {"line.1": {"text": "So antwortetest du. Sich nicht zu r\u00e4chen,", "tokens": ["So", "ant\u00b7wor\u00b7te\u00b7test", "du", ".", "Sich", "nicht", "zu", "r\u00e4\u00b7chen", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VVFIN", "PPER", "$.", "PRF", "PTKNEG", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-+-", "measure": "iambic.penta"}, "line.2": {"text": "War er schonend genung der Deutsche, deiner", "tokens": ["War", "er", "scho\u00b7nend", "ge\u00b7nung", "der", "Deut\u00b7sche", ",", "dei\u00b7ner"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word"], "pos": ["VAFIN", "PPER", "ADJD", "ADV", "ART", "NN", "$,", "PPOSAT"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Hier auch werther, als du ihn,", "tokens": ["Hier", "auch", "wert\u00b7her", ",", "als", "du", "ihn", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "ADJD", "$,", "KOUS", "PPER", "PPER", "$,"], "meter": "+-+-+-+", "measure": "trochaic.tetra"}, "line.4": {"text": "Fremdling im Heimischen, kenst.", "tokens": ["Fremd\u00b7ling", "im", "Hei\u00b7mi\u00b7schen", ",", "kenst", "."], "token_info": ["word", "word", "word", "punct", "word", "punct"], "pos": ["NN", "APPRART", "NN", "$,", "VVFIN", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.4": {"line.1": {"text": "Doch du selber hast ihn an dir ger\u00e4chet!", "tokens": ["Doch", "du", "sel\u00b7ber", "hast", "ihn", "an", "dir", "ge\u00b7r\u00e4\u00b7chet", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPER", "ADV", "VAFIN", "PPER", "APPR", "PPER", "VVPP", "$."], "meter": "--+-+-+--+-", "measure": "iambic.tetra.relaxed"}, "line.2": {"text": "Heiss schon war den Beginn; allein die letzte", "tokens": ["Heiss", "schon", "war", "den", "Be\u00b7ginn", ";", "al\u00b7lein", "die", "letz\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["NE", "ADV", "VAFIN", "ART", "NN", "$.", "ADV", "ART", "ADJA"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Rache gl\u00fchet, wie keine", "tokens": ["Ra\u00b7che", "gl\u00fc\u00b7het", ",", "wie", "kei\u00b7ne"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "VVFIN", "$,", "PWAV", "PIAT"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Sonst, von zerst\u00f6render Glut.", "tokens": ["Sonst", ",", "von", "zer\u00b7st\u00f6\u00b7ren\u00b7der", "Glut", "."], "token_info": ["word", "punct", "word", "word", "word", "punct"], "pos": ["ADV", "$,", "APPR", "ADJA", "NN", "$."], "meter": "-+-+--+", "measure": "iambic.tri.chol"}}, "stanza.5": {"line.1": {"text": "Wie der Geist dich auch hebt; er fliegt vergebens", "tokens": ["Wie", "der", "Geist", "dich", "auch", "hebt", ";", "er", "fliegt", "ver\u00b7ge\u00b7bens"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PWAV", "ART", "NN", "PPER", "ADV", "VVFIN", "$.", "PPER", "VVFIN", "ADV"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Wenn das Wort ihm nicht folgt. Der Ungeweihte", "tokens": ["Wenn", "das", "Wort", "ihm", "nicht", "folgt", ".", "Der", "Un\u00b7ge\u00b7weih\u00b7te"], "token_info": ["word", "word", "word", "word", "word", "word", "punct", "word", "word"], "pos": ["KOUS", "ART", "NN", "PPER", "PTKNEG", "VVFIN", "$.", "ART", "NN"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "In der Sprache Geheimniss", "tokens": ["In", "der", "Spra\u00b7che", "Ge\u00b7heim\u00b7niss"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "--+--+-", "measure": "anapaest.di.plus"}, "line.4": {"text": "T\u00f6dtet das lebendste Bild.", "tokens": ["T\u00f6d\u00b7tet", "das", "le\u00b7bends\u00b7te", "Bild", "."], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ART", "ADJA", "NN", "$."], "meter": "+-+-+-+", "measure": "trochaic.tetra"}}, "stanza.6": {"line.1": {"text": "Du erniedertest dich Ausl\u00e4ndert\u00f6ne", "tokens": ["Du", "er\u00b7nie\u00b7der\u00b7test", "dich", "Aus\u00b7l\u00e4n\u00b7der\u00b7t\u00f6\u00b7ne"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "PPER", "NN"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Nachzustammeln, daf\u00fcr den Hohn zu h\u00f6ren:", "tokens": ["Nach\u00b7zu\u00b7stam\u00b7meln", ",", "da\u00b7f\u00fcr", "den", "Hohn", "zu", "h\u00f6\u00b7ren", ":"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "$,", "PAV", "ART", "NN", "PTKZU", "VVINF", "$."], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Selbst nach Aruets S\u00e4ubrung,", "tokens": ["Selbst", "nach", "A\u00b7ruets", "S\u00e4u\u00b7brung", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "NE", "NN", "$,"], "meter": "+-+-+-", "measure": "trochaic.tri"}, "line.4": {"text": "Bleibe dein Lied noch t\u00fcdesk.", "tokens": ["Blei\u00b7be", "dein", "Lied", "noch", "t\u00fc\u00b7desk", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PPOSAT", "NN", "ADV", "ADJD", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.7": {"line.1": {"text": "Und die letzte? Dein Blatt von Deutschlands Sprache!", "tokens": ["Und", "die", "letz\u00b7te", "?", "Dein", "Blatt", "von", "Deutschlands", "Spra\u00b7che", "!"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ART", "ADJA", "$.", "PPOSAT", "NN", "APPR", "NE", "NN", "$."], "meter": "--+--+-++-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Die, die Rache ist selbst dem Widerrufe", "tokens": ["Die", ",", "die", "Ra\u00b7che", "ist", "selbst", "dem", "Wi\u00b7der\u00b7ru\u00b7fe"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word"], "pos": ["ART", "$,", "ART", "NN", "VAFIN", "ADV", "ART", "NN"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.3": {"text": "Nicht vertilgbar; beschleyern,", "tokens": ["Nicht", "ver\u00b7tilg\u00b7bar", ";", "be\u00b7schley\u00b7ern", ","], "token_info": ["word", "word", "punct", "word", "punct"], "pos": ["PTKNEG", "ADJD", "$.", "VVFIN", "$,"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Thust du ihn, kann er es nur.", "tokens": ["Thust", "du", "ihn", ",", "kann", "er", "es", "nur", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "PPER", "$,", "VMFIN", "PPER", "PPER", "ADV", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.8": {"line.1": {"text": "Widerrufe von dir? Dess sind wir sicher?", "tokens": ["Wi\u00b7der\u00b7ru\u00b7fe", "von", "dir", "?", "Dess", "sind", "wir", "si\u00b7cher", "?"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "PPER", "$.", "NE", "VAFIN", "PPER", "ADJD", "$."], "meter": "+-+-+-+--+-", "measure": "trochaic.penta.relaxed"}, "line.2": {"text": "Sicher, dass du auf dich aus voller Schale", "tokens": ["Si\u00b7cher", ",", "dass", "du", "auf", "dich", "aus", "vol\u00b7ler", "Scha\u00b7le"], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "word"], "pos": ["ADV", "$,", "KOUS", "PPER", "APPR", "PRF", "APPR", "ADJA", "NN"], "meter": "+--+-+-+-+-", "measure": "iambic.penta.invert"}, "line.3": {"text": "Rache str\u00f6mest, dem weisern", "tokens": ["Ra\u00b7che", "str\u00f6\u00b7mest", ",", "dem", "wei\u00b7sern"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NN", "VVFIN", "$,", "ART", "ADJA"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Enkel noch s\u00fcsser als uns.", "tokens": ["En\u00b7kel", "noch", "s\u00fcs\u00b7ser", "als", "uns", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADV", "ADJD", "KOKOM", "PPER", "$."], "meter": "+--+--+", "measure": "dactylic.tri"}}, "stanza.9": {"line.1": {"text": "Denn er m\u00f6chte vielleicht Erobrergr\u00f6sse", "tokens": ["Denn", "er", "m\u00f6ch\u00b7te", "viel\u00b7leicht", "E\u00b7ro\u00b7brer\u00b7gr\u00f6s\u00b7se"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PPER", "VMFIN", "ADV", "NN"], "meter": "--+--+-+-+-", "measure": "anapaest.di.plus"}, "line.2": {"text": "Anders \u00e4chten, als wir; Verdienst des Pflanzers", "tokens": ["An\u00b7ders", "\u00e4ch\u00b7ten", ",", "als", "wir", ";", "Ver\u00b7dienst", "des", "Pflan\u00b7zers"], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "word"], "pos": ["ADV", "VVFIN", "$,", "KOUS", "PPER", "$.", "NN", "ART", "NN"], "meter": "+-+--+-+-+-", "measure": "trochaic.penta.relaxed"}, "line.3": {"text": "Heller sehen, es sondern", "tokens": ["Hel\u00b7ler", "se\u00b7hen", ",", "es", "son\u00b7dern"], "token_info": ["word", "word", "punct", "word", "word"], "pos": ["NE", "VVINF", "$,", "PPER", "VVINF"], "meter": "+-+--+-", "measure": "pherekrateus"}, "line.4": {"text": "Von des Begiessers Verdienst", "tokens": ["Von", "des", "Be\u00b7gies\u00b7sers", "Ver\u00b7dienst"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "ART", "NN", "NN"], "meter": "+--+--+", "measure": "dactylic.tri"}}}}}