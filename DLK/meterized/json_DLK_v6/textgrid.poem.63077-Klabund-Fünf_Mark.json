{"textgrid.poem.63077": {"metadata": {"author": {"name": "Klabund", "birth": "N.A.", "death": "N.A."}, "title": "F\u00fcnf Mark", "genre": "verse", "period": "N.A.", "pub_year": 1909, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "In meiner Stra\u00dfe nachts steht eine", "tokens": ["In", "mei\u00b7ner", "Stra\u00b7\u00dfe", "nachts", "steht", "ei\u00b7ne"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["APPR", "PPOSAT", "NN", "ADV", "VVFIN", "ART"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.2": {"text": "(immer dieselbe) Lausekleine,", "tokens": ["(", "im\u00b7mer", "die\u00b7sel\u00b7be", ")", "Lau\u00b7se\u00b7klei\u00b7ne", ","], "token_info": ["punct", "word", "word", "punct", "word", "punct"], "pos": ["$(", "ADV", "PDAT", "$(", "NE", "$,"], "meter": "+--+-+-+-", "measure": "iambic.tetra.invert"}, "line.3": {"text": "Und gr\u00fc\u00dft mich kr\u00e4chzend mit Gepl\u00e4rr:", "tokens": ["Und", "gr\u00fc\u00dft", "mich", "kr\u00e4ch\u00b7zend", "mit", "Ge\u00b7pl\u00e4rr", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "VVPP", "APPR", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcnf Mark, mein Herr, f\u00fcnf Mark, mein Herr.", "tokens": ["F\u00fcnf", "Mark", ",", "mein", "Herr", ",", "f\u00fcnf", "Mark", ",", "mein", "Herr", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PPOSAT", "NN", "$,", "CARD", "NN", "$,", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Ich hab es mir mild verbeten,", "tokens": ["Ich", "hab", "es", "mir", "mild", "ver\u00b7be\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "PPER", "PPER", "ADJD", "VVFIN", "$,"], "meter": "-+--+-+-", "measure": "iambic.tri.relaxed"}, "line.2": {"text": "Da ist sie n\u00e4her nur getreten,", "tokens": ["Da", "ist", "sie", "n\u00e4\u00b7her", "nur", "ge\u00b7tre\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "ADJD", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Ihr d\u00fcrrer Leib schwoll schattengro\u00df:", "tokens": ["Ihr", "d\u00fcr\u00b7rer", "Leib", "schwoll", "schat\u00b7ten\u00b7gro\u00df", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJA", "NN", "ADJD", "VVFIN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcnf Mark ja blo\u00df, f\u00fcnf Mark ja blo\u00df.", "tokens": ["F\u00fcnf", "Mark", "ja", "blo\u00df", ",", "f\u00fcnf", "Mark", "ja", "blo\u00df", "."], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["CARD", "NN", "ADV", "ADV", "$,", "CARD", "NN", "ADV", "ADV", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Gr\u00fc\u00df Gott \u2013, der Leichenwagen rumpelt,", "tokens": ["Gr\u00fc\u00df", "Gott", "\u2013", ",", "der", "Lei\u00b7chen\u00b7wa\u00b7gen", "rum\u00b7pelt", ","], "token_info": ["word", "word", "punct", "punct", "word", "word", "word", "punct"], "pos": ["VVIMP", "NN", "$(", "$,", "ART", "NN", "VVFIN", "$,"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.2": {"text": "Ihr Schatz und eine Vettel humpelt", "tokens": ["Ihr", "Schatz", "und", "ei\u00b7ne", "Vet\u00b7tel", "hum\u00b7pelt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "KON", "ART", "NN", "VVFIN"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Stier gr\u00f6hlend hinter ihrem Sarg:", "tokens": ["Stier", "gr\u00f6h\u00b7lend", "hin\u00b7ter", "ih\u00b7rem", "Sarg", ":"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["NN", "ADJD", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcnf Mark, mein Herr, mein Herr, f\u00fcnf Mark.", "tokens": ["F\u00fcnf", "Mark", ",", "mein", "Herr", ",", "mein", "Herr", ",", "f\u00fcnf", "Mark", "."], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "CARD", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Man schmi\u00df sie in die Armenerde,", "tokens": ["Man", "schmi\u00df", "sie", "in", "die", "Ar\u00b7me\u00b7ner\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PIS", "VVFIN", "PPER", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+---", "measure": "unknown.measure.tri"}, "line.2": {"text": "Ihr Schatz gab ihr als Reisezehrde", "tokens": ["Ihr", "Schatz", "gab", "ihr", "als", "Rei\u00b7se\u00b7zehr\u00b7de"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPOSAT", "NN", "VVFIN", "PPER", "KOUS", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Zur Fahrt ins Dunkel in den Sarg:", "tokens": ["Zur", "Fahrt", "ins", "Dun\u00b7kel", "in", "den", "Sarg", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPRART", "NN", "APPRART", "NN", "APPR", "ART", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "F\u00fcnf Mark, mein Herr, mein Herr, f\u00fcnf Mark,", "tokens": ["F\u00fcnf", "Mark", ",", "mein", "Herr", ",", "mein", "Herr", ",", "f\u00fcnf", "Mark", ","], "token_info": ["word", "word", "punct", "word", "word", "punct", "word", "word", "punct", "word", "word", "punct"], "pos": ["CARD", "NN", "$,", "PPOSAT", "NN", "$,", "PPOSAT", "NN", "$,", "CARD", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "F\u00fcnf funkelnagelneue Mark....", "tokens": ["F\u00fcnf", "fun\u00b7kel\u00b7na\u00b7gel\u00b7neu\u00b7e", "Mark", "...."], "token_info": ["word", "word", "word", "punct"], "pos": ["CARD", "ADJA", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}}}}}