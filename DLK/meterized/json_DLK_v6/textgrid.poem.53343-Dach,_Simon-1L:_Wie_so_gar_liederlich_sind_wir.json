{"textgrid.poem.53343": {"metadata": {"author": {"name": "Dach, Simon", "birth": "N.A.", "death": "N.A."}, "title": "1L: Wie so gar liederlich sind wir", "genre": "verse", "period": "N.A.", "pub_year": 1632, "urn": "N.A.", "language": ["de:0.99"], "booktitle": "N.A."}, "text": null, "poem": {"stanza.1": {"line.1": {"text": "Wie so gar liederlich sind wir", "tokens": ["Wie", "so", "gar", "lie\u00b7der\u00b7lich", "sind", "wir"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "ADV", "ADV", "ADJD", "VAFIN", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vmb diesen Mann nun auch gekommen:", "tokens": ["Vmb", "die\u00b7sen", "Mann", "nun", "auch", "ge\u00b7kom\u00b7men", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "NN", "ADV", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "O Jammer, da\u00df der Vnschuld Zier", "tokens": ["O", "Jam\u00b7mer", ",", "da\u00df", "der", "Vn\u00b7schuld", "Zier"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NN", "$,", "KOUS", "ART", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "So h\u00e4uffig uns wird weg genommen!", "tokens": ["So", "h\u00e4uf\u00b7fig", "uns", "wird", "weg", "ge\u00b7nom\u00b7men", "!"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADJD", "PPER", "VAFIN", "ADV", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.2": {"line.1": {"text": "Wo r\u00fchrt di\u00df m\u00e4chtig Vngl\u00fcck her?", "tokens": ["Wo", "r\u00fchrt", "di\u00df", "m\u00e4ch\u00b7tig", "Vn\u00b7gl\u00fcck", "her", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "PDS", "ADJD", "NN", "PTKVZ", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sind wir zu Schlacht-Vieh denn erkohren?", "tokens": ["Sind", "wir", "zu", "Schlacht\u00b7Vieh", "denn", "er\u00b7koh\u00b7ren", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPER", "APPR", "NN", "KON", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "In vierzehn Tagen ohngefehr", "tokens": ["In", "vier\u00b7zehn", "Ta\u00b7gen", "ohn\u00b7ge\u00b7fehr"], "token_info": ["word", "word", "word", "word"], "pos": ["APPR", "CARD", "NN", "ADJD"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Sind in die dreissig Mann verlohren.", "tokens": ["Sind", "in", "die", "dreis\u00b7sig", "Mann", "ver\u00b7loh\u00b7ren", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "APPR", "ART", "CARD", "NN", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.3": {"line.1": {"text": "Vnd alle junge Leute zwar", "tokens": ["Vnd", "al\u00b7le", "jun\u00b7ge", "Leu\u00b7te", "zwar"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "PIAT", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sch\u00f6n, starck und frewdig von Geberden,", "tokens": ["Sch\u00f6n", ",", "starck", "und", "frew\u00b7dig", "von", "Ge\u00b7ber\u00b7den", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NE", "$,", "ADJD", "KON", "ADJD", "APPR", "NN", "$,"], "meter": "++-+--+--", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Von welcher jedem Hoffnung war,", "tokens": ["Von", "wel\u00b7cher", "je\u00b7dem", "Hoff\u00b7nung", "war", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PRELS", "PIAT", "NN", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Es w\u00fcrd aus ihm was stattlichs werden;", "tokens": ["Es", "w\u00fcrd", "aus", "ihm", "was", "statt\u00b7lichs", "wer\u00b7den", ";"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "APPR", "PPER", "PIS", "VVFIN", "VAINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.4": {"line.1": {"text": "Der wahren Frommheit Eigenthum,", "tokens": ["Der", "wah\u00b7ren", "Fromm\u00b7heit", "Ei\u00b7gen\u00b7thum", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ART", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Der Zierraht unsrer hohen Schulen,", "tokens": ["Der", "Zier\u00b7raht", "uns\u00b7rer", "ho\u00b7hen", "Schu\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "PPOSAT", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Vmb welche Lust und Fleisses-Ruhm", "tokens": ["Vmb", "wel\u00b7che", "Lust", "und", "Fleis\u00b7ses\u00b7Ruhm"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["APPR", "PWAT", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd alle Tugend schien zu buhlen.", "tokens": ["Vnd", "al\u00b7le", "Tu\u00b7gend", "schien", "zu", "buh\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PIAT", "NN", "VVFIN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.5": {"line.1": {"text": "Der Eltern Hoffnung, Ruh und Trost,", "tokens": ["Der", "El\u00b7tern", "Hoff\u00b7nung", ",", "Ruh", "und", "Trost", ","], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "NN", "$,", "NN", "KON", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Sind so erb\u00e4rmlich uns verblichen,", "tokens": ["Sind", "so", "er\u00b7b\u00e4rm\u00b7lich", "uns", "ver\u00b7bli\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "ADV", "ADJD", "PPER", "VVFIN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie Bl\u00fcmchen, die ein strenger Ost", "tokens": ["Wie", "Bl\u00fcm\u00b7chen", ",", "die", "ein", "stren\u00b7ger", "Ost"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWAV", "NN", "$,", "PRELS", "ART", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Zu hart umb kalte Nacht bestrichen.", "tokens": ["Zu", "hart", "umb", "kal\u00b7te", "Nacht", "be\u00b7stri\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PTKA", "ADJD", "APPR", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.6": {"line.1": {"text": "Wo bleibt nun unsre Wissenschafft?", "tokens": ["Wo", "bleibt", "nun", "uns\u00b7re", "Wis\u00b7sen\u00b7schafft", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "VVFIN", "ADV", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Hie hat sie Anla\u00df Ruhm zu kriegen.", "tokens": ["Hie", "hat", "sie", "An\u00b7la\u00df", "Ruhm", "zu", "krie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "PPER", "NN", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nein, Kunst, Raht, Hertz und Kr\u00e4uter Krafft", "tokens": ["Nein", ",", "Kunst", ",", "Raht", ",", "Hertz", "und", "Kr\u00e4u\u00b7ter", "Krafft"], "token_info": ["word", "punct", "word", "punct", "word", "punct", "word", "word", "word", "word"], "pos": ["PTKANT", "$,", "NN", "$,", "NN", "$,", "NN", "KON", "NN", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mu\u00df mit der Jugend gleich erliegen.", "tokens": ["Mu\u00df", "mit", "der", "Ju\u00b7gend", "gleich", "er\u00b7lie\u00b7gen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VMFIN", "APPR", "ART", "NN", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.7": {"line.1": {"text": "Der Himmel hat nicht Schuld daran,", "tokens": ["Der", "Him\u00b7mel", "hat", "nicht", "Schuld", "da\u00b7ran", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "PTKNEG", "NN", "PAV", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Die Lufft ist rein, und klar das Wetter,", "tokens": ["Die", "Lufft", "ist", "rein", ",", "und", "klar", "das", "Wet\u00b7ter", ","], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VAFIN", "ADJD", "$,", "KON", "ADJD", "ART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Der Mey kr\u00f6hnt alles umb und an,", "tokens": ["Der", "Mey", "kr\u00b7\u00f6hnt", "al\u00b7les", "umb", "und", "an", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NE", "VVFIN", "PIS", "APPR", "KON", "PTKVZ", "$,"], "meter": "--+-+-+-+", "measure": "anapaest.init"}, "line.4": {"text": "Der Acker gr\u00fcnt, der Wald kriegt Bl\u00e4tter.", "tokens": ["Der", "A\u00b7cker", "gr\u00fcnt", ",", "der", "Wald", "kriegt", "Bl\u00e4t\u00b7ter", "."], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "VVFIN", "$,", "ART", "NN", "VVFIN", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.8": {"line.1": {"text": "Was stirbt von andern Leuten gro\u00df?", "tokens": ["Was", "stirbt", "von", "an\u00b7dern", "Leu\u00b7ten", "gro\u00df", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "APPR", "ADJA", "NN", "ADJD", "$."], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So wenig sind fast nie begraben,", "tokens": ["So", "we\u00b7nig", "sind", "fast", "nie", "be\u00b7gra\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "ADV", "VAFIN", "ADV", "ADV", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mu\u00df unsre Schul allein und blo\u00df", "tokens": ["Mu\u00df", "uns\u00b7re", "Schul", "al\u00b7lein", "und", "blo\u00df"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VMFIN", "PPOSAT", "NN", "ADV", "KON", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die\u00df unverhoffte Hertzleid haben?", "tokens": ["Die\u00df", "un\u00b7ver\u00b7hoff\u00b7te", "Hertz\u00b7leid", "ha\u00b7ben", "?"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["PDS", "ADJA", "NN", "VAFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.9": {"line.1": {"text": "Vnd zwar die andre Schar ist rein,", "tokens": ["Vnd", "zwar", "die", "and\u00b7re", "Schar", "ist", "rein", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "ADV", "ART", "ADJA", "NN", "VAFIN", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gott wolle sie auch lang erhalten,", "tokens": ["Gott", "wol\u00b7le", "sie", "auch", "lang", "er\u00b7hal\u00b7ten", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "VMFIN", "PPER", "ADV", "ADJD", "VVPP", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nur der gemeine Tisch allein", "tokens": ["Nur", "der", "ge\u00b7mei\u00b7ne", "Tisch", "al\u00b7lein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "ART", "ADJA", "NN", "ADV"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mu\u00df wie durch eine Pest, erkalten.", "tokens": ["Mu\u00df", "wie", "durch", "ei\u00b7ne", "Pest", ",", "er\u00b7kal\u00b7ten", "."], "token_info": ["word", "word", "word", "word", "word", "punct", "word", "punct"], "pos": ["VMFIN", "KOKOM", "APPR", "ART", "NN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.10": {"line.1": {"text": "Was werden die so draussen sind", "tokens": ["Was", "wer\u00b7den", "die", "so", "draus\u00b7sen", "sind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWS", "VAFIN", "ART", "ADV", "ADV", "VAFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auff diese b\u00f6se Zeitung sagen?", "tokens": ["Auff", "die\u00b7se", "b\u00f6\u00b7se", "Zei\u00b7tung", "sa\u00b7gen", "?"], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PDAT", "ADJA", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Wie manche Mutter wird ihr Kind", "tokens": ["Wie", "man\u00b7che", "Mut\u00b7ter", "wird", "ihr", "Kind"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PWAV", "PIAT", "NN", "VAFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Mit Blut, an Thr\u00e4nen stat, beklagen?", "tokens": ["Mit", "Blut", ",", "an", "Thr\u00e4\u00b7nen", "stat", ",", "be\u00b7kla\u00b7gen", "?"], "token_info": ["word", "word", "punct", "word", "word", "word", "punct", "word", "punct"], "pos": ["APPR", "NN", "$,", "APPR", "NN", "VVFIN", "$,", "VVFIN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.11": {"line.1": {"text": "Kompt nun aus frembden Landen her,", "tokens": ["Kompt", "nun", "aus", "fremb\u00b7den", "Lan\u00b7den", "her", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "ADJA", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Lasst Euch die Reise nicht verdriessen,", "tokens": ["Lasst", "Euch", "die", "Rei\u00b7se", "nicht", "ver\u00b7dries\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "PTKNEG", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Geduldet Euch, flieht kein Beschwer,", "tokens": ["Ge\u00b7dul\u00b7det", "Euch", ",", "flieht", "kein", "Be\u00b7schwer", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "$,", "VVFIN", "PIAT", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Da\u00df ihr der Gutthat m\u00f6gt geniessen.", "tokens": ["Da\u00df", "ihr", "der", "Gut\u00b7that", "m\u00f6gt", "ge\u00b7nies\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "PPER", "ART", "NN", "VMFIN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.12": {"line.1": {"text": "Vnd wenn ihr eingenommen seyd,", "tokens": ["Vnd", "wenn", "ihr", "ein\u00b7ge\u00b7nom\u00b7men", "seyd", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["KON", "KOUS", "PPER", "VVPP", "VAFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So habet Gifft und Tod zur Speise,", "tokens": ["So", "ha\u00b7bet", "Gifft", "und", "Tod", "zur", "Spei\u00b7se", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "VAFIN", "NN", "KON", "NN", "APPRART", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Lasst ewer Hau\u00df in Hertzeleid'", "tokens": ["Lasst", "e\u00b7wer", "Hau\u00df", "in", "Hert\u00b7ze\u00b7leid'"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "APPR", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd unsre Schul in schlechtem Preise.", "tokens": ["Vnd", "uns\u00b7re", "Schul", "in", "schlech\u00b7tem", "Prei\u00b7se", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "PPOSAT", "NN", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.13": {"line.1": {"text": "O Gott, der du unschuldig Blut", "tokens": ["O", "Gott", ",", "der", "du", "un\u00b7schul\u00b7dig", "Blut"], "token_info": ["word", "word", "punct", "word", "word", "word", "word"], "pos": ["NE", "NN", "$,", "PRELS", "PPER", "ADJD", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auch bey den Thieren nimmst in Straffe,", "tokens": ["Auch", "bey", "den", "Thie\u00b7ren", "nimmst", "in", "Straf\u00b7fe", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "APPR", "ART", "NN", "VVFIN", "APPR", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "K\u00fchl an den Schuldigen den Muth,", "tokens": ["K\u00fchl", "an", "den", "Schul\u00b7di\u00b7gen", "den", "Muth", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "ART", "NN", "$,"], "meter": "+--+-+-+", "measure": "iambic.tetra.invert"}, "line.4": {"text": "Was thun dir diese arme Schaffe?", "tokens": ["Was", "thun", "dir", "die\u00b7se", "ar\u00b7me", "Schaf\u00b7fe", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWS", "VVFIN", "PPER", "PDAT", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.14": {"line.1": {"text": "Bring du die Warheit an das Licht,", "tokens": ["Bring", "du", "die", "War\u00b7heit", "an", "das", "Licht", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "PPER", "ART", "NN", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Halt ferner \u00fcber unserm Stande,", "tokens": ["Halt", "fer\u00b7ner", "\u00fc\u00b7ber", "un\u00b7serm", "Stan\u00b7de", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVIMP", "ADV", "APPR", "PPOSAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da\u00df ja durch Vrtheil und Gericht", "tokens": ["Da\u00df", "ja", "durch", "Vrtheil", "und", "Ge\u00b7richt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["KOUS", "ADV", "APPR", "NN", "KON", "NN"], "meter": "-+-+--+", "measure": "iambic.tri.chol"}, "line.4": {"text": "Die\u00df Blut nicht bleib auff diesem Lande.", "tokens": ["Die\u00df", "Blut", "nicht", "bleib", "auff", "die\u00b7sem", "Lan\u00b7de", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "NN", "PTKNEG", "VVFIN", "APPR", "PDAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.15": {"line.1": {"text": "Was bitt' ich? Wozu soll die Noht", "tokens": ["Was", "bitt'", "ich", "?", "Wo\u00b7zu", "soll", "die", "Noht"], "token_info": ["word", "word", "word", "punct", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER", "$.", "PWAV", "VMFIN", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wozu mein Zorn und Eiffer dienen?", "tokens": ["Wo\u00b7zu", "mein", "Zorn", "und", "Eif\u00b7fer", "die\u00b7nen", "?"], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PPOSAT", "NN", "KON", "NN", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Sie sind und bleiben auch wol todt,", "tokens": ["Sie", "sind", "und", "blei\u00b7ben", "auch", "wol", "todt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VAFIN", "KON", "VVFIN", "ADV", "ADV", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Auch Herr Bazelius mit ihnen.", "tokens": ["Auch", "Herr", "Ba\u00b7ze\u00b7lius", "mit", "ih\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "NN", "NE", "APPR", "PPER", "$."], "meter": "-++-+-+-", "measure": "unknown.measure.tetra"}}, "stanza.16": {"line.1": {"text": "O w\u00e4re dieser wehrte Mann", "tokens": ["O", "w\u00e4\u00b7re", "die\u00b7ser", "wehr\u00b7te", "Mann"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["NE", "VAFIN", "PDAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Zum wenigsten noch \u00fcberblieben!", "tokens": ["Zum", "we\u00b7nigs\u00b7ten", "noch", "\u00fc\u00b7berb\u00b7lie\u00b7ben", "!"], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPRART", "VVFIN", "ADV", "VVINF", "$."], "meter": "+-+--+-+-", "measure": "trochaic.tetra.relaxed"}, "line.3": {"text": "Ach nein, der Tod sieht keinen an,", "tokens": ["Ach", "nein", ",", "der", "Tod", "sieht", "kei\u00b7nen", "an", ","], "token_info": ["word", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "PTKANT", "$,", "ART", "NN", "VVFIN", "PIAT", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Er mu\u00df den andern gleich verstieben.", "tokens": ["Er", "mu\u00df", "den", "an\u00b7dern", "gleich", "ver\u00b7stie\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VMFIN", "ART", "ADJA", "ADV", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.17": {"line.1": {"text": "Weint die ihr von Ihm unterricht", "tokens": ["Weint", "die", "ihr", "von", "Ihm", "un\u00b7ter\u00b7richt"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "ART", "PPER", "APPR", "PPER", "VVFIN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "In K\u00fcnsten pflaget zu empfangen,", "tokens": ["In", "K\u00fcns\u00b7ten", "pfla\u00b7get", "zu", "emp\u00b7fan\u00b7gen", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "NN", "VVFIN", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Seht ewers Fleisses Brand und Licht", "tokens": ["Seht", "e\u00b7wers", "Fleis\u00b7ses", "Brand", "und", "Licht"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "PPOSAT", "NN", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist unanz\u00fcndlich au\u00dfgegangen.", "tokens": ["Ist", "un\u00b7an\u00b7z\u00fcnd\u00b7lich", "au\u00df\u00b7ge\u00b7gan\u00b7gen", "."], "token_info": ["word", "word", "word", "punct"], "pos": ["VAFIN", "ADJD", "VVPP", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.18": {"line.1": {"text": "Erkennt an ihm die Lieb' und Trew", "tokens": ["Er\u00b7kennt", "an", "ihm", "die", "Lieb'", "und", "Trew"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["VVFIN", "APPR", "PPER", "ART", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Durch eine sch\u00f6ne Todten-Gabe,", "tokens": ["Durch", "ei\u00b7ne", "sch\u00f6\u00b7ne", "Tod\u00b7ten\u00b7Ga\u00b7be", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "ART", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Nehmt Klage, Thr\u00e4nen, Angst und Rew", "tokens": ["Nehmt", "Kla\u00b7ge", ",", "Thr\u00e4\u00b7nen", ",", "Angst", "und", "Rew"], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word"], "pos": ["VVFIN", "NN", "$,", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd kompt damit zu seinem Grabe.", "tokens": ["Vnd", "kompt", "da\u00b7mit", "zu", "sei\u00b7nem", "Gra\u00b7be", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PAV", "APPR", "PPOSAT", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.19": {"line.1": {"text": "Ihr k\u00f6nnt doch seinen trewen Sinn", "tokens": ["Ihr", "k\u00f6nnt", "doch", "sei\u00b7nen", "tre\u00b7wen", "Sinn"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "PPOSAT", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Mit keiner andern M\u00fcntze Zahlen,", "tokens": ["Mit", "kei\u00b7ner", "an\u00b7dern", "M\u00fcnt\u00b7ze", "Zah\u00b7len", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PIAT", "ADJA", "NN", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Fallt \u00fcber seinen Leichnam hin,", "tokens": ["Fallt", "\u00fc\u00b7ber", "sei\u00b7nen", "Leich\u00b7nam", "hin", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "APPR", "PPOSAT", "NN", "PTKVZ", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd k\u00fcsset ihn zu tausent mahlen.", "tokens": ["Vnd", "k\u00fcs\u00b7set", "ihn", "zu", "tau\u00b7sent", "mah\u00b7len", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "PTKA", "ADJD", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.20": {"line.1": {"text": "Es wiederschalle gar die Lufft", "tokens": ["Es", "wie\u00b7der\u00b7schal\u00b7le", "gar", "die", "Lufft"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VVFIN", "ADV", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Durch ewre Klag' an allen Enden,", "tokens": ["Durch", "ew\u00b7re", "Klag'", "an", "al\u00b7len", "En\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "APPR", "PIAT", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Werfft dreymal Erd auff seine Grufft", "tokens": ["Werfft", "drey\u00b7mal", "Erd", "auff", "sei\u00b7ne", "Grufft"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["NN", "ADV", "NN", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Vnd deckt ihn zu mit trewen H\u00e4nden:", "tokens": ["Vnd", "deckt", "ihn", "zu", "mit", "tre\u00b7wen", "H\u00e4n\u00b7den", ":"], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "PPER", "APPR", "APPR", "ADJA", "NN", "$."], "meter": "-+--+--+-", "measure": "amphibrach.tri"}}, "stanza.21": {"line.1": {"text": "Sprecht! Vater, nimm die\u00df so f\u00fcr gut,", "tokens": ["Sprecht", "!", "Va\u00b7ter", ",", "nimm", "die\u00df", "so", "f\u00fcr", "gut", ","], "token_info": ["word", "punct", "word", "punct", "word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "$.", "NN", "$,", "VVIMP", "PDS", "ADV", "APPR", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wir wissen dir nicht mehr zu reichen,", "tokens": ["Wir", "wis\u00b7sen", "dir", "nicht", "mehr", "zu", "rei\u00b7chen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "PPER", "PTKNEG", "ADV", "PTKZU", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Kein \u00fcberflu\u00df an Geld und Gut", "tokens": ["Kein", "\u00fc\u00b7berf\u00b7lu\u00df", "an", "Geld", "und", "Gut"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["PIAT", "NN", "APPR", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Ist deinem Fleisse zu vergleichen.", "tokens": ["Ist", "dei\u00b7nem", "Fleis\u00b7se", "zu", "ver\u00b7glei\u00b7chen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VAFIN", "PPOSAT", "NN", "PTKZU", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.22": {"line.1": {"text": "Dein Lohn, Herr, \u00fcbertr\u00e4ff uns weit,", "tokens": ["Dein", "Lohn", ",", "Herr", ",", "\u00fc\u00b7bert\u00b7r\u00e4ff", "uns", "weit", ","], "token_info": ["word", "word", "punct", "word", "punct", "word", "word", "word", "punct"], "pos": ["PPOSAT", "NN", "$,", "NN", "$,", "VVFIN", "PPER", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Auch liessen wir ein Grabmal hawen", "tokens": ["Auch", "lies\u00b7sen", "wir", "ein", "Grab\u00b7mal", "ha\u00b7wen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "PPER", "ART", "NN", "VVINF"], "meter": "-+-+-+--+", "measure": "iambic.tetra.chol"}, "line.3": {"text": "Da\u00df, gleich wie Pharos vor der Zeit,", "tokens": ["Da\u00df", ",", "gleich", "wie", "Pha\u00b7ros", "vor", "der", "Zeit", ","], "token_info": ["word", "punct", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUS", "$,", "ADV", "KOKOM", "NE", "APPR", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Fern aus der See w\u00e4r' anzuschawen.", "tokens": ["Fern", "aus", "der", "See", "w\u00e4r'", "an\u00b7zu\u00b7scha\u00b7wen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["NN", "APPR", "ART", "NN", "VAFIN", "VVIZU", "$."], "meter": "+--+-+---", "measure": "iambic.tri.invert"}}, "stanza.23": {"line.1": {"text": "Gott wird das fromme Hertz in dir", "tokens": ["Gott", "wird", "das", "from\u00b7me", "Hertz", "in", "dir"], "token_info": ["word", "word", "word", "word", "word", "word", "word"], "pos": ["NN", "VAFIN", "ART", "ADJA", "NN", "APPR", "PPER"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Gewi\u00df nicht unvergolten lassen,", "tokens": ["Ge\u00b7wi\u00df", "nicht", "un\u00b7ver\u00b7gol\u00b7ten", "las\u00b7sen", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["ADV", "PTKNEG", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Er wird mit Frewde, Pracht und Zier", "tokens": ["Er", "wird", "mit", "Frew\u00b7de", ",", "Pracht", "und", "Zier"], "token_info": ["word", "word", "word", "word", "punct", "word", "word", "word"], "pos": ["PPER", "VAFIN", "APPR", "NN", "$,", "NN", "KON", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dich kr\u00f6hnen dort ohn Ziel und Massen.", "tokens": ["Dich", "kr\u00f6h\u00b7nen", "dort", "ohn", "Ziel", "und", "Mas\u00b7sen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["PPER", "VVFIN", "ADV", "APPR", "NN", "KON", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.24": {"line.1": {"text": "Vnd liegstu hie gleich tod und kalt,", "tokens": ["Vnd", "liegs\u00b7tu", "hie", "gleich", "tod", "und", "kalt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["KON", "VVFIN", "ADV", "ADV", "NN", "KON", "ADJD", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So lebstu doch in unsern Sinnen", "tokens": ["So", "lebs\u00b7tu", "doch", "in", "un\u00b7sern", "Sin\u00b7nen"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "VVFIN", "ADV", "APPR", "PPOSAT", "NN"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Mit deinen Gaben mannigfalt,", "tokens": ["Mit", "dei\u00b7nen", "Ga\u00b7ben", "man\u00b7nig\u00b7falt", ","], "token_info": ["word", "word", "word", "word", "punct"], "pos": ["APPR", "PPOSAT", "NN", "VVFIN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Bi\u00df da\u00df man uns auch tr\u00e4gt von hinnen.", "tokens": ["Bi\u00df", "da\u00df", "man", "uns", "auch", "tr\u00e4gt", "von", "hin\u00b7nen", "."], "token_info": ["word", "word", "word", "word", "word", "word", "word", "word", "punct"], "pos": ["APPR", "KOUS", "PIS", "PPER", "ADV", "VVFIN", "APPR", "ADV", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.25": {"line.1": {"text": "Wir werden r\u00fchmen alle Gunst", "tokens": ["Wir", "wer\u00b7den", "r\u00fch\u00b7men", "al\u00b7le", "Gunst"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PPER", "VAFIN", "VVFIN", "PIAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "So dir geschencket der Ebreer,", "tokens": ["So", "dir", "ge\u00b7schen\u00b7cket", "der", "E\u00b7breer", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ADV", "PPER", "VVFIN", "ART", "NN", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.3": {"text": "Auch deine Wei\u00dfheit in der Kunst", "tokens": ["Auch", "dei\u00b7ne", "Wei\u00df\u00b7heit", "in", "der", "Kunst"], "token_info": ["word", "word", "word", "word", "word", "word"], "pos": ["ADV", "PPOSAT", "NN", "APPR", "ART", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Der Griechen, Syrer und Chaldeer.", "tokens": ["Der", "Grie\u00b7chen", ",", "Sy\u00b7rer", "und", "Chal\u00b7deer", "."], "token_info": ["word", "word", "punct", "word", "word", "word", "punct"], "pos": ["ART", "NN", "$,", "NN", "KON", "NN", "$."], "meter": "-+-+--+-", "measure": "iambic.tri.relaxed"}}, "stanza.26": {"line.1": {"text": "Wie eiffrig man dir zugeh\u00f6rt,", "tokens": ["Wie", "eif\u00b7frig", "man", "dir", "zu\u00b7ge\u00b7h\u00f6rt", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADJD", "PIS", "PPER", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Wie nie dein Flei\u00df gekunt erm\u00fcden,", "tokens": ["Wie", "nie", "dein", "Flei\u00df", "ge\u00b7kunt", "er\u00b7m\u00fc\u00b7den", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "ADV", "PPOSAT", "NN", "ADJD", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Da als du \u00f6ffentlich gelehrt", "tokens": ["Da", "als", "du", "\u00f6f\u00b7fent\u00b7lich", "ge\u00b7lehrt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["ADV", "KOUS", "PPER", "ADJD", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Die Sprache der beschnittnen J\u00fcden.", "tokens": ["Die", "Spra\u00b7che", "der", "be\u00b7schnitt\u00b7nen", "J\u00fc\u00b7den", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["ART", "NN", "ART", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.27": {"line.1": {"text": "Wie man jetzt deinen Tod beklagt,", "tokens": ["Wie", "man", "jetzt", "dei\u00b7nen", "Tod", "be\u00b7klagt", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["PWAV", "PIS", "ADV", "PPOSAT", "NN", "VVPP", "$,"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Dein ehrlich und gerechtes Leben,", "tokens": ["Dein", "ehr\u00b7lich", "und", "ge\u00b7rech\u00b7tes", "Le\u00b7ben", ","], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PPOSAT", "ADJD", "KON", "ADJA", "NN", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Was r\u00fchmlich dir wird nachgesagt", "tokens": ["Was", "r\u00fchm\u00b7lich", "dir", "wird", "nach\u00b7ge\u00b7sagt"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["PWS", "VVFIN", "PPER", "VAFIN", "VVPP"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Dies alles wollen wir erheben.", "tokens": ["Dies", "al\u00b7les", "wol\u00b7len", "wir", "er\u00b7he\u00b7ben", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["PDS", "PIS", "VMFIN", "PPER", "VVINF", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}, "stanza.28": {"line.1": {"text": "Es m\u00fcsse steter Vorjahrs-Schein", "tokens": ["Es", "m\u00fcs\u00b7se", "ste\u00b7ter", "Vor\u00b7jahr\u00b7sSchein"], "token_info": ["word", "word", "word", "word"], "pos": ["PPER", "VMFIN", "ADJA", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.2": {"text": "Vmb dein geehrtes Grab her gl\u00e4ntzen,", "tokens": ["Vmb", "dein", "ge\u00b7ehr\u00b7tes", "Grab", "her", "gl\u00e4nt\u00b7zen", ","], "token_info": ["word", "word", "word", "word", "word", "word", "punct"], "pos": ["KOUI", "PPOSAT", "ADJA", "NN", "APZR", "VVINF", "$,"], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}, "line.3": {"text": "Vnd Pallas m\u00fcsse dein Gebein", "tokens": ["Vnd", "Pal\u00b7las", "m\u00fcs\u00b7se", "dein", "Ge\u00b7bein"], "token_info": ["word", "word", "word", "word", "word"], "pos": ["KON", "NN", "VMFIN", "PPOSAT", "NN"], "meter": "-+-+-+-+", "measure": "iambic.tetra"}, "line.4": {"text": "Beh\u00e4ngen stets mit frischen Kr\u00e4ntzen.", "tokens": ["Be\u00b7h\u00e4n\u00b7gen", "stets", "mit", "fri\u00b7schen", "Kr\u00e4nt\u00b7zen", "."], "token_info": ["word", "word", "word", "word", "word", "punct"], "pos": ["VVFIN", "ADV", "APPR", "ADJA", "NN", "$."], "meter": "-+-+-+-+-", "measure": "iambic.tetra"}}}}}